{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://nbviewer.jupyter.org/\n",
    "http://www.csszengarden.com/\n",
    "# bash reference manual: https://tiswww.case.edu/php/chet/bash/bashref.html\n",
    "#codepen thing: https://codepen.io/curiositypaths/pen/WddzQM?editors=1100\n",
    "#pep8 https://github.com/ericthansen/dsc-PEP8-online-ds-sp-000\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#sorting a dictionary by values:\n",
    "dict1 = {1: 1, 2: 9, 3: 4}\n",
    "sorted_tuples = sorted(dict1.items(), key=lambda item: item[1])\n",
    "print(sorted_tuples)  # [(1, 1), (3, 4), (2, 9)]\n",
    "sorted_dict = {k: v for k, v in sorted_tuples}\n",
    "\n",
    "print(sorted_dict)  # {1: 1, 3: 4, 2: 9}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pandas stuff: \n",
    "#pd.head, tail, info, index, columns, dtypes, shape, .iloc[index or slice], .loc['col name']\n",
    "#create a pandas \"series\" out of a dataframe\n",
    "#Filter on more than one column: #df.loc[(df['Salary_in_1000']>=100) & (df['Age']< 60) & (df['FT_Team'].str.startswith('S')),['Name','FT_Team']]\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df.info()\n",
    "df.index\n",
    "df.columns\n",
    "df.dtypes\n",
    "df.shape\n",
    "df.iloc[3]\n",
    "df.iloc[:, 3:7]\n",
    "df.loc[:, 'magnesium']\n",
    "df.loc[df['alcohol'] < 12]\n",
    "df.loc[df['alcohol'] < 12, ['color_intensity']]\n",
    "col_intensity = df['color_intensity']\n",
    "col_intensity[col_intensity > 8] \n",
    "df.loc[df['color_intensity'] > 10, 'color_intensity'] = 10\n",
    "df.loc[df['color_intensity'] > 7, 'shade'] = 'dark'\n",
    "df.loc[df['color_intensity'] <= 7, 'shade'] = 'light'\n",
    "pd.read_csv()\n",
    "pd.read_excel()\n",
    "pd.read_json()\n",
    "pd.DataFrame.from_dict()\n",
    "df.to_csv()\n",
    "df.to_excel()\n",
    "df.to_json()\n",
    "df.to_dict()\n",
    "df = df.drop(0)\n",
    "df.head(2)\n",
    "df = pd.read_csv('Data/ACS_16_5YR_B24011_with_ann.csv', header=1)\n",
    "df = pd.read_csv('Data/ACS_16_5YR_B24011_with_ann.csv', \n",
    "                 usecols=[0, 1, 2, 5, 6], encoding='latin-1')\n",
    "df = pd.read_csv('Data/ACS_16_5YR_B24011_with_ann.csv', usecols=['GEO.id', 'GEO.id2'], encoding='latin-1')\n",
    "df1 = pd.read_excel('Data/Yelp_Selected_Businesses.xlsx', header=2)\n",
    "df2 = pd.read_excel('Data/Yelp_Selected_Businesses.xlsx', sheet_name=2, header=2)\n",
    "df = pd.read_excel('Data/Yelp_Selected_Businesses.xlsx', sheet_name='Biz_id_RESDU', header=2)\n",
    "workbook = pd.ExcelFile('Data/Yelp_Selected_Businesses.xlsx')\n",
    "workbook.sheet_names\n",
    "df = workbook.parse(sheet_name=1, header=2)\n",
    "#Writedata out\n",
    "df.to_csv('NewSavedView.csv', index=False) \n",
    "df.to_excel('NewSavedView.xlsx')\n",
    "\n",
    "\n",
    "rankings_pd.rename(columns = {'test':'TEST'}, inplace = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Group the data by weekend/weekday and plot the sum of the numeric columns\n",
    "wkend = grouped.groupby('is_weekend').sum()\n",
    "#wkend\n",
    "wkend[['entries', 'exits']].plot(kind='barh')\n",
    "plt.show()#mapping\n",
    "df['On_N_Line'] = df['LINENAME'].map(contains_n)\n",
    "df['On_N_Line'].value_counts(normalize=True)\n",
    "df['On_N_Line'] = df['LINENAME'].map(lambda x: 'N' in x)\n",
    "\n",
    "# We can also check an individual column type rather then all \n",
    "print(df['ENTRIES'].dtype) \n",
    "\n",
    "# Changing the column to float\n",
    "df['ENTRIES'] = df['ENTRIES'].astype(float) \n",
    "# Converting Back\n",
    "print(df['ENTRIES'].dtype) \n",
    "df['ENTRIES'] = df['ENTRIES'].astype(int)\n",
    "print(df['ENTRIES'].dtype)\n",
    "\n",
    "df['LINENAME'] = df['LINENAME'].astype(int)\n",
    "\n",
    "pd.to_datetime(df['DATE']).head() \n",
    "# Notice we include delimiters (in this case /) between the codes \n",
    "pd.to_datetime(df['DATE'], format='%m/%d/%Y').head()\n",
    "\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "df['DATE'].dt.day_name().head()\n",
    "# If you don't pass the axis=1 parameter, pandas will try and drop a row with the specified index\n",
    "df = df.drop('C/A', axis=1) \n",
    "\n",
    "df = df.set_index('date')#but can be buggy with losing the new index\n",
    "df = df.reset_index()\n",
    "\n",
    "# Change the index to 'linename'\n",
    "df = df.set_index('linename', drop = False)\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "#\n",
    "# Group the data by day of week and plot the sum of the numeric columns\n",
    "grouped = df.groupby('day_of_week').sum()\n",
    "grouped.plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Use this dictionary to create a new column \n",
    "weekend_map = {'Monday':False, 'Tuesday':False, 'Wednesday':False, 'Thursday':False, \n",
    "               'Friday':False, 'Saturday':True, 'Sunday':True}\n",
    "\n",
    "# Add a new column 'is_weekend' that maps the 'day_of_week' column using weekend_map\n",
    "grouped['is_weekend'] = grouped['day_of_week'].map(weekend_map)\n",
    "#grouped\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.describe()\n",
    "df.mean()\n",
    "df['Fare'].mean()\n",
    "df['Age'].quantile(.9)\n",
    "df['Age'].median()\n",
    ".mode() -- the mode of the column\n",
    ".count() -- the count of the total number of entries in a column\n",
    ".std() -- the standard deviation for the column\n",
    ".var() -- the variance for the column\n",
    ".sum() -- the sum of all values in the column\n",
    ".cumsum() -- the cumulative sum, where each cell index contains the sum of all indices lower than, and including, itself.\n",
    "\n",
    "Summary Statistics for Categorical Columns\n",
    "df['Embarked'].unique()\n",
    "df['Embarked'].value_counts()\n",
    "\n",
    "Calculating on the Fly with .apply() and .applymap()\n",
    "# Quick function to convert every value in the DataFrame to a string\n",
    "string_df = df.applymap(lambda x: str(x))\n",
    "string_df.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = pd.DataFrame({'A':np.random.randn(365).cumsum(),\n",
    "                    'B':np.random.randn(365).cumsum() + 25,\n",
    "                    'C':np.random.randn(365).cumsum() - 25}, \n",
    "                     index = pd.date_range('1/1/2018', periods = 365))\n",
    "Scatter Plots\n",
    "The DataFrame.plot() allows us to plot a number of different kinds of plots. We can select which plot we want to use by specifying the kind parameter. Here is a complete list from the documentation:\n",
    "\n",
    "kind : str\n",
    "\n",
    "‘line’ : line plot (default)\n",
    "‘bar’ : vertical bar plot\n",
    "‘barh’ : horizontal bar plot\n",
    "‘hist’ : histogram\n",
    "‘box’ : boxplot\n",
    "‘kde’ : Kernel Density Estimation plot\n",
    "‘density’ : same as ‘kde’\n",
    "‘area’ : area plot\n",
    "‘pie’ : pie plot\n",
    "‘scatter’ : scatter plot\n",
    "‘hexbin’ : hexbin plot\n",
    "    \n",
    "data.plot('A', 'B', kind='scatter');\n",
    "\n",
    "data.plot.scatter('A', 'C', \n",
    "                  c = 'B',\n",
    "                  s = data['B'],\n",
    "                  colormap = 'viridis');\n",
    "\n",
    "ax = data.plot.scatter('A', 'C', \n",
    "                        c = 'B',\n",
    "                        s = data['B'],\n",
    "                        colormap = 'viridis');\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Manipulating Pandas plot objects in matplotlib')\n",
    "\n",
    "# Box Plots\n",
    "data.plot.box();\n",
    "\n",
    "# Histograms \n",
    "# Setting alpha level to inspect distribution overlap\n",
    "data.plot.hist(alpha = 0.7); \n",
    "\n",
    "# Kernel Density Estimate plots\n",
    "# Useful for visualizing an estimate of a variable's probability density function. \n",
    "# Kernel density estimation applications will be covered later\n",
    "data.plot.kde();\n",
    "\n",
    "pd.plotting.scatter_matrix(iris);\n",
    "\n",
    "\n",
    "# Set a colormap with 3 colors to show species\n",
    "colormap = ('skyblue', 'salmon', 'lightgreen')\n",
    "plt.figure()\n",
    "pd.plotting.parallel_coordinates(iris, 'species', color=colormap);\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cdf#stuff from project solution:\n",
    "pd.plotting.scatter_matrix(df[['LotArea', 'SalePrice', 'YrSold', 'YearBuilt']], figsize=(10,10));\n",
    "\n",
    "#nicer plots\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.hist(df['SalePrice'], bins='auto')\n",
    "ax.set_title('Distribution of Sale Prices')\n",
    "ax.set_xlabel('Sale Price')\n",
    "ax.set_ylabel('Number of houses')\n",
    "ax.axvline(df['SalePrice'].mean(), color='black');\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.hist(df['LotArea'], bins='auto');\n",
    "ax.set_title('Distribution of Sizes of Lot')\n",
    "ax.set_xlabel('Size of Lot')\n",
    "ax.set_ylabel('Number of Houses');\n",
    "\n",
    "\n",
    "df.corr()['SalePrice'].sort_values()\n",
    "\n",
    "# Perform an Exploration of home values by age\n",
    "df['age'] = df['YrSold'] - df['YearBuilt']\n",
    "df['decades'] = df.age // 10\n",
    "to_plot = df.groupby('decades').SalePrice.mean()\n",
    "to_plot.plot(kind='barh', figsize=(10,8))\n",
    "plt.ylabel('House Age in Decades')\n",
    "plt.xlabel('Average Sale Price of Homes')\n",
    "plt.title('Average Home Values by Home Age');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#data cleaning\n",
    "df.groupby('business_id')['stars'].mean().head()\n",
    "\n",
    "#check for dups\n",
    "df[df.duplicated(keep=False)].sort_values(by='business_id')\n",
    "#Remove dups:\n",
    "df = df[df.duplicated()]\n",
    "# Duplicates should no longer exist\n",
    "df[df.duplicated(keep=False)].sort_values(by='business_id')\n",
    "\n",
    "#PIVOT TABLESSSSS\n",
    "# This transforms the data into a person by person spreadsheet and what stars they gave various restaurants\n",
    "# Most values are NaN (null or missing) because people only review a few restaurants of those that exist\n",
    "usr_reviews = df.pivot(index='user_id', columns='business_id', values='stars')\n",
    "usr_reviews.head()\n",
    "\n",
    "\n",
    "\n",
    "#Count # of words in a review:\n",
    "df['text'].map(lambda x: len(x.split())).head()\n",
    "\n",
    "\n",
    "#THis line is a mess but combines a lot of good things\n",
    "df['text'].map(lambda x: 'Good' if any([word in x.lower() for word in ['awesome', 'love', 'good', 'great']]) else 'Bad').head()\n",
    "\n",
    "\n",
    "#sorting by last name - clever\n",
    "# Without a key\n",
    "names = ['Miriam Marks','Sidney Baird','Elaine Barrera','Eddie Reeves','Marley Beard',\n",
    "         'Jaiden Liu','Bethany Martin','Stephen Rios','Audrey Mayer','Kameron Davidson',\n",
    "         'Carter Wong','Teagan Bennett']\n",
    "sorted(names)\n",
    "# Sorting by last name\n",
    "names = ['Miriam Marks','Sidney Baird','Elaine Barrera','Eddie Reeves','Marley Beard',\n",
    "         'Jaiden Liu','Bethany Martin','Stephen Rios','Audrey Mayer','Kameron Davidson',\n",
    "'Teagan Bennett']\n",
    "sorted(names, key=lambda x: x.split()[1])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(10,10))\n",
    "x = np.linspace(start=-10, stop=10, num=10*83)\n",
    "for i in range(12):\n",
    "    row = i//4\n",
    "    col = i%4\n",
    "    ax = axes[row, col]\n",
    "    ax.scatter(x, x**i)\n",
    "    ax.set_title('Plot of x^{}'.format(i))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#aggregation/groupby\n",
    "df.groupby('Sex')\n",
    "df.groupby('Sex').sum()\n",
    "#can do sum, mean, median, min, max, etc, to see all the options:\n",
    "grouped_df = df.groupby('Sex')\n",
    "grouped_df.<TAB>\n",
    "\n",
    "#on multiple groups:\n",
    "df.groupby(['Sex', 'Pclass']).mean()\n",
    "\n",
    "#for specific columns\n",
    "df.groupby(['Sex', 'Pclass'])['Survived'].mean()\n",
    "\n",
    "#and to get specific values out of that output:\n",
    "grouped = df.groupby(['Sex', 'Pclass'])['Survived'].mean()\n",
    "print(grouped['female'])\n",
    "\n",
    "# Output:\n",
    "# Pclass\n",
    "# 1    0.968085\n",
    "# 2    0.921053\n",
    "# 3    0.500000\n",
    "# Name: Survived, dtype: float64\n",
    "\n",
    "print(grouped['female'][1])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#some join stuff\n",
    "some_dataframe.set_index('name_of_index_column', inplace=True)\n",
    "to_concat = [df1, df2, df3]\n",
    "big_df = pd.concat(to_concat)\n",
    "\n",
    "joined_df = df1.join(df2, how='inner')\n",
    "#NOTE: If both tables contain columns with the same name, the join will throw an error due to a naming collision, \n",
    "#since the resulting table would have multiple columns with the same name. To solve this, pass in a value to \n",
    "#lsuffix= or rsuffix=, which will append this suffix to the offending columns to resolve the naming collisions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#remember Wide format vs long format\n",
    "\n",
    "dataframe.groupby(['Embarked', 'Pclass']).mean()\n",
    "\n",
    "#pivot tables:\n",
    "some_dataframe.pivot(index='State', columns='Gender', values='Deaths_mean')\n",
    "#One of the quickest ways to manipulate the format of a dataset in python is to use the \n",
    "# .stack() and unstack() methods built into pandas DataFrames. - sometimes necessary to unstack \n",
    "multiple times!\n",
    "pivot.plot(kind='barh', figsize=(15,8), stacked=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Drop and reformat type\n",
    "# Your code here\n",
    "to_drop = df.loc[df['Population'] == 'Not Applicable']\n",
    "#joined_df = df.join(to_drop, how='left', rsuffix='_right')\n",
    "df.drop(to_drop.index, axis=0, inplace=True)\n",
    "df['Population']=df['Population'].astype('int64')\n",
    "df['Population'].dtype\n",
    "\n",
    "#slicing/pivot table\n",
    "grouped = df.groupby(['State','Gender'])['Deaths','Population'].agg(['mean','min','max','std'])\n",
    "\n",
    "# We could also flatten these:\n",
    "cols0 = grouped.columns.get_level_values(0)\n",
    "cols1 = grouped.columns.get_level_values(1)\n",
    "grouped.columns = [col0 + '_' + col1 if col1 != '' else col0 for col0, col1 in list(zip(cols0, cols1))]\n",
    "# The list comprehension above is more complicated then what we need but creates a nicer formatting and\n",
    "# demonstrates using a conditional within a list comprehension.\n",
    "# This simpler version works but has some tail underscores where col1 is blank:\n",
    "# grouped.columns = [col0 + '_' + col1 for col0, col1 in list(zip(cols0, cols1))]\n",
    "grouped.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "norm#detecting missing data\n",
    "df.isna()\n",
    "df.isna().sum()\n",
    "#for normally distribution, outliers: more than 3 std from the median\n",
    "#for nonnormal, things that are more than 1.5 IQR from q1 or q3\n",
    "\n",
    "\n",
    "#To detect placeholder values in categorical data, get the unique values in the column and see if there are any values that don't match up with your expectations. Pandas provides a built-in method for this. For instance, in the titanic dataset, we can check the unique values of the Embarked column by typing:\n",
    "\n",
    "df['Embarked'].unique()\n",
    "\n",
    "To drop all rows containing missing values in a DataFrame, use \n",
    "dataframe.dropna()\n",
    " Note that this returns a copy of the dataframe with the rows in question dropped -- however, you can mutate the DataFrame in place by passing in inplace=True as a parameter to the method call.\n",
    " \n",
    "Pandas provides an easy way for us to replace null values. For instance, if we wanted to replace all null values in the Fare column with the column median, we would type:\n",
    "\n",
    "df['Fare'].fillna(df['Fare'].median())\n",
    "\n",
    "Numerical data\n",
    "Often, missing values inside a continuously-valued column will cause all sorts of havoc in your models, so leaving the NaNs alone isn't usually an option here. Instead, consider using Coarse Classification, also referred to as Binning. This allows us to convert the entire column from a numerical column to a categorical column by binning our data into categories. For instance, we could deal with the missing values in the Age column by creating a categorical column that separates each person into 10-year age ranges. Anybody between the ages of 0 and 10 would be a 1, 11 to 20 would be a 2, and so on.\n",
    "\n",
    "Once we have binned the data in a new column, we can throw out the numerical version of the column, and just leave the missing values as one more valid category inside our new categorical column!\n",
    "\n",
    "pivoted = grouped.pivot(index='Pclass', columns = 'Sex', values='Age')\n",
    "pivoted\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pivoted.plot(kind='barh')\n",
    "\n",
    "print('Top 5 Values before:\\n', df['Cabin'].value_counts(normalize=True).reset_index()[:5])\n",
    "# Not a useful means of imputing in most cases, but a simple example to recap\n",
    "df.Cabin = df['Cabin'].fillna(value='?')\n",
    "print('Top 5 Values after:\\n', df.Cabin.value_counts(normalize=True).reset_index()[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Set plot space as inline for inline plots and qt for external plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Use plot() function to create a plot using above values on both x and y coordinates. Add a label.\n",
    "plt.plot(data, data, label='Sample Data')\n",
    "\n",
    "# Add a legend to the plot with legend()\n",
    "plt.legend()\n",
    "\n",
    "# Output the final plot\n",
    "plt.show()\n",
    "# Set plot space as inline for inline plots and qt for external plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Use plot() function to create a plot using above values on both x and y coordinates. Add a label.\n",
    "plt.plot(data, data, label='Sample Data')\n",
    "\n",
    "# Add labels for x and y axes\n",
    "plt.xlabel('X Axis Label')\n",
    "plt.ylabel('Y Axis Label')\n",
    "\n",
    "# Add a title for the plot\n",
    "plt.title('PLOT TITLE')\n",
    "\n",
    "# Add a legend to the plot with legend() in lower right corner\n",
    "plt.legend(loc=4)\n",
    "\n",
    "# Output the final plot\n",
    "plt.show()\n",
    "\n",
    "# Define a new figure with matplotlib's .figure() function. \n",
    "new_figure = plt.figure()\n",
    "\n",
    "# Add a subplot to the figure - a new axes\n",
    "ax = new_figure.add_subplot(111)\n",
    "\n",
    "# Generate a line plot \n",
    "ax.plot([1, 4, 6, 8], [10, 15, 27, 32], color='lightblue', linewidth=3, linestyle = '-.')\n",
    "\n",
    "# Draw a scatter plot on same axes\n",
    "ax.scatter([0.5, 2.2, 4.2, 6.5], [21, 19, 9, 26], color='red', marker='o')\n",
    "\n",
    "# Set the limits of x and y for axes\n",
    "ax.set_xlim(0, 9), ax.set_ylim(5,35)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#A standard plot\n",
    "x = np.linspace(start=0, stop=100, num=10**3)\n",
    "y = [xi**2 for xi in x]\n",
    "plt.scatter(x,y)\n",
    "\n",
    "#The same plot with new axes ticks\n",
    "x = np.linspace(start=0, stop=100, num=10**3)\n",
    "y = [xi**2 for xi in x]\n",
    "plt.scatter(x,y)\n",
    "\n",
    "xticks = np.linspace(start=0, stop=100, num=11)\n",
    "yticks = np.linspace(start=0, stop=100**2, num=11)\n",
    "plt.xticks(xticks); #Adding a semicolon after the call will prevent extraneous input from being displayed\n",
    "plt.yticks(yticks);\n",
    "\n",
    "x = np.linspace(start=0, stop=100, num=10**3)\n",
    "y = [xi**2 for xi in x]\n",
    "plt.scatter(x,y)\n",
    "\n",
    "xticks = np.linspace(start=0, stop=200, num=11)\n",
    "yticks = np.linspace(start=0, stop=10**5, num=11)\n",
    "plt.xticks(xticks); #Adding a semicolon after the call will prevent extraneous input from being displayed\n",
    "plt.yticks(yticks);\n",
    "plt.title('Displaying Terrible Use of plt.xticks() and plt.yticks()');\n",
    "\n",
    "x = np.linspace(start=0, stop=100, num=10**3)\n",
    "y = [xi**2 for xi in x]\n",
    "plt.scatter(x,y)\n",
    "\n",
    "xticks = np.linspace(start=0, stop=50, num=11)\n",
    "yticks = np.linspace(start=0, stop=.5*10**4, num=11)\n",
    "plt.title('More things to avoid')\n",
    "plt.xticks(xticks); #Adding a semicolon after the call will prevent extraneous input from being displayed\n",
    "plt.yticks(yticks);\n",
    "\n",
    "# Define a new figure with matplotlib's .plot() function. Set the size of figure space\n",
    "new_figure = plt.figure(figsize=(10,4))\n",
    "\n",
    "# Add a subplot to the figure - a new axes\n",
    "ax = new_figure.add_subplot(121)\n",
    "\n",
    "# Add a second subplot to the figure - a new axes\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "# Generate a line plot on first axes\n",
    "ax.plot([1, 4, 6, 8], [10, 15, 27, 32], color='lightblue', linewidth=3, linestyle = '-.')\n",
    "\n",
    "# Draw a scatter plot on 2nd axes\n",
    "ax2.scatter([0.5, 2.2, 4.2, 6.5], [21, 19, 9, 26], color='red', marker='o')\n",
    "\n",
    "# Set the limits of x and y for first axes\n",
    "ax.set_xlim(0, 9), ax.set_ylim(5,35)\n",
    "\n",
    "# Set the limits of x and y for 2nd axes\n",
    "ax2.set_xlim(0, 9), ax2.set_ylim(5,35)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##more subplots\n",
    "x = np.linspace(-10, 10, 101)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10,10))\n",
    "plt.title('Graphs of Various Polynomials')\n",
    "for n in range(1,9):\n",
    "    row = (n-1)//2\n",
    "    col = n%2-1\n",
    "    ax = axes[row][col]\n",
    "    y = [xi**n for xi in x]\n",
    "    ax.plot(x,y)\n",
    "    ax.set_title('x^{}'.format(n))\n",
    "    \n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "ax.plot(x, x+1, color=\"red\", linewidth=0.25)\n",
    "ax.plot(x, x+2, color=\"red\", linewidth=0.50)\n",
    "ax.plot(x, x+3, color=\"red\", linewidth=1.00)\n",
    "ax.plot(x, x+4, color=\"red\", linewidth=2.00)\n",
    "\n",
    "# possible linestype options ‘-‘, ‘–’, ‘-.’, ‘:’, ‘steps’\n",
    "ax.plot(x, x+5, color=\"green\", lw=3, linestyle='-')\n",
    "ax.plot(x, x+6, color=\"green\", lw=3, ls='-.')\n",
    "ax.plot(x, x+7, color=\"green\", lw=3, ls=':')\n",
    "\n",
    "# custom dash\n",
    "line, = ax.plot(x, x+8, color=\"black\", lw=1.50)\n",
    "line.set_dashes([5, 10, 15, 10]) # format: line length, space length, ...\n",
    "\n",
    "# possible marker symbols: marker = '+', 'o', '*', 's', ',', '.', '1', '2', '3', '4', ...\n",
    "ax.plot(x, x+9, color=\"blue\", lw=3, ls='-', marker='+')\n",
    "ax.plot(x, x+10, color=\"blue\", lw=3, ls='--', marker='o')\n",
    "ax.plot(x, x+11, color=\"blue\", lw=3, ls='-', marker='s')\n",
    "ax.plot(x, x+12, color=\"blue\", lw=3, ls='--', marker='1')\n",
    "\n",
    "# marker size and color\n",
    "ax.plot(x, x+13, color=\"purple\", lw=1, ls='-', marker='o', markersize=2)\n",
    "ax.plot(x, x+14, color=\"purple\", lw=1, ls='-', marker='o', markersize=4)\n",
    "ax.plot(x, x+15, color=\"purple\", lw=1, ls='-', marker='o', markersize=8, markerfacecolor=\"red\")\n",
    "ax.plot(x, x+16, color=\"purple\", lw=1, ls='-', marker='s', markersize=8, markerfacecolor=\"yellow\", markeredgewidth=3, markeredgecolor=\"green\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*lost a bunch of stuff*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "tips = sns.load_dataset('tips') # Seaborn comes prepackaged with several different datasets that are great for visualizing!\n",
    "\n",
    "boxplot = sns.boxplot(data=tips[\"total_bill\"])\n",
    "\n",
    "sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n",
    "\n",
    "sns.boxplot(x=\"day\", y=\"total_bill\", hue=\"smoker\", data=tips, palette=\"Set3\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#*SQL\n",
    "\n",
    "cur.execute(\"\"\"SELECT customerName,\n",
    "               COUNT(customerName) AS number_purchases,\n",
    "               MIN(amount) AS min_purchase,\n",
    "               MAX(amount) AS max_purchase,\n",
    "               AVG(amount) AS avg_purchase,\n",
    "               SUM(amount) AS total_spent\n",
    "               FROM customers\n",
    "               JOIN payments\n",
    "               USING(customerNumber)\n",
    "               GROUP BY customerName\n",
    "               ORDER BY SUM(amount) DESC;\"\"\")\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df. columns = [i[0] for i in cur.description]\n",
    "print(len(df))\n",
    "df.head()\n",
    "\n",
    "cur.execute(\"\"\"SELECT city, COUNT(customerNumber) AS number_customers\n",
    "               FROM customers\n",
    "               GROUP BY 1\n",
    "               HAVING COUNT(customerNumber)>=5;\"\"\")\n",
    "               \n",
    "cur.execute(\"\"\"SELECT customerName,\n",
    "               COUNT(amount) AS number_purchases_over_50K\n",
    "               FROM customers\n",
    "               JOIN payments\n",
    "               USING(customerNumber)\n",
    "               WHERE amount >= 50000\n",
    "               GROUP BY customerName\n",
    "               HAVING count(amount) >= 2\n",
    "               ORDER BY count(amount) DESC;\"\"\")\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df. columns = [i[0] for i in cur.description]\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independent stuff inspired by project:  Starting with a dataframe (that perhaps came from csv) then importing into\n",
    "#sql table to do joins\n",
    "import sqlite3\n",
    "from pandas import DataFrame\n",
    "\n",
    "conn = sqlite3.connect('TestDB1.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('CREATE TABLE CARS (Brand text, Price number)')\n",
    "conn.commit()\n",
    "\n",
    "Cars = {'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4'],\n",
    "        'Price': [22000,25000,27000,35000]\n",
    "        }\n",
    "\n",
    "df = DataFrame(Cars, columns= ['Brand', 'Price'])\n",
    "df.to_sql('CARS', conn, if_exists='replace', index = False)\n",
    " \n",
    "c.execute('''  \n",
    "SELECT * FROM CARS\n",
    "          ''')\n",
    "\n",
    "for row in c.fetchall():\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#SQL Joins - default SQL JOIN is an INNER JOIN\n",
    "#sqlite doesn't support OUTER JOINS!  \n",
    "#POSTGRESQL does.\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"SELECT * \n",
    "               FROM orderdetails\n",
    "               JOIN products\n",
    "               ON orderdetails.productCode = products.productCode\n",
    "               LIMIT 10;\n",
    "               \"\"\")\n",
    "\n",
    "# Take results and create DataFrame\n",
    "df = pd.DataFrame(cur.fetchall()) \n",
    "df.columns = [i[0] for i in cur.description]\n",
    "df.head()\n",
    "\n",
    "#more concise: - SQL will know you mean productCode in each table\n",
    "cur.execute(\"\"\"SELECT * FROM orderdetails\n",
    "               JOIN products\n",
    "               USING(productCode)\n",
    "               LIMIT 10;\n",
    "               \"\"\")\n",
    "df = pd.DataFrame(cur.fetchall()) \n",
    "df.columns = [i[0] for i in cur.description]\n",
    "df.head()\n",
    "\n",
    "\n",
    "###aliasing - not using AS\n",
    "cur.execute(\"\"\"SELECT * FROM orderdetails o\n",
    "               JOIN products p\n",
    "               ON o.productCode = p.productCode\n",
    "               LIMIT 10;\n",
    "               \"\"\")\n",
    "df = pd.DataFrame(cur.fetchall()) \n",
    "df.columns = [i[0] for i in cur.description]\n",
    "df.head()\n",
    "\n",
    "\n",
    "#Left Join\n",
    "cur.execute(\"\"\"SELECT * \n",
    "               FROM products\n",
    "               LEFT JOIN orderdetails\n",
    "               USING(productCode);\n",
    "               \"\"\")\n",
    "df = pd.DataFrame(cur.fetchall()) \n",
    "df.columns = [i[0] for i in cur.description]\n",
    "print(len(df))\n",
    "print(len(df[df.orderNumber.isnull()]))\n",
    "df[df.orderNumber.isnull()].head()\n",
    "\n",
    "\n",
    "#Multi-JOIN\n",
    "cur.execute('''SELECT contactFirstName, contactLastName, productName, quantityOrdered, orderDate\n",
    "                FROM orders\n",
    "                JOIN customers\n",
    "                USING(customerNumber)\n",
    "                JOIN orderdetails\n",
    "                USING(orderNumber)\n",
    "                JOIN products\n",
    "                USING(productCode)\n",
    "                ORDER BY orderDate DESC\n",
    "                \n",
    "                ;''')\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df.columns = [i[0] for i in cur.description]\n",
    "print('Number of results:', len(df))\n",
    "df.head()\n",
    "\n",
    "\n",
    "#SUBQUERIES\n",
    "cur.execute(\"\"\"SELECT lastName, firstName, officeCode\n",
    "               FROM employees\n",
    "               WHERE officeCode IN (SELECT officeCode \n",
    "                                    FROM offices \n",
    "                                    JOIN employees\n",
    "                                    USING(officeCode)\n",
    "                                    GROUP BY 1\n",
    "                                    HAVING COUNT(employeeNumber) >= 5);\n",
    "                                    \"\"\")\n",
    "cur.execute(\"\"\"SELECT AVG(customerAvgPayment) AS averagePayment\n",
    "               FROM (SELECT AVG(amount) AS customerAvgPayment\n",
    "                     FROM payments\n",
    "                     JOIN customers USING(customerNumber)\n",
    "                     GROUP BY customerNumber);\"\"\")                                    \n",
    "##SUBQUERIES From Lab:\n",
    "# Your code here\n",
    "cur.execute(''' SELECT employeeNumber, firstName, lastName, count(DISTINCT customerNumber)\n",
    "                FROM employees\n",
    "                JOIN customers\n",
    "                ON employeeNumber = salesRepEmployeeNumber\n",
    "                WHERE customerNumber IN          \n",
    "                    (\n",
    "                    SELECT customerNumber\n",
    "                    FROM customers\n",
    "                    GROUP BY customerNumber \n",
    "                    HAVING AVG(creditLimit) > 15000\n",
    "                    )\n",
    "                GROUP BY employeeNumber\n",
    "                ;''')\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df.columns = [i[0] for i in cur.description]\n",
    "df.head(20)\n",
    "\n",
    "cur.execute(''' \n",
    "                SELECT DISTINCT e.employeeNumber, e.firstName, e.lastName, office.city, e.officeCode\n",
    "                FROM employees e \n",
    "                JOIN offices office\n",
    "                USING(officeCode)\n",
    "                WHERE e.employeeNumber IN\n",
    "                    (--select employees who sold products\n",
    "                    SELECT employeeNumber \n",
    "                    FROM employees \n",
    "                    JOIN customers\n",
    "                    ON employees.employeeNumber = customers.salesRepEmployeeNumber\n",
    "                    JOIN orders\n",
    "                    USING(customerNumber)\n",
    "                    JOIN orderdetails\n",
    "                    USING(orderNumber)\n",
    "                    JOIN products\n",
    "                    USING(productCode)\n",
    "                    WHERE products.productCode IN\n",
    "                        (--select products that qualify\n",
    "                        SELECT products.productCode--,  count(DISTINCT orders.customerNumber) as numCust\n",
    "                        FROM products\n",
    "                        JOIN orderdetails\n",
    "                        USING(productCode)\n",
    "                        JOIN orders\n",
    "                        USING(orderNumber)\n",
    "                        --JOIN customers\n",
    "                        --USING(customerNumber)\n",
    "                        GROUP BY productCode\n",
    "                        HAVING count(DISTINCT orders.customerNumber) < 20\n",
    "                        )\n",
    "                    \n",
    "                    )\n",
    "                              \n",
    "            \n",
    "                ;''')\n",
    "\n",
    "\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df.columns = [i[0] for i in cur.description]\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Using SQL with Pandas\n",
    "\n",
    "# Getting Data using slicing syntax\n",
    "foo_df = bar_df[bar_df[bar_df['Col_1'] > bar_df['Col_2']]]\n",
    "\n",
    "# Using The query method\n",
    "foo_df = bar_df.query(\"Col_1 > Col_2\")\n",
    "\n",
    "# These two lines are equivalent!\n",
    "Note that if you want to use and and or statements with the .query() method, you'll need to use \"&\" and \"|\" instead.\n",
    "\n",
    "foo_df = bar_df.query(\"Col_1 > Col_2 & Col_2 <= Col_3\")\n",
    "\n",
    "\n",
    "\n",
    "from pandasql import sqldf\n",
    "\n",
    "Next, it's helpful to write a lambda function that will make it quicker and easier to write queries. Normally, you would have to pass in the global variables every time we use an object. In order to avoid doing this every time, here's how to write a lambda that does this for you:\n",
    "\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "To write a query, you just format it as a multi-line string!\n",
    "\n",
    "q = \"\"\"SELECT\n",
    "        m.date, m.beef, b.births\n",
    "     FROM\n",
    "        meats m\n",
    "     INNER JOIN\n",
    "        births b\n",
    "           ON m.date = b.date;\"\"\"\n",
    "In order to query DataFrames, you can just pass in the query string you've created to our sqldf object that you stored in pysqldf. This will return a DataFrame.\n",
    "\n",
    "results = pysqldf(q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#From Panda/SQL Query lab\n",
    "\n",
    "#Slicing using .query\n",
    "\n",
    "poor_male_survivors_df = df.query(\"Sex == 'male' & (Pclass == '2' | Pclass == '3')\")\n",
    "\n",
    "first_class_df = df.query('Pclass == \"1\"')\n",
    "second_third_class_df = df.query('Pclass != \"1\"')\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize=(14, 7))\n",
    "plt.title('Titanic Survivors by Ship Class')\n",
    "\n",
    "ax = axes[0][0]\n",
    "ax.hist(first_class_df.query('Survived == 1').Age)\n",
    "ax.set_title('First Class Survivors')\n",
    "\n",
    "ax = axes[1][0]\n",
    "ax.hist(first_class_df.query('Survived == 0').Age)\n",
    "ax.set_title('First Class Non-Survivors')\n",
    "\n",
    "ax = axes[0][1]\n",
    "ax.hist(second_third_class_df.query('Survived == 1').Age)\n",
    "ax.set_title('2/3 Class Survivors')\n",
    "\n",
    "ax = axes[1][1]\n",
    "ax.hist(second_third_class_df.query('Survived == 0').Age)\n",
    "ax.set_title('2/3 Class Non-Survivors')\n",
    "\n",
    "female_children_df = df.query('Sex == \"female\" & Age <= 15')\n",
    "df = df.eval('Age_x_Fare = Age*Fare')\n",
    "\n",
    "\n",
    "#########\n",
    "#SQL in pandas!\n",
    "\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "q3 = \"\"\"SELECT Pclass\n",
    "      FROM df\n",
    "      WHERE Sex = 'female' AND Survived = 1\n",
    "      ;\"\"\"\n",
    "\n",
    "q4 = \"\"\"SELECT Pclass --, Sex, Survived\n",
    "      FROM df\n",
    "      WHERE Sex = 'female' AND Survived != 1\n",
    "      ;\"\"\"\n",
    "#display(pysqldf(q3) )\n",
    "#display(pysqldf(q4) )\n",
    "survived_females_by_pclass_df = pysqldf(q3)\n",
    "died_females_by_pclass_df = pysqldf(q4)\n",
    "\n",
    "# Create and label the histograms for each below!\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize=(14, 7))\n",
    "plt.title('Female Titanic Passengers by Survival')\n",
    "\n",
    "ax = axes[0]\n",
    "y_ax = ['1','2','3','?']\n",
    "x_ax = survived_females_by_pclass_df.Pclass.value_counts().loc[y_ax]\n",
    "ax.barh(y_ax,x_ax)\n",
    "ax.set_title('Female Survivors')\n",
    "\n",
    "\n",
    "ax = axes[1]\n",
    "x_ax = died_females_by_pclass_df.Pclass.value_counts().loc[y_ax]\n",
    "ax.barh(y_ax, x_ax)\n",
    "ax.set_title('Female Non-Survivors')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SQL Data Types:\n",
    "In sqlite, \n",
    "text, integer, real, blob\n",
    "\n",
    "\n",
    "##\n",
    "Database Admin (in sqlite)\n",
    "Remember that you can use the bash ls command to preview files and folders in the current working directory.\n",
    "\n",
    "#Creating a db - just connect to a non-existing db\n",
    "import sqlite3 \n",
    "conn = sqlite3.connect('pets_database.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "#\n",
    "#Creating the cats table\n",
    "cur.execute(\"\"\"CREATE TABLE cats (\n",
    "                                id INTEGER PRIMARY KEY,\n",
    "                                name TEXT,\n",
    "                                age INTEGER,\n",
    "                                breed TEXT )          \n",
    "            \"\"\")\n",
    "       \n",
    "# insert Maru into the pet_database.db here\n",
    "cur.execute('''INSERT INTO cats (name, age, breed) \n",
    "                  VALUES ('Maru', 3, 'Scottish Fold');\n",
    "            ''')\n",
    "            \n",
    "#You can also update a table like this: cursor.execute('''ALTER TABLE cats ADD COLUMN notes text;''')\n",
    "\n",
    "#altering a table\n",
    "#The general pattern is ALTER TABLE table_name ADD COLUMN column_name column_type;\n",
    "\n",
    "\n",
    "#updating data\n",
    "cur.execute('''UPDATE [table name] \n",
    "                  SET [column name] = [new value]\n",
    "                  WHERE [column name] = [value];\n",
    "            ''')\n",
    "#eg:\n",
    "cur.execute('''UPDATE cats SET name = \"Hana\" WHERE name = \"Hannah\";''')\n",
    "\n",
    "\n",
    "You use the DELETE keyword to delete table rows.\n",
    "\n",
    "Similar to the UPDATE keyword, the DELETE keyword uses a WHERE clause to select rows.\n",
    "\n",
    "A boilerplate DELETE statement looks like this:\n",
    "\n",
    "cur.execute('''DELETE FROM [table name] WHERE [column name] = [value];''')\n",
    "Code Along III: DELETE\n",
    "Let's go ahead and delete Lil' Bub from our cats table (sorry Lil' Bub):\n",
    "\n",
    "cur.execute('''DELETE FROM cats WHERE id = 2;''')\n",
    "\n",
    "##Saving Changes\n",
    "While everything may look well and good, if you were to connect to the database from another Jupyter notebook (or elsewhere) the database would appear blank! That is, while the changes are reflected in your current session connection to the database you have yet to commit those changes to the master database so that other users and connections can also view the updates.\n",
    "\n",
    "Before you commit the changes, let's demonstrate this concept.\n",
    "\n",
    "First, preview the results of the table:\n",
    "\n",
    "cur.execute(\"\"\"SELECT * FROM cats;\"\"\").fetchall()\n",
    "\n",
    "#Preview the table via a second current cursor/connection \n",
    "#Don't overwrite the previous connection: you'll lose all of your work!\n",
    "conn2 = sqlite3.connect('pets_database.db')\n",
    "cur2 = conn2.cursor()\n",
    "cur2.execute(\"\"\"SELECT * FROM cats;\"\"\").fetchall()\n",
    "\n",
    "#As you can see, the second connection doesn't currently display any data in the cats table! To make the changes universally accessible commit the changes.\n",
    "\n",
    "#In this case:\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "#and verify it shows up\n",
    "#Preview the table via a reloaded second current cursor/connection \n",
    "conn2 = sqlite3.connect('pets_database.db')\n",
    "cur2 = conn2.cursor()\n",
    "cur2.execute(\"\"\"SELECT * FROM cats;\"\"\").fetchall()\n",
    "\n",
    "\n",
    "\n",
    "##dual key\n",
    "Create a Table for Student Grades\n",
    "Create a new table in the database called \"grades\". In the table, include the following fields: userId, courseId, grade.\n",
    "\n",
    "** This problem is a bit more tricky and will require a dual key. (A nuance you have yet to see.) Here's how to do that:\n",
    "\n",
    "CREATE TABLE table_name(\n",
    "   column_1 INTEGER NOT NULL,\n",
    "   column_2 INTEGER NOT NULL,\n",
    "   ...\n",
    "   PRIMARY KEY(column_1,column_2,...)\n",
    ");"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### SQL INterview QUESTIONS\n",
    "\n",
    "# SQL Interview Questions  - Quiz\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This quiz contains questions on topics you can expect to see in an interview pertaining to SQL and Relational Databases. Some of them are multiple choice, while some are short answer. For these short answer questions, double click on the Jupyter Notebook and type your answer below the line. \n",
    "\n",
    "## Question 1\n",
    "\n",
    "What are the 4 main datatypes in SQLite3? Can we use other common types from other kinds of SQL?\n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    "Integer, real, text, blob\n",
    "\n",
    "\n",
    "\n",
    "## Question 2\n",
    "\n",
    "Explain the relationship between **Primary Keys** and **Foreign Keys**.\n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    "A primary key of a table is a unique identifier for a given row of that table.  Foreign keys are columns of other tables that may not be unique but establishes a relationship between a given table and another table.\n",
    "\n",
    "\n",
    "\n",
    "## Question 3\n",
    "\n",
    "Explain the different types of relationships entities can have in a SQL database. \n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "One to one, one to many, many to many.  \n",
    "\n",
    "\n",
    "## Question 4\n",
    "\n",
    "Explain the various types of JOINs possible with SQL. \n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "In SQL in general, right, left, combined with inner/outer.  Depending on needs, can pull all rows of both tables, all rows in just one, or only rows that exist in both.\n",
    "\n",
    "\n",
    "\n",
    "## Question 5\n",
    "\n",
    "Explain the relationship between Aggregate functions and GROUP BY statements.\n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "Any variable that is being aggregrated should have an accompanying variable by which it is grouped.  I.e. the group by variable(s) determine the collections by which the aggregations are grouped.  \n",
    "E.g., one could aggregate total $ spent on orders by customer, or by salesperson, or all $ spent for entire company; each would entail a different way of grouping $ spent.\n",
    "\n",
    "\n",
    "## Question 6\n",
    "\n",
    "What role do Associative Entities play (JOIN Tables) in many-to-many JOINs?\n",
    "\n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "Associative Entities act to connect tables that don't have any keys in common necessarily, but may be connected through an intermediate table.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, we practiced answering open-ended interview questions for SQL and Relational Databases. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#MONGO DB \n",
    "Installing and Running MongoDB\n",
    "This part is easy -- in order to install mongoDB, we'll use our favorite package manager, conda! This part works the same, regardless of what operating system you're running.\n",
    "\n",
    "To install MongoDB on your machine, open a terminal or conda command prompt and just type:\n",
    "\n",
    "conda install mongodb\n",
    "\n",
    "Next, we have to create a directory to store our Mongo data files:\n",
    "\n",
    "sudo mkdir -p /data/db\n",
    "\n",
    "Give the directory the correct permission:\n",
    "\n",
    "sudo chown -R `id -un` /data/db\n",
    "\n",
    "###\n",
    "Create / Read / Update / Delete (CRUD) \n",
    "##\n",
    "######### MUST DO THIS - RUNNING MONGO in background\n",
    "start by running mondgod in conda prompt\n",
    "then mongo in gitbash\n",
    "#######\n",
    "then you can run mongo commands like...\n",
    "db.help()\n",
    "db.test.help()\n",
    "\n",
    "Typically, the main way you'll be working with MongoDB is through a Python library called pymongo that allows us to connect to and manipulate mongo databases in our code, just like sqlite3 allowed us to connect to and work with SQLite databases in the last section.\n",
    "\n",
    "#put in the terminal (gitbash) window running mongo: \n",
    "db.test.save( { a: 1 } )\n",
    "\n",
    "\n",
    "IMPORTANT SETUP\n",
    "import pymongo\n",
    "myclient = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "mydb = myclient['example_database']\n",
    "\n",
    "print(myclient.list_database_names())\n",
    "\n",
    "mycollection = mydb['example_collection']\n",
    "\n",
    "#CRUD ops\n",
    "example_customer_data = {'name': 'John Doe', 'address': '123 elm street', 'age': 28}\n",
    "\n",
    "results = mycollection.insert_one(example_customer_data)\n",
    "results\n",
    "results.inserted_id\n",
    "\n",
    "\n",
    "#More than one add at once:\n",
    "customer_2 = {'name': 'Jane Doe', 'address': '234 elm street', 'age': 7}\n",
    "customer_3 = {'name': 'Santa Claus', 'address': 'The North Pole', 'age': 547}\n",
    "customer_4 = {'name': 'John Doe jr.', 'address': '', 'age': 0.5}\n",
    "\n",
    "list_of_customers = [customer_2, customer_3, customer_4]\n",
    "\n",
    "results_2 = mycollection.insert_many(list_of_customers)\n",
    "\n",
    "#Note that we are allowed to assign the unique id for each new document ourselves by just including the key _id and the value we want to assign as that document's id. However, in general, it is a best practice to let the database create the unique keys for each document itself, and to leave that part alone.\n",
    "\n",
    "#finding data in the collection:\n",
    "query_1 = mycollection.find({})\n",
    "for x in query_1:\n",
    "    print(x)\n",
    "    \n",
    "#getting selections of the data:  (one represents which columns you want)\n",
    "query_2 = mycollection.find({}, {'_id': 1, 'name': 1, 'address': 1})\n",
    "for item in query_2:\n",
    "    print(item)\n",
    "\n",
    "(0 reps which columns you don't want)\n",
    "query_3 = mycollection.find({}, {'age': 0})\n",
    "for item in query_3:\n",
    "    print(item)\n",
    "    \n",
    "### filtering for certain records:\n",
    "query_4 = mycollection.find({'name': 'Santa Claus'})\n",
    "for item in query_4:\n",
    "    print(item)\n",
    "    \n",
    "#filtering for recs with certain conditions:\n",
    "query_5 = mycollection.find({\"age\": {\"$gt\": 20}})\n",
    "for item in query_5:\n",
    "    print(item)\n",
    "    \n",
    "#Can also use REGULAR EXPRESSIONS to filter - will learn those later\n",
    "\n",
    "\n",
    "#Updating docs:\n",
    "record_to_update = {'name' : 'John Doe'}\n",
    "update_1 = {'$set': {'age': 29}}\n",
    "update_2 = {'$set': {'birthday': '02/20/1986'}}\n",
    "\n",
    "mycollection.update_one(record_to_update, update_1)\n",
    "mycollection.update_one(record_to_update, update_2)\n",
    "query_6 = mycollection.find({'name': 'John Doe'})\n",
    "for item in query_6:\n",
    "    print(item)\n",
    "    \n",
    "---can also update_many but it's a little messy\n",
    "    \n",
    "##deletion\n",
    "\n",
    "deletion_1 = mycollection.delete_one({'name': 'John Doe'})\n",
    "print(deletion_1.deleted_count)\n",
    "\n",
    "#This will delete everything: (empty set is \"true\" for everything?)\n",
    "mycollection.delete_many({})\n",
    "\n",
    "#############\n",
    "#From lab:\n",
    "#this is neat, it combines a row condition and a selection of columns\n",
    "query_3 = mycollection.find({'Balance': {\"$gt\": 0}},{'_id':0, 'Name': 1, 'Email':1, 'Balance':1})\n",
    "for x in query_3:\n",
    "    print(x)\n",
    "    \n",
    "###A wacky update_one usage:  ###should also consider how to do with update_many\n",
    "names_list = ['John Smith', 'Jane Smith', 'Adam Enbar', 'Avi Flombaum', 'Steven S.']\n",
    "birthdays_list = ['02/20/1986','07/07/1983', '12/02/1982', '04/17/1983', '08/30/1991']\n",
    "\n",
    "def update_birthdays(names, birthdays):\n",
    "    #my_collection.update_many(zip(names_list, birthdays_list))\n",
    "    if(len(names_list) != len(birthdays_list)):\n",
    "        print('Error: incompatible lengths')\n",
    "        return None\n",
    "    for i in range(len(names_list)):\n",
    "\n",
    "        mycollection.update_one({'Name': names_list[i]}, {'$set': {'Birthday': birthdays_list[i]}})\n",
    "        \n",
    "update_birthdays(names_list, birthdays_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##NO-sql recap\n",
    "The four different kinds of NoSQL databases are:\n",
    "\n",
    "Document Stores\n",
    "Key-Value Stores\n",
    "Column Stores\n",
    "Graph Databases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON and APIs - Introduction\n",
    "Introduction\n",
    "In this section, you’ll learn about an additional data type: JSON (which stands for JavaScript Object Notation), as well as APIs (Application Programming Interfaces).\n",
    "\n",
    "JSON is the new standard data format for the web. An older data format that is still used on the web is XML, or Extensible Markup Language, which you’ll have a chance to learn more about in the Appendix. APIs are one of the many ways you’ll access data as a data scientist.\n",
    "\n",
    "Working with JSON files\n",
    "A substantial part of the job of a professional data scientist is to find and access data. You've spent a bunch of time looking at how to pull information from relational databases, but there is lots of information you might need to work with that is either not in a relational database, or that is not exposed to you via a relational database.\n",
    "\n",
    "For example, you might work with a third party website that has a lot of geographical data (perhaps points of interest near state highways). Within their company, they may well store the data within a relational database, but you might have to access it using an API (an Application Programming Interface - a way your computer can talk to their computer to go get some information!). Over the next couple of sections, we'll be looking at accessing data through APIs and enough HTML and CSS to get started with web scraping (downloading information automatically from websites). In this section, you'll look at a key data storage format, JSON, that you may well come across when retrieving data from other web applications or from inside your company.\n",
    "\n",
    "JSON\n",
    "You'll start off this section with a brief introduction to JSON so you know what this file format looks like. You'll then get some hands-on practice loading and parsing data from JSON files into Python.\n",
    "\n",
    "JSON Schemas\n",
    "Once you've learned how to import data that has been stored in the JSON format, you'll look at JSON schemas - a way to describe the expected structure of a given JSON file.\n",
    "\n",
    "Exploring JSON Schemas\n",
    "Finally, you'll get a lot more practice working with JSON schemas, exploring unknown schemas, accessing and manipulating data inside a JSON file and then converting JSON to alternate data formats such as pandas DataFrames. This lab will be a great chance for you to practice your Python programming skills and get comfortable with importing and transforming JSON data - something you may well have to do on a regular basis as a professional data scientist.\n",
    "\n",
    "APIs\n",
    "One of the many ways you'll find yourself accessing data as a professional data scientist is via APIs (Application Programming Interfaces). Typically, you'll send a request and get some data back, often in JSON or XML format. In this section, you'll get some hands-on experience retrieving and working with data provided by a range of different APIs.\n",
    "\n",
    "Introduction to APIs\n",
    "In this section, we'll provide a conceptual introduction to various kinds of APIs and some of the reasons that businesses create them.\n",
    "\n",
    "The Client Server Model\n",
    "We then look at the basic model of \"clients\" and \"servers\" to provide a framework for thinking about how your \"client\" retrieves information from an API \"server\".\n",
    "\n",
    "The Request/Response Cycle\n",
    "Next, we'll look at the fundamental mechanism by which web-based APIs are typically accessed - sending an HTTP request and then processing the response provided by the server. We'll also get a little experience working with HTTP requests using the Python .get() method within the requests package. We also get some hands-on experience retrieving information from NASA using Open Notify.\n",
    "\n",
    "APIs and OAuth\n",
    "Usually, access to a given API is limited to avoid abuse. One of the most common mechanisms for identifying your API requests to make sure they fit within acceptable usage guidelines is OAuth - Open Authorization - a standard for authorizing clients across web requests. In this section, we'll provide an overview of what OAuth is and how it works by looking at how it is implemented by Dropbox.\n",
    "\n",
    "Working with the Yelp API\n",
    "Next, we'll get some practice working with a real API, retrieving information from the Yelp API.\n",
    "\n",
    "Creating interactive maps with Folium\n",
    "We wrap up the section by creating interactive maps with Folium. In the Appendix, we include a Lab where you can build a Geographic Information System using Folium and data obtained from the Yelp API to display it on an interactive map.\n",
    "\n",
    "Summary\n",
    "Whether it’s from an API or a NoSQL store, it's quite possible that some of the data you find yourself working with will be stored using JSON. In this section, you'll build the confidence to be able to import and transform such data.\n",
    "\n",
    "Also, many companies provide access to their data via an API, so being able to connect to and work with data provided via an API is a critical skill as a professional data scientist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree\n",
    "    https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree\n",
    "        https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('nyc_2001_campaign_finance.json') as f:\n",
    "    data = json.load(f)\n",
    "print(type(data))\n",
    "\n",
    "\n",
    "#look at meta data\n",
    "import pandas as pd\n",
    "pd.set_option(\"max_colwidth\", 120)\n",
    "pd.DataFrame(\n",
    "    data=data['meta']['view'].values(),\n",
    "    index=data['meta']['view'].keys(),\n",
    "    columns=[\"value\"]\n",
    ")\n",
    "\n",
    "\n",
    "#look at main data\n",
    "\n",
    "len(data['data'])\n",
    "data['data'][0]\n",
    "data['data'][1]\n",
    "#looks tabular, import into pandas\n",
    "pd.DataFrame(data['data'])\n",
    "\n",
    "#looking at meta again\n",
    "data['meta']['view'].keys()\n",
    "#Ok, description is the 7th one! Let's pull the value associated with the description key:\n",
    "data['meta']['view']['description']\n",
    "\n",
    "\n",
    "\n",
    "############## FROM LAB\n",
    "\n",
    "print(f\"The overall data type is {type(data)}\")\n",
    "print(f\"The keys are {list(data.keys())}\")\n",
    "print()\n",
    "print(\"The value associated with the 'meta' key has metadata, including all of these attributes:\")\n",
    "print(list(data['meta']['view'].keys()))\n",
    "print()\n",
    "print(f\"The value associated with the 'data' key is a list of {len(data['data'])} records\")\n",
    "\n",
    "column_names = []\n",
    "for d in data['meta']['view']['columns']:\n",
    "    column_names.append(d['name'])\n",
    "column_names\n",
    "\n",
    "assert len(column_names) == 19\n",
    "\n",
    "# Print the top 10 candidates by total payments\n",
    "sorted(candidate_total_payments, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(data=data['data'][1:], columns=column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with known json schemas\n",
    "#e.g. new york times API\n",
    "import json\n",
    "with open('ny_times_response.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "data['response']['docs']['headline'] #this causes an error - we treated a list like a dictionary\n",
    "#can check types on these\n",
    "type(data['response'])\n",
    "\n",
    "#Flattening data - reducing depth of dictionary embedding.  combine names of dicts.\n",
    "#So, first let's write a function that takes in that complete dictionary, and returns a copy with only the 'main' and 'kicker' keys and values, now labeled 'headline_main' and 'headline_kicker':\n",
    "def extract_headline_info(headline_dict):\n",
    "    result = {}\n",
    "    result['headline_main'] = headline_dict['main']\n",
    "    result['headline_kicker'] = headline_dict['kicker']\n",
    "    return result\n",
    "\n",
    "#testing it:\n",
    "extract_headline_info(docs[2]['headline'])\n",
    "\n",
    "#another extractor/flattener:\n",
    "def extract_doc_info(doc):\n",
    "    info = extract_headline_info(doc['headline'])\n",
    "    info['pub_date'] = doc['pub_date']\n",
    "    info['word_count'] = doc['word_count']\n",
    "    return info\n",
    "doc_info_list = [extract_doc_info(doc) for doc in docs]\n",
    "doc_info_list\n",
    "\n",
    "#using pandas\n",
    "import pandas as pd\n",
    "pd.DataFrame(data['response']['docs'])\n",
    "\n",
    "#but since doc_info_list is flattened, it looks nicer in pandas\n",
    "pd.DataFrame(doc_info_list)\n",
    "\n",
    "#recreating this from raw data using pandas instead of python\n",
    "# Create dataframe of raw docs info\n",
    "df = pd.DataFrame(data['response']['docs'])\n",
    "\n",
    "# Make new headline_main and headline_kicker columns\n",
    "df['headline_main'] = df['headline'].apply(lambda headline_dict: headline_dict['main'])\n",
    "df['headline_kicker'] = df['headline'].apply(lambda headline_dict: headline_dict['kicker'])\n",
    "\n",
    "# Subset to only the relevant columns\n",
    "df = df[['headline_main', 'headline_kicker', 'pub_date', 'word_count']]\n",
    "df\n",
    "#This is a good general strategy for transforming nested JSON: create a DataFrame and then break out nested features into their own column features.\n",
    "\n",
    "#outputting to json!\n",
    "with open('doc_info_list.json', 'w') as f:\n",
    "    json.dump(doc_info_list, f)\n",
    "    \n",
    "#reopening\n",
    "with open('doc_info_list.json') as f:\n",
    "    doc_info_list_from_disk = json.load(f)\n",
    "\n",
    "doc_info_list_from_disk == doc_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b2067b4b554b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#when json schema is unknown:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output.json'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#check type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output.json'"
     ]
    }
   ],
   "source": [
    "#when json schema is unknown:\n",
    "import json\n",
    "with open('output.json') as f:\n",
    "    data = json.load(f)\n",
    "#check type\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From JSON Lab\n",
    "column_names = []\n",
    "for d in m['view']['columns']:\n",
    "    column_names.append(d['name'])\n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data=data['data'], columns=column_names)\n",
    "df_selected = df[(df['Question'] == 'Current asthma prevalence among adults aged >= 18 years') \n",
    "                 & (df['StratificationCategoryID1']=='OVERALL')\n",
    "                & (df['DataValueTypeID']=='CRDPREV')\n",
    "                 &(df['LocationDesc']!='United States')]\n",
    "print('Number of rows after filtering:',len(df_selected))\n",
    "\n",
    "df_selected=df_selected[['DataValue','LocationDesc']]\n",
    "df_selected['DataValue'] = df_selected['DataValue'].astype('float')\n",
    "df_selected.sort_values('DataValue', ascending=False)[:10]\n",
    "\n",
    "names = list(df_selected.sort_values('DataValue', ascending=False)[:10]['LocationDesc'])\n",
    "values = list(df_selected.sort_values('DataValue', ascending=False)[:10]['DataValue'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(names[::-1], values[::-1]) # Values inverted so highest is at top\n",
    "ax.set_title('Adult Asthma Rates by State in 2016')\n",
    "ax.set_xlabel('Percent 18+ with Asthma');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APIs - application programming interfaces\n",
    "#request/response cycle\n",
    "#client/server model\n",
    "\n",
    "#HTTP request/response cycle:\n",
    "#python has two modules for it: \n",
    "#urllib and urllib2 but these are confusing.\n",
    "#To make these things simpler, one easy-to-use third-party library, known asRequests, is available and most developers prefer to use it instead or urllib/urllib2. It is an Apache2 licensed HTTP library powered by urllib3 and httplib.\n",
    "\n",
    "# Uncomment and install requests if you don't have it already\n",
    "# !pip install requests\n",
    "\n",
    "# Import requests to working environment\n",
    "import requests\n",
    "### Making a request\n",
    "resp = requests.get('https://www.google.com')\n",
    "# Check the returned status code\n",
    "resp.status_code == requests.codes.ok\n",
    "#Once we know that our request was successful and we have a valid response, we can check the returned information using .text property of the response object.\n",
    "print (resp.text)\n",
    "# Read the header of the response - convert to dictionary for displaying k:v pairs neatly\n",
    "dict(resp.headers)\n",
    "#The content of the headers is our required element. You can see the key-value pairs holding various pieces of information about the resource and request. Let's try to parse some of these values using the requests library:\n",
    "print(resp.headers['Date'])  # Date the response was sent\n",
    "print(resp.headers['server'])   # Server type (google web service - GWS)\n",
    "\n",
    "#httpbin.org is a popular website to test different HTTP operations and practice with request-response cycles. Let's use httpbin/get to analyze the response to a GET request. First of all, let's find out the response header and inspect how it looks.\n",
    "\n",
    "r = requests.get('http://httpbin.org/get')\n",
    "\n",
    "response = r.json()  \n",
    "print(r.json())  \n",
    "print(response['args'])  \n",
    "print(response['headers'])  \n",
    "print(response['headers']['Accept'])  \n",
    "print(response['headers']['Accept-Encoding'])  \n",
    "print(response['headers']['Host'])  \n",
    "print(response['headers']['User-Agent'])  \n",
    "print(response['origin'])  \n",
    "print(response['url'])  \n",
    "\n",
    "#Let's use requests object structure to parse the values of headers as we did above.\n",
    "\n",
    "print(r.headers['Access-Control-Allow-Credentials'])  \n",
    "print(r.headers['Access-Control-Allow-Origin'])  \n",
    "print(r.headers['CONNECTION'])  \n",
    "print(r.headers['content-length'])  \n",
    "print(r.headers['Content-Type'])  \n",
    "print(r.headers['Date'])  \n",
    "print(r.headers['server'])  \n",
    "\n",
    "#In some cases, you'll need to pass parameters along with your GET requests. These extra parameters usually take the the form of query strings added to the requested URL. To do this, we need to pass these values in the params parameter. Let's try to access information from httpbin with some user information.\n",
    "credentials = {'user_name': 'FlatironSchool', 'password': 'learnlovecode'}  \n",
    "r = requests.get('http://httpbin.org/get', params=credentials)\n",
    "\n",
    "print(r.url)  \n",
    "print(r.text)  \n",
    "\n",
    "# HTTP POST method\n",
    "# Sometimes we need to send one or more files simultaneously to the server. For example, if a user is submitting a form and the form includes different fields for uploading files, like user profile picture, user resume, etc. Requests can handle multiple files on a single request. This can be achieved by putting the files to a list of tuples in the form (field_name, file_info).\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'http://httpbin.org/post'  \n",
    "file_list = [  \n",
    "    ('image', ('fi.png', open('images/fi.png', 'rb'), 'image/png')),\n",
    "    ('image', ('fi2.jpeg', open('images/fi2.jpeg', 'rb'), 'image/png'))\n",
    "]\n",
    "\n",
    "r = requests.post(url, files=file_list)  \n",
    "print(r.text)  \n",
    "\n",
    "\n",
    "#Open Notify is a cool project that gives good info about NASA stuff\n",
    "\n",
    "lat = 40.71\n",
    "lon = -74\n",
    "params = {'lat': lat, 'lon': lon}\n",
    "resp = requests.get('http://api.open-notify.org/iss-pass.json', params = params)\n",
    "resp.status_code == requests.codes.ok\n",
    "#print ('Response headers:',resp.headers, type(resp.headers))\n",
    "#print('Resp json:',resp.json())\n",
    "print ('Response text:',resp.text, type(resp.text),'\\n')\n",
    "r3=dict(resp.json())\n",
    "print('Responses as dictionary:',r)\n",
    "#print(r['passes'])\n",
    "print(\"Pass info for ISS passes over NYC is given.\")\n",
    "\n",
    "print('Current time:', r['timestamp'])\n",
    "print('Seconds(probably?) until next pass:',resp.json()['response'][0]['risetime']-r['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Auth\n",
    "#see explanation of auth cycle\n",
    "#Dropbox has good documentation of it.\n",
    "#examples using yelp:\n",
    "#Generate access token:\n",
    "# https://www.yelp.com/developers/v3/manage_app\n",
    "\n",
    "#for doing secret password/token stuff, \n",
    "# Move to your home (root) directory:\n",
    "# cd ~\n",
    "# Now make the .secret/ directory:\n",
    "# mkdir .secret\n",
    "# This will create a new folder in your home directory where you can store files for any of the API information you have.\n",
    "\n",
    "# Can you find the file you just made in your terminal? NOTE: dot files won't show up with just ls you must use the show all command as well ls -a\n",
    "\n",
    "# Move into the newly created .secret/ folder and create a file using vscode or any text editor to store your yelp API login info.¶\n",
    "# cd .secret/\n",
    "# code yelp_api.json\n",
    "# In this file, let's create a dictionary of values representing the client id and API key that looks something like this:\n",
    "\n",
    "# {\"api_key\": \"input api key here!\"}\n",
    "\n",
    "import json\n",
    "\n",
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "keys = get_keys(\"/Users/erict/.secret/yelp_api.json\")\n",
    "\n",
    "api_key = keys['api_key']\n",
    "\n",
    "###\n",
    "import requests\n",
    "term = 'Mexican'\n",
    "location = 'Astoria NY'\n",
    "SEARCH_LIMIT = 10\n",
    "\n",
    "url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "headers = {\n",
    "        'Authorization': 'Bearer {}'.format(api_key),\n",
    "    }\n",
    "\n",
    "url_params = {\n",
    "                'term': term.replace(' ', '+'),\n",
    "                'location': location.replace(' ', '+'),\n",
    "                'limit': SEARCH_LIMIT\n",
    "            }\n",
    "response = requests.get(url, headers=headers, params=url_params)\n",
    "print(response)\n",
    "print(type(response.text))\n",
    "print(response.text[:1000])\n",
    "response.json().keys()\n",
    "\n",
    "for key in response.json().keys():\n",
    "    print(key)\n",
    "    value = response.json()[key] #Use standard dictionary formatting\n",
    "    print(type(value)) #What type is it?\n",
    "    print('\\n\\n') #Separate out data\n",
    "    \n",
    "response.json()['businesses'][:2]\n",
    "response.json()['total']\n",
    "response.json()['region']\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(response.json()['businesses'])\n",
    "print(len(df)) #Print how many rows\n",
    "print(df.columns) #Print column names\n",
    "df.head() #Previews the first five rows. \n",
    "#You could also write df.head(10) to preview 10 rows or df.tail() to see the bottom\n",
    "\n",
    "#of course, every API has different parameters - so check out their documentation\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "#Our previous function for loading our api key file\n",
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "keys = get_keys(\"/Users/erict/.secret/yelp_api.json\")\n",
    "api_key = keys['api_key']\n",
    "import requests\n",
    "#https://www.yelp.com/developers/documentation/v3/business_search\n",
    "url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "headers = {\n",
    "        'Authorization': 'Bearer {}'.format(api_key),\n",
    "    }\n",
    "\n",
    "url_params = {\n",
    "                'location': 'NYC'\n",
    "            }\n",
    "response = requests.get(url, headers=headers, params=url_params)\n",
    "#can even pass more params\n",
    "url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "headers = {\n",
    "        'Authorization': 'Bearer {}'.format(api_key),\n",
    "    }\n",
    "\n",
    "url_params = {\n",
    "                'location': 'NYC',\n",
    "                'term' : 'pizza',\n",
    "                'limit' : 50,\n",
    "                'price' : \"1,2,3,4\",\n",
    "                'open_now' : True\n",
    "            }\n",
    "response = requests.get(url, headers=headers, params=url_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folium codealong\n",
    "import folium\n",
    "\n",
    "lat = 51.51\n",
    "long = -0.14\n",
    "\n",
    "#Create a map of the area\n",
    "base_map = folium.Map([lat, long], zoom_start=13)\n",
    "base_map\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Generate some random locations to add to our map\n",
    "x = [lat + np.random.uniform(-.1,.1) for i in range(20)]\n",
    "y = [long + np.random.uniform(-.1,.1) for i in range(20)]\n",
    "points = list(zip(x, y))\n",
    "for p in points:\n",
    "    lat = p[0]\n",
    "    long = p[1]\n",
    "    marker = folium.Marker(location=[lat, long])\n",
    "    marker.add_to(base_map)\n",
    "base_map\n",
    "\n",
    "#labels for points\n",
    "for p in points:\n",
    "    lat = p[0]\n",
    "    long = p[1]\n",
    "    popup_text = \"Latitude: {}, Longitude: {}\".format(lat,long)\n",
    "    popup = folium.Popup(popup_text, parse_html=True)\n",
    "    marker = folium.Marker(location=[lat, long], popup=popup)\n",
    "    marker.add_to(base_map)\n",
    "base_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML\n",
    "<p>Hello World</p>\n",
    "<p>This <a href=\"http://www.google.com\">link</a> will be a part of a separate paragraph.</p>\n",
    "\n",
    "#To use HTML5, the current up-to-date version, you can simply declare <!DOCTYPE html>.\n",
    "<!DOCTYPE html>\n",
    "<html>#then html open/close tags\n",
    "#and inside, head and body\n",
    "<head>\n",
    "        <!-- metadata about the HTML document as a whole -->\n",
    "\n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "        <!-- content of our page will be here! -->\n",
    "\n",
    "    </body>\n",
    "\n",
    "</html>\n",
    "#to make comments:\n",
    "<!-- NYC Pizza is world-famous, cheap, and loved by both vermin and human-like! -->\n",
    "\n",
    "#header levels:\n",
    "<h1>Dogs!</h1>\n",
    "<h3>Why Dogs are Great</h3>\n",
    "\n",
    "#images\n",
    "<img src=\"URL_TO_IMAGE\" alt=\"Picture of a Dog\">\n",
    "\n",
    "#lists\n",
    "<h5>My Favorite Things in No Particular Order</h5>\n",
    "<ul>\n",
    "    <li>Coffee</li>\n",
    "    <li>Vinyl Records</li>\n",
    "    <li>Pickling</li>\n",
    "</ul>\n",
    "\n",
    "#numbered lists\n",
    "<h5>Top 5 Pizza Places in NYC</h5>\n",
    "<ol>\n",
    "    <li>DiFara Pizza</li>\n",
    "    <li>Lucali's</li>\n",
    "    <li>Sal and Carmine's</li>\n",
    "    <li>Juliana's</li>\n",
    "    <li>Joe's</li>\n",
    "</ol>\n",
    "\n",
    "GOod resources:\n",
    "    w3schools \n",
    "    MDN\n",
    "    \n",
    "https://web.stanford.edu/group/csp/cs21/htmlcheatsheet.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Intro to CSS\n",
    "https://learn.co/tracks/module-1-data-science-career-2-1/intro-to-data-with-python-and-sql/section-10-html-css-and-web-scraping/intro-to-css\n",
    "\n",
    "How Does Writing CSS Differ From Writing HTML?\n",
    "As we write CSS these are the type of questions we might ask ourselves:\n",
    "\n",
    "Should the layout of the text be in a single or double column?\n",
    "Should we use a different font color for the header?\n",
    "How should the same content appear differently on a desktop vs. a mobile device?\n",
    "All of the questions above deal with the esthetic considerations of the page. These are the concerns of the presentation layer (CSS).\n",
    "\n",
    "As a contrast, let's consider the type of questions we might ask ourselves as we write HTML:\n",
    "\n",
    "Does the order of items within a list matter? Should it be a numbered list?\n",
    "Should we wrap a list of links inside a navigation tag?\n",
    "Is this the most important header in the whole HTML document?\n",
    "The last few questions deal with structure, hierarchy, and meaning. These are the concerns of the content layer (HTML).\n",
    "\n",
    "When we write CSS, we focus on esthetic and display considerations. When we write HTML, we focus on structure, hierarchy, and meaning.\n",
    "\n",
    "    \n",
    "\n",
    "CSS selectors are a way of declaring which HTML elements you wish to style. Selectors can appear a few different ways:\n",
    "\n",
    "The type of HTML element(h1, p, div, etc.)\n",
    "The value of an element's id or class (<p id='idvalue'></p>, <p\n",
    "class='classname'></p>)\n",
    "The value of an element's attributes (value=\"hello\")\n",
    "The element's relationship with surrounding elements (a p within an element with class of .infobox)\n",
    "For example, if you want the body of the page to have a black background, your selector syntax may be html or body. For anchors, your selector would be a. A few more examples are listed below:\n",
    "\n",
    "/*\n",
    "The CSS comment syntax is text between \"slash-star\" and \"star-slash\"\n",
    "*/\n",
    "\n",
    "Type selectors documentation https://developer.mozilla.org/en-US/docs/Web/CSS/Type_selectors\n",
    "    \n",
    "The element type class is a commonly used selector. Class selectors are used to select all elements that share a given class name. The class selector syntax is: .classname. Prefix the class name with a '.'(period).\n",
    "/*\n",
    "select all elements that have the 'important-topic' classname (e.g. <h1 class='important-topic'>\n",
    "and <h1 class='important-topic'>)\n",
    "*/\n",
    ".important-topic\n",
    "/*\n",
    "select all elements that have the 'welcome-message' classname (e.g. <p class='helpful-hint'>\n",
    "and <p class='helpful-hint'>) - #I think they meant helpful hint instead of welcome message\n",
    "*/\n",
    ".helpful-hint \n",
    "\n",
    "\n",
    "You can also use the id selector to style elements. However, there should be only one element with a given id in an HTML document. This can make styling with the ID selector ideal for one-off styles. The id selector syntax is: #idvalue. Prefix the id attribute of an element with a # (which is called \"octothorpe,\" \"pound sign\", or \"hashtag\").\n",
    "\n",
    "/*\n",
    "selects the HTML element with the id 'main-header' (e.g. <h1 id='main-header'>)\n",
    "*/\n",
    "#main-header\n",
    "\n",
    "/*\n",
    "selects the HTML element with the id 'welcome-message' (e.g. <p id='welcome-message'>)\n",
    "*/\n",
    "#welcome-message\n",
    "\n",
    "\n",
    "\n",
    "ID Selectors docn: https://developer.mozilla.org/en-US/docs/Web/CSS/ID_selectors\n",
    "        \n",
    "Declaring css properties:\n",
    "A CSS property name with a CSS property value is a CSS declaration. To apply a CSS declaration like color: blue to a specific HTML element, you need to combine your CSS declaration with a CSS selector. The association between one or more CSS declarations and a CSS selector is called a CSS declaration block. CSS declarations (one or more) that applied to a specific selector are wrapped by curly braces ({ }). Each declaration inside a declaration block must be separated by a semi-colon (;).\n",
    "\n",
    "Below is a sample CSS declaration block.\n",
    "\n",
    "selector {\n",
    "  color: blue;\n",
    "}\n",
    "/*\n",
    "This is a css declaration for a selector\n",
    "'color' is a property name and 'blue' is a css property value\n",
    "!!!!! CSS declarations must end with a semi-colon (;) !!!!!\n",
    "*/\n",
    "Here's a more complete example declaration block.\n",
    "\n",
    "/*\n",
    "The CSS declaration block below:\n",
    "* Will apply to all `h1` elements\n",
    "* Will change the text color to blue\n",
    "* Will set the font family to Georgia\n",
    "*/\n",
    "h1 {\n",
    "  color: blue;\n",
    "  font-family: Georgia;\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "https://codepen.io/\n",
    "    \n",
    "Sample:\n",
    "https://codepen.io/curiositypaths/pen/WddzQM?editors=1100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The css lab seems written by someone else.  Doesn't explain well.\n",
    "Making a Rainbow\n",
    "First off make sure you have forked and cloned this repo. Next, create a new branch, and switch to it; it's git checkout -b your_solution_branch_name in case you forgot.\n",
    "\n",
    "In that directory, you'll see three files. index.html, main.css, and this README.md. Open them in your text editor via your command line. Also, open index.html in your browser; if everything is working correctly you should see a white page. Good job!\n",
    "\n",
    "see lab documents.\n",
    "#css snippet:\n",
    "div {\n",
    "  border: 20px solid #000; /* this is short hand for setting\n",
    "                            the border's width, its type, and color */\n",
    "  display: inline-block; /* a way of positioning elements */\n",
    "  min-width: 20em; /* the two min styles make the shape */\n",
    "  min-height: 25em; /* of the divs oblong*/\n",
    "  border-radius: 50%; /* this makes the normally square div round */\n",
    "  border-left-color: transparent; /* these remove the color from the left side */\n",
    "  border-right-color: transparent; /* the right side */\n",
    "  border-bottom-color: transparent; /* and the bottom of the circle */\n",
    "}\n",
    "#red { /* this selects any elements with the red id */\n",
    "  border-top-color: #f00;\n",
    "}\n",
    "#orange { /* this selects any elements with the red id */\n",
    "  border-top-color: #ffa500;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beautiful Soup - a Python package for web scraping\n",
    "#DOM: Document Object Model.  https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction\n",
    "from bs4 import BeautifulSoup\n",
    "with open('sample_page.html') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "print(soup.prettify())\n",
    "##\n",
    "print(soup.title)\n",
    "# <title>The Dormouse's story</title>\n",
    "\n",
    "print(soup.title.name)\n",
    "# u'title'\n",
    "\n",
    "print(soup.title.string)\n",
    "# u'The Dormouse's story'\n",
    "\n",
    "print(soup.title.parent.name)\n",
    "# u'head'\n",
    "\n",
    "print(soup.p)\n",
    "# <p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "print(soup.p['class'])\n",
    "# u'title'\n",
    "\n",
    "print(soup.a)\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    "\n",
    "print(soup.find_all('a'))\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "print(soup.find(id=\"link3\"))\n",
    "# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
    "###\n",
    "Beautiful soup is the preliminary tool for web scraping. That said, there are more complex examples where you may wish to either scrape larger amounts of data through full-on web crawling or trickier examples involving javascript. For these and other scenarios, alternative tools such as Selenium and Scrapy are worth investigating.\n",
    "\n",
    "#\n",
    "Beautiful Soup - A good go-to tool for parsing the DOM\n",
    "https://www.crummy.com/software/BeautifulSoup/?\n",
    "\n",
    "Selenium - Browser automation (useful when you need to interact with javascript for more complex scraping)\n",
    "https://www.seleniumhq.org/\n",
    "\n",
    "Scrapy - Another package for scraping larger datasets at scale\n",
    "https://scrapy.org/\n",
    "    \n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping in practice\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "html_page = requests.get('http://books.toscrape.com/') # Make a get request to retrieve the page\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser') # Pass the page contents to beautiful soup for parsing\n",
    "\n",
    "soup.prettify\n",
    "\n",
    "warning = soup.find('div', class_=\"alert alert-warning\")\n",
    "warning # Previewing is optional but can help you verify you are selecting what you think you are\n",
    "\n",
    "# This code is a bit brittle but works for now; in general, ask, are you confident that this will work for all pages?\n",
    "book_container = warning.nextSibling.nextSibling \n",
    "book_container\n",
    "\n",
    "titles = book_container.findAll('h3') # Make a selection\n",
    "titles[0] # Preview the first entry it\n",
    "\n",
    "titles[0].find('a')\n",
    "titles[0].find('a').attrs['title']\n",
    "final_titles = [h3.find('a').attrs['title'] for h3 in book_container.findAll('h3')]\n",
    "print(len(final_titles), final_titles[:5])\n",
    "#Passing regular expressions\n",
    "import re\n",
    "regex = re.compile(\"star-rating (.*)\")\n",
    "book_container.findAll('p', {\"class\" : regex}) # Initial Trial in developing the script\n",
    "star_ratings = []\n",
    "for p in book_container.findAll('p', {\"class\" : regex}):\n",
    "    star_ratings.append(p.attrs['class'][-1])\n",
    "star_ratings\n",
    "star_dict = {'One': 1, 'Two': 2, 'Three':3, 'Four': 4, 'Five':5} # Manually create a dictionary to translate to numeric\n",
    "star_ratings = [star_dict[s] for s in star_ratings]\n",
    "star_ratings\n",
    "book_container.findAll('p', class_=\"price_color\") # First preview\n",
    "prices = [p.text for p in book_container.findAll('p', class_=\"price_color\")] # Keep cleaning it up\n",
    "print(len(prices), prices[:5])\n",
    "prices = [float(p[1:]) for p in prices] # Removing the pound sign and converting to float\n",
    "print(len(prices), prices[:5])\n",
    "avails = book_container.findAll('p', class_=\"instock availability\")\n",
    "avails[:5] # Preview our selection\n",
    "avails[0].text # Dig a little deeper into the structure\n",
    "avails = [a.text.strip() for a in book_container.findAll('p', class_=\"instock availability\")] # Finalize the selection\n",
    "print(len(avails), avails[:5])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([final_titles, star_ratings, prices, avails]).transpose()\n",
    "df.columns = ['Title', 'Star_Rating', 'Price_(pounds)', 'Availability']\n",
    "df\n",
    "\n",
    "#pseudocode:\n",
    "df = pd.DataFrame()\n",
    "for i in range(2,51):\n",
    "    url = \"http://books.toscrape.com/catalogue/page-{}.html\".format(i)\n",
    "    html_page = requests.get(url)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    warning = soup.find('div', class_=\"alert alert-warning\")\n",
    "    book_container = warning.nextSibling.nextSibling\n",
    "    new_titles = retrieve_titles(book_container)\n",
    "    new_star_ratings = retrieve_ratings(book_container)\n",
    "    new_prices = retrieve_prices(book_container)\n",
    "    new_avails = retrieve_avails(book_container)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Scraping lab - hardcoded URL hack vs scrape for next page url:\n",
    "\n",
    "#hacked next page:\n",
    "#Your code here\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_info = []\n",
    "book_info = {}\n",
    "final_titles = []\n",
    "star_ratings = []\n",
    "prices = []\n",
    "avails = []\n",
    "df = pd.DataFrame()\n",
    "for i in range(1,51):\n",
    "    url = \"http://books.toscrape.com/catalogue/page-{}.html\".format(i)\n",
    "    html_page = requests.get(url)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    new_titles = retrieve_titles(soup)\n",
    "    new_star_ratings = retrieve_ratings(soup)\n",
    "    new_prices = retrieve_prices(soup)\n",
    "    new_avails = retrieve_availabilities(soup)\n",
    "    page_info.append([new_titles, new_star_ratings, new_prices, new_avails])\n",
    "    final_titles+=new_titles\n",
    "    star_ratings+=new_star_ratings\n",
    "    prices+=new_prices\n",
    "    avails+=new_avails\n",
    "    \n",
    "    for i in range(len(new_titles)):\n",
    "        book_info[new_titles[i]]= [new_star_ratings[i], new_prices[i], new_avails]\n",
    "        \n",
    "df = pd.DataFrame([final_titles, star_ratings, prices, avails]).transpose()\n",
    "df.columns = ['Title', 'Star_Rating', 'Price_(pounds)', 'Availability']\n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Scrape for next page url:\n",
    "#Your code here\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_info = []\n",
    "book_info = {}\n",
    "final_titles = []\n",
    "star_ratings = []\n",
    "prices = []\n",
    "avails = []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "url = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "html_page = requests.get(url)\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "\n",
    "def nextUrl(soup):\n",
    "    \n",
    "    localwarning = soup.find('div', class_=\"alert alert-warning\")\n",
    "    localbook_container = localwarning.nextSibling.nextSibling\n",
    "    try:\n",
    "        #print('url',url)\n",
    "        rel_extension = localbook_container.findAll('li')[-1].find('a').attrs['href']#.find('href=')\n",
    "        #print('relex:',rel_extension)\n",
    "        #print('url',url)\n",
    "        return rel_extension\n",
    "    except:\n",
    "        #print(\"Error\")\n",
    "        return None\n",
    "        \n",
    "while(True):\n",
    "    #print('while url:',url)\n",
    "    html_page = requests.get(url)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    new_titles = retrieve_titles(soup)\n",
    "    new_star_ratings = retrieve_ratings(soup)\n",
    "    new_prices = retrieve_prices(soup)\n",
    "    new_avails = retrieve_availabilities(soup)\n",
    "    page_info.append([new_titles, new_star_ratings, new_prices, new_avails])\n",
    "    final_titles+=new_titles\n",
    "    star_ratings+=new_star_ratings\n",
    "    prices+=new_prices\n",
    "    avails+=new_avails\n",
    "    \n",
    "    for i in range(len(new_titles)):\n",
    "        book_info[new_titles[i]]= [new_star_ratings[i], new_prices[i], new_avails]\n",
    "        \n",
    "    #Next URL logic:\n",
    "    try:\n",
    "        rel_extension = nextUrl(soup)\n",
    "        #print('rel_extension:',rel_extension)\n",
    "        if rel_extension == None:\n",
    "            break\n",
    "        rel_path = url[0:url.rfind('/')]+'/'\n",
    "        #print('relpath',rel_path)\n",
    "        url = rel_path+rel_extension\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "df = pd.DataFrame([final_titles, star_ratings, prices, avails]).transpose()\n",
    "df.columns = ['Title', 'Star_Rating', 'Price_(pounds)', 'Availability']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping for images\n",
    "#normal setup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "html_page = requests.get('http://books.toscrape.com/') # Make a get request to retrieve the page\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser') # Pass the page contents to beautiful soup for parsing\n",
    "warning = soup.find('div', class_=\"alert alert-warning\")\n",
    "book_container = warning.nextSibling.nextSibling\n",
    "#look for img\n",
    "images = book_container.findAll('img')\n",
    "ex_img = images[0] # Preview an entry\n",
    "ex_img\n",
    "#use tab complete to look at options\n",
    " ex_img.\n",
    "check out source url\n",
    "ex_img.attrs['src']\n",
    "\n",
    "\n",
    "#download image locally\n",
    "import shutil\n",
    "url_base = \"http://books.toscrape.com/\"\n",
    "url_ext = ex_img.attrs['src']\n",
    "full_url = url_base + url_ext\n",
    "r = requests.get(full_url, stream=True)\n",
    "if r.status_code == 200:\n",
    "    with open(\"images/book1.jpg\", 'wb') as f:\n",
    "        r.raw.decode_content = True\n",
    "        shutil.copyfileobj(r.raw, f)\n",
    "        \n",
    "#Bash to see if its there\n",
    "ls images/\n",
    "\n",
    "#preview it\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('images/book1.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "#this using PIL doesn't work - PIL doesn't seem to integrate with learn-env\n",
    "#can preview using pandas\n",
    "import pandas as pd\n",
    "from IPython.display import Image, HTML\n",
    "row1 = [ex_img.attrs['alt'], '<img src=\"images/book1.jpg\"/>']\n",
    "df = pd.DataFrame(row1).transpose()\n",
    "df.columns = ['title', 'cover']\n",
    "HTML(df.to_html(escape=False))\n",
    "\n",
    "#\"all together now\"\n",
    "data = []\n",
    "for n, img in enumerate(images):\n",
    "    url_base = \"http://books.toscrape.com/\"\n",
    "    url_ext = img.attrs['src']\n",
    "    full_url = url_base + url_ext\n",
    "    r = requests.get(full_url, stream=True)\n",
    "    path = \"images/book{}.jpg\".format(n+1)\n",
    "    title = img.attrs['alt']\n",
    "    if r.status_code == 200:\n",
    "        with open(path, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "        row = [title, '<img src=\"{}\"/>'.format(path)]\n",
    "        data.append(row)\n",
    "df = pd.DataFrame(data)\n",
    "print('Number of rows: ', len(df))\n",
    "df.columns = ['title', 'cover']\n",
    "HTML(df.to_html(escape=False))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping lab 2 - concerts\n",
    "#so far, this lab seems not workable because of cloudflare protections against scraping.  But here's the solution code\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "#Exploration; designing/testing function parts\n",
    "response = requests.get(\"https://www.residentadvisor.net/events/us/newyork\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "event_listings = soup.find('div', id=\"event-listing\")\n",
    "entries = event_listings.findAll('li')\n",
    "print(len(entries), entries[0])\n",
    "#Successive exploration in function development\n",
    "rows = []\n",
    "for entry in entries:\n",
    "    #Is it a date? If so, set current date.\n",
    "    date = entry.find('p', class_=\"eventDate date\")\n",
    "    event = entry.find('h1', class_=\"event-title\")\n",
    "    if event:\n",
    "        details = event.text.split(' at ')\n",
    "        event_name = details[0].strip()\n",
    "        venue = details[1].strip()\n",
    "        try:\n",
    "            n_attendees = int(re.match(\"(\\d*)\", entry.find('p', class_=\"attending\").text)[0])\n",
    "        except:\n",
    "            n_attendees = np.nan\n",
    "        rows.append([event_name, venue, cur_date, n_attendees])\n",
    "    elif date:\n",
    "        cur_date = date.text\n",
    "    else:\n",
    "        continue\n",
    "df = pd.DataFrame(rows)\n",
    "df.head()\n",
    "\n",
    "#Final function\n",
    "def scrape_events(events_page_url):\n",
    "    #Your code here\n",
    "    response = requests.get(events_page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    entries = event_listings.findAll('li')\n",
    "    rows = []\n",
    "    for entry in entries:\n",
    "        #Is it a date? If so, set current date.\n",
    "        date = entry.find('p', class_=\"eventDate date\")\n",
    "        event = entry.find('h1', class_=\"event-title\")\n",
    "        if event:\n",
    "            details = event.text.split(' at ')\n",
    "            event_name = details[0].strip()\n",
    "            venue = details[1].strip()\n",
    "            try:\n",
    "                n_attendees = int(re.match(\"(\\d*)\", entry.find('p', class_=\"attending\").text)[0])\n",
    "            except:\n",
    "                n_attendees = np.nan\n",
    "            rows.append([event_name, venue, cur_date, n_attendees])\n",
    "        elif date:\n",
    "            cur_date = date.text\n",
    "        else:\n",
    "            continue\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.head()\n",
    "    df.columns = [\"Event_Name\", \"Venue\", \"Event_Date\", \"Number_of_Attendees\"]\n",
    "    return df\n",
    "#Write a Function to Retrieve the URL for the Next Page\n",
    "#Function development cell\n",
    "soup.find('a', attrs={'ga-event-action':\"Next \"}).attrs['href']\n",
    "def next_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    url_ext = soup.find('a', attrs={'ga-event-action':\"Next \"}).attrs['href']\n",
    "    next_page_url = \"https://www.residentadvisor.net\" + url_ext\n",
    "    #Your code here\n",
    "    return next_page_url\n",
    "\n",
    "#Scrape the Next 1000 Events for Your Area\n",
    "#Your code here\n",
    "dfs = []\n",
    "total_rows = 0\n",
    "cur_url = \"https://www.residentadvisor.net/events/us/newyork\"\n",
    "while total_rows <= 1000:\n",
    "    df = scrape_events(cur_url)\n",
    "    dfs.append(df)\n",
    "    total_rows += len(df)\n",
    "    cur_url = next_page(cur_url)\n",
    "    time.sleep(.2)\n",
    "df = pd.concat(dfs)\n",
    "df = df.iloc[:1000]\n",
    "print(len(df))\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3resource.com/sqlite/sqlite-dot-commands.php\n",
    "#useful sqlite page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project tricks - taking raw dfs into sql tables to join #I did actually put this in above in the sql section\n",
    "import sqlite3\n",
    "from pandas import DataFrame\n",
    "\n",
    "conn = sqlite3.connect('TestDB1.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('CREATE TABLE CARS (Brand text, Price number)')\n",
    "conn.commit()\n",
    "\n",
    "Cars = {'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4'],\n",
    "        'Price': [22000,25000,27000,35000]\n",
    "        }\n",
    "\n",
    "df = DataFrame(Cars, columns= ['Brand', 'Price'])\n",
    "df.to_sql('CARS', conn, if_exists='replace', index = False)\n",
    " \n",
    "c.execute('''  \n",
    "SELECT * FROM CARS\n",
    "          ''')\n",
    "\n",
    "for row in c.fetchall():\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project Hints from Jeff:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook') #and other options\n",
    "plt.style.available\n",
    "with plt.style.context('dark_background'):\n",
    "    plt.scatter([1,2,3],[4,5,6])\n",
    "    plt.xlabel('Xlabel')\n",
    "    plt.ylabel('Ylabel');\n",
    "    \n",
    "#pandas explode function\n",
    "#Docstrings for functions\n",
    "#dataframes have built in .copy function, don't use copy module"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############Permutations/combinations and Probability\n",
    "#combinations - order doesn't matter; permutations - order does\n",
    "In this section, we dug into a number of foundational concepts:\n",
    "\n",
    "Probability is \"how likely\" it is that an event will happen\n",
    "Sets in Python are unordered collections of unique elements\n",
    "A sample space is a collection of every single possible outcome in a trial\n",
    "The inclusion exclusion principle is a counting technique used to calculate the number of elements in a collection of sets with overlapping elements.\n",
    "Factorials provide the basis for calculating permutations\n",
    "The difference between permutations and combinations is that with combinations, order is not important\n",
    "The \"sum rule\" of probability states that \n",
    "Independent events don't affect each other - e.g. consecutive coin tosses\n",
    "Dependent events do affect each other - e.g. picking consecutive colored marbles from a bag\n",
    "The product rule is useful when the conditional probability is easy to compute, but the probability of intersections of events is not\n",
    "The chain rule (also called the general product rule) permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities.\n",
    "Bayes theorem describes the probability of an event based on prior knowledge of conditions that might be related to the event\n",
    "The law of total probability states that the probability for a sample space is the sum of the probabilities for partitions of that sample space\n",
    "In this section, we introduced the ideas of combinatorics and probability. In the next section, you'll use this knowledge and take it a step further by learning about statistical distributions and their applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-distributions-use-cases/master/images/dists.png\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prob Mass Funct\n",
    "\n",
    "# Count the frequency of values in a given dataset\n",
    "import collections\n",
    "x = [1,1,1,1,2,2,2,2,3,3,4,5,5]\n",
    "counter = collections.Counter(x)\n",
    "print(counter)\n",
    "\n",
    "print(len(x))\n",
    "\n",
    "# Convert frequency to probability - divide each frequency value by total number of values\n",
    "pmf = []\n",
    "for key,val in counter.items():\n",
    "    pmf.append(round(val/len(x), 2))\n",
    "    \n",
    "print(counter.keys(), pmf)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.array(pmf).sum()\n",
    "\n",
    "####\n",
    "def p(x_i):\n",
    "    frequency = counter[x_i]\n",
    "    total_number = len(x)\n",
    "    return frequency / total_number\n",
    "\n",
    "print(\"p(1) =\", p(1))\n",
    "print(\"p(3) =\", p(3))\n",
    "\n",
    "###\n",
    "#normalized bars\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "outcomes = counter.keys()\n",
    "\n",
    "plt.bar(outcomes, [p(x_i) for x_i in outcomes]);\n",
    "plt.title(\"A Probability Mass Function\")\n",
    "plt.xlabel(\"Outcomes\")\n",
    "plt.ylabel(\"Probabilities of Outcomes\");\n",
    "\n",
    "###\n",
    "#or as a histogram\n",
    "plt.hist(x);\n",
    "plt.title(\"Histogram of Outcomes\")\n",
    "plt.xlabel(\"Bins of Outcomes\")\n",
    "plt.ylabel(\"Frequencies of Outcomes\");\n",
    "\n",
    "###\n",
    "#customizing vis\n",
    "xtick_locations = range(1,6)\n",
    "bins = np.arange(6)+0.5\n",
    "plt.hist(x, bins=bins,  rwidth=0.25, density=True)\n",
    "plt.xticks(ticks=xtick_locations)\n",
    "plt.xlabel('Bins of Outcomes')\n",
    "plt.ylabel('Probabilities of Bins of Outcomes')\n",
    "plt.title(\"Adjusted Histogram with `density=True`\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas series fun - \n",
    "#start with a series, like sizes. make a function (e.g. p_perceived) then use .apply \n",
    "#on it to make a new series\n",
    "sum(sizes.apply(p_perceived) * sizes)\n",
    "\n",
    "\n",
    "#\n",
    "pmf_df.plot.bar(x='Class Size')\n",
    "\n",
    "\n",
    "###neat thing to plot two bar plots atop each other with transparency:\n",
    "# Setting up shared axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Your code here\n",
    "pmf_df.plot.bar(ax = ax, x='Class Size', y = 'Overall Probability', color = 'tab:red', alpha = 0.5)\n",
    "pmf_df.plot.bar(ax = ax, x='Class Size', y = 'Perceived Probability', color = 'tab:blue', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric methods use parameters like mean and standard deviation of given data and attempt to work out the shape of the distribution that the data belongs to. These may implement maximum likelihood methods to fit a distribution to the given data. You'll learn more about this later.\n",
    "\n",
    "Kernel density estimation or KDE is a common non-parametric estimation technique to plot a curve (the kernel) at every individual data point. These curves are then added to plot a smooth density estimation. The kernel most often used is a Gaussian (which produces a bell curve at each data point). Other kernels can be used in special cases when the underlying distribution is not normal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('weight-height.csv')\n",
    "\n",
    "print(data.head())\n",
    "data.describe()\n",
    "\n",
    "##\n",
    "welch\n",
    "# Create two vertical subplots sharing 15% and 85% of plot space\n",
    "# sharex allows sharing of axes i.e. building multiple plots on same axes\n",
    "fig, (ax, ax2) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)}, figsize = (10,8) )\n",
    "\n",
    "sns.distplot(data.Height, \n",
    "             hist=True, hist_kws={\n",
    "                                  \"linewidth\": 2,\n",
    "                                  \"edgecolor\" :'red',\n",
    "                                  \"alpha\": 0.4, \n",
    "                                  \"color\":  \"w\",\n",
    "                                  \"label\": \"Histogram\",\n",
    "                                  },\n",
    "             kde=True, kde_kws = {'linewidth': 3,\n",
    "                                  'color': \"blue\",\n",
    "                                  \"alpha\": 0.7,\n",
    "                                  'label':'Kernel Density Estimation Plot'\n",
    "                                 },\n",
    "             fit= stats.norm, fit_kws = {'color' : 'green',\n",
    "                                         'label' : 'parametric fit',\n",
    "                                         \"alpha\": 0.7,\n",
    "                                          'linewidth':3},\n",
    "             ax=ax2)\n",
    "ax2.set_title('Density Estimations')\n",
    "\n",
    "sns.boxplot(x=data.Height, ax = ax,color = 'red')\n",
    "ax.set_title('Box and Whiskers Plot')\n",
    "ax2.set(ylim=(0, .08))\n",
    "plt.ylim(0,0.11)\n",
    "plt.legend();\n",
    "##\n",
    "#interpolation\n",
    "import numpy as np\n",
    "n, bins = np.histogram(data.Height, 20, density=1)\n",
    "n , bins\n",
    "\n",
    "# Initialize numpy arrays according to number of bins with zeros to store interpolated values\n",
    "pdfx = np.zeros(n.size)\n",
    "pdfy = np.zeros(n.size)\n",
    "\n",
    "# Interpolate through histogram bins \n",
    "# identify middle point between two neighbouring bins, in terms of x and y coords\n",
    "for k in range(n.size):\n",
    "    pdfx[k] = 0.5*(bins[k]+bins[k+1])\n",
    "    pdfy[k] = n[k]\n",
    "\n",
    "# plot the calculated curve\n",
    "plt.plot(pdfx, pdfy);\n",
    "###\n",
    "plt.figure(figsize=(7,5))\n",
    "data.Height.plot.hist(bins = 20, density=True, label = 'Normalized histogram', alpha = 0.7)\n",
    "# plot the calculated curve\n",
    "plt.plot(pdfx, pdfy, label = 'Density function')\n",
    "plt.ylabel ('Probabilities')\n",
    "plt.legend()\n",
    "plt.title ('PDF for height data')\n",
    "plt.show()\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CDFs (and PMFs) can be calculated using built-in NumPy and matplotlib methods. So we don't have create custom functions to calculate these. We can draw a histogram styled CDF as shown below using the following steps\n",
    "\n",
    "# You would need to perform these steps\n",
    "\n",
    "# Use np.histogram() to automatically calculate the histogram with probabilities. Here is numpy histogram documentation to help you dig deeper.\n",
    "\n",
    "# Use plt.scatter() method with np.cumsum() to calculate and plot cumulative probabilities (just like we did above).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n, bins = np.histogram(dice_lst, bins=[1,2,3,4,5,6,7], density=True)\n",
    "#display('n:',n, 'bins',bins)\n",
    "plt.scatter(dice_lst, np.cumsum(n))\n",
    "#plt.scatter(s=\n",
    "#display(np.cumsum(n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli and binomial dist\n",
    "A general rule for the Bernoulli distribution is that:  𝐸(𝑋)=𝑝  and  𝜎2=𝑝∗(1−𝑝) .\n",
    "\n",
    "Note how the Bernoulli distribution describes a single coin flip, a single penalty shot, etc. What if we repeat this process multiple times and are interested in the probability of obtaining a certain number of 1s/successes/tails? This process is described by the binomial distribution.\n",
    "\n",
    "#use numpy to generate binomial/bernoulli trials\n",
    "import numpy as np\n",
    "np.random.seed(123) # set a seed to get the same results\n",
    "np.random.binomial(100, 0.8)\n",
    "#and again - for different result\n",
    "np.random.binomial(100, 0.8)\n",
    "\n",
    "#or a bunch of times\n",
    "iteration = []\n",
    "for loop in range(500):\n",
    "    iteration.append(np.random.binomial(100, 0.8))\n",
    "    np_it = np.array(iteration)\n",
    "    \n",
    "## a different experiment - success on 3 penalty shots\n",
    "n = 10000\n",
    "iteration = []\n",
    "for loop in range(n):\n",
    "    iteration.append(np.random.binomial(3, 0.8))\n",
    "    np_it = np.array(iteration)\n",
    "#unique gets counts of the unique values\n",
    "values, counts = np.unique(np_it, return_counts=True)\n",
    "print(values)\n",
    "print(counts)\n",
    "\n",
    "#and visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(values, counts/10000, align='center', alpha=0.9)\n",
    "plt.xticks(values)\n",
    "plt.ylabel('Fraction')\n",
    "plt.title('Number of penalty goals')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal dist = gaussian\n",
    "he Probability Density Function\n",
    "The probability density function equation for the normal distribution is given by the following expression:\n",
    "\n",
    "𝑁(𝑥)= (prob dens function doesnt display nicely)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making normal dist vis\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "mu, sigma = 0.5, 0.1\n",
    "n = 1000\n",
    "s = np.random.normal(mu, sigma, n)\n",
    "sns.distplot(s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting normal dist to std normal dist.\n",
    "#z=(x-mu)/sigma\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "mean1, sd1 = 5, 3 # dist 1 \n",
    "mean2, sd2 = 10, 2 # dist 2 \n",
    "d1 = np.random.normal(mean1, sd1, 1000)\n",
    "d2 = np.random.normal(mean2, sd2, 1000)\n",
    "sns.distplot(d1);\n",
    "sns.distplot(d2);\n",
    "\n",
    "# Stardardizing and visualizing distributions\n",
    "\n",
    "sns.distplot([(x - d1.mean())/d1.std() for x in d1]);\n",
    "sns.distplot([(x - d2.mean())/d2.std() for x in d2]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bring in SciPy¶\n",
    "In the previous lesson, you have seen formulas to calculate skewness and kurtosis for your data. SciPy comes packaged with these functions and provides an easy way to calculate these two quantities, see scipy.stats.kurtosis and scipy.stats.skew. Check out the official SciPy documentation to dig deeper into this. Otherwise, simply pull up the documentation within the Jupyter notebook using shift+tab within the function call or pull up the full documentation with kurtosis? or skew?, once you have imported these methods from the SciPy package.\n",
    "\n",
    "You'll generate two datasets and measure/visualize and compare their skew and kurtosis in this lab.\n",
    "\n",
    "\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "x_random = np.random.normal(0,2,10000)\n",
    "plt.hist(x_random, bins='auto')\n",
    "display('skew',skew(x_random))\n",
    "display('kurtosis',kurtosis(x_random))\n",
    "\n",
    "\n",
    "#manually doing a normal dist\n",
    "x = np.linspace( -5, 5, 10000 )\n",
    "y = 1./(np.sqrt(2.*np.pi)) * np.exp( -.5*(x)**2  )  # normal distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlighting some specific distributions:\n",
    "Bernoulli Trials deal with a series of boolean events, which is a type of discrete distribution\n",
    "The normal distribution is the classic \"bell curve\" with 68% of the probability mass within 1 SD (standard deviation) of the mean, 95% within 2 SDs, and 99.7% within 3 SDs.\n",
    "The standard normal distribution is a standardized version of the normal distribution, where the mean is 0 and the SD is 1\n",
    "\n",
    "The z-score can be used to understand how extreme a certain result is\n",
    "Skewness and kurtosis (number of outliers)can be used to measure how different a given distribution is from a normal distribution\n",
    "\n",
    "The uniform distribution, which represents processes where each outcome is equally likely, like rolling dice\n",
    "The Poisson distribution, which can be used to represent the likelihood of a given number of successes over a given time period\n",
    "The exponential distribution, which can be used to represent the amount of time it may take before an event occurs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/learn-co-curriculum/dsc-central-limit-theorem/master/images/new_CentralLimitTheorem.png\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Any of the read_ methods in pandas will store 1-dimensional in a Series instead of a DataFrame if passed the optimal parameter squeeze=True.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "np.random.seed(0) #set a random seed for reproducibility\n",
    "\n",
    "#checking for normality\n",
    "data = pd.read_csv('non_normal_dataset.csv', squeeze=True)\n",
    "data.head()\n",
    "sns.distplot(data)\n",
    "\n",
    "st.normaltest(data)\n",
    "\n",
    "#sampling with replacement\n",
    "def get_sample(data, n):\n",
    "    sample=[]\n",
    "    l = len(data)\n",
    "    for i in range(n):\n",
    "        r = np.random.randint(l)\n",
    "        sample.append(data[r])\n",
    "    return sample\n",
    "    \n",
    "\n",
    "test_sample = get_sample(data, 30)\n",
    "print(test_sample[:5]) \n",
    "\n",
    "#generating sample mean\n",
    "def get_sample_mean(sample):\n",
    "    return np.mean(sample)\n",
    "\n",
    "test_sample2 = get_sample(data, 30)\n",
    "test_sample2_mean = get_sample_mean(test_sample2)\n",
    "print(test_sample2_mean) \n",
    "\n",
    "#create sample dist of sample means\n",
    "def create_sample_distribution(data, dist_size=100, n=30):\n",
    "    sample_dist = []\n",
    "    for i in range(dist_size):\n",
    "        s = get_sample(data, n)\n",
    "        sample_dist.append(get_sample_mean(s))\n",
    "    return sample_dist\n",
    "        \n",
    "\n",
    "test_sample_dist = create_sample_distribution(data)\n",
    "print(test_sample_dist[:5]) \n",
    "\n",
    "#creating different samples and visualizing\n",
    "sample_dist = create_sample_distribution(data, 10, 3)\n",
    "sns.distplot(sample_dist)\n",
    "\n",
    "sample_dist = create_sample_distribution(data, 100, 30)\n",
    "sns.distplot(sample_dist)\n",
    "\n",
    "#normal curving\n",
    "mu=0\n",
    "sigma=1\n",
    "x=np.linspace(\n",
    "    stats.norm(mu,sigma).ppf(0.01),\n",
    "    stats.norm(mu,sigma).ppf(0.99),\n",
    "    100\n",
    ")\n",
    "y=stats.norm(mu,sigma).pdf(x)\n",
    "ax.plot(x,y,'r-')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Calculate the frequency of each mean value\n",
    "freq, c = np.unique(means, return_counts=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Standard Error (SE) is very similar to the standard deviation. Both are measures of spread. The higher the number, the more spread out your data is. To put it simply, the two terms are essentially equal — but there is one important difference. While the standard error uses statistics (sample data), standard deviations use parameters (population data). We achieve this by dividing the standard deviation by the square root of the sample size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from conf interval video\n",
    "pop = list(stats.norm.rvs(size=1000, random_state=42))\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax = sns.kdeplot(pop, ax=ax, label='Label')\n",
    "plt.axvline(pop_mean, ls='-.', c='r', label='$\\mu$')\n",
    "\n",
    "#zscore for certain percentages\n",
    "z=stats.norm.ppf(0.95) #z score of 95% values\n",
    "\n",
    "\n",
    "#confidence intervals:\n",
    "sample mean +/- standard error * z score (or t-score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "student's t-dist function - similar to normal but with shorter peak and thicker tails; it has a pdfunction containing the Gamma function.\n",
    "\n",
    "We can use the normal dist when either:\n",
    "population std. dev is known or\n",
    "sample size is greater than 30\n",
    "\n",
    "if you don't know either one, then use the t-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###from Confidence Intervals lab\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Plot styles\n",
    "plt.style.use('fivethirtyeight')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "#The Poisson distribution is the discrete probability distribution of the number of events occurring in a given time period, given the average number of times the event occurs over that time period. We shall use a Poisson distribution to construct a bimodal distribution.\n",
    "np.random.seed(15)\n",
    "population_ages1 = stats.poisson.rvs(loc=18, mu=35, size=150000)\n",
    "population_ages2 = stats.poisson.rvs(loc=18, mu=10, size=100000)\n",
    "population_ages = np.concatenate((population_ages1, population_ages2))\n",
    "\n",
    "pop_ages = pd.DataFrame(population_ages)\n",
    "np.random.seed(15)\n",
    "\n",
    "# Take random sample of size 500\n",
    "sample_size = 500\n",
    "sample = pop_ages.sample(500)\n",
    "\n",
    "np.random.seed(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student t-distribution stuff\n",
    "sample_chol_levels = np.random.normal(loc=54, scale=17, size=1000)\n",
    "# or just given some sample values\n",
    "x_bar = np.mean(sample_chol_levels)\n",
    "s = np.std(sample_chol_levels, ddof = 1)\n",
    "print('Sample mean:', x_bar)\n",
    "print('Sample standard deviation:', s)\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "stats.t.interval(alpha = 0.95,                              # Confidence level\n",
    "                 df= len(sample_chol_levels)-1,             # Degrees of freedom\n",
    "                 loc = x_bar,                               # Sample mean\n",
    "                 scale = s)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Critical Value\n",
    "For the sake of example, let's say that we are calculating the confidence interval solely based on information in the sample. In other words, unlike when we calculated confidence intervals using the z-distribution, we do not have the population standard deviation.\n",
    "\n",
    "We can calculate a confidence interval without the population standard deviation using the t-distribution, represented by the stats.t.ppf(q, df) function. This function takes in a value for the confidence level required (q) with \"degrees of freedom\" (df).\n",
    "\n",
    "Hints:\n",
    "\n",
    "In this case, we want 95% confidence level for a two-tail test. This means the confidence level (q) for this function needs to be  (1−0.95)/2 , i.e.  0.975 \n",
    "In this case, the number of degrees of freedom (df) is equal to the sample size minus 1, or df = sample_size - 1.\n",
    "Calculate the t-critical value for a 95% confidence level based on the sample taken above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a0b61401311e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Get the t-critical value by using 95% confidence level and degree of freedom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mt_critical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.975\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;31m#stats.t.interval(alpha = 0.95,                              # Confidence level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m              \u001b[1;31m#    df= len(sample)-1,             # Degrees of freedom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# Get the t-critical value by using 95% confidence level and degree of freedom\n",
    "t_critical = stats.t.ppf(q=.975, df=sample_size-1)\n",
    "            #stats.t.interval(alpha = 0.95,                              # Confidence level\n",
    "             #    df= len(sample)-1,             # Degrees of freedom\n",
    "             #    loc = sample_mean,                               # Sample mean\n",
    "             #    scale = np.std(sample, ddof = 1))   \n",
    "\n",
    "# Check the t-critical value\n",
    "print(\"t-critical value:\")\n",
    "print(t_critical)     \n",
    "\n",
    "# t-critical value:\n",
    "# 2.0638985616280205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Get the sample standard deviation\n",
    "sample_stdev = sample.std(ddof=1)\n",
    "\n",
    "\n",
    "# Calculate the standard error using the formula described above\n",
    "se = sample_stdev/np.sqrt(sample_size)\n",
    "\n",
    "# Check the SE\n",
    "print(\"Sample Standard Error of the Mean:\")\n",
    "print(se)\n",
    "\n",
    "# Sample Standard Error of the Mean:\n",
    "# 0.697197803193802\n",
    "\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Calculate margin of error using t_critical and se\n",
    "margin_of_error = t_critical*se\n",
    "\n",
    "# Calculate the confidence interval using margin_of_error\n",
    "confidence_interval = (sample_mean-margin_of_error, sample_mean+margin_of_error)\n",
    "\n",
    "# Check the confidence interval\n",
    "print(\"Confidence Interval:\")\n",
    "print(confidence_interval)\n",
    "\n",
    "# Confidence Interval:\n",
    "# (18.431843086289952, 21.309734172653762)\n",
    "\n",
    "\n",
    "####OR USING BUILT IN\n",
    "\n",
    "# Run this cell without changes\n",
    "\n",
    "ci = stats.t.interval(\n",
    "    alpha=0.95,         # Confidence level\n",
    "    df=sample_size - 1, # Degrees of freedom\n",
    "    loc=sample_mean,    # Sample mean\n",
    "    scale=se            # Standard error\n",
    ")\n",
    "\n",
    "print(\"True Population Mean:\")\n",
    "print(population_mean)\n",
    "print(\"95% Confidence Interval of Mean Based on Sample:\")\n",
    "print(ci)\n",
    "\n",
    "# True Population Mean:\n",
    "# 21.00857750766395\n",
    "# 95% Confidence Interval of Mean Based on Sample:\n",
    "# (18.431843086289952, 21.309734172653762)\n",
    "\n",
    "\n",
    "\n",
    "###Now, to refactor and iterate for repeated samples\n",
    "# Replace None with appropriate code\n",
    "\n",
    "def conf_interval(sample):\n",
    "    '''\n",
    "    Input:  Sample data\n",
    "    Output: Confidence interval for the mean of the\n",
    "            population that the sample was drawn from\n",
    "    '''\n",
    "    \n",
    "    # Sample size\n",
    "    n = len(sample)\n",
    "    # Sample mean\n",
    "    x_hat = sample.mean()\n",
    "    \n",
    "    # Standard error of the mean\n",
    "    standard_error = sample.std()/np.sqrt(n)\n",
    "    \n",
    "    # Compute confidence interval with stats.t.interval\n",
    "    conf = stats.t.interval(\n",
    "            alpha=0.95,         # Confidence level\n",
    "            df=sample_size - 1, # Degrees of freedom\n",
    "            loc=x_hat,    # Sample mean\n",
    "            scale=standard_error            # Standard error\n",
    "        )\n",
    "    \n",
    "    return conf\n",
    "\n",
    "# Confirm that this produces the same interval as the previous code\n",
    "conf_interval(sample)\n",
    "\n",
    "# (18.431843086289952, 21.309734172653762)\n",
    "\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(12)\n",
    "\n",
    "# Select the sample size \n",
    "sample_size = 25\n",
    "\n",
    "# Initialize lists to store interval and mean values\n",
    "sample_means = []\n",
    "intervals = []\n",
    "\n",
    "# Run a for loop for sampling 20 times and calculate + store \n",
    "# confidence interval and sample mean values in lists initialized above\n",
    "\n",
    "for sample in range(20):\n",
    "    # Take a random sample of chosen size from population_ages\n",
    "    #population_ages.random_sample(sample_size)\n",
    "    s=np.random.choice(population_ages, sample_size)\n",
    "    \n",
    "    # Calculate sample mean and confidence_interval\n",
    "    sample_mean = s.mean()\n",
    "    ci = conf_interval(s)\n",
    "  \n",
    "    # Append sample means and conf intervals for each iteration\n",
    "    sample_means.append(sample_mean)\n",
    "    intervals.append(ci)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# Run this cell without changes\n",
    "\n",
    "# Set up figure\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "# Draw the means and confidence intervals for each sample\n",
    "ax.errorbar(\n",
    "    x=np.arange(1, 21, 1),\n",
    "    y=sample_means,\n",
    "    yerr=[(upper-lower)/2 for upper, lower in intervals],\n",
    "    fmt='o',\n",
    "    color=\"gray\",\n",
    "    markerfacecolor=\"blue\"\n",
    ")\n",
    "\n",
    "# Draw the population mean as a horizontal line \n",
    "ax.hlines(\n",
    "    xmin=0,\n",
    "    xmax=21,\n",
    "    y=population_ages.mean(), \n",
    "    linewidth=2.0,\n",
    "    color=\"red\"\n",
    ")\n",
    "\n",
    "# Label plot\n",
    "ax.set_xlabel(\"Samples\")\n",
    "ax.set_ylabel(\"Means\")\n",
    "\n",
    "# Customize legend appearance\n",
    "legend_elements = [\n",
    "    # Sample mean (blue circle with gray edge)\n",
    "    Line2D(\n",
    "        [0], # \"Dummy\" line being graphed\n",
    "        [0], # for use in the legend\n",
    "        marker=\"o\",\n",
    "        color=\"w\",\n",
    "        markerfacecolor=\"blue\",\n",
    "        markeredgecolor=\"gray\"\n",
    "    ),\n",
    "    # Confidence interval (gray vertical line)\n",
    "    Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        marker=\"|\",\n",
    "        markersize=15,\n",
    "        color=\"w\",\n",
    "        markeredgewidth=1.5,\n",
    "        markeredgecolor=\"gray\"\n",
    "    ),\n",
    "    # Population mean (red horizontal line)\n",
    "    Line2D([0],[0], color=\"red\")\n",
    "]\n",
    "\n",
    "ax.legend(\n",
    "    handles=legend_elements,\n",
    "    labels=[\"Sample Mean\", \"Sample Confidence Interval for Mean\", \"True Population Mean\"],\n",
    "    loc=\"lower left\", \n",
    "    fontsize=\"large\"\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intro to statistical significance\n",
    "import numpy as np\n",
    "\n",
    "sample = np.array([15.46097664, 15.5930238 , 19.55426936, 16.54231132, 18.40712804,\n",
    "                   13.33092129, 13.79141786, 12.44315636, 15.70385525, 12.48722755,\n",
    "                   11.81594655, 15.22694063, 18.25064431, 15.93900563, 17.38471543,\n",
    "                   15.02334988, 14.55826229, 16.64212199, 16.50657618, 13.44759329,\n",
    "                   13.05467437, 14.151049  , 13.55036322, 13.37386788, 10.25090132,\n",
    "                   16.45380807, 12.63016764, 11.90102614, 15.34426397, 15.2048003 ,\n",
    "                   11.60623705, 16.10720081, 16.42266283, 13.74686281, 14.51850311,\n",
    "                   15.59951107, 18.03269318, 14.35882143, 17.95626942, 14.06849303,\n",
    "                   14.4507767 , 17.27149508, 15.06747021, 13.82402614, 12.40651465,\n",
    "                   15.94104138, 16.8003216 , 18.45973001, 14.24757027, 14.06031845])\n",
    "sample_mean = sample.mean()\n",
    "sample_mean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots()\n",
    "ax = sns.kdeplot(sample, ax=ax, label='Sample PDF')\n",
    "ax.axvline(sample_mean, color=\"red\", label=r'$\\bar{x}$')\n",
    "ax.legend();\n",
    "\n",
    "from scipy import stats\n",
    "stats.t.interval(\n",
    "    alpha=0.9999999999999999,\n",
    "    df=len(sample)-1,\n",
    "    loc=sample_mean,\n",
    "    scale=stats.sem(sample)\n",
    ")\n",
    "\n",
    "stats.t.interval(\n",
    "    alpha=0.999,\n",
    "    df=len(sample)-1,\n",
    "    loc=sample_mean,\n",
    "    scale=stats.sem(sample)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental design:\n",
    "    https://github.com/ericthansen/dsc-experimental-design.git  \n",
    "    \n",
    "Other - p values intro\n",
    "    https://github.com/ericthansen/dsc-z-score-p-value\n",
    "    \n",
    "population/parameters, sample/statistics\n",
    "\n",
    "p-vals and null hyp\n",
    "https://github.com/ericthansen/dsc-p-values-and-null-hypothesis\n",
    "\n",
    "\"null hyp loves you\":\n",
    "https://byrslf.co/the-null-hypothesis-loves-you-and-wants-you-to-be-happy-3189413d8cd0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-table in python\n",
    "# Z-table in Python \n",
    "import scipy.stats as stats\n",
    "\n",
    "# Probabilities up to z-score of 1.5\n",
    "print(stats.norm.cdf(1.5))\n",
    "\n",
    "# Probabilities greater than z-score of 1.34\n",
    "print (1-stats.norm.cdf(1.34))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statsignif\n",
    "import scipy.stats as stats\n",
    "from math import sqrt\n",
    "x_bar = 103 # sample mean \n",
    "n = 40 # number of students\n",
    "sigma = 16 # sd of population\n",
    "mu = 100 # Population mean \n",
    "\n",
    "z = (x_bar - mu)/(sigma/sqrt(n))\n",
    "z\n",
    "\n",
    "#fancy z-plotting (statistical significance) \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.fill_between(x=np.arange(-4,1.19,0.01),\n",
    "                 y1= stats.norm.pdf(np.arange(-4,1.19,0.01)) ,\n",
    "                 facecolor='red',\n",
    "                 alpha=0.35,\n",
    "                 label= 'Area below z-statistic'\n",
    "                 )\n",
    "\n",
    "plt.fill_between(x=np.arange(1.19,4,0.01), \n",
    "                 y1= stats.norm.pdf(np.arange(1.19,4,0.01)) ,\n",
    "                 facecolor='blue',\n",
    "                 alpha=0.35, \n",
    "                 label= 'Area above z-statistic')\n",
    "plt.legend()\n",
    "plt.title ('z-statistic = 1.19');\n",
    "\n",
    "\n",
    "#comparing\n",
    "stats.norm.cdf(z)\n",
    "pval = 1 - stats.norm.cdf(z)\n",
    "pval #pval 0.117\n",
    "#but since pval is > 0.05, we can't reject null hyp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-value and effect size\n",
    "\n",
    "Effect size is used to quantify the size of the difference between two groups under observation. Effect sizes are easy to calculate, understand and apply to any measured outcome and are applicable to a multitude of study domains. It is highly valuable towards quantifying the effectiveness of a particular intervention, relative to some comparison. Measuring effect size allows scientists to go beyond the obvious and simplistic 'Does it work or not?' to the far more sophisticated, 'How well does it work in a range of contexts?'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9788e1b6d9ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m#but relative to which one?  Could do the average,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m#or the place where pdfs cross:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mthresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstd1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmean2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstd2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmean1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstd1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstd2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mmale_below_thresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmale_sample\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'std1' is not defined"
     ]
    }
   ],
   "source": [
    "#scipy imports\n",
    "#documentation: https://docs.scipy.org/doc/scipy/reference/index.html\n",
    "# Import necessary modules \n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "\n",
    "# Import SciPy stats and matplotlib for calculating and visualising effect size\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# seed the random number generator so you get the same results\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "##creating a random variable object\n",
    "#Mean height and sd for males\n",
    "male_mean = 178\n",
    "male_sd = 7.7\n",
    "\n",
    "# Generate a normal distribution for male heights \n",
    "male_height = scipy.stats.norm(male_mean, male_sd)\n",
    "\n",
    "\n",
    "\n",
    "#function to eval a gaussian pdf \n",
    "def evaluate_PDF(rv, x=4):\n",
    "    '''Input: a random variable object, standard deviation\n",
    "    output : x and y values for the normal distribution\n",
    "    '''\n",
    "    \n",
    "    # Identify the mean and standard deviation of random variable \n",
    "    mean = rv.mean()\n",
    "    std = rv.std()\n",
    "\n",
    "    # Use numpy to calculate evenly spaced numbers over the specified interval (4 sd) and generate 100 samples.\n",
    "    xs = np.linspace(mean - x*std, mean + x*std, 100)\n",
    "    \n",
    "    # Calculate the peak of normal distribution i.e. probability density. \n",
    "    ys = rv.pdf(xs)\n",
    "\n",
    "    return xs, ys # Return calculated values\n",
    "\n",
    "\n",
    "#Use scipy stats rvs method to generate a random sample from a pop dist\n",
    "male_sample = male_height.rvs(1000)\n",
    "#the result is a numpy array\n",
    "\n",
    "#one way to express difference between 2 means is by \n",
    "#percentage difference of the means.\n",
    "#but relative to which one?  Could do the average,\n",
    "#or the place where pdfs cross:\n",
    "thresh = (std1 * mean2 + std2 * mean1) / (std1 + std2)\n",
    "\n",
    "male_below_thresh = sum(male_sample < thresh)\n",
    "male_below_thresh\n",
    "female_above_thresh = sum(female_sample > thresh)\n",
    "female_above_thresh\n",
    "#now plot the overlap:\n",
    "# Male height\n",
    "m_xs, male_ys = evaluate_PDF(male_height)\n",
    "plt.plot(m_xs, male_ys, label='male', linewidth=4, color='#beaed4') \n",
    "\n",
    "#Female height \n",
    "f_xs, female_ys = evaluate_PDF(female_height)\n",
    "plt.plot(f_xs, female_ys, label='female', linewidth=4, color='#fdc086')\n",
    "plt.vlines(thresh,ymin=0,ymax=0.06)\n",
    "plt.fill_betweenx(male_ys,x1 = m_xs,x2=thresh, where = m_xs < thresh,color='b')\n",
    "plt.fill_betweenx(female_ys,x1=f_xs,x2=thresh, where = f_xs > thresh,color='b')\n",
    "plt.xlabel('height (cm)')\n",
    "# Calculate the overlap \n",
    "overlap = male_below_thresh / len(male_sample) + female_above_thresh / len(female_sample)\n",
    "overlap\n",
    "#Or in more practical terms, you might report the fraction of people who would be misclassified if you tried to use height to guess sex:\n",
    "misclassification_rate = overlap / 2\n",
    "misclassification_rate\n",
    "\n",
    "#short way to find: if chose m and f samples at random, what is prob that males taller than females\n",
    "# Python zip() The zip() function take iterables (can be zero or more), \n",
    "# makes iterator that aggregates elements based on the iterables passed, \n",
    "# and returns an iterator of tuples.\n",
    "\n",
    "sum(x > y for x, y in zip(male_sample, female_sample)) / len(male_sample)\n",
    "\n",
    "\n",
    "#Cohen's d\n",
    "Cohen’s d is one of the most common ways to measure effect size. As an effect size, Cohen's d is typically used to represent the magnitude of differences between two (or more) groups on a given variable, with larger values representing a greater differentiation between the two groups on that variable.\n",
    "\n",
    "The basic formula to calculate Cohen’s  𝑑  is:\n",
    "\n",
    "𝑑  = effect size (difference of means) / pooled standard deviation\n",
    "\n",
    "The denominator is the standardiser, and it is important to select the most appropriate one for a given dataset. The pooled standard deviation is the average spread of all data points around their group mean (not the overall mean).\n",
    "#there is some discussion about different standardizers\n",
    "def Cohen_d(group1, group2):\n",
    "\n",
    "    # Compute Cohen's d.\n",
    "\n",
    "    # group1: Series or NumPy array\n",
    "    # group2: Series or NumPy array\n",
    "\n",
    "    # returns a floating point number \n",
    "\n",
    "    diff = group1.mean() - group2.mean()\n",
    "\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1 = group1.var()\n",
    "    var2 = group2.var()\n",
    "\n",
    "    # Calculate the pooled threshold as shown earlier\n",
    "    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "    \n",
    "    # Calculate Cohen's d statistic\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    \n",
    "    return d\n",
    "\n",
    "#Interpreting d: 0.2 small effect, .5 medium effect, .8 large\n",
    "\n",
    "\n",
    "#Plotting pdfs\n",
    "def plot_pdfs(cohen_d=2):\n",
    "    \"\"\"Plot PDFs for distributions that differ by some number of stds.\n",
    "    \n",
    "    cohen_d: number of standard deviations between the means\n",
    "    \"\"\"\n",
    "    group1 = scipy.stats.norm(0, 1)\n",
    "    group2 = scipy.stats.norm(cohen_d, 1)\n",
    "    xs, ys = evaluate_PDF(group1)\n",
    "    plt.fill_between(xs, ys, label='Group1', color='#ff2289', alpha=0.7)\n",
    "\n",
    "    xs, ys = evaluate_PDF(group2)\n",
    "    plt.fill_between(xs, ys, label='Group2', color='#376cb0', alpha=0.7)\n",
    "    \n",
    "    o, s = overlap_superiority(group1, group2)\n",
    "    print('overlap', o)\n",
    "    print('superiority', s)\n",
    "    \n",
    "https://github.com/ericthansen/dsc-t-tests    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ericthansen/dsc-t-tests\n",
    "\n",
    "Two Sample t-tests\n",
    "The two-sample t-test is used to determine if two population means are equal. The main types of two-sampled t-tests are paired and independent tests. Paired tests are useful for determining how different a sample is affected by a certain treatment. In other words, the individual items/people in the sample will remain the same and researchers are comparing how they change after treatment. Here is an example of a scenario where a two-sample paired t-test could be applied:\n",
    "\n",
    "The US Olympic weightlifting team is trying out a new workout technique to in an attempt to improve everyone's powerlifting abilities. Did the program have an effect at a 95% significance level?\n",
    "\n",
    "Because we are looking at how specific individuals were affected by a treatment, we would use the paired t-test.\n",
    "\n",
    "Independent two-sample t-tests are for when we are comparing two different, unrelated samples to one another. Unlike paired t-tests, we are not taking paired differences because there is no way to pair two unrelated samples! Here is an example of a scenario where a two-sample independent t-test could be applied:\n",
    "\n",
    "Agricultural scientists are trying to compare the difference in soybean yields in two different counties in Mississippi.\n",
    "\n",
    "\n",
    "Samples vs. Tails\n",
    "Note that we now have two different labels where \"one\" and \"two\" appear repeatedly. Let's make sure the difference is clear!\n",
    "\n",
    "Previously we learned about one-tail and two-tail tests. A one-tail test means that the alternative hypothesis contains something like  >  or  < , which means that we are only looking at the area under the curve on one side. Whereas a two-tail test means that the alternative hypothesis contains something like  ≠  (which could mean greater than or less than), which means that we are looking at the area under the curve in two places, one on each side.\n",
    "\n",
    "One-sample tests can be one-tail or two-tail tests, as can two-sample tests. Some quick examples:\n",
    "\n",
    "One-sample one-tail:  𝐻𝑎:𝜇<3 \n",
    "One-sample two-tail:  𝐻𝑎:𝜇≠3 \n",
    "Two-sample one-tail:  𝐻𝑎:𝜇1<𝜇2 \n",
    "Two-sample two-tail:  𝐻𝑎:𝜇1≠𝜇2\n",
    "\n",
    "Assumptions for Performing t-tests\n",
    "When performing various kinds of t-tests, you assume that the sample observations have numeric and continuous values. You also assume that the sample observations are independent from each other (that is, that you have a simple random sample) and that the samples have been drawn from normal distributions. You can visually inspect the distribution of your sample using a histogram, for example.\n",
    "\n",
    "In the case of unpaired two-sample t-tests, you also assume that the populations the samples have been drawn from have the same variance. For paired two-sample t-tests, you assume that the difference between the two sets of samples are normally distributed.\n",
    "\n",
    "Regardless of the type of t-test you are performing, there are 5 main steps to executing them:\n",
    "1) Set up null and alternative hypotheses\n",
    "\n",
    "2) Choose a significance level\n",
    "\n",
    "3) Calculate the test statistic (t-value)\n",
    "\n",
    "4) Determine the critical t-value (find the rejection region)\n",
    "\n",
    "5) Compare t-value with critical t-value to determine if we can reject the null hypothesis.\n",
    "\n",
    "Hypotheses are written in terms of population parameters like mu\n",
    "rather than sample stats like xbar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEy look, you can put latex in here too!\n",
    "### Step 3: Calculate the t-statistic\n",
    "\n",
    "Assuming that we are fulfilling the three requirements for a t-test mentioned above (i.e. normality, independence, and randomness), we are ready to calculate our t statistic using the formula for one-sample t-test given as:\n",
    "\n",
    "# $$t = \\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}}$$\n",
    " \n",
    "(The *t-statistic* is also known as the *t-value*.)\n",
    "\n",
    "Using the formula given above, calculate the t-statistic in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From lab\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats \n",
    "import math\n",
    "\n",
    "# For visualizing distributions - optional \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def one_sample_ttest(sample, popmean, alpha):\n",
    "    '''print out stats for sample and identify if significant, also visualize'''\n",
    "    \n",
    "\n",
    "    # Visualize sample distribution for normality \n",
    "    sns.set(color_codes=True)\n",
    "    sns.histplot(sample, kde=True, bins=5);\n",
    "    \n",
    "    # Population mean \n",
    "    mu = popmean#not sure what else supposed to do here\n",
    "\n",
    "    # Sample mean (x̄) using NumPy mean()\n",
    "    x_bar = sample.mean()\n",
    "\n",
    "    # Sample Standard Deviation (sigma) using Numpy\n",
    "    sigma = np.std(sample, ddof=1)\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    n = len(sample)\n",
    "    dof = n-1\n",
    "    \n",
    "    # Calculate the critical t-value\n",
    "    t_crit = stats.t.ppf(1 - alpha, df=dof)\n",
    "    print('Critical t-value: {}'.format(t_crit))\n",
    "    \n",
    "    # Calculate the t-value and p-value      \n",
    "    t = (x_bar -  mu)/(sigma/np.sqrt(n))\n",
    "    print('T-value: {}'.format(t))\n",
    "    \n",
    "    p= stats.t.sf(t, df=dof)\n",
    "    print('P-value: {}'.format(p))\n",
    "    ###alternately,  and to check\n",
    "    results = stats.ttest_1samp(\n",
    "    a=sample,   # the entire array-like sample\n",
    "    popmean=mu # the mean you are testing the sample against\n",
    "    )\n",
    "    print(\"Checking t and p value: t:{}, p:{}, results:{} & {}\".format(t, p, results.statistic, results.pvalue/2))\n",
    "    \n",
    "    # return results\n",
    "    if t>t_crit:\n",
    "        results='Significant'\n",
    "    else:\n",
    "        results=\"Not Significant\"\n",
    "    return results\n",
    "\n",
    "##\n",
    "\n",
    "sample = np.array([84.0, 92.4, 74.3, 79.4, 86.7, 75.3, 90.9, 86.1, 81.0, 85.1, \n",
    "      78.7, 73.5, 86.9, 87.4, 82.7, 81.9, 69.9, 77.2, 79.3, 83.3])\n",
    "popmean = 65\n",
    "alpha = 0.05\n",
    "results=one_sample_ttest(sample, popmean, alpha)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating pooled sample variance\n",
    "\n",
    "def sample_variance(sample):\n",
    "    n=len(sample)\n",
    "    x_bar=sample.mean()\n",
    "    num=0\n",
    "    for s in sample:\n",
    "        num += (s-x_bar)**2\n",
    "    den=n-1\n",
    "    \n",
    "    return num/den\n",
    "\n",
    "def pooled_variance(sample1, sample2):\n",
    "    n1=len(sample1)\n",
    "    n2=len(sample2)\n",
    "    num=(n1-1)*sample_variance(sample1) + (n2-1)*sample_variance(sample2)\n",
    "    den=n1+n2-2\n",
    "    return num/den\n",
    "\n",
    "def twosample_tstatistic(expr, ctrl):\n",
    "    num=expr.mean()-ctrl.mean()\n",
    "    den=np.sqrt(pooled_variance(expr, ctrl)*(1/len(expr)+1/len(ctrl)))\n",
    "    return num/den\n",
    "\n",
    "t_stat = twosample_tstatistic(experimental, control)\n",
    "t_stat\n",
    "# -1.8915462966190268\n",
    "\n",
    "\n",
    "\n",
    "#All built in t-test:\n",
    "stats.ttest_ind(experimental, control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick p-valu check\n",
    "import scipy.stats as st\n",
    "\n",
    "n = 20 #Number of flips\n",
    "p = .75 #Simulating an unfair coin\n",
    "coin1 = np.random.binomial(n, p)\n",
    "sigma = np.sqrt(n*.5*(1-.5))#For a binomial\n",
    "st.norm.sf(np.abs(z))\n",
    "z = (coin1 - 10) / (sigma / np.sqrt(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c18557f0d09c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.75\u001b[0m \u001b[1;31m# Simulating an unfair coin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mn_heads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#more p-value stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "#checking pvalue change over time with more flips\n",
    "\n",
    "#How many times would you have to flip a 75% heads coin to determine it was false?\n",
    "p_vals = []\n",
    "#Iterate through various numbers of trials\n",
    "for n in range(1,50):\n",
    "    #Do multiple runs for that number of samples to compare\n",
    "    p_val = []\n",
    "    for i in range(200):\n",
    "        p = .75 # Simulating an unfair coin\n",
    "        n_heads = np.random.binomial(n, p)\n",
    "        mu = n / 2\n",
    "        sigma = np.sqrt(n*.5*(1-.5))\n",
    "        z  = (n_heads - mu) / (sigma / np.sqrt(n))\n",
    "        p_val.append(st.norm.sf(np.abs(z)))\n",
    "    p_vals.append(np.mean(p_val))\n",
    "plt.plot(list(range(1,50)), p_vals)\n",
    "plt.title('Average P-Values Associated with Hypothesis Testing of a .75 Unfair Coin by Number of Trials')\n",
    "plt.ylabel('Average P-Value of Simulations')\n",
    "plt.xlabel('Number of Coin Flips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t test with power, etc using 2 random vars\n",
    "\n",
    "import scipy.stats as stats\n",
    "def run_ttest_sim(p1, p2, std, nobs, alpha=0.05, n_sim=10**5):\n",
    "    \"\"\"p1 and p2 are the underlying means probabilities for 2 normal variables\n",
    "    Samples will be generated using these parameters.\"\"\"\n",
    "    # Calculate Normalized Effect Size\n",
    "    effect_size = np.abs(p1-p2)/std\n",
    "    \n",
    "    # Run a Simulation\n",
    "    # Initialize array to store results\n",
    "    p = (np.empty(n_sim))\n",
    "    p.fill(np.nan)\n",
    "\n",
    "    #  Run a for loop for range of values in n_sim\n",
    "    for s in range(n_sim):\n",
    "        control = np.random.normal(loc= p1, scale=std, size=nobs)\n",
    "        experimental = np.random.normal(loc= p2, scale=std, size=nobs)\n",
    "        t_test = stats.ttest_ind(control, experimental)\n",
    "        p[s] = t_test[1]\n",
    "    \n",
    "    num_null_rejects = np.sum(p < alpha)\n",
    "    power = num_null_rejects/n_sim\n",
    "    # Store results\n",
    "    stat_dict = {'alpha':alpha,\n",
    "                 'nobs':nobs,\n",
    "                 'effect_size':effect_size,\n",
    "                 'power': power}\n",
    "    return stat_dict\n",
    "\n",
    "run_ttest_sim(.5, .7, 1, 50)\n",
    "\n",
    "###power/alpha/samplesize/effectsize\n",
    "from statsmodels.stats.power import TTestIndPower, TTestPower\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid') # Nice background styling on plots\n",
    "power_analysis = TTestIndPower()\n",
    "power_analysis.plot_power(dep_var='nobs',\n",
    "                          nobs = np.array(range(5,1500)),\n",
    "                          effect_size=np.array([.05, .1, .2,.3,.4,.5]),\n",
    "                          alpha=0.05)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate power\n",
    "power_analysis.solve_power(effect_size=.2, nobs1=80, alpha=.05)\n",
    "# Calculate minimum effect size to satisfy desired alpha and power as well as respect sample size limitations\n",
    "power_analysis.solve_power(nobs1=25, alpha=.05, power=.8)\n",
    "#can solve for any of the 4\n",
    "\n",
    "\n",
    "#initialize empty numpy array\n",
    "p = (np.empty(n_sim))\n",
    "p.fill(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welch's t-test  \n",
    "https://github.com/ericthansen/dsc-welchs-ttest.git\n",
    "\n",
    "Welch's t score:\n",
    "![text](https://render.githubusercontent.com/render/math?math=%5CLarge%20t%20=%20%5Cfrac%7B%5Cbar%7BX_1%7D-%5Cbar%7BX_2%7D%7D%7B%5Csqrt%7B%5Cfrac%7Bs_1%5E2%7D%7BN_1%7D%20%2b%20%5Cfrac%7Bs_2%5E2%7D%7BN_2%7D%7D%7D%20=%20%5Cfrac%7B%5Cbar%7BX_1%7D-%5Cbar%7BX_2%7D%7D%7B%5Csqrt%7Bse_1%5E2%2bse_2%5E2%7D%7D)\n",
    "Calculate the degrees of freedom\n",
    "Once the t-score has been calculated for the experiment using the above formula, you then must calculate the degrees of freedom for the t-distribution. Under the two-sample Student's t-test, this is simply the total number of observations in the samples size minus two, but given that the sample sizes may vary using the Welch's t-test, the calculation is a bit more complex:\n",
    "![Alt text](https://render.githubusercontent.com/render/math?math=%5CLarge%20v%20%5Capprox%20%5Cfrac%7B%5Cleft(%20%5Cfrac%7Bs_1%5E2%7D%7BN_1%7D%20%2b%20%5Cfrac%7Bs_2%5E2%7D%7BN_2%7D%5Cright)%5E2%7D%7B%5Cfrac%7Bs_1%5E4%7D%7BN_1%5E2v_1%7D%20%2b%20%5Cfrac%7Bs_2%5E4%7D%7BN_2%5E2v_2%7D%7D \"a title\")\n",
    "\n",
    "Then calculate p-stat for a t dist with that tscore and those deg of freedom\n",
    "One good way is with  \n",
    "p = 1 - stats.t.cdf(t, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the welch t test lab\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(82)\n",
    "control = np.random.normal(loc=10, scale=1, size=8)\n",
    "treatment = np.random.normal(loc=10.5, scale=1.2, size=12)\n",
    "def welch_t(a, b):\n",
    "    \n",
    "    \"\"\" Calculate Welch's t-statistic for two samples. \"\"\"\n",
    "    num = abs(np.mean(control) - np.mean(treatment))\n",
    "    denom = np.sqrt((np.var(control))/len(control) + (np.var(treatment)/len(treatment)))\n",
    "    \n",
    "\n",
    "    \n",
    "    return num/denom# Return the t-score!\n",
    "\n",
    "welch_t(control, treatment)\n",
    "# 2.0997990691576858\n",
    "def welch_df(a, b):\n",
    "    \n",
    "    \"\"\" Calculate the effective degrees of freedom for two samples. \"\"\"\n",
    "    na = len(a)\n",
    "    nb = len(b)\n",
    "    vara = np.var(a)\n",
    "    varb = np.var(b)\n",
    "    nua = na-1\n",
    "    nub = nb-1\n",
    "    num = (vara/na + varb/nb)**2\n",
    "    den = (vara**2)/(na**2 * nua) + (varb**2)/(nb**2 * nub)\n",
    "    return num/den # Return the degrees of freedom\n",
    "\n",
    "welch_df(control, treatment)\n",
    "# 17.673079085111\n",
    "\n",
    "# Your code here; calculate t-score and the degrees of freedom for the two samples, a and b\n",
    "t = welch_t(control, treatment)\n",
    "df = welch_df(control, treatment)\n",
    "print(t, df)\n",
    "# 2.0997990691576858 17.673079085111\n",
    "\n",
    "# Your code here; calculate the p-value for the two samples defined above\n",
    "\n",
    "from scipy import stats\n",
    "p = 1 - stats.t.cdf(t, df)\n",
    "print(p)\n",
    "# 0.025191666225846454\n",
    "\n",
    "def p_value(a, b, two_sided=False):\n",
    "    # Your code here\n",
    "    from scipy import stats\n",
    "    t = welch_t(a, b)\n",
    "    df = welch_df(a, b)\n",
    "    p = 1 - stats.t.cdf(t, df)\n",
    "    if two_sided:\n",
    "        return p*2\n",
    "    else:\n",
    "        return p\n",
    "     # Return the p-value!\n",
    "p_value(treatment, control)\n",
    "# 0.025191666225846454\n",
    "\n",
    "p_value(treatment, control, two_sided=True)\n",
    "# 0.05038333245169291\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolmogorov/Smirnof Stat - representation of difference from normality (or difference between two distributions) based on comparing the max of the vertical distance between their CDFs\n",
    "\n",
    "For one sample vs a known distribution (eg. Normal aka 'norm')\n",
    "scipy.stats.kstest(rvs, cdf, args=(), N=20, alternative='two-sided', mode='approx')\n",
    "\n",
    "A two-sample K-S test is available in SciPy using following function:\n",
    "\n",
    "scipy.stats.ks_2samp(data1, data2)[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANOVA\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "df = pd.read_csv('IT_salaries.csv')\n",
    "df.head()\n",
    "\n",
    "\n",
    "#in this example:\n",
    "# S - the individuals salary\n",
    "# X - years of experience\n",
    "# E - education level (1-Bachelors, 2-Masters, 3-PHD)\n",
    "# M - management (0-no management, 1-yes management)\n",
    "#In order to generate the ANOVA table, you first fit a linear model and then generate the table from this object. Our formula will be written as:\n",
    "\n",
    "#Control_Column ~ C(factor_col1) + factor_col2 + C(factor_col3) + ... + X\n",
    "\n",
    "#We indicate categorical variables by wrapping them with C().\n",
    "formula = 'S ~ C(E) + C(M) + X'\n",
    "lm = ols(formula, df).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2)\n",
    "print(table)\n",
    "\n",
    "#anova video: \n",
    "https://learning.flatironschool.com/courses/1888/pages/video-anova?module_item_id=261345\n",
    "    \n",
    "check out seaborn swarmplot\n",
    "boxplot\n",
    "violinplot\n",
    "\n",
    "#also, if we have a bunch of different groups and want\n",
    "#to see if they significantly differ\n",
    "groups = {'1':data1, ...}\n",
    "f_stat, p = stats.f_oneway(*groups.values()) \n",
    "#this uses the Fstat (at the heart of ANOVA testing)\n",
    "\n",
    "#remember, it's an Omnibus test - it only tells us\n",
    "#if there's a sig diff, but doesn't tell us where\n",
    "#aha!  so the ols(ordinary least squares) code above breaks out by group - so it\n",
    "#is a little more comprehensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi-Squared!\n",
    "#chi sq dist can be used for lots of things\n",
    "\n",
    "#one of them is comparing if some observations differ\n",
    "#from expectations\n",
    "x=np.linspace(stats.chi2.ppf(0.00001, dof), stats.chi2.ppf(0.9999, dof), 500)\n",
    "f,ax=plt.subplots()\n",
    "ax.set_title('chi^2, dof deg of freedom')\n",
    "ax.plot(x, stats.chi2.pdf(x, dof, 'r-', lw=5))\n",
    "plt.tight_layout()\n",
    "\n",
    "#can get p score from\n",
    "#calculating the chisq_stat yourself, then\n",
    "p=1-stats.chi2.cdf(chisq_stat, df=dof)\n",
    "#or\n",
    "stats.chi2.sf(chisq)\n",
    "\n",
    "#or go straight to the built in:\n",
    "obs=[50,100,180,70]\n",
    "exp = [60,80,180,80]\n",
    "result = stats.chisquare(f_obs=obs, f_exp = exp)\n",
    "\n",
    "#the other thing is test for independence.\n",
    "#e.g. if you had pc/mac users who create profiles abcd..\n",
    "#ie. two categories, and each profile type we expect to be\n",
    "#independent of pc/mac, you can test if they are indep.\n",
    "\n",
    "stats.chi2.pdf()\n",
    "#Lots of good functions to use, but one for 2 category\n",
    "#e.g. 2 rows, 3 cols\n",
    "chi, p, dof, exp = stats.contingency.chi2_contingency(observations)\n",
    "#where observations are, e.g., [[category 1 yes, category 1 no], [cat 2 yes, cat 2 no]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AB Testing - i.e. what sample size do we need to observe\n",
    "#A given alpha/power/effectsize\n",
    "#developed for the lab\n",
    "\n",
    "# Calculate the required sample size\n",
    "from statsmodels.stats.power import TTestIndPower, TTestPower\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid') # Nice background styling on plots\n",
    "power_analysis = TTestIndPower()\n",
    "\n",
    "\n",
    "alpha=0.05\n",
    "power=0.8\n",
    "effect_size=0.01\n",
    "std=0.0475\n",
    "#Not sure how to make normalized effect size using Cohen's d since we don't have current population to make pooled variance\n",
    "#could potentially just use \"number of stdevs between the means\" idea.  If both pops have same std, can do...\n",
    "effect_size = effect_size/std\n",
    "\n",
    "res = power_analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
    "res\n",
    "\n",
    "#now do some plotting of different scenarios\n",
    "#Your code; plot power curves for the various alpha and effect size combinations\n",
    "alphas=[0.01, 0.05, 0.1]\n",
    "power=0.8\n",
    "effect_sizes=np.array([0.005, .01, .02, .03])/std\n",
    "\n",
    "# res = power_analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
    "# res\n",
    "for i in range(len(alphas)):\n",
    "    power_analysis.plot_power(dep_var='nobs',\n",
    "                              nobs = np.array(range(10,int(res*5),10)),\n",
    "                              effect_size=effect_sizes,\n",
    "                              alpha=alphas[i]\n",
    "                             )\n",
    "    plt.title('Power vs. Sample Size; Alpha: {}'.format(alphas[i]))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####flatiron_stats.py\n",
    "#flatiron_stats\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def welch_t(a, b):\n",
    "    \n",
    "    \"\"\" Calculate Welch's t statistic for two samples. \"\"\"\n",
    "\n",
    "    numerator = a.mean() - b.mean()\n",
    "    \n",
    "    # “ddof = Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, \n",
    "    #  where N represents the number of elements. By default ddof is zero.\n",
    "    \n",
    "    denominator = np.sqrt(a.var(ddof=1)/a.size + b.var(ddof=1)/b.size)\n",
    "    \n",
    "    return np.abs(numerator/denominator)\n",
    "\n",
    "def welch_df(a, b):\n",
    "    \n",
    "    \"\"\" Calculate the effective degrees of freedom for two samples. This function returns the degrees of freedom \"\"\"\n",
    "    \n",
    "    s1 = a.var(ddof=1) \n",
    "    s2 = b.var(ddof=1)\n",
    "    n1 = a.size\n",
    "    n2 = b.size\n",
    "    \n",
    "    numerator = (s1/n1 + s2/n2)**2\n",
    "    denominator = (s1/ n1)**2/(n1 - 1) + (s2/ n2)**2/(n2 - 1)\n",
    "    \n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "def p_value_welch_ttest(a, b, two_sided=False):\n",
    "    \"\"\"Calculates the p-value for Welch's t-test given two samples.\n",
    "    By default, the returned p-value is for a one-sided t-test. \n",
    "    Set the two-sided parameter to True if you wish to perform a two-sided t-test instead.\n",
    "    \"\"\"\n",
    "    t = welch_t(a, b)\n",
    "    df = welch_df(a, b)\n",
    "    \n",
    "    p = 1-stats.t.cdf(np.abs(t), df)\n",
    "    \n",
    "    if two_sided:\n",
    "        return 2*p\n",
    "    else:\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Resources\n",
    "Choosing Statistical Tests (Links to an external site.)\n",
    "https://stats.idre.ucla.edu/other/mult-pkg/whatstat/\n",
    "\n",
    "How to choose the right statistical test?\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3116565/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random musing:\n",
    "What about something like the reverse monty hall problem - i.e. a situation where you get more info but still would not want to switch.  idea - what if, instead of a car, it's a hungry tiger?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes Theorem!\n",
    "# Bayes vs frequentist.  difference in philosophy as well as  b's theorem\n",
    "def bayes(P_a, P_b, P_b_given_a):\n",
    "    # Your code here\n",
    "    P_a_given_b = P_b_given_a * P_a/P_b\n",
    "    return P_a_given_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usual imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max likelihood estimation\n",
    "establish equation for parameters, use derivative to maximize - can take monotonic funciton (e.g. log) of this to make taking the derivtive easier - extrema will still happen at same place.\n",
    "other resources:\n",
    "https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1\n",
    "\n",
    "https://www.statisticshowto.datasciencecentral.com/iid-statistics/\n",
    "\n",
    "https://opencurriculum.org/5512/monotonically-increasing-and-decreasing-functions-an-algebraic-approach/\n",
    "\n",
    "https://mathbitsnotebook.com/Algebra2/Exponential/EXLogFunctions.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maximum aposteriori \n",
    "https://github.com/ericthansen/dsc-map-multinomial-bayes.git  \n",
    "Maximum A Posteriori Estimation\n",
    "Maximum A Posteriori Estimation (MAP) is similar to Maximum Likelihood Estimation but extends this concept by allowing one to also account for prior beliefs regarding the distribution of the variable in question. Recall Bayes' theorem:\n",
    "\n",
    "\n",
    "\n",
    "The Bayesian interpretation of this formula is\n",
    "likelihood*prior/evidence\n",
    "\n",
    "\n",
    "\n",
    "With MAP, you then attempt to optimize a parameter  for the assumed distribution in order to maximize the posterior probability.\n",
    "\n",
    "Multinomial Bayes\n",
    "Multinomial Bayes also extends the notions within Bayes' theorem, allowing one to chain inferences. The primary assumption for this is assuming that your variables are independent of one another. Recall that if you assume two events A and B are independent of one another, then  . Similarly, if independence is assumed when extending Bayes theorem to a multivariate case, one can multiply the successive probability estimates. Mathematically, this can be summarized as:\n",
    "\n",
    "\n",
    "\n",
    "Summary\n",
    "This lesson briefly introduced the concept of Maximum A Posteriori Estimation and extending Bayes' theorem to multivariate cases. In later sections, you'll investigate these ideas in practice, working with practical examples and coding your own implementations to gain a full understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical learning theory\n",
    "nice representation of fitting a model\n",
    "https://learning.flatironschool.com/courses/1888/pages/statistical-learning-theory?module_item_id=261365  \n",
    "https://github.com/ericthansen/dsc-stat-learning-theory?organization=ericthansen&organization=ericthansen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "#m = sy/sx\n",
    "#or\n",
    "def calc_slope(xs,ys):\n",
    "    \n",
    "    return ((X.mean()*Y.mean() - (X*Y).mean())/((X.mean())**2 - (X**2).mean()))\n",
    "\n",
    "calc_slope(X,Y)\n",
    "\n",
    "\n",
    "###########from lab\n",
    "# import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Initialize arrays X and Y with given values\n",
    "# X = Independent Variable\n",
    "X = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n",
    "# Y = Dependent Variable\n",
    "Y = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(X, Y)\n",
    "\n",
    "# Write the function to calculate slope as: \n",
    "# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\n",
    "def calc_slope(xs,ys):\n",
    "    \n",
    "    return ((X.mean()*Y.mean() - (X*Y).mean())/((X.mean())**2 - (X**2).mean()))\n",
    "\n",
    "calc_slope(X,Y)\n",
    "\n",
    "# 0.5393518518518512\n",
    "\n",
    "# use the slope function with intercept formula to return calculate slope and intercept from data points\n",
    "\n",
    "def best_fit(xs,ys):\n",
    "    \n",
    "    m = calc_slope(xs, ys)\n",
    "    #y = mx + c => c = y-mx\n",
    "    c = ys.mean() - m * xs.mean()\n",
    "    return (m, c)\n",
    "\n",
    "# Uncomment below to test your function\n",
    "\n",
    "m, c = best_fit(X,Y)\n",
    "m, c\n",
    "\n",
    "# (0.5393518518518512, 6.379629629629633)\n",
    "\n",
    "def reg_line (m, c, xs):\n",
    "    \n",
    "    ys = np.zeros(len(xs), dtype=np.float64)\n",
    "    \n",
    "    for i in range(len(xs)):\n",
    "        ys[i] = m * xs[i] + c\n",
    "    return ys\n",
    "    \n",
    "\n",
    "# Uncomment below\n",
    "regression_line = reg_line(m,c,X)\n",
    "regression_line\n",
    "list(zip(X, regression_line))\n",
    "print(regression_line)\n",
    "print(X)\n",
    "\n",
    "# Plot data and regression line\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.scatter(X,Y)\n",
    "ax.plot(X, regression_line)\n",
    "\n",
    "x_new = 7\n",
    "y_predicted = m*x_new + c\n",
    "y_predicted\n",
    "\n",
    "# 10.155092592592592\n",
    "\n",
    "# Plot as above and show the predicted value\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.scatter(X,Y)\n",
    "ax.plot(X, regression_line)\n",
    "ax.scatter(x_new, y_predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R squared = coefficient of determination\n",
    "# r^2 = 1 - ss_res/ss_tot = ss_exp/ss_tot \n",
    "#ss_res = sum (y_i-yhat_i)^2\n",
    "#ss_tot = sum( y_i - ybar)^2\n",
    "#respectively, residual, explained, total\n",
    "\n",
    "###From Lab\n",
    "\n",
    "# Calculate sum of squared errors between regression and mean line \n",
    "import numpy as np\n",
    "\n",
    "def sq_err(y_real, y_predicted):\n",
    "    \"\"\"\n",
    "    input\n",
    "    y_real : true y values\n",
    "    y_predicted : regression line\n",
    "\n",
    "    \n",
    "    return\n",
    "    squared error between regression and true line (ss_tot)\n",
    "    \"\"\"\n",
    "    if len(y_real) != len(y_predicted):\n",
    "        return \"Error\"\n",
    "    else:\n",
    "        sq_e = 0\n",
    "        for r, p in zip(y_real, y_predicted):\n",
    "            sq_e += (r-p)**2\n",
    "    return sq_e\n",
    "\n",
    "\n",
    "# Calculate Y_mean , squared error for regression and mean line , and calculate r-squared\n",
    "\n",
    "def r_squared(y_real, y_predicted):\n",
    "    \"\"\"\n",
    "    input\n",
    "    y_real: real values\n",
    "    y_predicted: regression values\n",
    "    \n",
    "    return\n",
    "    r_squared value\n",
    "    \"\"\"\n",
    "    if len(y_real) != len(y_predicted):\n",
    "        return \"Error\"\n",
    "    else:\n",
    "        y_bar = y_real.mean()\n",
    "        y_mean = np.zeros(len(y_real))\n",
    "        y_mean += y_bar\n",
    "        \n",
    "        ssr = sq_err(y_real, y_predicted)\n",
    "        sst = sq_err(y_real, y_mean)\n",
    "        #print(sst, y_real.var()*len(y_real))\n",
    "        r_squared = 1-ssr/sst\n",
    "        return r_squared\n",
    "\n",
    "# Check the output with some example data\n",
    "Y = np.array([1, 3, 5, 7])\n",
    "Y_pred = np.array([4.1466666666666665, 2.386666666666667, 3.56, 5.906666666666666])\n",
    "\n",
    "r_squared(Y, Y_pred)\n",
    "print(r_squared(Y, Y_pred))\n",
    "\n",
    "# 0.32\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ericthansen/dsc-regression-assumptions\n",
    "\n",
    "Heteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n",
    "\n",
    "When there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.  \n",
    "\n",
    "A scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line). The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. You will learn about p-values later, but for now, you can remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary least squares in Statsmodels\n",
    "https://github.com/ericthansen/dsc-ols-statsmodels  \n",
    "https://github.com/ericthansen/dsc-ols-statsmodels-lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#check linearity\n",
    "df = pd.read_csv('heightWeight.csv')\n",
    "plt.scatter(df.height, df.weight)\n",
    "plt.title(\"Linearity check\")\n",
    "plt.show()\n",
    "\n",
    "#get a general sense of distribution- not sufficient to assume normality here!!!\n",
    "df.plot.kde()\n",
    "plt.title(\"distribution check for dependent and independent variable\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Statsmodels allows users to fit statistical models using R-style formulas. The formula framework is quite powerful and for simple regression it is written using a ~ as Y ~ X.\n",
    "#The formula gives instruction for a general structure for a regression call. For a statsmodels ols calls, you'll need a Pandas dataframe with column names that you will add to your formula.\n",
    "\n",
    "#regression formula\n",
    "f = 'weight~height'\n",
    "\n",
    "#pass to OLS with fit method\n",
    "model = ols(formula=f, data=df).fit()\n",
    "\n",
    "model.summary()\n",
    "dir(model)# to see specific parts!! very useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a lot of information. Statsmodels performs a ton of tests and calculates measures to identify goodness of fit. \n",
    "\n",
    "* You can find the R-Squared, which is 0.95 i.e. the data are very linearly related\n",
    "* You can also look at the coefficients of the model for intercept and slope (next to \"height\")\n",
    "* Kurtosis and Skew values are shown here\n",
    "* A lot of significance testing is being done here\n",
    "\n",
    "\n",
    "**Here is a brief description of these measures:**\n",
    "\n",
    "The left part of the first table gives some specifics on the data and the model:\n",
    "\n",
    "* **Dep. Variable**: Singular. Which variable is the point of interest of the model\n",
    "* **Model**: Technique used, an abbreviated version of Method (see methods for more).\n",
    "* **Method**: The loss function optimized in the parameter selection process. Least Squares since it picks the parameters that reduce the training error. This is also known as Mean Square Error [MSE].\n",
    "* **No. Observations**: The number of observations used by the model, or size of the training data.\n",
    "* **Degrees of Freedom Residuals**: Degrees of freedom of the residuals, which is the number of observations – number of parameters. Intercept is a parameter. The purpose of Degrees of Freedom is to reflect the impact of descriptive/summarizing statistics in the model, which in regression is the coefficient. Since the observations must \"live up\" to these parameters, they only have so many free observations, and the rest must be reserved to \"live up\" to the parameters' prophecy. This internal mechanism ensures that there are enough observations to match the parameters.\n",
    "* **Degrees of Freedom Model**: The number of parameters in the model (not including the constant/intercept term if present)\n",
    "* **Covariance Type**: Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process. Since this model is Ordinary Least Squares, it is non-robust and therefore highly sensitive to outliers.\n",
    "\n",
    "The right part of the first table shows the goodness of fit: \n",
    "\n",
    "* **R-squared**: The coefficient of determination, the Sum Squares of Regression divided by Total Sum Squares. This translates to the percent of variance explained by the model. The remaining percentage represents the variance explained by error, the E term, the part that model and predictors fail to grasp.\n",
    "* **Adj. R-squared**: Version of the R-Squared that penalizes additional independent variables. \n",
    "* **F-statistic**: A measure of how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals. Feeds into the calculation of the P-Value.\n",
    "* **Prob (F-statistic) or P-Value**: The probability that a sample like this would yield the above statistic, and whether the model's verdict on the null hypothesis will consistently represent the population. Does not measure effect magnitude, instead measures the integrity and consistency of this test on this group of data.\n",
    "* **Log-likelihood**: The log of the likelihood function.\n",
    "* **AIC**: The Akaike Information Criterion. Adjusts the log-likelihood based on the number of observations and the complexity of the model. Penalizes the model selection metrics when more independent variables are added.\n",
    "* **BIC**: The Bayesian Information Criterion. Similar to the AIC, but has a higher penalty for models with more parameters. Penalizes the model selection metrics when more independent variables are added.\n",
    "\n",
    "The second table shows the coefficient report: \n",
    "\n",
    "* **coef**: The estimated value of the coefficient. By how much the model multiplies the independent value by.\n",
    "* **std err**: The basic standard error of the estimate of the coefficient. Average distance deviation of the points from the model, which offers a unit relevant way to gauge model accuracy.\n",
    "* **t**: The t-statistic value. This is a measure of how statistically significant the coefficient is.\n",
    "* **P > |t|**: P-value that the null-hypothesis that the coefficient = 0 is true. If it is less than the confidence level, often 0.05, it indicates that there is a statistically significant relationship between the term and the response.\n",
    "* **[95.0% Conf. Interval]**: The lower and upper values of the 95% confidence interval. Specific range of the possible coefficient values.\n",
    "\n",
    "The third table shows information about the residuals, autocorrelation, and multicollinearity: \n",
    "\n",
    "* **Skewness**: A measure of the symmetry of the data about the mean. Normally-distributed errors should be symmetrically distributed about the mean (equal amounts above and below the line). The normal distribution has 0 skew.\n",
    "* **Kurtosis**: A measure of the shape of the distribution. Compares the amount of data close to the mean with those far away from the mean (in the tails), so model \"peakiness\". The normal distribution has a Kurtosis of 3, and the greater the number, the more the curve peaks.\n",
    "* **Omnibus D’Angostino’s test**: Provides a combined statistical test for the presence of skewness and kurtosis.\n",
    "* **Prob(Omnibus)**: The above statistic turned into a probability\n",
    "* **Jarque-Bera**: A different test of the skewness and kurtosis\n",
    "* **Prob (JB)**: The above statistic turned into a probability\n",
    "* **Durbin-Watson**: A test for the presence of autocorrelation (that the errors are not independent), which is often important in time-series analysis\n",
    "* **Cond. No**: A test for multicollinearity (if in a fit with multiple parameters, the parameters are related to each other).\n",
    "\n",
    "The interpretation of some of these measures will be explained in the next lessons. For others, you'll get a better insight into them in the lessons on statistics. \n",
    "\n",
    "\n",
    "## Visualize error terms\n",
    "\n",
    "You can also plot some visualizations to check the regression assumptions with respect to the error terms. You'll use `sm.graphics.plot_regress_exog()` for some built-in visualization capabilities of statsmodels. Here is how to do it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-761c9fefae30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_regress_exog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"height\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"height\", fig=fig)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the four graphs we see above:\n",
    "\n",
    "* The **Y and Fitted vs. X** graph plots the dependent variable against our predicted values with a confidence interval. The positive relationship shows that height and weight are correlated, i.e., when one variable increases the other increases.\n",
    "\n",
    "* The **Residuals versus height** graph shows our model's errors versus the specified predictor variable. Each dot is an observed value; the line represents the mean of those observed values. Since there's no pattern in the distance between the dots and the mean value, the OLS assumption of homoskedasticity holds.\n",
    "\n",
    "* The **Partial regression plot** shows the relationship between height and weight, taking in to account the impact of adding other independent variables on our existing height coefficient. You'll later learn how this same graph changes when you add more variables.\n",
    "\n",
    "* The **Component and Component Plus Residual (CCPR)** plot is an extension of the partial regression plot. It shows where the trend line would lie after adding the impact of adding our other independent variables on the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QQ Plots\n",
    "import scipy.stats as stats\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more with regression stats and diagnostics\n",
    "#  https://github.com/ericthansen/dsc-ols-regression-diagnostics\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "data = pd.read_csv('advertising.csv', index_col=0)\n",
    "f = 'sales~TV'\n",
    "f2 = 'sales~radio'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "model2 = smf.ols(formula=f2, data=data).fit()\n",
    "\n",
    "resid1 = model.resid\n",
    "resid2 = model2.resid\n",
    "fig = sm.graphics.qqplot(resid1, dist=stats.norm, line='45', fit=True)\n",
    "fig = sm.graphics.qqplot(resid2, dist=stats.norm, line='45', fit=True)\n",
    "\n",
    "#interpreting qq plots - ccu: peak is to the left (left skew- long right tail)\n",
    "#ccd: peak os to the right - long left tail\n",
    "#like an x^3 graph: too clustered around center than normal or with weird plateau in center\n",
    "#like an x^1/3 graph: too flat / triangular\n",
    "\n",
    "#jarque-bera test for normality - it inspects the skewness and kurtosis.  JB of >=6 indicates difference from normality.\n",
    "# JB test for TV\n",
    "name = ['Jarque-Bera','Prob','Skew', 'Kurtosis']\n",
    "test = sms.jarque_bera(model.resid)\n",
    "list(zip(name, test))\n",
    "# JB test for radio\n",
    "name = ['Jarque-Bera','Prob','Skew', 'Kurtosis']\n",
    "test2 = sms.jarque_bera(model2.resid)\n",
    "list(zip(name, test2))\n",
    "\n",
    "#The Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. \n",
    "#The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\n",
    "#In the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\n",
    "lwr_thresh = data.TV.quantile(q=.45)\n",
    "upr_thresh = data.TV.quantile(q=.55)\n",
    "middle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n",
    "# len(middle_10percent_indices)\n",
    "\n",
    "indices = [x-1 for x in data.index if x not in middle_10percent_indices]\n",
    "plt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\n",
    "plt.xlabel('TV')\n",
    "plt.ylabel('Model Residuals')\n",
    "plt.title(\"Residuals versus TV Feature\")\n",
    "plt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles='dashed',linewidth=2)\n",
    "plt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles='dashed',linewidth=2);\n",
    "\n",
    "#Here is a brief description of the steps involved:\n",
    "# Order the data in ascending order\n",
    "# Split your data into three parts and drop values in the middle part.\n",
    "# Run separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\n",
    "# Calculate the ratio of the Residual sum of squares of two parts.\n",
    "# Apply the F-test. (more on F-Test later)\n",
    "# For now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small. However, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\n",
    "\n",
    "# Run Goldfeld Quandt test\n",
    "name = ['F statistic', 'p-value']\n",
    "test = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\n",
    "list(zip(name, test))\n",
    "# Run Goldfeld Quandt test\n",
    "import statsmodels.stats.api as sms\n",
    "name = ['F statistic', 'p-value']\n",
    "test = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\n",
    "list(zip(name, test))\n",
    "\n",
    "# The null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\n",
    "\n",
    "# The p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\n",
    "\n",
    "# Statsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More on p-value significance?\n",
    "#https://github.com/ericthansen/dsc-significance-p-value\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "data = pd.read_csv('Advertising.csv', index_col=0)\n",
    "f = 'sales~TV'\n",
    "f2 = 'sales~radio'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "\n",
    "model\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Have a look at the probability for the F-statistic in the upper right plane. This is a p-value for the overall model fit. You can see how this probability value is the exact same as the p-value for TV. You'll see that this is always the case if there is only one independent variable. Here, TV drives the entire model, so the model p-value is the same as the TV coefficient p-value.\n",
    "\n",
    "# In the plane with the coefficient estimates, p-values are in the column P > |t| and rounded to 3 digits (hence 0.000 for Intercept and TV). You can find the confidence intervals in the two last columns: The confidence interval for intercept is [6,13, 7.935] meaning that there is a 95% chance that the actual coefficient value is in that range. For TV, there is a 95% chance that the actual coefficient value is in the interval [0.042, 0.053]. Note that 0 is in none of these intervals as expected given the very low p-value.\n",
    "    \n",
    "#see also this lab which was somewhat comprehensive:\n",
    "#    https://github.com/ericthansen/dsc-regression-boston-lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Takeaways\n",
    "In this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\n",
    "\n",
    "Key takeaways include:\n",
    "\n",
    "Statistical learning theory deals with the problem of finding a predictive function based on data  \n",
    "A loss function calculates how well a given model represents the relationship between data values  \n",
    "A linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)  \n",
    "The Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set  \n",
    "Certain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity  \n",
    "Q-Q plots can check for normality in residual errors  \n",
    "The Jarque-Bera test can be used to test for normality - especially when the number of data points is large  \n",
    "The Goldfeld-Quant test can be used to check for homoscedasticity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lecture: https://learning.flatironschool.com/courses/1888/pages/video-simple-linear-regression-part-2?module_item_id=261379\n",
    "# repo: https://github.com/ericthansen/ds-simple_linear_regression-nbz32.git\n",
    "## This has some good explanations of OLS report terms!! #\n",
    "#desmos tool link: https://www.desmos.com/calculator/jwquvmikhr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1234)\n",
    "sen = np.random.uniform(18, 65, 100)\n",
    "income = np.random.normal((sen/10), 0.5)\n",
    "sen = sen.reshape(-1, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "fig.suptitle('seniority vs. income', fontsize=16)\n",
    "plt.scatter(sen, income)\n",
    "plt.plot(sen, sen/10, c='black')\n",
    "plt.xlabel('seniority', fontsize=14)\n",
    "plt.ylabel('monthly income', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with categorical variables\n",
    "#https://github.com/ericthansen/dsc-dealing-with-categorical-variables\n",
    "#Namely, one-hot variables\n",
    "\n",
    "#Sample code:\n",
    "import pandas as pd\n",
    "data = pd.read_csv('auto-mpg.csv')\n",
    "# First convert horsepower into a string and then to int\n",
    "data['horsepower'].astype(str).astype(int)\n",
    "data.head()\n",
    "data.info()\n",
    "\n",
    "print(data['origin'].describe())\n",
    "print(data['origin'].nunique())\n",
    "#Values range from 1 to 3, moreover, actually the only values that are in the dataset are 1, 2 and 3! it turns out that \"origin\" is a so-called categorical variable. It does not represent a continuous number but refers to a location - say 1 may stand for US, 2 for Europe, 3 for Asia (note: for this dataset the actual meaning is not disclosed).\n",
    "\n",
    "#id-ing cat vars\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(16,3))\n",
    "\n",
    "for xcol, ax in zip(['acceleration', 'displacement', 'horsepower', 'weight'], axes):\n",
    "    data.plot(kind='scatter', x=xcol, y='mpg', ax=ax, alpha=0.4, color='b')\n",
    "    \n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,3))\n",
    "\n",
    "for xcol, ax in zip([ 'cylinders', 'model year', 'origin'], axes):\n",
    "    data.plot(kind='scatter', x=xcol, y='mpg', ax=ax, alpha=0.4, color='b')\n",
    "\n",
    "#To id categoricals, its helpful to look at histogram or the number of unique vals\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.gca()\n",
    "data.hist(ax = ax);\n",
    "\n",
    "data[['cylinders', 'model year', 'origin']].nunique()\n",
    "\n",
    "# #Transforming categorical variables\n",
    "# When you want to use categorical variables in regression models, they need to be transformed. There are two approaches to this:\n",
    "\n",
    "# 1) Perform label encoding\n",
    "# 2) Create dummy variables / one-hot-encoding\n",
    "\n",
    "\n",
    "# Label encoding\n",
    "# Let's illustrate label encoding and dummy creation with the following Pandas Series with 3 categories: \"USA\", \"EU\" and \"ASIA\".\n",
    "origin = ['USA', 'EU', 'EU', 'ASIA','USA', 'EU', 'EU', 'ASIA', 'ASIA', 'USA']\n",
    "origin_series = pd.Series(origin)\n",
    "\n",
    "#Now you'll want to make sure Python recognizes there strings as categories. This can be done as follows:\n",
    "cat_origin = origin_series.astype('category')\n",
    "cat_origin\n",
    "\n",
    "# Sometimes you'll want to represent your labels as numbers. This is called label encoding.\n",
    "\n",
    "# You'll perform label encoding in a way that numerical labels are always between 0 and (number_of_categories)-1. There are several ways to do this, one way is using .cat.codes\n",
    "\n",
    "cat_origin.cat.codes\n",
    "\n",
    "#Another way is to use scikit-learn's LabelEncoder:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "origintrap_df = pd.get_dummies(cat_origin)\n",
    "trap_df_encoded = lb_make.fit_transform(cat_origin)\n",
    "\n",
    "#Note that while .cat.codes can only be used on variables that are transformed using .astype(category), this is not a requirement to use LabelEncoder.\n",
    "\n",
    "#Creating Dummy Variables\n",
    "#Another way to transform categorical variables is through using one-hot encoding or \"dummy variables\". The idea is to convert each category into a new column, and assign a 1 or 0 to the column. There are several libraries that support one-hot encoding, let's take a look at two:\n",
    "\n",
    "pd.get_dummies(cat_origin)\n",
    "#See how the label name has become the column name! Another method is through using the LabelBinarizer in scikit-learn.\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "origin_dummies = lb.fit_transform(cat_origin)\n",
    "# You need to convert this back to a dataframe\n",
    "origin_dum_df = pd.DataFrame(origin_dummies,columns=lb.classes_)\n",
    "origin_dum_df\n",
    "#The advantage of using dummies is that, whatever algorithm you'll be using, your numerical values cannot be misinterpreted as being continuous. Going forward, it's important to know that for linear regression (and most other algorithms in scikit-learn), one-hot encoding is required when adding categorical variables in a regression model!\n",
    "\n",
    "#The Dummy Variable Trap\n",
    "#Due to the nature of how dummy variables are created, one variable can be predicted from all of the others. This is known as perfect multicollinearity and it can be a problem for regression. Multicollinearity will be covered in depth later but the basic idea behind perfect multicollinearity is that you can perfectly predict what one variable will be using some combination of the other variables. If this isn't super clear, go back to the one-hot encoded origin data above:\n",
    "trap_df = pd.get_dummies(cat_origin)\n",
    "trap_df\n",
    "# Predict ASIA column from EU and USA\n",
    "predicted_asia = 1 - (trap_df['EU'] + trap_df['USA'])\n",
    "predicted_asia.to_frame(name='Predicted_ASIA')\n",
    "#You are probably wondering why this is a problem for regression. Recall that the coefficients derived from a regression model are used to make predictions. In a multiple linear regression, the coefficients represent the average change in the dependent variable for each 1 unit change in a predictor variable, assuming that all the other predictor variables are kept constant. This is no longer the case when predictor variables are related which, as you've just seen, happens automatically when you create dummy variables. This is what is known as the Dummy Variable Trap.\n",
    "#Fortunately, the dummy variable trap can be avoided by simply dropping one of the dummy variables. You can do this by subsetting the dataframe manually or, more conveniently, by passing drop_first=True to get_dummies():\n",
    "pd.get_dummies(cat_origin, drop_first=True)\n",
    "\n",
    "\n",
    "#Back to our auto-mpg data\n",
    "#Let's go ahead and change our \"cylinders\", \"model year\", and \"origin\" columns over to dummies and drop the first variable.\n",
    "cyl_dummies = pd.get_dummies(data['cylinders'], prefix='cyl', drop_first=True)\n",
    "yr_dummies = pd.get_dummies(data['model year'], prefix='yr', drop_first=True)\n",
    "orig_dummies = pd.get_dummies(data['origin'], prefix='orig', drop_first=True)\n",
    "#Next, let's remove the original columns from our data and add the dummy columns instead\n",
    "data = data.drop(['cylinders','model year','origin'], axis=1)\n",
    "data = pd.concat([data, cyl_dummies, yr_dummies, orig_dummies], axis=1)\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity example (more)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "data = pd.read_csv('auto-mpg.csv')\n",
    "data['horsepower'].astype(str).astype(int) # don't worry about this for now\n",
    "data.head()\n",
    "#removing the target variable (and car name)\n",
    "data_pred = data.iloc[:,1:8]\n",
    "data_pred.head()\n",
    "#initial look at how predictors relate\n",
    "pd.plotting.scatter_matrix(data_pred,figsize  = [9, 9]);\n",
    "plt.show()\n",
    "#and the corresponding correlation numbers\n",
    "data_pred.corr()\n",
    "#Generally, a correlation with an absolute value around 0.7-0.8 or higher is considered a high correlation. If we take 0.75 as a cut-off, how many high correlations do we have?\n",
    "abs(data_pred.corr()) > 0.75\n",
    "\n",
    "#what if the set was too large to manage manually?\n",
    "####\n",
    "####THIS IS HIGHLY SLICK CODE THAT CREATES HIGHLY CORRELATED PAIRS.  KEEP IT HANDY!###\n",
    "# save absolute value of correlation matrix as a data frame\n",
    "# converts all values to absolute value\n",
    "# stacks the row:column pairs into a multindex\n",
    "# reset the index to set the multindex to seperate columns\n",
    "# sort values. 0 is the column automatically generated by the stacking\n",
    "\n",
    "\n",
    "df=data_pred.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "# zip the variable name columns (Which were only named level_0 and level_1 by default) in a new column named \"pairs\"\n",
    "df['pairs'] = list(zip(df.level_0, df.level_1))\n",
    "\n",
    "# set index to pairs\n",
    "df.set_index(['pairs'], inplace = True)\n",
    "\n",
    "#d rop level columns\n",
    "df.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# rename correlation column as cc rather than 0\n",
    "df.columns = ['cc']\n",
    "\n",
    "# drop duplicates. This could be dangerous if you have variables perfectly correlated with variables other than themselves.\n",
    "# for the sake of exercise, kept it in.\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "#Then subset as before\n",
    "df[(df.cc>.75) & (df.cc <1)]\n",
    "\n",
    "#####\n",
    "#With the variables 'cylinder', 'displacement', 'horsepower', and 'weight' so highly correlated, you would typically remove three of them in order to remove collinear features.\n",
    "\n",
    "#Another option is to use a heatmap to render the correlation matrix as a visualization.\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create a df with the target as the first column,\n",
    "# then compute the correlation matrix\n",
    "heatmap_data = pd.concat([y_train, X_train], axis=1)\n",
    "corr = heatmap_data.corr()\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, ax = plt.subplots(figsize=(5, 8))\n",
    "\n",
    "# Plot a heatmap of the correlation matrix, with both\n",
    "# numbers and colors indicating the correlations\n",
    "sns.heatmap(\n",
    "    # Specifies the data to be plotted\n",
    "    data=corr,\n",
    "    # The mask means we only show half the values,\n",
    "    # instead of showing duplicates. It's optional.\n",
    "    mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "    # Specifies that we should use the existing axes\n",
    "    ax=ax,\n",
    "    # Specifies that we want labels, not just colors\n",
    "    annot=True,\n",
    "    # Customizes colorbar appearance\n",
    "    cbar_kws={\"label\": \"Correlation\", \"orientation\": \"horizontal\", \"pad\": .2, \"extend\": \"both\"}\n",
    ")\n",
    "\n",
    "# Customize the plot appearance\n",
    "ax.set_title(\"Heatmap of Correlation Between Attributes (Including Target)\");\n",
    "#crap - must have overwritten this\n",
    "\n",
    "\n",
    "##lab on multicollinearity/vis\n",
    "\n",
    "#trick for making readable huge scatterplot\n",
    "sm = pd.plotting.scatter_matrix(ames_preprocessed, figsize=[20, 20]);\n",
    "\n",
    "# Rotates the text\n",
    "[s.xaxis.label.set_rotation(90) for s in sm.reshape(-1)]\n",
    "[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n",
    "\n",
    "#May need to offset label when rotating to prevent overlap of figure\n",
    "[s.get_yaxis().set_label_coords(-1,0.5) for s in sm.reshape(-1)]\n",
    "\n",
    "#Hide all ticks\n",
    "[s.set_xticks(()) for s in sm.reshape(-1)]\n",
    "[s.set_yticks(()) for s in sm.reshape(-1)]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transformations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "'''Linear Regression Assumptions\n",
    "Remember that linear regression operates under various assumptions including that the dependent variable can be decomposed into a linear combination of the independent features. Additionally, data should be homoscedastic and the residuals should follow a normal distribution.\n",
    "\n",
    "One thing we briefly touched upon previously is the distributions of the predictors. In previous labs, you have looked at these distributions to have an understanding of what the distributions look like. In fact, you'll often find that having the data more normally distributed will benefit your model and model performance in general. So while normality of the predictors is not a mandatory assumption, having (approximately) normal features may be helpful for your model!'''\n",
    "\n",
    "#using raw features that aren't normal\n",
    "data = pd.read_csv('auto-mpg.csv')\n",
    "data.head()\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "outcome = 'mpg'\n",
    "x_cols = ['displacement', 'horsepower', 'weight', 'acceleration']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=data).fit()\n",
    "model.summary()\n",
    "pd.plotting.scatter_matrix(data[x_cols], figsize=(10,12));\n",
    "\n",
    "#reminder about log\n",
    "x = np.linspace(start=-100, stop=100, num=10**3)\n",
    "y = np.log(x)\n",
    "plt.plot(x, y);\n",
    "\n",
    "#Transforming nonnormal features\n",
    "non_normal = ['displacement', 'horsepower', 'weight']\n",
    "for feat in non_normal:\n",
    "    data[feat] = data[feat].map(lambda x: np.log(x))\n",
    "pd.plotting.scatter_matrix(data[x_cols], figsize=(10,12));\n",
    "#after transforming, model\n",
    "outcome = 'mpg'\n",
    "x_cols = ['displacement', 'horsepower', 'weight', 'acceleration']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=data).fit()\n",
    "model.summary()\n",
    "'''While not dramatic, you can observe that simply by transforming non-normally distributed features using log transformations, we have increased our  𝑅2  value of the model from 0.707 to 0.748.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other kinds of scaling\n",
    "'''Min-max scaling\n",
    "standardization by mean and std\n",
    "mean norm(subtract mean, divide by diff of max and min)\n",
    "unit vector normalization (dvide by magnitude of x)\n",
    "'''\n",
    "#applying transform to auto-mpg data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "data = pd.read_csv('auto-mpg.csv')\n",
    "data['horsepower'].astype(str).astype(int) # don't worry about this for now\n",
    "data_pred = data.iloc[:,1:8]\n",
    "data_pred.head()\n",
    "data_pred[['acceleration', 'displacement', 'horsepower', 'weight']].hist(figsize  = [6, 6]); \n",
    "#skewness is an issue\n",
    "#first make it more normal, then feature scale to manage the diff in magnitude\n",
    "import numpy as np\n",
    "data_log = pd.DataFrame([])\n",
    "data_log['logdisp'] = np.log(data_pred['displacement'])\n",
    "data_log['loghorse'] = np.log(data_pred['horsepower'])\n",
    "data_log['logweight'] = np.log(data_pred['weight'])\n",
    "data_log.hist(figsize  = [6, 6]);\n",
    "'''Now, let's perform min-max scaling (on 'acceleration'), standardization (on 'logdisp' and 'logweight'), and mean normalization (on 'loghorse').'''\n",
    "acc = data_pred['acceleration']\n",
    "logdisp = data_log['logdisp']\n",
    "loghorse = data_log['loghorse']\n",
    "logweight = data_log['logweight']\n",
    "\n",
    "scaled_acc = (acc - min(acc)) / (max(acc) - min(acc))\n",
    "scaled_disp = (logdisp - np.mean(logdisp)) / np.sqrt(np.var(logdisp))\n",
    "scaled_weight = (logweight - np.mean(logweight)) / np.sqrt(np.var(logweight))\n",
    "scaled_horse = (loghorse - np.mean(loghorse)) / (max(loghorse) - min(loghorse))\n",
    "\n",
    "data_cont_scaled = pd.DataFrame([])\n",
    "data_cont_scaled['acc'] = scaled_acc\n",
    "data_cont_scaled['disp'] = scaled_disp\n",
    "data_cont_scaled['horse'] = scaled_horse\n",
    "data_cont_scaled['weight'] = scaled_weight\n",
    "\n",
    "data_cont_scaled.hist(figsize = [6, 6]);\n",
    "\n",
    "'''Scikit-learn provides automatic tools to scale features, see, among others, MinMaxScaler, StandardScaler, and Normalizer. Have a look at these built-in functions and some code examples here: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing!\n",
    "\n",
    "To learn more about feature scaling in general, you can have a look at this blogpost: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html (up until \"bottom-up approaches\").'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple Lin Regression Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('auto-mpg.csv') \n",
    "data['horsepower'].astype(str).astype(int)\n",
    "\n",
    "acc = data['acceleration']\n",
    "logdisp = np.log(data['displacement'])\n",
    "loghorse = np.log(data['horsepower'])\n",
    "logweight= np.log(data['weight'])\n",
    "\n",
    "scaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\n",
    "scaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\n",
    "scaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\n",
    "scaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n",
    "\n",
    "data_fin = pd.DataFrame([])\n",
    "data_fin['acc'] = scaled_acc\n",
    "data_fin['disp'] = scaled_disp\n",
    "data_fin['horse'] = scaled_horse\n",
    "data_fin['weight'] = scaled_weight\n",
    "cyl_dummies = pd.get_dummies(data['cylinders'], prefix='cyl', drop_first=True)\n",
    "yr_dummies = pd.get_dummies(data['model year'], prefix='yr', drop_first=True)\n",
    "orig_dummies = pd.get_dummies(data['origin'], prefix='orig', drop_first=True)\n",
    "mpg = data['mpg']\n",
    "data_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\n",
    "\n",
    "data_fin.info()\n",
    "\n",
    "#simplifying the model to fewer things\n",
    "data_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\n",
    "data_ols.head()\n",
    "\n",
    "#now use statsmodels api\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "formula = 'mpg ~ acceleration+weight+orig_2+orig_3'\n",
    "model = ols(formula=formula, data=data_ols).fit()\n",
    "\n",
    "'''Having to type out all the predictors isn't practical when you have many. Another better way than to type them all out is to seperate out the outcome variable 'mpg' out of your DataFrame, and use the a '+'.join() command on the predictors, as done below:'''\n",
    "outcome = 'mpg'\n",
    "predictors = data_ols.drop('mpg', axis=1)\n",
    "pred_sum = '+'.join(predictors.columns)\n",
    "formula = outcome + '~' + pred_sum\n",
    "model = ols(formula=formula, data=data_ols).fit()\n",
    "model.summary()\n",
    "#to avoid making the summation string,  use the ols function\n",
    "'''The advantage is that you don't have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.'''\n",
    "import statsmodels.api as sm\n",
    "predictors_int = sm.add_constant(predictors)\n",
    "model = sm.OLS(data['mpg'],predictors_int).fit()\n",
    "model.summary()\n",
    "'''Just like for single multiple regression, the coefficients for the model should be interpreted as \"how does  𝑦  change for each additional unit  𝑋 \"? However, do note that since  𝑋  was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed  𝑋 , the actual relationship is \"how does  𝑦  change for each additional unit  𝑋′ \", where  𝑋′  is the (log- and min-max, standardized,...) transformed data matrix.'''\n",
    "'''Linear regression using scikit-learn\n",
    "You can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn't have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "y = data_ols['mpg']\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(predictors, y)\n",
    "# coefficients\n",
    "linreg.coef_\n",
    "# intercept\n",
    "linreg.intercept_\n",
    "\n",
    "\n",
    "#Predicting - from lab\n",
    "#I just took dot product of the inputs and the coefficients (after manually one-hot-ting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelling for inference (understanding) vs. Prediction (application).  The latter often becomes\n",
    "#a black box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model fit in linear regression\n",
    "\n",
    "# https://github.com/ericthansen/dsc-model-fit-linear-regression.git\n",
    "'''You will be able to:\n",
    "\n",
    "Use stepwise selection methods to determine the most important features for a model\n",
    "Use recursive feature elimination to determine the most important features for a model'''\n",
    "\n",
    "\n",
    "##summary of previous work, including log mapping and categorical handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('auto-mpg.csv')\n",
    "\n",
    "acc = data['acceleration']\n",
    "logdisp = np.log(data['displacement'])\n",
    "loghorse = np.log(data['horsepower'])\n",
    "logweight = np.log(data['weight'])\n",
    "\n",
    "scaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\n",
    "scaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\n",
    "scaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\n",
    "scaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n",
    "\n",
    "data_fin = pd.DataFrame([])\n",
    "data_fin['acc'] = scaled_acc\n",
    "data_fin['disp'] = scaled_disp\n",
    "data_fin['horse'] = scaled_horse\n",
    "data_fin['weight'] = scaled_weight\n",
    "cyl_dummies = pd.get_dummies(data['cylinders'], prefix='cyl', drop_first=True)\n",
    "yr_dummies = pd.get_dummies(data['model year'], prefix='yr', drop_first=True)\n",
    "orig_dummies = pd.get_dummies(data['origin'], prefix='orig', drop_first=True)\n",
    "mpg = data['mpg']\n",
    "data_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\n",
    "\n",
    "###\n",
    "data_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\n",
    "data_ols.head(3)\n",
    "# Import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "#predictors\n",
    "outcome = 'mpg'\n",
    "predictors = data_ols.drop('mpg', axis=1)\n",
    "pred_sum = '+'.join(predictors.columns)\n",
    "formula = outcome + '~' + pred_sum\n",
    "\n",
    "#model fit\n",
    "model = ols(formula=formula, data=data_ols).fit()\n",
    "model.summary()\n",
    "\n",
    "'''Note that with 6 variables, we can sub-select some of those variables, each to create a different model.'''\n",
    "'''This means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we'll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.'''\n",
    "\n",
    "#stepwise selection links:\n",
    "https://en.wikipedia.org/wiki/Stepwise_regression\n",
    "https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm\n",
    "#stepwise selection - not yet built in to python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "\n",
    "result = stepwise_selection(predictors, data_fin['mpg'], verbose=True)\n",
    "print('resulting features:')\n",
    "print(result)\n",
    "\n",
    "#link to scikit learn for feature ranking\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection\n",
    "    \n",
    "#from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "selector = RFE(linreg, n_features_to_select=3)\n",
    "selector = selector.fit(predictors, data_fin['mpg'])\n",
    "\n",
    "#this tells you which variables are selected\n",
    "selector.support_ \n",
    "#and the ranking\n",
    "selector.ranking_\n",
    "\n",
    "#you can get access to the parameter estimates\n",
    "estimators = selector.estimator_\n",
    "print(estimators.coef_)\n",
    "print(estimators.intercept_)\n",
    "\n",
    "'''Note that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.'''\n",
    "n_features_to_select=4\n",
    "\n",
    "https://planspace.org/20150423-forward_selection_with_statsmodels/\n",
    "    provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test sets\n",
    "# default 70/30 split\n",
    "#a big diff between test and training (Root)mean square error is a sign of overfitting\n",
    "\n",
    "#applying again to auto-mpg data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('auto-mpg.csv') \n",
    "data['horsepower'].astype(str).astype(int)\n",
    "\n",
    "acc = data['acceleration']\n",
    "logdisp = np.log(data['displacement'])\n",
    "loghorse = np.log(data['horsepower'])\n",
    "logweight = np.log(data['weight'])\n",
    "\n",
    "scaled_acc = (acc-min(acc))/(max(acc)-min(acc))\n",
    "scaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\n",
    "scaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\n",
    "scaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n",
    "\n",
    "data_fin = pd.DataFrame([])\n",
    "data_fin['acc'] = scaled_acc\n",
    "data_fin['disp'] = scaled_disp\n",
    "data_fin['horse'] = scaled_horse\n",
    "data_fin['weight'] = scaled_weight\n",
    "cyl_dummies = pd.get_dummies(data['cylinders'], prefix='cyl', drop_first=True)\n",
    "yr_dummies = pd.get_dummies(data['model year'], prefix='yr', drop_first=True)\n",
    "orig_dummies = pd.get_dummies(data['origin'], prefix='orig', drop_first=True)\n",
    "mpg = data['mpg']\n",
    "data_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\n",
    "\n",
    "data = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\n",
    "y = data[['mpg']]\n",
    "X = data.drop(['mpg'], axis=1)\n",
    "\n",
    "#Scikitlearn has a handy function train_test_split\n",
    "imp\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "\n",
    "#doing regression on the training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "y_hat_train = linreg.predict(X_train)\n",
    "y_hat_test = linreg.predict(X_test)\n",
    "\n",
    "#look at residuals, calc MSE (mean square error) for each\n",
    "train_residuals = y_hat_train - y_train\n",
    "test_residuals = y_hat_test - y_test\n",
    "mse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\n",
    "mse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\n",
    "print('Train Mean Squarred Error:', mse_train)\n",
    "print('Test Mean Squarred Error:', mse_test)\n",
    "\n",
    "#or calc directly from scikitlearn function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_mse = mean_squared_errorcross_val\n",
    "test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "print('Train Mean Squarred Error:', train_mse)\n",
    "print('Test Mean Squarred Error:', test_mse)\n",
    "\n",
    "#for this sample, not a big difference\n",
    "\n",
    "#A nice blog post with some handy visualizations and workflow with model building, test sets, etc\n",
    "https://towardsdatascience.com/linear-regression-in-python-9a1f5f000606\n",
    "'''summary from that blog: Replace feature_cols & X\n",
    "Train_test_split your data\n",
    "Fit the model to linreg again using linreg.fit\n",
    "Make predictions using (y_pred = linreg.predict(X_test))\n",
    "Compute RMSE\n",
    "Repeat until RMSE satisfactory'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lab.  including sub loop to smooth out, averaging over several different traintest splits\n",
    "#https://github.com/ericthansen/dsc-regression-model-validation-lab\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Your code here\n",
    "# Your code here\n",
    "iters = 10\n",
    "results = pd.DataFrame(columns = ['Training Size', 'Train Error', 'Test Error'])\n",
    "#pd.DataFrame(data, columns = ['Name', 'Age'])\n",
    "for split in range(5, 96, 5):\n",
    "    s = split/100\n",
    "    avg_train_mse = 0\n",
    "    avg_test_mse = 0\n",
    "    for _ in range(iters):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=s)#, random_state=42)\n",
    "        linreg = LinearRegression()\n",
    "        linreg.fit(X_train, y_train)\n",
    "        y_hat_train = linreg.predict(X_train)\n",
    "        y_hat_test = linreg.predict(X_test)\n",
    "        train_residuals = y_hat_train - y_train\n",
    "        test_residuals = y_hat_test - y_test\n",
    "    #     mse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\n",
    "    #     mse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\n",
    "        train_mse = mean_squared_error(y_train, y_hat_train)\n",
    "        test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "        avg_train_mse += train_mse\n",
    "        avg_test_mse += test_mse\n",
    "    train_mse = avg_train_mse / iters\n",
    "    test_mse = avg_test_mse / iters\n",
    "    \n",
    "    results = results.append({'Training Size':s, 'Train Error': train_mse, 'Test Error': test_mse},ignore_index=True)\n",
    "#results\n",
    "results.plot(x='Training Size', y = ['Train Error', 'Test Error'])\n",
    "    #['Train Error', 'Test Error'], x=['Training Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We can see from this that different random test splits can results in dramatically different\n",
    "results.  One way to address this: K-Fold Cross Validation\n",
    "In order to deal with the issues that random sampling can introduce into interpreting the quality of our models, we'll use a more advanced technique called K-Fold Cross Validation.\n",
    "'''\n",
    "'''K-Fold Cross Validation expands on the idea of training and test splits by splitting the entire dataset into {K} equal sections of data. We'll then iteratively train {K} linear regression models on the data, with each linear model using a different section of data as the test set, and all other sections combined as the training set.\n",
    "We can then average the individual results frome each of these linear models to get a Cross-Validation MSE. This will be closer to the model's actual MSE, since \"noisy\" results that are higher than average will cancel out the \"noisy\" results that are lower than average.'''\n",
    "\n",
    "'''You can easily do this in scikit-learn using cross_val_score(). If you want the mean squared error as an output, you need to set the scoring argument to 'neg_mean_squared_error'. Note that this negates your mean squared error, so larger means better!'''\n",
    "cross_val_score()\n",
    "\n",
    "#for example\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_5_results  = np.mean(polynom)\n",
    "cv_10_results = np.mean(cross_val_score(linreg, X, y, cv=10, scoring='neg_mean_squared_error'))\n",
    "cv_20_results = np.mean(cross_val_score(linreg, X, y, cv=20, scoring='neg_mean_squared_error'))\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/cross_validation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle!\n",
    "#basically a way to save objects (or virtually anything in the Python interpreter) to a file\n",
    "import pickle\n",
    "data_object = {\n",
    "    'a': [1, 2.0, 3, 4+6j],\n",
    "    'b': ('character string', b'byte string'),\n",
    "    'c': {None, True, False}\n",
    "}\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(data_object, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#with scikit learn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# y = 1 * x_0 + 2 * x_1 + 3\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "import pickle\n",
    "# Save\n",
    "with open('regression_model.pickle', 'wb') as f:\n",
    "    pickle.dump(reg, f)\n",
    "\n",
    "# Load\n",
    "with open('regression_model.pickle', 'rb') as file:\n",
    "    reg2 = pickle.load(file)\n",
    "reg2.predict(X)\n",
    "\n",
    "\n",
    "#links\n",
    "https://docs.python.org/3/library/pickle.html\n",
    "https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science workflow\n",
    "Lots of visuals here, worthwhile to see\n",
    "https://github.com/ericthansen/dsc-data-science-processes\n",
    "\n",
    "Data Science projects are often complex, with many stakeholders, data sources, and goals. Due to this, the Data Science community has created several methodologies for helping organize and structure Data Science Projects. In this lesson, you'll explore three of the most popular methodologies -- CRISP-DM, KDD, and OSEMN, and explore how you can make use of them to keep your projects well-structured and organized.\n",
    "\n",
    "CRoss-Industry Standard Process for Data Mining (CRISP-DM)\n",
    "-business understanding\n",
    "Good questions for this stage include:\n",
    "\n",
    "Who are the stakeholders in this project? Who will be directly affected by the creation of this project?\n",
    "What business problem(s) will this Data Science project solve for the organization?\n",
    "What problems are inside the scope of this project?\n",
    "What problems are outside the scope of this project?\n",
    "What data sources are available to us?\n",
    "What is the expected timeline for this project? Are there hard deadlines (e.g. \"must be live before holiday season shopping\") or is this an ongoing project?\n",
    "Do stakeholders from different parts of the company or organization all have the exact same understanding about what this project is and isn't?\n",
    "\n",
    "Data Understanding:\n",
    "What data is available to us? Where does it live? Do we have the data, or can we scrape/buy/source the data from somewhere else?\n",
    "Who controls the data sources, and what steps are needed to get access to the data?\n",
    "What is our target?\n",
    "What predictors are available to us?\n",
    "What data types are the predictors we'll be working with?\n",
    "What is the distribution of our data?\n",
    "How many observations does our dataset contain? Do we have a lot of data? Only a little?\n",
    "Do we have enough data to build a model? Will we need to use resampling methods?\n",
    "How do we know the data is correct? How is the data collected? Is there a chance the data could be wrong?\n",
    "\n",
    "\n",
    "Data Preparation:\n",
    "\n",
    "Once we have a strong understanding of our data, we can move onto preparing the data for our modeling steps.\n",
    "\n",
    "During this stage, we'll want to handle the following issues:\n",
    "\n",
    "Detecting and dealing with missing values\n",
    "Data type conversions (e.g. numeric data mistakenly encoded as strings)\n",
    "Checking for and removing multicollinearity (correlated predictors)\n",
    "Normalizing our numeric data\n",
    "Converting categorical data to numeric format through one-hot encoding\n",
    "\n",
    "Modeling:\n",
    "\n",
    "Once we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task.\n",
    "\n",
    "Consider the following questions during the modeling step:\n",
    "\n",
    "Is this a classification task? A regression task? Something else?\n",
    "What models will we try?\n",
    "How do we deal with overfitting?\n",
    "Do we need to use regularization or not?\n",
    "What sort of validation strategy will we be using to check that our model works well on unseen data?\n",
    "What loss functions will we use?\n",
    "What threshold of performance do we consider as successful?\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "During this step, we'll evaluate the results of our modeling efforts. Does our model solve the problems that we outlined all the way back during step 1? Why or why not? Often times, evaluating the results of our modeling step will raise new questions, or will cause us to consider changing our approach to the problem. Notice from the CRISP-DM diagram above, that the \"Evaluation\" step is unique in that it points to both Business Understanding and Deployment. As we mentioned before, Data Science is an iterative process -- that means that given the new information our model has provided, we'll often want to start over with another iteration, armed with our newfound knowledge! Perhaps the results of our model showed us something important that we had originally failed to consider the goal of the project or the scope. Perhaps we learned that the model can't be successful without more data, or different data. Perhaps our evaluation shows us that we should reconsider our approach to cleaning and structuring the data, or how we frame the project as a whole (e.g. realizing we should treat the problem as a classification rather than a regression task). In any of these cases, it is totally encouraged to revisit the earlier steps.\n",
    "\n",
    "Of course, if the results are satisfactory, then we instead move onto deployment!\n",
    "\n",
    "Deployment:\n",
    "\n",
    "During this stage, we'll focus on moving our model into production and automating as much as possible. Everything before this serves as a proof-of-concept or an investigation. If the project has proved successful, then you'll work with stakeholders to determine the best way to implement models and insights. For example, you might set up an automated ETL (Extract-Transform-Load) pipelines of raw data in order to feed into a database and reformat it so that it is ready for modeling. During the deployment step, you'll actively work to determine the best course of action for getting the results of your project into the wild, and you'll often be involved with building everything needed to put the software into production.\n",
    "\n",
    "This is one of the most rewarding steps of the entire Data Science process -- getting to see your work go live!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Knowledge Discovery in Databases, or KDD is considered the oldest Data Science process. \n",
    "\n",
    "The creation of this process is credited to Gregory Piatetsky-Shapiro, who also runs the ever-popular Data Science blog, kdnuggets (Links to an external site.). If you're interested, read the original white paper on KDD, which can be found here (Links to an external site.)!\n",
    "data\n",
    "-Selection  \n",
    "target data\n",
    "-preprocessing  \n",
    "preprocessed data\n",
    "-Transformation  \n",
    "transformed data\n",
    "-data mining  \n",
    "patterns\n",
    "-interpretation/evaluation  \n",
    "knowledge\n",
    "\n",
    "\n",
    "\n",
    "OSEMN model\n",
    "\n",
    "Obtain/scrub/explore/model/interpret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing specific lab\n",
    "\n",
    "# Run this cell without changes\n",
    "\n",
    "#heck, this entire notebook:\n",
    "https://github.com/ericthansen/dsc-sklearn-preprocessing-lab\n",
    "\n",
    "#of note - good guidance on  Missing indicator technique\n",
    "# Replace None with appropriate code\n",
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "# (1) Identify data to be transformed\n",
    "# We only want missing indicators for LotFrontage\n",
    "frontage_train = X_train[[\"LotFrontage\"]]\n",
    "\n",
    "# (2) Instantiate the transformer object\n",
    "missing_indicator = MissingIndicator()\n",
    "\n",
    "# (3) Fit the transformer object on frontage_train\n",
    "missing_indicator.fit(frontage_train)\n",
    "\n",
    "# (4) Transform frontage_train and assign the result\n",
    "# to frontage_missing_train\n",
    "frontage_missing_train = missing_indicator.transform(frontage_train)\n",
    "\n",
    "# Visually inspect frontage_missing_train\n",
    "frontage_missing_train\n",
    "\n",
    "#repeated several times.  see notebook for detail.\n",
    "\n",
    "'''neat convention:\n",
    "    The .categories_ attribute of OrdinalEncoder is only present once the .fit method has been called. (The trailing _ indicates this convention.)'''\n",
    "'''lots of useful workflow from video'''\n",
    "'''https://github.com/ericthansen/ds-multiple_linear_regression-nbz32/blob/main/multiple_linear_regression.ipynb'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactions\n",
    "#In statistics, an interaction is a particular property of three or more variables, where two or more variables interact in a non-additive manner when affecting a third variable. In other words, the two variables interact to have an effect that is more (or less) than the sum of their parts.\n",
    "#sometimes there are \"confounding factors\" - ie new paramters that account for otherwise unpredictable behavior\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv('auto-mpg.csv') \n",
    "\n",
    "\n",
    "acc = data['acceleration']\n",
    "logdisp = np.log(data['displacement'])\n",
    "loghorse = np.log(data['horsepower'])\n",
    "logweight= np.log(data['weight'])\n",
    "\n",
    "scaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\n",
    "scaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\n",
    "scaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\n",
    "scaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n",
    "\n",
    "data_fin = pd.DataFrame([])\n",
    "data_fin['acc']= scaled_acc\n",
    "data_fin['disp']= scaled_disp\n",
    "data_fin['horse'] = scaled_horse\n",
    "data_fin['weight'] = scaled_weight\n",
    "mpg = data['mpg']\n",
    "data_fin = pd.concat([mpg, data_fin, data['cylinders'], data['model year'], data['origin']], axis=1)\n",
    "y = data_fin[['mpg']]\n",
    "X = data_fin.drop(['mpg'], axis=1)\n",
    "\n",
    "regression = LinearRegression()\n",
    "crossvalidation = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "baseline = np.mean(cross_val_score(regression, X, y, scoring='r2', cv=crossvalidation))\n",
    "baseline\n",
    "\n",
    "'''See how we built a baseline model using some log-transformed predictors and some categorical predictors. We didn't properly convert the categorical variables to categorical yet, which we should do in the end, but we want to start with a baseline model and a baseline  𝑅2  just to get a sense of what a baseline model looks like.'''\n",
    "'''to see how horsepower and origin work together...'''\n",
    "origin_1 = data_fin[data_fin['origin'] == 1]\n",
    "origin_2 = data_fin[data_fin['origin'] == 2]\n",
    "origin_3 = data_fin[data_fin['origin'] == 3]\n",
    "origin_1.head()\n",
    "regression_1 = LinearRegression()\n",
    "regression_2 = LinearRegression()\n",
    "regression_3 = LinearRegression()\n",
    "\n",
    "horse_1 = origin_1['horse'].values.reshape(-1, 1)\n",
    "horse_2 = origin_2['horse'].values.reshape(-1, 1)\n",
    "horse_3 = origin_3['horse'].values.reshape(-1, 1)\n",
    "\n",
    "regression_1.fit(horse_1, origin_1['mpg'])\n",
    "regression_2.fit(horse_2, origin_2['mpg'])\n",
    "regression_3.fit(horse_3, origin_3['mpg'])\n",
    "\n",
    "# Make predictions using the testing set\n",
    "pred_1 = regression_1.predict(horse_1)\n",
    "pred_2 = regression_2.predict(horse_2)\n",
    "pred_3 = regression_3.predict(horse_3)\n",
    "\n",
    "# The coefficients\n",
    "print(regression_1.coef_)\n",
    "print(regression_2.coef_)\n",
    "print(regression_3.coef_)\n",
    "\n",
    "\n",
    "'''look at the plots'''\n",
    "# Plot outputs\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.scatter(horse_1, origin_1['mpg'],  color='blue', alpha=0.3, label = 'origin = 1')\n",
    "plt.scatter(horse_2, origin_2['mpg'],  color='red', alpha=0.3, label = 'origin = 2')\n",
    "plt.scatter(horse_3, origin_3['mpg'],  color='orange', alpha=0.3, label = 'origin = 3')\n",
    "\n",
    "plt.plot(horse_1, pred_1, color='blue', linewidth=2)\n",
    "plt.plot(horse_2, pred_2, color='red', linewidth=2)\n",
    "plt.plot(horse_3, pred_3, color='orange', linewidth=2)\n",
    "plt.ylabel('mpg')\n",
    "plt.xlabel('horsepower')\n",
    "plt.legend();\n",
    "'''looking at these plots, since they are parallel, no need for interaction(multiplicative) effect'''\n",
    "'''so how would we do it, if we needed it?'''\n",
    "regression = LinearRegression()\n",
    "crossvalidation = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "X_interact = X.copy()\n",
    "X_interact['horse_origin'] = X['horse'] * X['origin']\n",
    "\n",
    "interact_horse_origin = np.mean(cross_val_score(regression, X_interact, y, scoring='r2', cv=crossvalidation))\n",
    "interact_horse_origin\n",
    "\n",
    "#by including this interaction, we bump up the R^2 score from .83 to .84, about 1%, good!\n",
    "\n",
    "\n",
    "#can do similar thing with other pairs\n",
    "https://github.com/ericthansen/dsc-interaction-terms\n",
    "    \n",
    "#The lab on this:\n",
    "https://github.com/ericthansen/dsc-interaction-terms-lab\n",
    "#    with some workflow examples.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yield.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-de405503d6f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0myld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'yield.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\s+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0myld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#separating\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yield.csv'"
     ]
    }
   ],
   "source": [
    "#polynomial regression\n",
    "#an example with one predictor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "yld = pd.read_csv('yield.csv', sep='\\s+', index_col=0)\n",
    "yld.head()\n",
    "#separating\n",
    "y = yld['Yield']\n",
    "X = yld.drop(columns='Yield', axis=1)\n",
    "plt.scatter(X, y, color='green')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "#looks like no linear relationship, let's plot just to see\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X, y)\n",
    "plt.scatter(X, y, color='green')\n",
    "plt.plot(X, reg.predict(X))\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "#and get an r^2 value\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mean_squared_error(y, reg.predict(X))\n",
    "r2_score(y, reg.predict(X)) #.08\n",
    "#yep, not great.\n",
    "\n",
    "#polynomials can give a better fit\n",
    "X['Temp_sq'] = X['Temp']**2\n",
    "X.head()\n",
    "reg_q = LinearRegression().fit(X, y)\n",
    "plt.scatter(X['Temp'], y, color='green')\n",
    "plt.plot(X['Temp'], reg_q.predict(X))\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "mean_squared_error(y, reg_q.predict(X)) #.04\n",
    "r2_score(y, reg_q.predict(X)) #.69\n",
    "\n",
    "#note the model generates a smooth curve, not just a piecewise linear thing like in graph\n",
    "import numpy as np\n",
    "plt.scatter(X['Temp'], y, color='green')\n",
    "\n",
    "X_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=['Temp'])\n",
    "X_pred['Temp_sq'] = X_pred**2 \n",
    "y_pred = reg_q.predict(X_pred)\n",
    "\n",
    "plt.plot(X_pred['Temp'], y_pred)\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "\n",
    "#not just limited to quadratics!\n",
    "#How about order 6\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "y = yld['Yield']\n",
    "X = yld.drop(columns='Yield', axis=1)\n",
    "\n",
    "poly = PolynomialFeatures(6)\n",
    "X_fin = poly.fit_transform(X)\n",
    "\n",
    "print('The transformed feature names are: {}'.format(poly.get_feature_names()))\n",
    "print('------------------')\n",
    "print('The first row of transformed data is: {}'.format(X_fin[0]))\n",
    "\n",
    "#now you can fit a linear regression to the various powers columns\n",
    "reg_poly = LinearRegression().fit(X_fin, y)\n",
    "X_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=['Temp'])\n",
    "\n",
    "X_linspace_fin = poly.fit_transform(X_linspace)\n",
    "y_poly_pred = reg_poly.predict(X_linspace_fin)\n",
    "plt.scatter(X['Temp'], y, color='green')\n",
    "plt.plot(X_linspace, y_poly_pred)\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "\n",
    "mean_squared_error(y, reg_poly.predict(X_fin)) #0.03\n",
    "\n",
    "r2_score(y, reg_poly.predict(X_fin))#.759\n",
    "\n",
    "\n",
    "# and this is great workflow\n",
    "https://flatiron.illumidesk.com/user/ericthansen/notebooks/dsc-polynomial-regression-lab/index.ipynb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bias/variance trade-off\n",
    "https://flatiron.illumidesk.com/user/ericthansen/notebooks/dsc-bias-variance-trade-off/index.ipynb\n",
    "'''    You will be able to:\n",
    "\n",
    "Describe the bias-variance tradeoff in machine learning\n",
    "Discuss how bias and variance are related to over and underfitting\n",
    "List the three components of error'''\n",
    "\n",
    "#one reason for train/test split is to keep honest about overfitting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "yld = pd.read_csv('yield2.csv', sep='\\s+', index_col = 0)\n",
    "\n",
    "print(yld.head())\n",
    "y = yld['Yield']\n",
    "X = yld['Temp']\n",
    "\n",
    "plt.scatter(X, y, color = 'green')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "#this creates a parabolic dist.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=34567)\n",
    "reg = LinearRegression().fit(X_train.values.reshape(-1, 1), y_train)\n",
    "#simple linreg makes a bad fit for train and test\n",
    "# Plot the simple model\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, color='green')\n",
    "plt.plot(X_train.values.reshape(-1, 1), reg.predict(X_train.values.reshape(-1, 1)))\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='green')\n",
    "plt.plot(X_test.values.reshape(-1, 1), reg.predict(X_test.values.reshape(-1, 1)))\n",
    "plt.xlabel('Temperature');\n",
    "\n",
    "#but if we include polynomials, it gets better.  below, they use degree 6 (probably too much)\n",
    "# 6th degree polynomial\n",
    "poly = PolynomialFeatures(6)\n",
    "X_fin = poly.fit_transform(X_train.values.reshape(-1, 1))\n",
    "reg_poly = LinearRegression().fit(X_fin, y_train)\n",
    "'''Let's formalize this:\n",
    "\n",
    "Underfitting happens when a model cannot learn the training data, nor can it generalize to new data.\n",
    "\n",
    "The simple linear regression model fitted earlier was an underfit model.\n",
    "\n",
    "\n",
    "Overfitting happens when a model learns the training data too well. In fact, so well that it is not generalizeable to new data'''\n",
    "#the linear reg was underfit; the 6degree was overfit\n",
    "\n",
    "# 2nd degree polynomial\n",
    "poly = PolynomialFeatures(2)  \n",
    "X_fin = poly.fit_transform(X_train.values.reshape(-1, 1))\n",
    "reg_poly = LinearRegression().fit(X_fin, y_train)\n",
    "X_linspace_fin = poly.fit_transform(X_linspace)\n",
    "y_poly_pred = reg_poly.predict(X_linspace_fin)\n",
    "# Plot 2nd degree polynomial fit\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, color='green')\n",
    "plt.plot(X_linspace, y_poly_pred)\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='green')\n",
    "plt.plot(X_linspace, y_poly_pred);\n",
    "\n",
    "X_fin_test = poly.fit_transform(X_test.values.reshape(-1, 1))\n",
    "y_pred = reg_poly.predict(X_fin_test)\n",
    "mean_squared_error(y_test, y_pred)#.06\n",
    "#training result is worse but test is improved\n",
    "\n",
    "#bias-variance tradeoff\n",
    "'''Another perspective on this problem of overfitting versus underfitting is the bias-variance tradeoff. The idea is that We can decompose the mean squared error as the sum of:\n",
    "\n",
    "bias\n",
    "variance, and\n",
    "irreducible error'''\n",
    "# https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation\n",
    "# https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\n",
    "\n",
    "\n",
    "#lab, which doesn't make a ton of sense...\n",
    "https://github.com/ericthansen/dsc-bias-variance-trade-off-lab\n",
    "    \n",
    "    \n",
    "#and the full complete process lab\n",
    "https://github.com/ericthansen/dsc-linear-regression-lab\n",
    "#    (includes fun correlation heatmap, with just lowerleft triangle and numbers!)\n",
    "sns.heatmap(df.corr() #just the basic version\n",
    "            #if that's not so helpful..\n",
    "df.corr()['metric'].map(abs).sort_values(ascending=False)\n",
    "\n",
    "    \n",
    "#One handy little trick to get only numeric columns...\n",
    "X_train.select_dtypes(['int64', 'float64'])\n",
    "#When using statsmodels, summary shows a \"cond. no.\":\n",
    "#A condition number of 10-30 indicates multicollinearity, and a condition number above 30 indicates strong multicollinearity. This print-out shows a condition number of 2.77e+03, i.e. 2770, which is well above 30.\n",
    "\n",
    "\n",
    "'''Selecting Features with sklearn.feature_selection\n",
    "Let's try a different approach. Scikit-learn has a submodule called feature_selection that includes tools to help reduce the feature set.\n",
    "\n",
    "We'll use RFECV (documentation here). \"RFE\" stands for \"recursive feature elimination\", meaning that it repeatedly scores the model, finds and removes the feature with the lowest \"importance\", then scores the model again. If the new score is better than the previous score, it continues removing features until the minimum is reached. \"CV\" stands for \"cross validation\" here, and we can use the same splitter we have been using to test our data so far.'''\n",
    "\n",
    "'''Investigating Multicollinearity (Independence Assumption)\n",
    "Another way to measure multicollinearity is with variance inflation factor (StatsModels documentation here). A \"rule of thumb\" for VIF is that 5 is too high (i.e. strong multicollinearity).\n",
    "\n",
    "Run the code below to find the VIF for each feature.'''\n",
    "# Run this cell without changes\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = [variance_inflation_factor(X_train_final.values, i) for i in range(X_train_final.shape[1])]\n",
    "pd.Series(vif, index=X_train_final.columns, name=\"Variance Inflation Factor\")\n",
    "\n",
    "\n",
    "#A video on model validation, includes \n",
    "#balancing bias and variance, Train-Test split, overfitting, underfitting, k-fold cross validation,\n",
    "https://github.com/ericthansen/ds-model_validation-nbz32\n",
    "    \n",
    "#Feature selection and engineering\n",
    "''' Topics covered in this lecture include: model selection, correlation and multicollinearity, Recursive Feature Elimination, products of features, polynomial features, Variance Inflation Factors, model condition number, exploratory data analysis'''\n",
    "https://github.com/ericthansen/ds-feature_selection_and_feature_engineering-nbz32\n",
    "'''underscored attributes of models dont exist until you've fit the model'''\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''appendix stuff on uniform distribution, poisson dist, '''\n",
    "https://github.com/ericthansen/dsc-uniform-distribution\n",
    "https://github.com/ericthansen/dsc-poisson-distribution\n",
    "import numpy as np\n",
    "from math import factorial\n",
    "def poisson_probability(lambd, x):\n",
    "return lambd**x * np.exp(1)**(-lambd)/factorial(x)\n",
    "\n",
    "https://github.com/ericthansen/dsc-exponential-distribution\n",
    "import numpy as np\n",
    "\n",
    "def exp_pdf(mu, x):\n",
    "    decay_rate = 1 / mu\n",
    "    return decay_rate * np.exp(-decay_rate * x)\n",
    "    \n",
    "def exp_cdf(mu, x):\n",
    "    decay_rate = 1 / mu\n",
    "    return 1 - np.exp(-decay_rate * x)\n",
    "\n",
    "#Monte Carlo Simulations\n",
    "https://github.com/ericthansen/dsc-monte-carlo-simulations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-obtaining-your-data-lab\n",
    "#pre-lab EDL:\n",
    "#some more sql refreshers/tricks\n",
    "# Your code here\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "####THIS is very useful to see all the tables in a db###\n",
    "# Create a connection\n",
    "con = sqlite3.connect('lego.db')\n",
    "# Create a cursor\n",
    "cur = con.cursor()\n",
    "# Select some data\n",
    "cur.execute(\"\"\"SELECT name FROM sqlite_master\n",
    "            WHERE type='table'\n",
    "            ORDER BY name;\n",
    "            \"\"\")\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df.columns = [i[0] for i in cur.description]\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"SELECT * \n",
    "                FROM product_details\n",
    "                JOIN product_info \n",
    "                USING(prod_id)\n",
    "                JOIN product_pricing\n",
    "                USING(prod_id)\n",
    "                JOIN product_reviews\n",
    "                USING(prod_id);\n",
    "            \"\"\")\n",
    "df4 = pd.DataFrame(cur.fetchall())\n",
    "df4.columns = [i[0] for i in cur.description]\n",
    "print(df.shape)\n",
    "df4.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-965048325fad>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-965048325fad>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    https://github.com/ericthansen/dsc-scrubbing-and-cleaning-data\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#scrubbing and cleaning data review\n",
    "https://github.com/ericthansen/dsc-scrubbing-and-cleaning-data\n",
    "'''You will be able to:\n",
    "\n",
    "Cast columns to appropriate data types\n",
    "Identify and deal with null values appropriately\n",
    "Remove unnecessary columns'''\n",
    "\n",
    "\n",
    "'''A first step to uncover and investigate such issues is to use the .info() method available for all Pandas DataFrames. This will tell what type of data each column contains, as well as the number of values contained within that column (which can also help us identify columns that contain missing data)! Here's an example response:'''\n",
    "'''From here, a good next step would be to look at examples from each column encoded as strings (remember, Pandas refers to string columns as object) and confirm that this data is supposed to be encoded as strings. One method to do this is to preview a truncated version of the output from .value_counts(). For example, you could preview the 5 most frequent entries from each column with a simple loop like this:'''\n",
    "\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        print(col, df[col].value_counts()[:5])\n",
    "    except:\n",
    "        print(col, df[col].value_counts())\n",
    "        # If there aren't 5+ unique values for a column the first print statement\n",
    "        # will throw an error for an invalid idx slice\n",
    "    print('\\n') # Break up the output between columns\n",
    "    \n",
    "'''If you've identified numeric data encoded as strings, it's typically a pretty easy problem to solve. Often it's as simple as casting the string data to a numeric type:\n",
    "'''\n",
    "df['numeric_string_col'] = df['numeric_string_col'].astype('float')\n",
    "'''Sadly, it's not always that simple. For example, if there is even a single cell that contains a letter or non-numeric character such as a comma or monetary symbol ($) the above statement will fail. In such cases, a more complex cleaning function must be manually created. This could involve stripping extraneous symbols such as ',$/%', or simply casting non convertible strings as null. Recall that when NumPy sees multiple data types in an array, it defaults to casting everything as a string. If you try to cast a column from string to numeric data types and get an error, consider checking the unique values in that column -- it's likely that you may have a single letter hiding out somewhere that needs to be removed!'''\n",
    "\n",
    "\n",
    "##scrubbing and cleaning lab\n",
    "https://github.com/ericthansen/dsc-scrubbing-and-cleaning-data-lab\n",
    "    #goes through the main workflow, cleaning, etc, for numeric and categoricals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring your data\n",
    "#hist plots are cool\n",
    "#joint plots are also quite cool\n",
    "sns.jointplot(x= <column>, y= <column>, data=<dataset>, kind='reg')\n",
    "# https://seaborn.pydata.org/generated/seaborn.jointplot.html\n",
    "\n",
    "#exploring your data lab\n",
    "# https://github.com/ericthansen/dsc-exploring-your-data-lab\n",
    "\n",
    "#select just numeric columns\n",
    "result = df.select_dtypes(include='number')p\n",
    "\n",
    "#used joinplot\n",
    "sns.jointplot(x= 'piece_count', y= 'list_price', data=df, kind='reg')\n",
    "\n",
    "#did corr matrix\n",
    "df[numeric].corr()\n",
    "#used heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create a df with the target as the first column,\n",
    "# then compute the correlation matrix\n",
    "##heatmap_data = pd.concat([y_train, X_train], axis=1)\n",
    "##corr = heatmap_data.corr()\n",
    "corr = df[numeric].corr()\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, ax = plt.subplots(figsize=(5, 8))\n",
    "\n",
    "# Plot a heatmap of the correlation matrix, with both\n",
    "# numbers and colors indicating the correlations\n",
    "sns.heatmap(\n",
    "    # Specifies the data to be plotted\n",
    "    data=corr,\n",
    "    # The mask means we only show half the values,\n",
    "    # instead of showing duplicates. It's optional.\n",
    "    mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "    # Specifies that we should use the existing axes\n",
    "    ax=ax,\n",
    "    # Specifies that we want labels, not just colors\n",
    "    annot=True,\n",
    "    # Customizes colorbar appearance\n",
    "    cbar_kws={\"label\": \"Correlation\", \"orientation\": \"horizontal\", \"pad\": .2, \"extend\": \"both\"}\n",
    ")\n",
    "\n",
    "# Customize the plot appearance\n",
    "ax.set_title(\"Heatmap of Correlation Between Attributes (Including Target)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling your data\n",
    "# https://github.com/ericthansen/dsc-modeling-your-data\n",
    "\n",
    "### *** come back here ***9/6/21\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "df = pd.read_excel('mpg excercise.xls')\n",
    "df.head()\n",
    "# Define the problem\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Length', 'Wheelbase', 'Width', 'U_Turn_Space',\n",
    "          'Rear_seat', 'Luggage', 'Weight', 'Horsepower', 'Fueltank']\n",
    "# Some brief preprocessing\n",
    "df.columns = [col.replace(' ', '_') for col in df.columns]\n",
    "for col in x_cols:\n",
    "    df[col] = (df[col] - df[col].mean())/df[col].std()\n",
    "df.head()\n",
    "from statsmodels.formula.api import ols\n",
    "# Fitting the actual model\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=df).fit()\n",
    "model.summary()\n",
    "'''Assessing the Model\n",
    "When performing an initial assessment of the model you might focus on a number of different perspectives. There are metrics assessing the overall accuracy of the model including  𝑟2  and mean square error. There are also many metrics when analyzing how various features contribute to the overall model. These are essential to building a story and intuition behind the model so that educated business strategies can be implemented to optimize the target variable. After all, typically you aren't solely interested in predicting a quantity in a black box given said information. Rather, you would often like to know the underlying influencers and how those can be adjusted in order to increase or decrease the final measured quantity whether it be sales, customer base, costs, or risk. Such metrics would include p-values associated with the various features, comparing models with features removed and investigating potential multicollinearity in the model. Multicollinearity also touches upon checking model assumptions. One underlying intuition motivating the regression model is that the features constitute a set of levers which, if appropriately adjusted, account for the target variable. The theory then goes that the errors should be simply the cause of noise in our measurements, or smaller unaccounted factors. These errors are then assumed to be normally distributed.\n",
    "\n",
    "Comments on P-Values\n",
    "Based on the p-values above, you can see that there are a number of extraneous features. Recall that a common significance cutoff is 0.05. The refined model should eliminate these irrelevant features.'''\n",
    "#Initial Refinement\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Wheelbase', 'Weight', 'Fueltank']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=df).fit()\n",
    "model.summary()\n",
    "#checking for multicollinearity\n",
    "'''While you've examined the bi-variable relations previously by examining pair-wise correlation between features, you haven't fully accounted for multicollinearity which is a relation of 3 or more variables. One test for this is the variance inflation factor. Typically, variables with a vif of 5 or greater (or more definitively 10 or greater) are displaying multicollinearity with other variables in the feature set. We we'll check this here:'''\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X = df[x_cols]\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "list(zip(x_cols, vif)) #variables with vif of 5 or greater display multicollinearity\n",
    "#\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Wheelbase', 'Fueltank']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=df).fit()\n",
    "model.summary()\n",
    "#\n",
    "X = df[x_cols]\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "list(zip(x_cols, vif))\n",
    "'''results:\n",
    "[('Passengers', 1.972330344357365),\n",
    " ('Wheelbase', 5.743165022553869),\n",
    " ('Weight', 9.016035842933373),\n",
    " ('Fueltank', 5.032060527995974)]'''\n",
    "'''Comment: While the p-values indicate that all of the current features are impactful, the variance inflation factor indicates that there is moderate multicollinearity between our variables. With that, it makes sense to briefly update the features once again and recheck for multicollinearity.'''\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Wheelbase', 'Fueltank']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=df).fit()\n",
    "model.summary()\n",
    "#we removed the category with highest VIF (close to 10)\n",
    "X = df[x_cols]\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "list(zip(x_cols, vif))\n",
    "'''[('Passengers', 1.955034462110378),\n",
    " ('Wheelbase', 3.567043045106437),\n",
    " ('Fueltank', 2.378966703427496)]'''\n",
    "'''doing so, we reduce the Rsquared of the model, but reduce multicollinearity'''\n",
    "'''for now, we will return to the original model'''\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Wheelbase', 'Weight', 'Fueltank']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=df).fit()\n",
    "model.summary()\n",
    "####Checking for normality\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)\n",
    "'''looks pretty good but maybe variation at the tails'''\n",
    "\n",
    "#Check for homoscedasticity (and avoid heteroscedasticity)\n",
    "plt.scatter(model.predict(df[x_cols]), model.resid)\n",
    "plt.plot(model.predict(df[x_cols]), [0 for i in range(len(df))])\n",
    "'''some outliers have concerning variation; may want to identify and handle those outliers?'''\n",
    "\n",
    "#model refinement 3\n",
    "'''Due to the particularly large errors visible above ~37MPG, it's reasonable to remove these outliers and retrain the model on the remaining subset. While the model will be specific to this subset, it could prove to be more accurate and reflective of the general domain.'''\n",
    "#Finding a cutoff point\n",
    "for i in range(90, 99):\n",
    "    q = i / 100\n",
    "    print('{} percentile: {}'.format(q, df['MPG_Highway'].quantile(q=q)))\n",
    "\n",
    "#remove the outliers!  This is worth returning to\n",
    "subset = df[df['MPG_Highway'] < 38]\n",
    "print('Percent removed:',(len(df) - len(subset))/len(df))\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Wheelbase', 'Weight', 'Fueltank']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=subset).fit()\n",
    "model.summary()\n",
    "#rechecking normality\n",
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)\n",
    "#rechecking homosced\n",
    "plt.scatter(model.predict(subset[x_cols]), model.resid)\n",
    "plt.plot(model.predict(subset[x_cols]), [0 for i in range(len(subset))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blogging\n",
    "https://github.com/ericthansen/dsc-blogging-overview\n",
    "#project 2 submission\n",
    "https://github.com/ericthansen/dsc-project-submissions-online\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for process functionization:\n",
    "df.date =  pd.to_datetime(df['date'])\n",
    "df.loc[df.yr_renovated==0,'yr_renovated'] = df.yr_built\n",
    "target = 'price'\n",
    "X = df.drop(target, axis = 1)\n",
    "y = df[target]\n",
    "SPLIT_IS_RANDOM = True\n",
    "if SPLIT_IS_RANDOM:\n",
    "    random_state = randint(1,2**32 - 2)\n",
    "    #print(random_state)\n",
    "else:\n",
    "    random_state = 14\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = random_state)\n",
    "#do missing col fixin here#\n",
    "#X_train['yr_renovated']\n",
    "X_train[X_train['yr_renovated']==0][col]  = X_train[X_train['yr_renovated']==0]['yr_built']\n",
    "\n",
    "dropped_cols = ['id',]\n",
    "num_cols = ['date',  'sqft_living', 'sqft_lot',\n",
    "        'condition', 'grade', 'sqft_above',\n",
    "       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n",
    "       'sqft_living15', 'sqft_lot15',\n",
    "            'bedrooms', 'bathrooms',\n",
    "       ]\n",
    "#originally put bedrooms/bathrooms as categoricals...but moved back up to numeric\n",
    "cat_cols = ['floors', 'waterfront', 'view', 'waterfront_Missing', 'view_Missing','yr_renovated_Missing']\n",
    "for col in cat_cols:\n",
    "    col_train = X_train[[col]]\n",
    "    ohe = OneHotEncoder(categories='auto', sparse=False, drop='first', handle_unknown='error')\n",
    "    ohe.fit(col_train)\n",
    "    if verbose:\n",
    "        display(col, ohe.categories_)\n",
    "    # (4) Transform fireplace_qu_train using the encoder and\n",
    "    # assign the result to fireplace_qu_encoded_train\n",
    "    col_train_encoded = ohe.transform(col_train)\n",
    "    \n",
    "    if verbose:\n",
    "        pass\n",
    "        #display('featurenames:', ohe.get_feature_names([col]))\n",
    "    \n",
    "\n",
    "    # Visually inspect fireplace_qu_encoded_train\n",
    "    #fireplace_qu_encoded_train\n",
    "    \n",
    "    col_train_encoded = pd.DataFrame(\n",
    "        # Pass in NumPy array\n",
    "        col_train_encoded,\n",
    "        # Set the column names to the categories found by OHE\n",
    "        #columns=ohe.categories_[0][1:], #old and busted; instead use this cool thing:\n",
    "        columns = ohe.get_feature_names([col]),\n",
    "        # Set the index to match X_train's index\n",
    "        index=X_train.index\n",
    "    )\n",
    "    # (5b) Drop original FireplaceQu column\n",
    "    X_train.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # (5c) Concatenate the new dataframe with current X_train\n",
    "    X_train = pd.concat([X_train, col_train_encoded], axis=1)\n",
    "X_train = X_train.drop('date', axis = 1)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "cross_val_score(model, X_train, y_train, cv=10)\n",
    "\n",
    "\n",
    "#######################\n",
    "#Preprocess Test data\n",
    "#waterfront, view, yr_renovated\n",
    "\n",
    "# Add missing indicator for lot frontage\n",
    "\n",
    "\n",
    "\n",
    "wf_test = X_test[[\"waterfront\"]]\n",
    "wf_missing_test = missing_indicator1.transform(wf_test)\n",
    "X_test[\"waterfront_Missing\"] = wf_missing_test\n",
    "# Impute missing lot frontage values\n",
    "wf_imputed_test = imputer1.transform(wf_test)\n",
    "X_test[\"waterfront\"] = wf_imputed_test\n",
    "\n",
    "view_test = X_test[[\"view\"]]\n",
    "view_missing_test = missing_indicator2.transform(view_test)\n",
    "X_test[\"view_Missing\"] = view_missing_test\n",
    "# Impute missing lot frontage values\n",
    "view_imputed_test = imputer2.transform(view_test)\n",
    "X_test[\"view\"] = view_imputed_test\n",
    "\n",
    "yr_test = X_test[[\"yr_renovated\"]]\n",
    "yr_missing_test = missing_indicator3.transform(yr_test)\n",
    "X_test[\"yr_renovated_Missing\"] = yr_missing_test\n",
    "# Impute missing lot frontage values\n",
    "yr_imputed_test = imputer3.transform(yr_test)\n",
    "X_test[\"yr_renovated\"] = yr_imputed_test\n",
    "\n",
    "# Check that there are no more missing values\n",
    "X_test.isna().sum()\n",
    "\n",
    "X_test[X_test['yr_renovated']==0][col]  = X_test[X_test['yr_renovated']==0]['yr_built']\n",
    "\n",
    "for col in cat_cols:\n",
    "    col_train = X_test[[col]]\n",
    "    ohe = OneHotEncoder(categories='auto', sparse=False, drop='first', handle_unknown='error')\n",
    "    ohe.fit(col_train)\n",
    "    if verbose:\n",
    "        display(col, ohe.categories_)\n",
    "    # (4) Transform fireplace_qu_train using the encoder and\n",
    "    # assign the result to fireplace_qu_encoded_train\n",
    "    col_train_encoded = ohe.transform(col_train)\n",
    "    \n",
    "    if verbose:\n",
    "        pass\n",
    "        #display('featurenames:', ohe.get_feature_names([col]))\n",
    "    \n",
    "\n",
    "    # Visually inspect fireplace_qu_encoded_train\n",
    "    #fireplace_qu_encoded_train\n",
    "    \n",
    "    col_train_encoded = pd.DataFrame(\n",
    "        # Pass in NumPy array\n",
    "        col_train_encoded,\n",
    "        # Set the column names to the categories found by OHE\n",
    "        #columns=ohe.categories_[0][1:], #old and busted; instead use this cool thing:\n",
    "        columns = ohe.get_feature_names([col]),\n",
    "        # Set the index to match X_train's index\n",
    "        index=X_test.index\n",
    "    )\n",
    "    # (5b) Drop original FireplaceQu column\n",
    "    X_test.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # (5c) Concatenate the new dataframe with current X_train\n",
    "    X_test = pd.concat([X_test, col_train_encoded], axis=1)\n",
    "\n",
    "#clean date\n",
    "X_test = X_test.drop('date', axis = 1)\n",
    "\n",
    "\n",
    "######################## fit and score\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functionize this too:\n",
    "col = 'waterfront'\n",
    "\n",
    "# (1) Identify data to be transformed\n",
    "# We only want missing indicators for column\n",
    "col1_train = X_train[[col]]\n",
    "\n",
    "# (2) Instantiate the transformer object\n",
    "missing_indicator1 = MissingIndicator()\n",
    "\n",
    "# (3) Fit the transformer object on col\n",
    "missing_indicator1.fit(col1_train)\n",
    "\n",
    "# (4) Transform col and assign the result\n",
    "# to col_missing_train\n",
    "col1_missing_train = missing_indicator1.transform(col1_train)\n",
    "\n",
    "# Visually inspect col_missing_train\n",
    "#col1_missing_train\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "# col_missing_train should be a NumPy array\n",
    "assert type(col1_missing_train) == np.ndarray\n",
    "\n",
    "# We should have the same number of rows as the full X_train\n",
    "assert col1_missing_train.shape[0] == X_train.shape[0]\n",
    "\n",
    "# But we should only have 1 column\n",
    "assert col1_missing_train.shape[1] == 1\n",
    "\n",
    "X_train[\"waterfront_Missing\"] = col1_missing_train\n",
    "#X_train\n",
    "\n",
    "\n",
    "# (1) col1_train was created previously, so we don't\n",
    "# need to extract the relevant data again\n",
    "\n",
    "# (2) Instantiate a SimpleImputer with strategy=\"median\"\n",
    "imputer1 = SimpleImputer(strategy='median')\n",
    "\n",
    "# (3) Fit the imputer on col_train\n",
    "imputer1.fit(col1_train)\n",
    "\n",
    "# (4) Transform frontage_train using the imputer and\n",
    "# assign the result to col_imputed_train\n",
    "col1_imputed_train = imputer1.transform(col1_train)\n",
    "\n",
    "# Visually inspect col_imputed_train\n",
    "display(col1_imputed_train)\n",
    "\n",
    "# Run this cell without changes\n",
    "\n",
    "# (5) Replace value of col\n",
    "X_train[\"waterfront\"] = col1_imputed_train\n",
    "\n",
    "# Visually inspect X_train\n",
    "# display(X_train)\n",
    "\n",
    "display(X_train.waterfront.value_counts())\n",
    "display(X_train.waterfront.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snazzy project tricks and visualizations\n",
    "# Price distribution\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.distplot(df['price'], bins=50, hist_kws=dict(edgecolor=\"blue\", linewidth=1))\n",
    "\n",
    "plt.ticklabel_format(style='plain')\n",
    "\n",
    "\n",
    "#scatter plot\n",
    "#A quick look at sqft vs price\n",
    "plt.figure(figsize=(9, 5))\n",
    "sns.scatterplot(x='sqft_living', y='price', data=df).set_title('Square Feet vs. Price')\n",
    "plt.ticklabel_format(style='plain')#style='plain', 'sci', 'scientific'\n",
    "\n",
    "#2 useful plots\n",
    "#Investigating bedrooms vs price\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].set_title('Bedrooms Count')\n",
    "sns.countplot(df['bedrooms'], ax=axes[0], color='purple')\n",
    "\n",
    "axes[1].set_title('Bedrooms vs. Price')\n",
    "sns.boxplot(x='bedrooms', y='price', data=df, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#latitude/longitude heatmap!\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "# Create basic Folium heat map\n",
    "price_map = folium.Map(location=[47.5112,-122.257], \n",
    "                       tiles = \"Stamen Terrain\",\n",
    "                      zoom_start = 9)\n",
    "\n",
    "# Add data for heatmp \n",
    "data_heatmap = df[['lat','long','price']]\n",
    "data_heatmap = df.dropna(axis=0, subset=['lat','long','price'])\n",
    "data_heatmap = [[row['lat'],row['long']] for index, row in data_heatmap.iterrows()]\n",
    "HeatMap(data_heatmap, radius=10, \n",
    "        gradient = {.35: 'purple',.55: 'blue',.68:'yello',.78:'red'}).add_to(price_map)\n",
    "# Plot!\n",
    "price_map\n",
    "\n",
    "#notes from intro-chat with Jeff Herman\n",
    "\n",
    "'''non-tech presentation and insights:\n",
    "    think about who audience is - buyers/sellers/flippers/realtors, etch?  to guide insights on EDA\n",
    "    are there months when avg house price is lower, months or weekend/weekdays?  \n",
    "    \n",
    "    for bedrooms - it's numeric, but not continuous - can use  features - only one-hot it if it doesn't seem to follow a line or have a numerical underlying meaning\n",
    "    \n",
    "    residutals should be nornal, hetroscedas, no-multicollinear\n",
    "    \n",
    "    \n",
    "    consider cutting off high price housers \n",
    "    \n",
    "    blog ideas around linear regression model - assumptions of lienar regression, how you test for them \n",
    "    scedactitycy, \n",
    "    multicollin, \n",
    "    how we validated model, train/test/cross validation, mse, \n",
    "    summary table in stats models. \n",
    "    could do recursive feature elim, but better to do through EDA \n",
    "    \n",
    "    at least 3 different models\n",
    "    \n",
    "    many get an r^2 between .55 and .75'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### SECTION 3!!!! ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-introduction-to-object-orientation\n",
    "#oop - python and ruby are oop.  haskell and clojure are \"functional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Note: By convention, you should use CamelCase to name the class. Also, you can\\'t create an \"empty\" class. At the least, you need to specify the pass keyword to ensure the class definition is syntactically valid.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/ericthansen/dsc-classes-and-instances\n",
    "# https://github.com/ericthansen/dsc-classes-and-instances-lab\n",
    "# from driver import Driver\n",
    "'''Note: By convention, you should use CamelCase to name the class. Also, you can't create an \"empty\" class. At the least, you need to specify the pass keyword to ensure the class definition is syntactically valid.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-instance-methods\n",
    "# https://github.com/ericthansen/dsc-instance-methods-lab\n",
    "# https://github.com/ericthansen/dsc-instance-variables-lab\n",
    "# https://github.com/ericthansen/dsc-oop-recap-v2-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-423b95e3bc61>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-423b95e3bc61>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/ericthansen/dsc-linalg-introduction\n",
    "# https://github.com/ericthansen/dsc-lingalg-motivation - this has good additional linalg links\n",
    "# https://github.com/ericthansen/dsc-lingalg-linear-equations\n",
    "# https://github.com/ericthansen/dsc-lingalg-linear-equations-quiz\n",
    "# https://github.com/ericthansen/dsc-scalars-vectors-matrices-tensors-codealong\n",
    "    # for regular numpy matrix:\n",
    "    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "    print(X)\n",
    "    # for matlab code: \n",
    "    Y = np.mat([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "    print(Y) \n",
    "    '''Numpy matrices are always 2-dimensional, while numpy arrays (also referred to as ndarrays) are N-dimensional. Matrix objects are a subclass of ndarray, so they inherit all the attributes and methods of ndarrays. For multidimensional arrays/matrices with more than 2 dimensions, it is always best to use arrays. Arrays are the standard vector/matrix/tensor type of NumPy, and most NumPy functions return arrays and not matrices.'''\n",
    "    print (X[0, 0]) # element at first row and first column\n",
    "    print (X[-1, -1]) # element from the last row and last column \n",
    "    print (X[0, :]) # first row and all columns\n",
    "    print (X[:, 0]) # all rows and first column \n",
    "    print (X[:]) # all rows and all columns\n",
    "    A_transposed = A.T\n",
    "    A_transposed_2 = np.transpose(A)\n",
    "    print(x.shape)\n",
    "\n",
    "# https://github.com/ericthansen/dsc-linalg-mat-multiplication-codealong\n",
    "    #hadamard product : elementwise product. uses a hollow circle dot\n",
    "    import numpy as np\n",
    "    A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    print(A)\n",
    "    B = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    print(B)\n",
    "    print ('\\nHadamard product\\n\\n', A * B)\n",
    "    \n",
    "    #dot product : dot prod.  really just regular row-col multiplication\n",
    "    A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "    B = np.array([[2, 7], [1, 2], [3, 6]])\n",
    "    C = A.dot(B)\n",
    "    print(A, '\\ndot', '\\n', B, '\\n = \\n', C)     \n",
    "    \n",
    "    # matrix-vector mult\n",
    "    A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "    v = np.array([0.5, 0.5])\n",
    "    C = A.dot(v)\n",
    "    \n",
    "    # Cross product between two vectors\n",
    "    x = np.array([0, 0, 1])\n",
    "    y = np.array([0, 1, 0])\n",
    "\n",
    "    print(np.cross(x, y))\n",
    "    print(np.cross(y, x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-lineq-numpy-codealong\n",
    "#identiy matrix yay\n",
    "import numpy as np\n",
    "A = np.array([[2,1],[3,4]])\n",
    "I = np.array([[1,0],[0,1]])\n",
    "print(I.dot(A))\n",
    "print('\\n', A.dot(I))\n",
    "#inverse matrix\n",
    "A = np.array([[4, 2, 1],[4, 8, 3],[1, 1, 0]])\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(A_inv) \n",
    "#you can solve a system of linear equations with inverse - ax = b, mult both sides by a^-1\n",
    "#But there are lots of reasons to NOT calculate the inverse\n",
    "#better to \"solve directly\"; eg. use numpy builtin\n",
    "# Use Numpy's built in function solve() to solve linear equations\n",
    "x = np.linalg.solve(A, B)\n",
    "# https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/\n",
    "\n",
    "\n",
    "#https://github.com/ericthansen/dsc-lineq-numpy-lab\n",
    "'''Exercise 3\n",
    "You want to make a soup containing tomatoes, carrots, and onions.\n",
    "\n",
    "Suppose you don't know the exact mix to put in, but you know there are 7 individual pieces of vegetables, and there are twice as many tomatoes as onions, and that the 7 pieces of vegetables cost 5.25 USD in total. You also know that onions cost 0.5 USD each, tomatoes cost 0.75 USD and carrots cost 1.25 USD each.\n",
    "\n",
    "Create a system of equations to find out exactly how many of each of the vegetables are in your soup.'''\n",
    "# Create and solve the relevant system of equations\n",
    "v = np.array([[.5, .75, 1.25],[1, 1, 1],[2,-1,0]])\n",
    "total = np.array([5.25,7,0])\n",
    "np.linalg.solve(v, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-linalg-regression-codealong\n",
    "# http://math.mit.edu/~gs/linearalgebra/ila0403.pdf\n",
    "#   https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.polyfit.html\n",
    "# Code here \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([1,2,2])\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()\n",
    "#for lin regression with polyfit:\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "# Fit with polyfit function to get c(intercept) and m(slope)\n",
    "# the degree parameter = 1 to models this as a straight line\n",
    "c, m = polyfit(x, y, 1)\n",
    "\n",
    "# Plot the data points and line calculated from ployfit\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, c + (m * x), '-')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()\n",
    "print(c, m)\n",
    "##\n",
    "X = np.array([[1, 1],[1, 2],[1, 3]])\n",
    "y = np.array([1, 2, 2])\n",
    "Xt = X.T\n",
    "XtX = Xt.dot(X)\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "Xty = Xt.dot(y)\n",
    "x_hat = XtX_inv.dot(Xty) # the value for b shown above\n",
    "x_hat\n",
    "\n",
    "# Define data points\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([1, 2, 2])\n",
    "\n",
    "# Plot the data points and line parameters calculated above\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, x_hat[0] + (x_hat[1] * x), '-')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##### https://github.com/ericthansen/dsc-linalg-regression-lab\n",
    "\n",
    "beta = np.linalg.inv((X_train.T).dot(X_train)).dot(X_train.T).dot(y_train)\n",
    "y_pred = []\n",
    "for row in X_test:\n",
    "    pred = row.dot(beta)\n",
    "    y_pred.append(pred)\n",
    "y_pred[:5]\n",
    "\n",
    "# Plot predicted and actual values as line plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.scatter(y_train, y_pred)\n",
    "# Calculate RMSE\n",
    "mse = 0\n",
    "for i in range(len(y_pred)):\n",
    "    mse += (y_pred[i] - y_train[i])**2\n",
    "mse /= len(y_pred)\n",
    "rmse = mse ** (1/2)\n",
    "rmse\n",
    "\n",
    "# Calculate NRMSE\n",
    "nrmse = rmse / (max(y_train) - min(y_train))\n",
    "nrmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-computational-complexity\n",
    "    # https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\n",
    "    # https://medium.com/karuna-sehgal/a-simplified-explanation-of-the-big-o-notation-82523585e835\n",
    "# https://github.com/ericthansen/dsc-linalg-section-recap\n",
    "\n",
    "# https://github.com/ericthansen/dsc-calculus-introduction\n",
    "# https://github.com/ericthansen/dsc-derivatives-intro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-derivatives-intro-lab\n",
    "import numpy as np\n",
    "#this seems to be a way to find derivatives of polynomial functions\n",
    "array_1 = np.array([[4, 2], [4, 1], [-10, 0]])\n",
    "np.shape(array_1)\n",
    "\n",
    "def term_output(array, input_value):\n",
    "    return array[0] * (input_value)**array[1]\n",
    "term_output(np.array([3, 2]), 2) # 12\n",
    "#\n",
    "def output_at(array_of_terms, x_value):\n",
    "    s = 0\n",
    "    for t in array_of_terms:\n",
    "        s += term_output(t, x_value)\n",
    "    return s\n",
    "\n",
    "array_3 = np.array([[3, 2], [-11, 0]])\n",
    "output_at(array_3, 2)\n",
    "# 1 \n",
    "\n",
    "##\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "x_values = np.linspace(-30, 30, 100)\n",
    "y_values = list(map(lambda x: output_at(array_3, x), x_values))\n",
    "\n",
    "plt.plot(x_values, y_values, label = \"3x^2 - 11\")\n",
    "\n",
    "ax.legend(loc=\"upper center\",fontsize='large')\n",
    "plt.show()\n",
    "\n",
    "####  For a linear function\n",
    "lin_function = np.array([[4, 1], [15, 0]])\n",
    "#graphing\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "x_values = np.linspace(0, 5, 100)\n",
    "y_values = list(map(lambda x: output_at(lin_function, x), x_values))\n",
    "plt.plot(x_values, y_values, label = \"4x + 15\")\n",
    "ax.legend(loc=\"upper center\",fontsize='large')\n",
    "plt.show()\n",
    "\n",
    "### now to do derivatives (this is a little janky, with delta_x not truly a limit)\n",
    "def delta_f(array_of_terms, x_value, delta_x):\n",
    "    return output_at(array_of_terms, x_value + delta_x) - output_at(array_of_terms, x_value)\n",
    "delta_f(lin_function, 2, 1) # 4\n",
    "# plotting\n",
    "x_value = 2\n",
    "delta_x = 1\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "x_values = np.linspace(0, 5, 100)\n",
    "y_values = list(map(lambda x: output_at(lin_function, x), x_values))\n",
    "\n",
    "plt.plot(x_values, y_values, label = \"4x + 15\")\n",
    "\n",
    "# delta x\n",
    "y_val = output_at(lin_function, x_value)\n",
    "hline_lab= 'delta x = ' + str(delta_x)\n",
    "plt.hlines(y=y_val, xmin= x_value, xmax= x_value + delta_x, color=\"lightgreen\", label = hline_lab)\n",
    "\n",
    "# delta f\n",
    "y_val_max = output_at(lin_function, x_value + delta_x)\n",
    "vline_lab =  'delta f = ' + str(y_val_max-y_val)\n",
    "plt.vlines(x = x_value + delta_x , ymin= y_val, ymax=y_val_max, color=\"darkorange\", label = vline_lab)\n",
    "ax.legend(loc='upper left', fontsize='large')\n",
    "\n",
    "plt.show()\n",
    "#deriv funct\n",
    "def derivative_of(array_of_terms, x_value, delta_x):\n",
    "    del_f = delta_f(array_of_terms, x_value, delta_x)\n",
    "    return del_f/delta_x\n",
    "derivative_of(lin_function, x_value, delta_x)\n",
    "# 4.0\n",
    "\n",
    "#now the tan line\n",
    "def tangent_line(array_of_terms, x_value, line_length = 4, delta_x = .01):\n",
    "    y = output_at(array_of_terms, x_value)\n",
    "    derivative_at = derivative_of(array_of_terms, x_value, delta_x)\n",
    "    \n",
    "    x_dev = np.linspace(x_value - line_length/2, x_value + line_length/2, 50)\n",
    "    tan = y + derivative_at *(x_dev - x_value)\n",
    "    return {'x_dev':x_dev, 'tan':tan, 'lab': \" f' (x) = \" + str(derivative_at)}\n",
    "tan_line = tangent_line(lin_function, 2, line_length = 2, delta_x = .1)\n",
    "tan_line\n",
    "#and plot it\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "x_values = np.linspace(0, 5, 100)\n",
    "y_values = list(map(lambda x: output_at(lin_function, x), x_values))\n",
    "\n",
    "plt.plot(x_values, y_values, label = \"4x + 15\")\n",
    "# tangent_line\n",
    "plt.plot(tan_line['x_dev'], tan_line['tan'], color = \"yellow\", label = tan_line['lab'])\n",
    "\n",
    "# delta x\n",
    "y_val = output_at(lin_function, x_value)\n",
    "hline_lab= 'delta x = ' + str(delta_x)\n",
    "plt.hlines(y=y_val, xmin= x_value, xmax= x_value + delta_x, color=\"lightgreen\", label = hline_lab)\n",
    "\n",
    "# delta f\n",
    "y_val_max = output_at(lin_function, x_value + delta_x)\n",
    "vline_lab =  'delta f = ' + str(y_val_max-y_val)\n",
    "plt.vlines(x = x_value + delta_x , ymin= y_val, ymax=y_val_max, color=\"darkorange\", label = vline_lab)\n",
    "ax.legend(loc='upper left', fontsize='large')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "####and now original function and f' side by side\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "x_values = np.linspace(0, 5, 100)\n",
    "function_values = list(map(lambda x: output_at(lin_function, x),x_values))\n",
    "derivative_values = list(map(lambda x: derivative_of(lin_function, x, delta_x), x_values))\n",
    "\n",
    "# plot 1\n",
    "plt.subplot(121)\n",
    "plt.plot(x_values, function_values, label = \"f (x)\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=[0, 1], ncol=2, fancybox=True)\n",
    "\n",
    "# plot 2\n",
    "plt.subplot(122)\n",
    "plt.plot(x_values, derivative_values,color=\"darkorange\", label = \"f '(x)\")\n",
    "plt.legend(loc=\"upper left\");\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-derivatives-of-non-linear-functions\n",
    "#derivatives?  wow! :)\n",
    "# https://github.com/ericthansen/dsc-rules-for-derivatives\n",
    " #obvs, i know this.  but here's some code\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def jog(miles):\n",
    "    return 6*miles\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5,5.5))\n",
    "\n",
    "x = np.linspace(0, 6, 7)\n",
    "c1= np.linspace(1,2,20)\n",
    "c2= np.linspace(4,5,20)\n",
    "\n",
    "plt.plot(x, jog(x), label = \"distance given # hours\", marker=\"|\", markersize=12)\n",
    "plt.plot(c1, jog(c1), label = \"slope = 6\", color=\"green\")\n",
    "plt.plot(c2, jog(c2), label = \"slope = 6\", color=\"red\")\n",
    "\n",
    "ax.legend(loc='upper left', fontsize='large')\n",
    "plt.ylabel(\"distance in miles\")\n",
    "plt.xlabel(\"number of hours\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#\n",
    "def f(x):\n",
    "    return np.square(x)\n",
    "fig, ax = plt.subplots(figsize=(9.5,6.5))\n",
    "\n",
    "x = np.linspace(-3, 6, 100)\n",
    "c1= np.linspace(1.5,2.5,20)\n",
    "c2= np.linspace(4.5,5.5,20)\n",
    "c3= np.linspace(-2.5,-1.5,20)\n",
    "\n",
    "plt.plot(x, f(x), label = \"distance given # seconds\")\n",
    "\n",
    "x_dev = np.linspace(1.5, 3.2, 100)\n",
    "a1 = 2\n",
    "a2 = 5\n",
    "a3 = -2\n",
    "delta_a=0.001\n",
    "fprime1 = (f(a1+delta_a)-f(a1))/delta_a \n",
    "fprime2 = (f(a2+delta_a)-f(a2))/delta_a \n",
    "fprime3 = (f(a3+delta_a)-f(a3))/delta_a \n",
    "\n",
    "tan1 = f(a1)+fprime1*(c1-a1)\n",
    "tan2 = f(a2)+fprime2*(c2-a2)\n",
    "tan3 = f(a3)+fprime3*(c3-a3)\n",
    "\n",
    "# plot of the function and the tangent\n",
    "plt.plot(c1, tan1, color = \"green\", label=\"slope = 4\")\n",
    "plt.plot(c2, tan2, color = \"red\", label=\"slope = 10\")\n",
    "plt.plot(c3, tan3, color = \"orange\", label=\"slope = -4\")\n",
    "\n",
    "ax.legend(loc='upper left', fontsize='large')\n",
    "\n",
    "plt.ylabel(\"distance in feet\")\n",
    "plt.xlabel(\"number of seconds\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#also, https://github.com/ericthansen/dsc-derivatives-conclusion\n",
    "from derivatives import *\n",
    "import numpy as np\n",
    "tuple_sq_pos  = np.array([[2, 2], [-8, 1]])\n",
    "x_values = np.linspace(-6, 10, 100)\n",
    "function_values = list(map(lambda x: output_at(tuple_sq_pos, x), x_values))\n",
    "derivative_values = list(map(lambda x: derivative_at(tuple_sq_pos, x),x_values))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "# plot 1\n",
    "plt.subplot(121)\n",
    "plt.axhline(y=0, color='lightgrey', )\n",
    "plt.axvline(x=0, color='lightgrey')\n",
    "plt.plot(x_values, function_values, label = \"f (x) = 2x^2−8x \")\n",
    "\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=[0, 1], ncol=2, fancybox=True)\n",
    "\n",
    "# plot 2\n",
    "plt.subplot(122)\n",
    "plt.axhline(y=0, color='lightgrey')\n",
    "plt.axvline(x=0, color='lightgrey')\n",
    "plt.plot(x_values, derivative_values,color=\"darkorange\", label = \"f '(x) = 4x-8\")\n",
    "\n",
    "ax.grid(True, which='both')\n",
    "\n",
    "plt.legend(loc=\"upper left\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-gradient-descent-intro\n",
    "# https://github.com/ericthansen/dsc-gradient-descent-step-sizes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "def regression_formula(x):\n",
    "    return 1.319 + 52*x\n",
    "\n",
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30,1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3+ 50* x + y_randterm\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x, y, '.b')\n",
    "plt.plot(x, regression_formula(x), '-')\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);\n",
    "\n",
    "\n",
    "table_sm = np.zeros((401,2))\n",
    "for idx, val in enumerate(np.linspace(40, 60, 401)):\n",
    "    table_sm[idx,0] = val\n",
    "    table_sm[idx,1] = residual_sum_squares(x, y, val, 1.319)\n",
    "def tan_line(start, stop, delta_a):\n",
    "    x_dev = np.linspace(start, stop, 100)\n",
    "    a = (start+stop)/2 \n",
    "    f_a= table_sm[(table_sm[:,0]==a),1]\n",
    "    rounded_a_delta_a = round(a+delta_a,2)\n",
    "    f_a_delta= table_sm[(table_sm[:,0]== (rounded_a_delta_a)),1]\n",
    "    fprime = (f_a_delta-f_a)/delta_a \n",
    "    tan = f_a+fprime*(x_dev-a)\n",
    "    return fprime, x_dev, tan\n",
    "fprime_1, x_dev_1, y_dev_1 = tan_line(41, 43.5, 0.05)\n",
    "fprime_2, x_dev_2,  y_dev_2 = tan_line(45, 48, 0.05)\n",
    "fprime_3, x_dev_3,  y_dev_3 = tan_line(49, 52, 0.05)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(table[:,0], table[:,1], '-')\n",
    "plt.plot(x_dev_1, y_dev_1, color = \"red\",  label = \"slope =\" + str(fprime_1))\n",
    "plt.plot(x_dev_2, y_dev_2, color = \"green\",  label = \"slope =\" + str(fprime_2))\n",
    "plt.plot(x_dev_3, y_dev_3, color = \"orange\", label = \"slope =\" + str(fprime_3))\n",
    "\n",
    "plt.xlabel(\"m-values\", fontsize=14)\n",
    "plt.ylabel(\"RSS\", fontsize=14)\n",
    "plt.legend(loc='upper right', fontsize='large')\n",
    "\n",
    "plt.title(\"RSS with changes to slope\", fontsize=16);\n",
    "\n",
    "# https://github.com/ericthansen/dsc-gradient-descent-step-sizes-lab\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(225)\n",
    "###setup and first approx\n",
    "x = np.random.rand(30, 1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3 + 50*x + y_randterm\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, marker=\".\", c=\"b\")\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"y\", fontsize=14)\n",
    "fig.suptitle(\"Revenues\");\n",
    "def regression_formula(x):\n",
    "    return 43*x + 12\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, marker=\".\", c=\"b\")\n",
    "ax.plot(x, regression_formula(x), color=\"orange\", label=r'$y = 43x + 12$')\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"y\", fontsize=14)\n",
    "fig.suptitle(\"Revenues\", fontsize=16)\n",
    "ax.legend();\n",
    "##further approx's\n",
    "def errors(x_values, y_values, m, b):\n",
    "    y_line = (b + m*x_values)\n",
    "    return (y_values - y_line)\n",
    "\n",
    "def squared_errors(x_values, y_values, m, b):\n",
    "    return errors(x_values, y_values, m, b)**2\n",
    "\n",
    "def residual_sum_squares(x_values, y_values, m, b):\n",
    "    return sum(squared_errors(x_values, y_values, m, b))\n",
    "## cost curve\n",
    "def rss_values(x_values, y_values, m, b_values):\n",
    "    # Make a NumPy array to contain the data\n",
    "    data = np.zeros(shape=(len(b_values),2))\n",
    "    # Loop over all of the values in b_values\n",
    "    for idx, b_val in enumerate(b_values):\n",
    "        # Add the current b value and associated RSS to the\n",
    "        # NumPy array\n",
    "        data[idx] = [b_val, residual_sum_squares(x_values, y_values, m, b_val)]\n",
    "        \n",
    "    # Return the NumPy array\n",
    "    return data\n",
    "# Run this cell without changes\n",
    "example_rss = rss_values(x, y, 43, [1,2,3])\n",
    "\n",
    "# Should return a NumPy array\n",
    "assert type(example_rss) == np.ndarray\n",
    "\n",
    "# Specifically a 2D array\n",
    "assert example_rss.ndim == 2\n",
    "\n",
    "# The shape should match the number of b values passed in\n",
    "assert example_rss.shape == (3, 2)\n",
    "\n",
    "example_rss\n",
    "# Replace None with appropriate code\n",
    "b_val = np.arange(0,14.5, 0.5)\n",
    "# Replace None with appropriate code\n",
    "bval_rss = rss_values(x,y, 43, b_val)\n",
    "np.savetxt(sys.stdout, bval_rss, '%16.2f') # this line is to round your result, which will make things look nicer.\n",
    "#costcurve plot\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.plot(bval_rss[:,0], bval_rss[:,1])\n",
    "ax.set_xlabel(r'$b$ values', fontsize=14)\n",
    "ax.set_ylabel(\"RSS\", fontsize=14)\n",
    "fig.suptitle(\"RSS with Changes to Intercept\", fontsize=16);\n",
    "\n",
    "#slopes at different vals\n",
    "def slope_at(x_values, y_values, m, b):\n",
    "    delta = .001\n",
    "    base_rss = residual_sum_squares(x_values, y_values, m, b)\n",
    "    delta_rss = residual_sum_squares(x_values, y_values, m, b + delta)\n",
    "    numerator = delta_rss - base_rss\n",
    "    slope = numerator/delta\n",
    "    return slope\n",
    "#plotting it\n",
    "# Setting up to repeat the same process for 3 and 6\n",
    "# (You can change these values to see other tangent lines)\n",
    "b_vals = [3, 6]\n",
    "\n",
    "def plot_slope_at_b_vals(x, y, m, b_vals, bval_rss):\n",
    "    # Find the slope at each of these values\n",
    "    slopes = [slope_at(x, y, m, b) for b in b_vals]\n",
    "    # Find the RSS at each of these values\n",
    "    rss_values = [residual_sum_squares(x, y, m, b) for b in b_vals]\n",
    "\n",
    "    # Calculate the actual x and y locations for plotting\n",
    "    x_values = [np.linspace(b-1, b+1, 100) for b in b_vals]\n",
    "    y_values = [rss_values[i] + slopes[i]*(x_values[i] - b) for i, b in enumerate(b_vals)]\n",
    "    \n",
    "    # Plotting the same RSS curve as before\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    ax.plot(bval_rss[:,0], bval_rss[:,1])\n",
    "    ax.set_xlabel(r'$b$ values', fontsize=14)\n",
    "    ax.set_ylabel(\"RSS\", fontsize=14)\n",
    "\n",
    "    # Adding tangent lines for the selected b values\n",
    "    for i in range(len(b_vals)):\n",
    "        ax.plot(x_values[i], y_values[i], label=f\"slope={round(slopes[i], 2)}\", linewidth=3)\n",
    "\n",
    "    ax.legend(loc='upper right', fontsize='large')\n",
    "    fig.suptitle(f\"RSS with Intercepts {[round(b, 3) for b in b_vals]} Highlighted\", fontsize=16)\n",
    "    \n",
    "plot_slope_at_b_vals(x, y, 43, b_vals, bval_rss)\n",
    "##### toward gradient descent\n",
    "def updated_b(b, learning_rate, cost_curve_slope):\n",
    "    return -cost_curve_slope*learning_rate + b\n",
    "b_vals = []\n",
    "\n",
    "current_b = 3\n",
    "b_vals.append(current_b)\n",
    "\n",
    "current_cost_slope = slope_at(x, y, 43, current_b)\n",
    "new_b = updated_b(current_b, .01, current_cost_slope)\n",
    "print(f\"\"\"\n",
    "Current b: {round(current_b, 3)}\n",
    "Cost slope for current b: {round(current_cost_slope, 3)}\n",
    "Updated b: {round(new_b, 3)}\n",
    "\"\"\")\n",
    "# Current b: 3\n",
    "# Cost slope for current b: -232.731\n",
    "# Updated b: 5.327\n",
    "plot_slope_at_b_vals(x, y, 43, b_vals, bval_rss)\n",
    "\n",
    "#grad descent!\n",
    "def gradient_descent(x_values, y_values, steps, current_b, learning_rate, m):\n",
    "    listd = []\n",
    "    b = current_b\n",
    "    for step in range(steps):\n",
    "        dic = {}\n",
    "        #rss_values(x_values, y_values, m, b_values)\n",
    "        r = residual_sum_squares(x_values, y_values, m, b)\n",
    "        cost_curve_slope = slope_at(x_values, y_values, m, b)\n",
    "        dic['b'] = b\n",
    "        dic['rss'] = r\n",
    "        dic['slope'] = cost_curve_slope\n",
    "        listd.append(dic)\n",
    "        b = updated_b(b, learning_rate, cost_curve_slope)\n",
    "    #listd = np.savetxt(sys.stdout, listd, '%16.2f')\n",
    "    return listd\n",
    "descent_steps = gradient_descent(x, y, 15, 0, learning_rate = .005, m = 43)\n",
    "descent_steps\n",
    "#final result plot\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.scatter(x, y, marker=\".\", c=\"b\")\n",
    "colors = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#0000FF']\n",
    "for i, b in enumerate(b_vals+[6.83]):\n",
    "    ax.plot(x, x*43 + b, color=colors[i], label=f'$y = 43x + {round(b, 3)}$', linewidth=3)\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"y\", fontsize=14)\n",
    "fig.suptitle(\"Revenues\", fontsize=16)\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-gradient-descent-in-3d\n",
    "# https://github.com/ericthansen/dsc-the-gradient-in-gradient-descent.git\n",
    "# https://github.com/ericthansen/dsc-gradient-to-cost-function-v2-1\n",
    "#residual sum of squares as a loss function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def regression_formula(x):\n",
    "    return 1.8 + 46*x\n",
    "\n",
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30,1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3+ 50* x + y_randterm\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(x, y, '.b')\n",
    "plt.plot(x, regression_formula(x), '-', color=\"red\")\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);\n",
    "\n",
    "# plot errors\n",
    "def errors(x_values, y_values, m, b):\n",
    "    y_line = (b + m*x_values)\n",
    "    return (y_values - y_line)\n",
    "\n",
    "def squared_errors(x_values, y_values, m, b):\n",
    "    return np.round(errors(x_values, y_values, m, b)**2, 2)\n",
    "\n",
    "def residual_sum_squares(x_values, y_values, m, b):\n",
    "    return round(sum(squared_errors(x_values, y_values, m, b)), 2)\n",
    "\n",
    "table = np.zeros((20,2))\n",
    "for idx, val in enumerate(range(40, 60)):\n",
    "    table[idx,0] = val\n",
    "    table[idx,1] = residual_sum_squares(x, y, val, 1.319)\n",
    "    \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(table[:,0], table[:,1], '-')\n",
    "plt.xlabel(\"m-values\", fontsize=14)\n",
    "plt.ylabel(\"RSS\", fontsize=14)\n",
    "plt.title(\"RSS with changes to slope\", fontsize=16);\n",
    "\n",
    "#plotting it all\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def regression_formula(x):\n",
    "    return 1.319 + 52*x\n",
    "\n",
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30,1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3+ 50* x + y_randterm\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(x, y, '.b')\n",
    "plt.plot(x, regression_formula(x), '-', color=\"red\")\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-applying-gradient-descent-lab\n",
    "# see top for theory; they're still calculating partials just for one function.\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30, 1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3 + 50* x + y_randterm\n",
    "\n",
    "data = np.array([y, x])\n",
    "data = np.transpose(data)\n",
    "\n",
    "plt.plot(x, y, '.b')\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);\n",
    "\n",
    "##first pass at setting up gradient changes\n",
    "# initial variables of our regression line\n",
    "m_current = 0; b_current = 0\n",
    "\n",
    "#amount to update our variables for our next step\n",
    "update_to_b = 0; update_to_m = 0\n",
    "\n",
    "# Define the error_at function\n",
    "def error_at(point, b, m):\n",
    "    return point[1]*m + b - point[0]\n",
    "\n",
    "# iterate through data to change update_to_b and update_to_m\n",
    "for p in data:\n",
    "    #print(p)\n",
    "    error_at_p = error_at(p, b_current, m_current)\n",
    "    update_to_b += error_at_p\n",
    "    update_to_m += error_at_p * p[1]\n",
    "\n",
    "# Create new_b and new_m by subtracting the updates from the current estimates\n",
    "new_b = b_current + 2*update_to_b\n",
    "new_m = m_current + 2*update_to_m\n",
    "\n",
    "###refining the process\n",
    "#amount to update our variables for our next step\n",
    "m_current = 0; b_current = 0\n",
    "update_to_b = 0; update_to_m = 0\n",
    "\n",
    "# define learning rate and n\n",
    "eta = 0.01; n = len(data)\n",
    "\n",
    "# create update_to_b and update_to_m\n",
    "for p in data:\n",
    "    #print(p)\n",
    "    error_at_p = error_at(p, b_current, m_current)\n",
    "    update_to_b += error_at_p\n",
    "    update_to_m += error_at_p * p[1]\n",
    "    \n",
    "# create new_b and new_m\n",
    "new_b = b_current + eta*update_to_b/n\n",
    "new_m = m_current + eta*update_to_m/n\n",
    "\n",
    "### defining step_gradient function\n",
    "def step_gradient(b_current, m_current, points):\n",
    "    eta = 0.1 #learning rate\n",
    "    #amount to update our variables for our next step\n",
    "    #m_current = 0; b_current = 0\n",
    "    update_to_b = 0; update_to_m = 0\n",
    "\n",
    "    # define learning rate and n\n",
    "    #eta = 0.01; \n",
    "    n = len(data)\n",
    "\n",
    "    # create update_to_b and update_to_m\n",
    "    for p in data:\n",
    "        #print(p)\n",
    "        error_at_p = error_at(p, b_current, m_current)\n",
    "        update_to_b += error_at_p\n",
    "        update_to_m += error_at_p * p[1]\n",
    "\n",
    "    # create new_b and new_m\n",
    "    new_b = b_current - eta*update_to_b/n\n",
    "    new_m = m_current - eta*update_to_m/n\n",
    "    return (new_b, new_m)\n",
    "b = 0; m = 0\n",
    "b, m = step_gradient(b, m, 1)\n",
    "b, m\n",
    "# b= 3.02503, m= 2.07286\n",
    "\n",
    "###let's include 2 predictors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(11)\n",
    "\n",
    "x1 = np.random.rand(100,1).reshape(100)\n",
    "x2 = np.random.rand(100,1).reshape(100)\n",
    "y_randterm = np.random.normal(0,0.2,100)\n",
    "y = 2+ 3* x1+ -4*x2 + y_randterm\n",
    "\n",
    "data = np.array([y, x1, x2])\n",
    "data = np.transpose(data)\n",
    "#data\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "ax1.set_title('x_1')\n",
    "ax1.plot(x1, y, '.b')\n",
    "ax2.set_title('x_2')\n",
    "ax2.plot(x2, y, '.b');\n",
    "##### modified fnctions\n",
    "import numpy as np\n",
    "def error_at_multi(point, b, ms):\n",
    "    mxs = 0\n",
    "    for i in range(1,len(point)):\n",
    "        mxs += point[i] * ms[i-1]\n",
    "        #print(i, mxs)\n",
    "    return -(mxs + b - point[0])\n",
    "\n",
    "def step_gradient_multi(b_current, m_current, points):\n",
    "    eta = 0.1 #learning rate\n",
    "    update_to_b = 0\n",
    "    update_to_m = list(np.zeros(len(m_current)))\n",
    "    n = len(points)\n",
    "    #print('n', n)\n",
    "    \n",
    "    #for mj in range(len(m_current)):\n",
    "    for i in range(len(points)):\n",
    "        #print(p)\n",
    "        p = points[i]\n",
    "        error_at_p = error_at_multi(p, b_current, m_current)\n",
    "        #print('error at p', error_at_p)\n",
    "        update_to_b += error_at_p\n",
    "        update_to_m += error_at_p * points[i, 1:] #np.array(p).dot(error_at_p)#m_current * error_at_p # \n",
    "\n",
    "    # create new_b and new_m\n",
    "    new_b = b_current + eta*update_to_b/n\n",
    "    new_ms = m_current + eta*update_to_m/n\n",
    "    return (new_b, new_ms)\n",
    "error_at_multi([0,0,0,0], 10, [0,0,0])\n",
    "\n",
    "#np.array([1,2,3]).dot([2,3,4])\n",
    "new_b, new_ms = step_gradient_multi(0, np.zeros(2), data)\n",
    "new_b, new_ms\n",
    "for _ in range(500):\n",
    "    new_b, new_ms = step_gradient_multi(new_b, new_ms, data)\n",
    "    \n",
    "# https://github.com/ericthansen/dsc-calculus-section-recap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-regularization-intro-v2-1\n",
    "#REGULARIZATION TECHNIQUES\n",
    "#lasso and ridge regression - penalized estimation - penalties on large coefficients; prevents overfitting\n",
    "#AIC and BIC - Hirotugu Akaike and Bayes Information Criterion\n",
    "# The formula for the AIC, invented by Hirotugu Akaike in 1973 and short for \"Akaike's Information Criterion\" is given by:\n",
    "# The BIC (Bayesian Information Criterion) is very similar to the AIC and emerged as a Bayesian response to the AIC, but can be used for the exact same purposes.\n",
    "# The Lower the values of AIC and BIC, the better your model is performing.\n",
    "\n",
    "# https://github.com/ericthansen/dsc-ridge-and-lasso-regression\n",
    "# for ridge, lambda is a hyperparam.  \n",
    "# ridge also called R2 norm regularization\n",
    "\n",
    "# for lasso, similar but coefficients not squared for penalty, just absval. \n",
    "# The name \"Lasso\" comes from \"Least Absolute Shrinkage and Selection Operator\".\n",
    "# performs estimate and selection(i.e. removing some coefficients) simulanteously\n",
    "# Lasso aka L1 norm reg\n",
    "\n",
    "\n",
    "# an example using MPG auto data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('auto-mpg.csv') \n",
    "\n",
    "y = data[['mpg']]\n",
    "X = data.drop(['mpg', 'car name', 'origin'], axis=1)\n",
    "\n",
    "# Perform test train split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "#after splitting, use minmaxscalar to transform\n",
    "scale = MinMaxScaler()\n",
    "X_train_transformed = scale.fit_transform(X_train)\n",
    "X_test_transformed = scale.transform(X_test)\n",
    "\n",
    "#now fit ridge to the transformed data.\n",
    "# alpha is the lambda hyperparam\n",
    "# Build a Ridge, Lasso and regular linear regression model  \n",
    "# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X_train_transformed, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train_transformed, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train_transformed, y_train)\n",
    "#generate predictions\n",
    "# Generate preditions for training and test sets\n",
    "y_h_ridge_train = ridge.predict(X_train_transformed)\n",
    "y_h_ridge_test = ridge.predict(X_test_transformed)\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train_transformed)\n",
    "y_h_lin_test = lin.predict(X_test_transformed)\n",
    "#look at rss for each set\n",
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train_transformed))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test_transformed))**2))\n",
    "\n",
    "##note that unpenalized>ridge>lasso\n",
    "\n",
    "#if we look at coeffs\n",
    "print('Ridge parameter coefficients:', ridge.coef_)\n",
    "print('Lasso parameter coefficients:', lasso.coef_)\n",
    "print('Linear model parameter coefficients:', lin.coef_)\n",
    "#lasso shrunk a few to 0\n",
    "#ridge brought down the large 4th param by 25% ish\n",
    "# more resources:\n",
    "# https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-ridge-and-lasso-regression-lab\n",
    "##see also https://github.com/ericthansen/dsc-ridge-and-lasso-regression-lab/tree/solution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('Housing_Prices/train.csv')\n",
    "# Your code here\n",
    "df.info()\n",
    "# Create X and y\n",
    "target = 'SalePrice'\n",
    "y = df[target]\n",
    "X = df.drop(target, axis=1)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)\n",
    "\n",
    "# Remove \"object\"-type features from X\n",
    "cont_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Remove \"object\"-type features from X_train and X_test\n",
    "X_train_cont = X_train.select_dtypes(include=np.number)\n",
    "X_test_cont = X_test.select_dtypes(include=np.number)\n",
    "# X_train_cont.info()\n",
    "\n",
    "#build a naive linreg\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute missing values with median using SimpleImputer\n",
    "impute = SimpleImputer(strategy='median')#.fit(X_train_cont)\n",
    "X_train_imputed = impute.fit(X_train_cont).transform(X_train_cont)\n",
    "X_test_imputed = impute.fit(X_test_cont).transform(X_test_cont)\n",
    "\n",
    "# Fit the model and print R2 and MSE for training and test sets\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Print R2 and MSE for training and test sets\n",
    "display('r2 train:',linreg.score(X_train_imputed, y_train), \n",
    "        'mse train:', mean_squared_error(y_train, linreg.predict(X_train_imputed)))\n",
    "display('r2 test', linreg.score(X_test_imputed, y_test), \n",
    "        'mse test', mean_squared_error(y_test, linreg.predict(X_test_imputed)))\n",
    "##normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the train and test data\n",
    "ss = StandardScaler()\n",
    "X_train_imputed_scaled = ss.fit(X_train_imputed).transform(X_train_imputed)\n",
    "X_test_imputed_scaled = ss.fit(X_test_imputed).transform(X_test_imputed)\n",
    "\n",
    "# Fit the model\n",
    "linreg_norm = linreg.fit(X_train_imputed_scaled, y_train)\n",
    "\n",
    "\n",
    "# Print R2 and MSE for training and test sets\n",
    "display('r2 train:',linreg_norm.score(X_train_imputed_scaled, y_train), \n",
    "        'mse train:', mean_squared_error(y_train, linreg_norm.predict(X_train_imputed_scaled)))\n",
    "display('r2 test', linreg_norm.score(X_test_imputed_scaled, y_test), \n",
    "        'mse test', mean_squared_error(y_test, linreg_norm.predict(X_test_imputed_scaled)))\n",
    "##include categoricals\n",
    "# Create X_cat which contains only the categorical variables\n",
    "features_cat = X_train.select_dtypes(include='object').columns.tolist()\n",
    "X_train_cat = X_train[features_cat]\n",
    "X_test_cat = X_test[features_cat]\n",
    "\n",
    "# Fill missing values with the string 'missing'\n",
    "X_train_cat.fillna('missing')\n",
    "X_test_cat.fillna('missing')\n",
    "#X_train[features_cat].info()\n",
    "##onehot encode\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# OneHotEncode categorical variables\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')#categories='auto', sparse=True)#, drop='first', handle_unknown='error')\n",
    "\n",
    "# Transform training and test sets\n",
    "X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)\n",
    "'''ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Transform training and test sets\n",
    "X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)'''\n",
    "# Convert these columns into a DataFrame\n",
    "columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n",
    "# cat_train_df = pd.DataFrame(X_train_ohe, columns=columns)\n",
    "# cat_test_df = pd.DataFrame(X_test_ohe, columns=columns)\n",
    "\n",
    "##combine\n",
    "# Your code here\n",
    "X_train_all = pd.concat([pd.DataFrame(X_train_imputed_scaled), cat_train_df], axis=1)\n",
    "X_test_all = pd.concat([pd.DataFrame(X_test_imputed_scaled), cat_test_df], axis=1)\n",
    "##build new linreg\n",
    "# Your code here\n",
    "linreg_final = LinearRegression()\n",
    "linreg_final.fit(X_train_all, y_train)\n",
    "display('r2 train:',linreg_final.score(X_train_all, y_train), \n",
    "        'mse train:', mean_squared_error(y_train, linreg_final.predict(X_train_all)))\n",
    "display('r2 test', linreg_final.score(X_test_all, y_test), \n",
    "        'mse test', mean_squared_error(y_test, linreg_final.predict(X_test_all)))\n",
    "\n",
    "###Now with lasso and ridge\n",
    "# Your code here\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso() \n",
    "lasso.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', lasso.score(X_train_all, y_train))\n",
    "print('Test r^2:', lasso.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, lasso.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, lasso.predict(X_test_all)))\n",
    "#with higher alpha\n",
    "# Your code here\n",
    "lasso = Lasso(alpha=10) \n",
    "lasso.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', lasso.score(X_train_all, y_train))\n",
    "print('Test r^2:', lasso.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, lasso.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, lasso.predict(X_test_all)))\n",
    "##Now ridge\n",
    "# Your code here\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge() \n",
    "ridge.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', ridge.score(X_train_all, y_train))\n",
    "print('Test r^2:', ridge.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, ridge.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, ridge.predict(X_test_all)))\n",
    "#and ridge with higher alpha\n",
    "# Your code here\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha = 10) \n",
    "ridge.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', ridge.score(X_train_all, y_train))\n",
    "print('Test r^2:', ridge.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, ridge.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, ridge.predict(X_test_all)))\n",
    "\n",
    "##looking at metrics - lasso removes about 25% of predictors\n",
    "# Number of Ridge params almost zero\n",
    "near0 = 10**(-10)\n",
    "print(sum(abs(ridge.coef_)<near0))\n",
    "print(len(abs(ridge.coef_)))\n",
    "# Number of Lasso params almost zero\n",
    "print(sum(abs(lasso.coef_)<near0))\n",
    "print(len(lasso.coef_))\n",
    "print(len(lasso.coef_))\n",
    "print(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\n",
    "\n",
    "###PUT IT ALL TOGETHER\n",
    "def preprocess(X, y):\n",
    "    '''Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n",
    "    train and test DataFrames with targets'''\n",
    "    \n",
    "    # Train-test split (75-25), set seed to 10\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
    "    \n",
    "    # Remove \"object\"-type features and SalesPrice from X\n",
    "    cont_features = [col for col in X.columns if X[col].dtype in [np.float64, np.int64]]\n",
    "\n",
    "    X_train_cont = X_train.loc[:, cont_features]\n",
    "    X_test_cont = X_test.loc[:, cont_features]\n",
    "\n",
    "    # Impute missing values with median using SimpleImputer\n",
    "    impute = SimpleImputer(strategy='median')\n",
    "\n",
    "    X_train_imputed = impute.fit_transform(X_train_cont)\n",
    "    X_test_imputed = impute.transform(X_test_cont)\n",
    "\n",
    "    # Scale the train and test data\n",
    "    ss = StandardScaler()\n",
    "\n",
    "    X_train_imputed_scaled = ss.fit_transform(X_train_imputed)\n",
    "    X_test_imputed_scaled = ss.transform(X_test_imputed)\n",
    "\n",
    "    # Create X_cat which contains only the categorical variables\n",
    "    features_cat = [col for col in X.columns if X[col].dtype in [np.object]]\n",
    "    X_train_cat = X_train.loc[:, features_cat]\n",
    "    X_test_cat = X_test.loc[:, features_cat]\n",
    "\n",
    "    # Fill nans with a value indicating that that it is missing\n",
    "    X_train_cat.fillna(value='missing', inplace=True)\n",
    "    X_test_cat.fillna(value='missing', inplace=True)\n",
    "\n",
    "    # OneHotEncode Categorical variables\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "    X_test_ohe = ohe.transform(X_test_cat)\n",
    "\n",
    "    columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "    cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "    cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n",
    "    \n",
    "    # Combine categorical and continuous features into the final dataframe\n",
    "    X_train_all = pd.concat([pd.DataFrame(X_train_imputed_scaled), cat_train_df], axis=1)\n",
    "    X_test_all = pd.concat([pd.DataFrame(X_test_imputed_scaled), cat_test_df], axis=1)\n",
    "    \n",
    "    return X_train_all, X_test_all, y_train, y_test\n",
    "\n",
    "\n",
    "##Now, graph over different alphas to find optimal\n",
    "X_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n",
    "\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "alphas = []\n",
    "\n",
    "for alpha in np.linspace(0, 200, num=50):\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train_all, y_train)\n",
    "    \n",
    "    train_preds = lasso.predict(X_train_all)\n",
    "    train_mse.append(mean_squared_error(y_train, train_preds))\n",
    "    \n",
    "    test_preds = lasso.predict(X_test_all)\n",
    "    test_mse.append(mean_squared_error(y_test, test_preds))\n",
    "    \n",
    "    alphas.append(alpha)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alphas, train_mse, label='Train')\n",
    "ax.plot(alphas, test_mse, label='Test')\n",
    "ax.set_xlabel('Alpha')\n",
    "ax.set_ylabel('MSE')\n",
    "\n",
    "# np.argmin() returns the index of the minimum value in a list\n",
    "optimal_alpha = alphas[np.argmin(test_mse)]\n",
    "\n",
    "# Add a vertical line where the test MSE is minimized\n",
    "ax.axvline(optimal_alpha, color='black', linestyle='--')\n",
    "ax.legend();\n",
    "\n",
    "print(f'Optimal Alpha Value: {int(optimal_alpha)}')\n",
    "#Nice graphs yo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-feature-and-model-selection-aic-and-bic.git\n",
    "\n",
    "# https://github.com/ericthansen/dsc-feature-selection-methods\n",
    "#wrapper methods, filter methods, embedded methods, remember interaction terms?  cool.  lots of \n",
    "  #other things come into play with these feature selections\n",
    "#The overview:\n",
    "#Practice on a diabetes dataset\n",
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import the data\n",
    "df = pd.read_csv('diabetes.tab.txt', sep='\\t')\n",
    "df.head()\n",
    "\n",
    "# Obtain the target and features from the DataFrame\n",
    "target = df['Y']\n",
    "features = df.drop(columns='Y')\n",
    "\n",
    "# Create dummy variable for sex\n",
    "features['female'] = pd.get_dummies(features['SEX'], drop_first=True)\n",
    "features.drop(columns=['SEX'], inplace=True)\n",
    "features.head()\n",
    "\n",
    "#For both regularization (an embedded method) and various filters, it is important to standardize the data. This next cell is fitting a StandardScaler from sklearn to the data.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=20, test_size=0.2)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale every feature except the binary column - female\n",
    "transformed_training_features = scaler.fit_transform(X_train.iloc[:,:-1])\n",
    "transformed_testing_features = scaler.transform(X_test.iloc[:,:-1])\n",
    "\n",
    "# Convert the scaled features into a DataFrame\n",
    "X_train_transformed = pd.DataFrame(scaler.transform(X_train.iloc[:,:-1]), \n",
    "                                   columns=X_train.columns[:-1], \n",
    "                                   index=X_train.index)\n",
    "X_train_transformed['female'] = X_train['female']\n",
    "\n",
    "X_test_transformed = pd.DataFrame(scaler.transform(X_test.iloc[:,:-1]), \n",
    "                                  columns=X_train.columns[:-1], \n",
    "                                  index=X_test.index)\n",
    "X_test_transformed['female'] = X_test['female']\n",
    "##see how baseline performs before doing relgularization\n",
    "def run_model(model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print('Training R^2 :', model.score(X_train, y_train))\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    print('Training Root Mean Square Error', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))\n",
    "    print('\\n----------------\\n')\n",
    "    print('Testing R^2 :', model.score(X_test, y_test))\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    print('Testing Root Mean Square Error', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test)))\n",
    "    lm = LinearRegression()\n",
    "lm.fit(X_train_transformed, y_train)\n",
    "run_model(lm, X_train_transformed, X_test_transformed, y_train, y_test)\n",
    "#not great results, so...\n",
    "##add some more features, like polynomial ones\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly_train = pd.DataFrame(poly.fit_transform(X_train_transformed), columns=poly.get_feature_names(features.columns))\n",
    "X_poly_test = pd.DataFrame(poly.transform(X_test_transformed), columns=poly.get_feature_names(features.columns))\n",
    "X_poly_train.head()\n",
    "#now we have 65 columsn and probably overfit!\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(X_poly_train, y_train)\n",
    "\n",
    "run_model(lr_poly, X_poly_train, X_poly_test, y_train, y_test)\n",
    "#training better, testing worse\n",
    "\n",
    "#let's add some filter methods - the first one is variance threshold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "threshold_ranges = np.linspace(0, 2, num=6)\n",
    "\n",
    "for thresh in threshold_ranges:\n",
    "    print(thresh)\n",
    "    selector = VarianceThreshold(thresh)\n",
    "    reduced_feature_train = selector.fit_transform(X_poly_train)\n",
    "    reduced_feature_test = selector.transform(X_poly_test)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(reduced_feature_train, y_train)\n",
    "    run_model(lr, reduced_feature_train, reduced_feature_test, y_train, y_test)\n",
    "    print('--------------------------------------------------------------------')\n",
    "#That does a lousy job.\n",
    "\n",
    "#looks like you can automate it, with different selection approaches...\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, SelectKBest\n",
    "selector = SelectKBest(score_func=f_regression)\n",
    "X_k_best_train = selector.fit_transform(X_poly_train, y_train)\n",
    "X_k_best_test= selector.transform(X_poly_test)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_k_best_train ,y_train)\n",
    "run_model(lr,X_k_best_train,X_k_best_test,y_train,y_test)\n",
    "\n",
    "selector = SelectKBest(score_func=mutual_info_regression)\n",
    "X_k_best_train = selector.fit_transform(X_poly_train, y_train)\n",
    "X_k_best_test= selector.transform(X_poly_test)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_k_best_train ,y_train)\n",
    "run_model(lr,X_k_best_train,X_k_best_test,y_train,y_test)\n",
    "\n",
    "#Now, wrapper method using Recursive feature elim\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "rfe = RFECV(LinearRegression(),cv=5)\n",
    "X_rfe_train = rfe.fit_transform(X_poly_train, y_train)\n",
    "X_rfe_test = rfe.transform(X_poly_test)\n",
    "lm = LinearRegression().fit(X_rfe_train, y_train)\n",
    "run_model(lm, X_rfe_train, X_rfe_test, y_train, y_test)\n",
    "print ('The optimal number of features is: ', rfe.n_features_)\n",
    "#slightly better\n",
    "\n",
    "#now let's try an embedded method - ie lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "lasso = LassoCV(max_iter=100000, cv=5)\n",
    "lasso.fit(X_train_transformed, y_train)\n",
    "run_model(lasso,X_train_transformed, X_test_transformed, y_train, y_test)\n",
    "print('The optimal alpha for the Lasso Regression is: ', lasso.alpha_)\n",
    "\n",
    "#now try lasso on the fully polynomial model\n",
    "lasso2 = LassoCV(max_iter=100000, cv=5)\n",
    "\n",
    "lasso2.fit(X_poly_train, y_train)\n",
    "run_model(lasso2, X_poly_train, X_poly_test, y_train, y_test)\n",
    "print('The optimal alpha for the Lasso Regression is: ', lasso2.alpha_)\n",
    "\n",
    "\n",
    "\n",
    "#mrore good links: \n",
    "# https://www.researchgate.net/profile/Amparo_Alonso-Betanzos/publication/221252792_Filter_Methods_for_Feature_Selection_-_A_Comparative_Study/links/543fd9ec0cf21227a11b8e05.pdf\n",
    "# http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-extensions-to-linear-models-lab\n",
    "# and https://github.com/ericthansen/dsc-extensions-to-linear-models-lab/blob/solution/index.ipynb\n",
    "\n",
    "#also https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html\n",
    "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py\n",
    "# https://xavierbourretsicotte.github.io/subset_selection.html\n",
    "\n",
    "#this one was...elaborate; not sure how we were meant to do it without more guidance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "df = pd.read_csv(\"ames.csv\")\n",
    "df = df[['LotArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF',\n",
    "         '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd',\n",
    "         'GarageArea', 'Fireplaces', 'SalePrice']]\n",
    "#df.head()\n",
    "#baseline\n",
    "# Your code here\n",
    "y = pd.DataFrame(df['SalePrice'])\n",
    "X = df.drop(\"SalePrice\", axis=1)\n",
    "\n",
    "X_scaled = scale(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "all_data = pd.concat([y, X_scaled], axis=1)\n",
    "all_data.head()\n",
    "regression = LinearRegression()\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "baseline = np.mean(cross_val_score(regression, X_scaled, y, scoring='r2', cv=crossvalidation))\n",
    "baseline\n",
    "##include interactions\n",
    "# Your code here\n",
    "combs = list(combinations(X.columns, 2))\n",
    "\n",
    "interactions = []\n",
    "data = X_scaled.copy()\n",
    "for comb in combs:\n",
    "    data['interaction'] = data[comb[0]] * data[comb[1]]\n",
    "    score = np.mean(cross_val_score(regression, data, y, scoring='r2', cv=crossvalidation))\n",
    "    if score > baseline: interactions.append((comb[0], comb[1], round(score, 3)))\n",
    "            \n",
    "print('Top 7 interactions: %s' %sorted(interactions, key=lambda inter: inter[2], reverse=True)[:7])\n",
    "\n",
    "#use top 7\n",
    "# Your code here\n",
    "top7 = sorted(interactions, key=lambda inter: inter[2], reverse=True)[:7]\n",
    "df_inter = X_scaled.copy()\n",
    "for inter in top7:\n",
    "    df_inter[inter[0]+'_'+inter[1]]=df_inter[inter[0]] * df_inter[inter[1]]\n",
    "df_inter.head()\n",
    "\n",
    "#include polynoms\n",
    "# Your code here\n",
    "polynomials = []\n",
    "for col in X.columns:\n",
    "    for degree in [2, 3, 4]:\n",
    "        data = X_scaled.copy()\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_transformed = poly.fit_transform(X[[col]])\n",
    "        data = pd.concat([data.drop(col, axis=1),pd.DataFrame(X_transformed)], axis=1)\n",
    "        score = np.mean(cross_val_score(regression, data, y, scoring='r2', cv=crossvalidation))\n",
    "        if score > baseline: polynomials.append((col, degree, round(score, 3)))\n",
    "print('Top 10 polynomials: %s' %sorted(polynomials, key=lambda poly: poly[2], reverse=True)[:10])\n",
    "# Your code here\n",
    "polynom = pd.DataFrame(polynomials)\n",
    "polynom.groupby([0], sort=False)[2].max()\n",
    "# Your code here\n",
    "for col in ['OverallQual', 'GrLivArea']:\n",
    "    poly = PolynomialFeatures(4, include_bias=False)\n",
    "    X_transformed = poly.fit_transform(X[[col]])\n",
    "    colnames= [col, col + '_' + '2',  col + '_' + '3', col + '_' + '4']\n",
    "    df_inter = pd.concat([df_inter.drop(col, axis=1), pd.DataFrame(X_transformed, columns=colnames)], axis=1)\n",
    "# Your code here\n",
    "df_inter.head()\n",
    "df_inter.info()\n",
    "\n",
    "#full model\n",
    "# Your code here\n",
    "full_model_cv_score = np.mean(cross_val_score(regression, df_inter, y, scoring='r2', cv=crossvalidation))\n",
    "full_model_cv_score\n",
    "\n",
    "#find best lasso parameter\n",
    "from sklearn.linear_model import Lasso, LassoCV, LassoLarsCV, LassoLarsIC\n",
    "# Your code here \n",
    "model_bic = LassoLarsIC(criterion='bic')\n",
    "model_bic.fit(df_inter, y)\n",
    "alpha_bic_ = model_bic.alpha_\n",
    "\n",
    "model_aic = LassoLarsIC(criterion='aic')\n",
    "model_aic.fit(df_inter, y)\n",
    "alpha_aic_ = model_aic.alpha_\n",
    "\n",
    "\n",
    "def plot_ic(model, name, color):\n",
    "    alpha_ = model.alpha_\n",
    "    alphas_ = model.alphas_\n",
    "    criterion_ = model.criterion_\n",
    "    plt.plot(-np.log10(alphas_), criterion_, '--', color=color, linewidth=2, label= name)\n",
    "    plt.axvline(-np.log10(alpha_), color=color, linewidth=2,\n",
    "                label='alpha for %s ' % name)\n",
    "    plt.xlabel('-log(alpha)')\n",
    "    plt.ylabel('criterion')\n",
    "\n",
    "plt.figure()\n",
    "plot_ic(model_aic, 'AIC', 'green')\n",
    "plot_ic(model_bic, 'BIC', 'blue')\n",
    "plt.legend()\n",
    "plt.title('Information-criterion for model selection');\n",
    "\n",
    "#analyze final result\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split X_scaled and y into training and test sets\n",
    "# Set random_state to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Code for baseline model\n",
    "linreg_f = LinearRegression()\n",
    "linreg_f.fit(X_train, y_train)\n",
    "\n",
    "# Print R-Squared and RMSE\n",
    "print('Train R^2:', linreg_f.score(X_train, y_train))\n",
    "print('Test R^2:', linreg_f.score(X_test, y_test))\n",
    "print('Train RMSE:', mean_squared_error(y_train, linreg_f.predict(X_train), squared=False))\n",
    "print('Test RMSE:', mean_squared_error(y_test, linreg_f.predict(X_test), squared=False))\n",
    "\n",
    "# Split df_inter and y into training and test sets\n",
    "# Set random_state to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_inter, y, random_state=1)\n",
    "\n",
    "# Code for lasso with alpha from AIC\n",
    "lasso = Lasso(alpha= model_aic.alpha_) \n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Print R-Squared and RMSE\n",
    "print('Training R-Squared:', lasso.score(X_train, y_train))\n",
    "print('Test R-Squared:', lasso.score(X_test, y_test))\n",
    "print('Training RMSE:', mean_squared_error(y_train, lasso.predict(X_train), squared=False))\n",
    "print('Test RMSE:', mean_squared_error(y_test, lasso.predict(X_test), squared=False))\n",
    "\n",
    "# Code for lasso with alpha from BIC\n",
    "lasso2 = Lasso(alpha= model_bic.alpha_) \n",
    "lasso2.fit(X_train, y_train)\n",
    "\n",
    "# Print R-Squared and RMSE\n",
    "print('Training R-Squared:', lasso2.score(X_train, y_train))\n",
    "print('Test R-Squared:', lasso2.score(X_test, y_test))\n",
    "print('Training RMSE:', mean_squared_error(y_train, lasso2.predict(X_train), squared=False))\n",
    "print('Test RMSE:', mean_squared_error(y_test, lasso2.predict(X_test), squared=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-generating-data\n",
    "\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import make_blobs\n",
    "from sklearn.datasets import make_blobs#from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=3, n_features=2)\n",
    "\n",
    "#now visualize\n",
    "# Plot a scatter plot, color \n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()\n",
    "\n",
    "#now, make moons - not linearly separable so good for nonlinear like tanh or sigmoid\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "# Plot a scatter plot, color \n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#now make circles\n",
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(n_samples=100, noise=0.05)\n",
    "\n",
    "# Plot a scatter plot, color \n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#now make regression\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "#and plot\n",
    "# Plot regression dataset\n",
    "plt.scatter(X,y)\n",
    "plt.show()\n",
    "\n",
    "#can make nonlinear\n",
    "# Generate new y \n",
    "y2 = y**2\n",
    "y3 = y**3\n",
    "\n",
    "# Visualize this data\n",
    "plt.scatter(X, y2)\n",
    "plt.show()\n",
    "plt.scatter(X, y3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#finally, see also https://scikit-learn.org/stable/datasets/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-generating-data-lab\n",
    "# Your code here \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import make_blobs\n",
    "from sklearn.datasets import make_blobs#from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state = 42)\n",
    "\n",
    "# Your code here \n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "df.head()\n",
    "# Your code here \n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()\n",
    "\n",
    "#diffefent clustering values\n",
    "# Your code here: \n",
    "cluster_std = 0.5\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state = 42, cluster_std = cluster_std)\n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#now do regression\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def reg_simulation(n, random_state):\n",
    "    # Generate X and y\n",
    "    X, y = make_regression(n_samples=100, n_features=1, noise=n, random_state=random_state)\n",
    "\n",
    "\n",
    "    # Use X,y to draw a scatter plot\n",
    "    plt.scatter(X[:, 0], y, color='red', s=10, label='Data')\n",
    "    \n",
    "    # Fit a linear regression model to X , y and calculate r2\n",
    "    # label and plot the regression line \n",
    "    linreg = LinearRegression().fit(X, y)\n",
    "    \n",
    "    plt.plot(X[:, 0], linreg.predict(X), color='black', label='Model')\n",
    "    plt.title('Noise: ' + str(n) + ', R-Squared: ' + str(round(linreg.score(X,y), 2)))\n",
    "    plt.tick_params(labelbottom=False, labelleft=False)\n",
    "    plt.xlabel('Variable X')\n",
    "    plt.ylabel('Variable Y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "random_state = random_state = np.random.RandomState(42)\n",
    "\n",
    "for n in [10, 25, 40, 50, 100, 200]:\n",
    "    reg_simulation(n, random_state)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-ml-fundamentals-lab\n",
    "# Like.  the whole thing.  USEFUL WORKFLOW!\n",
    "#also https://github.com/ericthansen/dsc-ml-fundamentals-lab/tree/solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-regularization-recap-v2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-logistic-regression-intro \n",
    "# https://github.com/ericthansen/dsc-intro-to-supervised-learning-v2-1 - VERY useful background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-linear-to-logistic-regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "age = np.random.uniform(18, 65, 100)\n",
    "income = np.random.normal((age/10), 0.5)\n",
    "age = age.reshape(-1,1)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "fig.suptitle('age vs income', fontsize=16)\n",
    "plt.scatter(age, income)\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.suptitle('linear regression', fontsize=16)\n",
    "plt.scatter(age, income)\n",
    "plt.plot(age, age/10, c='black')\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "income_bin = income > 4\n",
    "income_bin = income_bin.astype(int)  \n",
    "print(income_bin)\n",
    "\n",
    "\n",
    "#plots are just 0s and 1s\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.suptitle('age vs binary income', fontsize=16)\n",
    "plt.scatter(age, income_bin)\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income (> or < 4000)', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#fit a logistic regression (s-curve lookin' thing)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(age, income_bin)\n",
    "# Store the coefficients\n",
    "coef = lin_reg.coef_\n",
    "interc = lin_reg.intercept_\n",
    "# Create the line\n",
    "lin_income = (interc + age * coef)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.suptitle('linear regression', fontsize=16)\n",
    "plt.scatter(age, income_bin)\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income', fontsize=14)\n",
    "plt.plot(age, lin_income, c='black')\n",
    "plt.show()\n",
    "\n",
    "# Instantiate a Logistic regression model\n",
    "# Solver must be specified to avoid warning, see documentation for more information\n",
    "# liblinear is recommended for small datasets\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "regr = LogisticRegression(C=1e5, solver='liblinear')\n",
    "\n",
    "# Fit the model to the training set\n",
    "regr.fit(age, income_bin)\n",
    "\n",
    "# Store the coefficients\n",
    "coef = regr.coef_\n",
    "interc = regr.intercept_\n",
    "\n",
    "# Create the linear predictor\n",
    "lin_pred = (age * coef + interc)\n",
    "\n",
    "# Perform the log transformation\n",
    "mod_income = 1 / (1 + np.exp(-lin_pred))\n",
    "\n",
    "# Sort the numbers to make sure plot looks right\n",
    "age_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.suptitle('logistic regression', fontsize=16)\n",
    "plt.scatter(age, income_bin)\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income', fontsize=14)\n",
    "plt.plot(age_ordered, mod_income_ordered, c='black')\n",
    "plt.show()\n",
    "\n",
    "###a real world example\n",
    "import statsmodels as sm\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "salaries = pd.read_csv('salaries_final.csv', index_col=0)\n",
    "salaries.head()\n",
    "\n",
    "# Convert race and sex using get_dummies() \n",
    "x_feats = ['Race', 'Sex', 'Age']\n",
    "X = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n",
    "\n",
    "# Convert target using get_dummies\n",
    "y = pd.get_dummies(salaries['Target'], drop_first=True, dtype=float)\n",
    "y = y['>50K']\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Create intercept term required for sm.Logit, see documentation for more information\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit model\n",
    "logit_model = sm.Logit(y, X)\n",
    "\n",
    "# Get results of the fit\n",
    "result = logit_model.fit()\n",
    "\n",
    "result.summary()\n",
    "\n",
    "np.exp(result.params)\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept = False, C = 1e15, solver='liblinear')\n",
    "model_log = logreg.fit(X, y)\n",
    "model_log\n",
    "\n",
    "model_log.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://github.com/ericthansen/dsc-fitting-a-logistic-regression-model-lab\n",
    "# Import the data\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()\n",
    "\n",
    "# Total number of people who survived/didn't survive\n",
    "df['Survived'].value_counts()\n",
    "total_surv = df.Survived.sum()\n",
    "total_surv\n",
    "\n",
    "# Create dummy variables\n",
    "relevant_columns = ['Pclass', 'Age', 'SibSp', 'Fare', 'Sex', 'Embarked', 'Survived']\n",
    "dummy_dataframe = pd.get_dummies(df[relevant_columns], drop_first=True, dtype=float)\n",
    "\n",
    "dummy_dataframe.shape\n",
    "\n",
    "# Drop missing rows\n",
    "dummy_dataframe = dummy_dataframe.dropna()\n",
    "dummy_dataframe.shape\n",
    "\n",
    "# Split the data into X and y\n",
    "y = dummy_dataframe['Survived']\n",
    "X = dummy_dataframe.drop('Survived', axis=1)\n",
    "\n",
    "# Build a logistic regression model using statsmodels\n",
    "import statsmodels.api as sm\n",
    "X = sm.tools.add_constant(X)\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Summary table\n",
    "result.summary()\n",
    "###now reduce to using only the most relevant predictors\n",
    "# Your code here\n",
    "relevant_columns = ['Pclass', 'Age', 'SibSp', 'Sex', 'Survived']\n",
    "dummy_dataframe = pd.get_dummies(df[relevant_columns], drop_first=True, dtype=float)\n",
    "\n",
    "dummy_dataframe = dummy_dataframe.dropna()\n",
    "\n",
    "y = dummy_dataframe['Survived']\n",
    "X = dummy_dataframe.drop(columns=['Survived'], axis=1)\n",
    "\n",
    "X = sm.tools.add_constant(X)\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit()\n",
    "\n",
    "result.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-logistic-regression-in-scikit-learn\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()\n",
    "\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "print(df.columns)\n",
    "df.head()\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()\n",
    "\n",
    "x_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\n",
    "X = pd.get_dummies(df[x_feats], drop_first=True)\n",
    "y = df['Survived']\n",
    "X.head() # Preview our data to make sure it looks reasonable\n",
    "\n",
    "# Fill missing values\n",
    "X = X.fillna(value=0) \n",
    "for col in X.columns:\n",
    "    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n",
    "    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n",
    "\n",
    "X.head()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "model_log\n",
    "\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "\n",
    "import numpy as np\n",
    "# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \n",
    "residuals = np.abs(y_train - y_hat_train)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))\n",
    "\n",
    "residuals = np.abs(y_test - y_hat_test)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-logistic-regression-in-scikit-learn-lab\n",
    "# Import necessary functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv('heart.csv')\n",
    "df.head()\n",
    "\n",
    "# Split the data into target and predictors\n",
    "y = df.target\n",
    "X = df.drop('target', axis = 1)\n",
    "\n",
    "# Your code here\n",
    "X = X.apply(lambda x : (x - x.min()) /(x.max() - x.min()), axis=0)\n",
    "X.head()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Instantiate the model\n",
    "C = 10**10\n",
    "logreg = LogisticRegression(fit_intercept=False, C=C, solver='liblinear')\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "##check results\n",
    "# Your code here\n",
    "import numpy as np\n",
    "residuals = np.abs(y_train - y_hat_train)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))\n",
    "##and on test set\n",
    "# Your code here\n",
    "residuals = np.abs(y_test - y_hat_test)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-confusion-matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "example_labels = [0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1]\n",
    "example_preds  = [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "cf = confusion_matrix(example_labels, example_preds)\n",
    "cf\n",
    "\n",
    "ex2_labels = [0, 1, 2, 2, 3, 1, 0, 2, 1, 2, 3, 3, 1, 0]\n",
    "ex2_preds =  [0, 1, 1, 2, 3, 3, 2, 2, 1, 2, 3, 0, 2, 0]\n",
    "\n",
    "cf2 = confusion_matrix(ex2_labels, ex2_preds)\n",
    "cf2\n",
    "\n",
    "# https://github.com/ericthansen/dsc-visualizing-confusion-matrices-lab\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Define appropriate X and y\n",
    "X = df[df.columns[:-1]]\n",
    "y = df.target\n",
    "\n",
    "# Normalize the data\n",
    "for col in df.columns:\n",
    "    df[col] = (df[col] - min(df[col]))/ (max(df[col]) - min(df[col]))\n",
    "\n",
    "# Split the data into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Fit a model\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "\n",
    "# Preview model params\n",
    "print(model_log) \n",
    "\n",
    "# Predict\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "print(\"\")\n",
    "# Data preview\n",
    "df.head()\n",
    "\n",
    "##manually create confusion matrix\n",
    "def conf_matrix(y_true, y_pred):\n",
    "\n",
    "    cmatrix = {}\n",
    "    lt = list(y_true)\n",
    "    lp = list(y_pred)\n",
    "    for i in range(len(lt)):\n",
    "        if lt[i] == 1 and lp[i] == 1:\n",
    "            cmatrix['TP'] = cmatrix.get('TP', 0) + 1\n",
    "        elif lt[i] == 1 and lp[i] == 0:\n",
    "            cmatrix['FN'] = cmatrix.get('FN', 0) + 1\n",
    "        elif lt[i] == 0 and lp[i] == 1:\n",
    "            cmatrix['FP'] = cmatrix.get('FP', 0) + 1            \n",
    "        elif lt[i] == 0 and lp[i] == 0:\n",
    "            cmatrix['TN'] = cmatrix.get('TN', 0) + 1      \n",
    "    return cmatrix\n",
    "# Test the function\n",
    "conf_matrix(y_test, y_hat_test)\n",
    "# Expected output: {'TP': 39, 'TN': 24, 'FP': 9, 'FN': 4}\n",
    "\n",
    "##check work\n",
    "# Import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_hat_test)\n",
    "print('Confusion Matrix:\\n', cnf_matrix)\n",
    "\n",
    "##create a nice visual\n",
    "# Import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Visualize your confusion matrix\n",
    "cm = confusion_matrix(y_test, y_hat_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-evaluation-metrics\n",
    "#Precision and Recall are two of the most basic evaluation metrics available to us. Precision measures how precise the predictions are, while Recall indicates what percentage of the classes we're interested in were actually captured by the model.\n",
    "# precision = true positives / all predicted positives\n",
    "# recall = true pos / actual total positives\n",
    "# accuracy  = (number of true pos + true neg)/ total obs\n",
    "# F1 score = F1 score represents the Harmonic Mean of Precision and Recall.\n",
    "#   = 2(precision*recall)/(precision+recall)\n",
    "#   penalizes either being low\n",
    "# Scikit-learn has a built-in function that will create a Classification Report. This classification report even breaks down performance by individual class predictions for your model. You can find the classification_report() function in the sklearn.metrics module, which takes labels and predictions and returns the precision, recall, F1 score and support (number of occurrences of each label in y_true) for the results of a model.\n",
    "\n",
    "\n",
    "# https://github.com/ericthansen/dsc-evaluating-logistic-regression-models-lab\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('heart.csv')\n",
    "df.head()\n",
    "\n",
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate LogisticRegression\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "\n",
    "# Fit to training data\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "model_log\n",
    "\n",
    "def precision(y, y_hat):\n",
    "    # Your code here\n",
    "    # precision = true positives / all predicted positives\n",
    "#     resid = y-y_hat\n",
    "    cmatrix = {}\n",
    "    lt = list(y)\n",
    "    lp = list(y_hat)\n",
    "    for i in range(len(lt)):\n",
    "        if lt[i] == 1 and lp[i] == 1:\n",
    "            cmatrix['TP'] = cmatrix.get('TP', 0) + 1\n",
    "        elif lt[i] == 1 and lp[i] == 0:\n",
    "            cmatrix['FN'] = cmatrix.get('FN', 0) + 1\n",
    "        elif lt[i] == 0 and lp[i] == 1:\n",
    "            cmatrix['FP'] = cmatrix.get('FP', 0) + 1            \n",
    "        elif lt[i] == 0 and lp[i] == 0:\n",
    "            cmatrix['TN'] = cmatrix.get('TN', 0) + 1    \n",
    "    return cmatrix['TP']/(cmatrix['TP']+cmatrix['FP'])\n",
    "    \n",
    "    \n",
    "def recall(y, y_hat):\n",
    "    # Your code here\n",
    "    # recall = true pos / actual total positives\n",
    "    cmatrix = {}\n",
    "    lt = list(y)\n",
    "    lp = list(y_hat)\n",
    "    for i in range(len(lt)):\n",
    "        if lt[i] == 1 and lp[i] == 1:\n",
    "            cmatrix['TP'] = cmatrix.get('TP', 0) + 1\n",
    "        elif lt[i] == 1 and lp[i] == 0:\n",
    "            cmatrix['FN'] = cmatrix.get('FN', 0) + 1\n",
    "        elif lt[i] == 0 and lp[i] == 1:\n",
    "            cmatrix['FP'] = cmatrix.get('FP', 0) + 1            \n",
    "        elif lt[i] == 0 and lp[i] == 0:\n",
    "            cmatrix['TN'] = cmatrix.get('TN', 0) + 1    \n",
    "    return cmatrix['TP']/(cmatrix['TP']+cmatrix['FN'])\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    # Your code here\n",
    "    # accuracy  = (number of true pos + true neg)/ total obs\n",
    "    cmatrix = {}\n",
    "    lt = list(y)\n",
    "    lp = list(y_hat)\n",
    "    for i in range(len(lt)):\n",
    "        if lt[i] == 1 and lp[i] == 1:\n",
    "            cmatrix['TP'] = cmatrix.get('TP', 0) + 1\n",
    "        elif lt[i] == 1 and lp[i] == 0:\n",
    "            cmatrix['FN'] = cmatrix.get('FN', 0) + 1\n",
    "        elif lt[i] == 0 and lp[i] == 1:\n",
    "            cmatrix['FP'] = cmatrix.get('FP', 0) + 1            \n",
    "        elif lt[i] == 0 and lp[i] == 0:\n",
    "            cmatrix['TN'] = cmatrix.get('TN', 0) + 1    \n",
    "    return (cmatrix['TP']+cmatrix['TN'])/len(y)\n",
    "\n",
    "def f1_score(y, y_hat):\n",
    "    # Your code here\n",
    "    # 2(precision*recall)/(precision+recall)\n",
    "    prec = precision(y, y_hat)\n",
    "    rec = recall(y, y_hat)\n",
    "    return 2*(prec*rec)/(prec+rec)\n",
    "\n",
    "# Your code here\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "display('prec train',precision(y_train, y_hat_train))\n",
    "display('prec test',precision(y_test, y_hat_test))\n",
    "\n",
    "display('recall train',recall(y_train, y_hat_train))\n",
    "display('recall test',recall(y_test, y_hat_test))\n",
    "\n",
    "\n",
    "display('acc train',accuracy(y_train, y_hat_train))\n",
    "display('acc test',accuracy(y_test, y_hat_test))\n",
    "\n",
    "\n",
    "display('F1 train', f1_score(y_train, y_hat_train))\n",
    "display('F1 test', f1_score(y_test, y_hat_test))\n",
    "\n",
    "###now with sklearn\n",
    "# Your code here\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "for [yt, yp, yset] in [[y_train, y_hat_train, 'train'],[y_test, y_hat_test, 'test']]:\n",
    "    display(yset.upper())\n",
    "    display('precision:', precision_score(yt, yp))\n",
    "    display('recall:', recall_score(yt, yp))\n",
    "    display('accuracy:', accuracy_score(yt, yp))\n",
    "    display('f1 score:', f1_score(yt, yp))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#calculate for different test sizes\n",
    "training_precision = []\n",
    "testing_precision = []\n",
    "training_recall = []\n",
    "testing_recall = []\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "training_f1 = []\n",
    "testing_f1 = []\n",
    "\n",
    "for i in range(10, 95):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= i) # replace the \"None\" here\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver='liblinear')\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    y_hat_test = model_log.predict(X_test)\n",
    "    y_hat_train = model_log.predict(X_train)\n",
    "    \n",
    "    # Your code here\n",
    "    training_precision.append(precision_score(y_train, y_hat_train))\n",
    "    testing_precision.append(precision_score(y_test, y_hat_test))\n",
    "    training_recall.append(recall_score(y_train, y_hat_train))\n",
    "    testing_recall.append(recall_score(y_test, y_hat_test))\n",
    "    training_accuracy.append(accuracy_score(y_train, y_hat_train))\n",
    "    testing_accuracy.append(accuracy_score(y_test, y_hat_test))\n",
    "    training_f1.append(f1_score(y_train, y_hat_train))\n",
    "    testing_f1.append(f1_score(y_test, y_hat_test))\n",
    "    \n",
    "# Train and test precision\n",
    "plt.scatter(list(range(10, 95)), training_precision, label='training_precision')\n",
    "plt.scatter(list(range(10, 95)), testing_precision, label='testing_precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Train and test recall\n",
    "plt.scatter(list(range(10, 95)), training_recall, label='training_recall')\n",
    "plt.scatter(list(range(10, 95)), testing_recall, label='testing_recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Train and test accuracy\n",
    "plt.scatter(list(range(10, 95)), training_accuracy, label='training_accuracy')\n",
    "plt.scatter(list(range(10, 95)), testing_accuracy, label='testing_accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Train and test F1 score\n",
    "plt.scatter(list(range(10, 95)), training_f1, label='training_f1')\n",
    "plt.scatter(list(range(10, 95)), testing_f1, label='testing_f1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-roc-curves-and-auc\n",
    "# ROC curves plot true pos rate vs false pos rate.\n",
    "# true pos rate = true pos /(true pos + false neg)\n",
    "# false pos rate = false pos / (false pos + true neg)\n",
    "# plot tpr on y axis, fpr on x.\n",
    "\n",
    "#AUC is area under curve, when plotting positives/negatives/decision boundary,\n",
    "# basically it compares the area of all false results to total\n",
    "# smaller false total, better\n",
    "# AUC of .5 is random, AUC of 1 is perfect (higher is better)\n",
    "\n",
    "\n",
    "#see this embedded below:\n",
    "https://flatiron.illumidesk.com/user/ericthansen/notebooks/dsc-roc-curves-and-auc/images/Image_146_recall.png\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC diagram <img src=\"https://flatiron.illumidesk.com/user/ericthansen/notebooks/dsc-roc-curves-and-auc/images/Image_146_recall.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continued - drawing the curve in practice\n",
    "#train a classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Define appropriate X and y\n",
    "y = df['target']\n",
    "X = df.drop(columns='target', axis=1)\n",
    "\n",
    "# Normalize the Data\n",
    "X = X.apply(lambda x : (x - x.min()) /(x.max() - x.min()),axis=0)\n",
    "\n",
    "# Split the data into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Fit a model\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "print(logreg) # Preview model params\n",
    "\n",
    "# Predict\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "# Data preview\n",
    "print(\"\")\n",
    "df.head()\n",
    "\n",
    "##Draw the ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Scikit-learn's built in roc_curve method returns the fpr, tpr, and thresholds\n",
    "# for various decision boundaries given the case member probabilites\n",
    "\n",
    "# First calculate the probability scores of each of the datapoints:\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "##calculate the AUC\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "\n",
    "##put it all together as cohesive visual\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn's beautiful styling\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-roc-curves-and-auc-lab\n",
    "## sample lab\n",
    "# Import and preview the data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('mushrooms.csv')\n",
    "display(df.head(5))\n",
    "display(df.info())\n",
    "\n",
    "#df['cap-shape'].value_counts()\n",
    "##because everything is Objects, we make dummies\n",
    "# Define y\n",
    "y = pd.get_dummies(df['class'], drop_first=True)\n",
    "y = y['p']\n",
    "\n",
    "# Define X\n",
    "X = pd.get_dummies(df.drop('class', axis=1), drop_first=True)\n",
    "#X = None\n",
    "#display(X.head())\n",
    "# Import train_test_split\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "##import logistic regression\n",
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate\n",
    "logreg = LogisticRegression(fit_intercept=False, solver='liblinear')\n",
    "\n",
    "# Fit the model to training data\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "## calculate Tpr and Fpr\n",
    "# Import roc_curve, auc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Calculate the probability scores of each point in the training set\n",
    "y_train_score = model_log.decision_function(X_train)\n",
    "\n",
    "# Calculate the fpr, tpr, and thresholds for the training set\n",
    "train_fpr, train_tpr, thresholds = roc_curve(y_train, y_train_score)\n",
    "\n",
    "# Calculate the probability scores of each point in the test set\n",
    "y_score = model_log.decision_function(X_test)\n",
    "\n",
    "## Calculate the fpr, tpr, and thresholds for the test set\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "##check out AUC\n",
    "print('AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "##draw the ROC curves\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn's beautiful styling\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "# ROC curve for training set\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(train_fpr, train_tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve for Training Set')\n",
    "plt.legend(loc='lower right')\n",
    "print('AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "plt.show()\n",
    "# ROC curve for test set\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve for Testing Set')\n",
    "plt.legend(loc='lower right')\n",
    "print('AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-class-imbalance-problems\n",
    "## synthetic minority oversampling SMOTE https://jair.org/index.php/jair/article/view/10302/24590\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = train_test\n",
    "\n",
    "# Data preview\n",
    "df.head()\n",
    "###look at level of class imbalance\n",
    "print('Raw counts: \\n')\n",
    "print(df['is_attributed'].value_counts())\n",
    "print('-----------------------------------')\n",
    "print('Normalized counts: \\n')\n",
    "print(df['is_attributed'].value_counts(normalize=True))\n",
    "##define x and y\n",
    "# Define appropriate X and y\n",
    "y = df['is_attributed']\n",
    "X = df[['ip', 'app', 'device', 'os', 'channel']]\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "##compare some different reg parameters\n",
    "# Now let's compare a few different regularization performances on the dataset:\n",
    "weights = [None, 'balanced', {1:2, 0:1}, {1:10, 0:1}, {1:100, 0:1}, {1:1000, 0:1}]\n",
    "names = ['None', 'Balanced', '2 to 1', '10 to 1', '100 to 1', '1000 to 1']\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for n, weight in enumerate(weights):\n",
    "    # Fit a model\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e20, class_weight=weight, solver='lbfgs')\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    print(model_log)\n",
    "\n",
    "    # Predict\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "    y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    \n",
    "    print('AUC for {}: {}'.format(names[n], auc(fpr, tpr)))\n",
    "    print('-------------------------------------------------------------------------------------')\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=colors[n],\n",
    "             lw=lw, label='ROC curve {}'.format(names[n]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "##Oversampling and undersampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# https://jair.org/index.php/jair/article/view/10302/24590\n",
    "#\n",
    "# Previous original class distribution\n",
    "print('Original class distribution: \\n')\n",
    "print(y.value_counts())\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train) \n",
    "# Preview synthetic sample class distribution\n",
    "print('-----------------------------------------')\n",
    "print('Synthetic sample class distribution: \\n')\n",
    "print(pd.Series(y_train_resampled).value_counts()) \n",
    "\n",
    "#now see some different ratios\n",
    "# Now let's compare a few different ratios of minority class to majority class\n",
    "ratios = [0.1, 0.25, 0.33, 0.5, 0.7, 1]\n",
    "names = ['0.1', '0.25', '0.33','0.5','0.7','even']\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for n, ratio in enumerate(ratios):\n",
    "    # Fit a model\n",
    "    smote = SMOTE(sampling_strategy=ratio)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train) \n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver ='lbfgs')\n",
    "    model_log = logreg.fit(X_train_resampled, y_train_resampled)\n",
    "    print(model_log)\n",
    "\n",
    "    # Predict\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "    y_score = logreg.decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    \n",
    "    print('AUC for {}: {}'.format(names[n], auc(fpr, tpr)))\n",
    "    print('-------------------------------------------------------------------------------------')\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=colors[n],\n",
    "             lw=lw, label='ROC curve {}'.format(names[n]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-class-imbalance-problems-lab\n",
    "# similar to above.  predicting credit card fraud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Load a compressed csv file\n",
    "df = pd.read_csv('creditcard.csv.gz', compression='gzip')\n",
    "\n",
    "# Print the first five rows of data\n",
    "df.head(5)\n",
    "\n",
    "#preview class imbalance\n",
    "# Count the number of fraudulent/infraudulent purchases\n",
    "df.Class.value_counts()\n",
    "#define target/predictors\n",
    "# Your code here\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "#find imbalance in train/test\n",
    "# Training set\n",
    "print('Raw counts: \\n')\n",
    "print(y_train.value_counts())\n",
    "print('-----------------------------------')\n",
    "print('Normalized counts: \\n')\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print('\\n')\n",
    "# Test set\n",
    "print('Raw counts: \\n')\n",
    "print(y_test.value_counts())\n",
    "print('-----------------------------------')\n",
    "print('Normalized counts: \\n')\n",
    "print(y_test.value_counts(normalize=True))\n",
    "print('\\n')\n",
    "\n",
    "#create initial model\n",
    "# Initial Model\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e20, class_weight=None, solver='liblinear')#, solver='lbfgs')\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "# Probability scores for test set\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "# False positive rate and true positive rate\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "# Seaborn's beautiful styling\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "# Print AUC\n",
    "print('AUC for {}: {}'.format(\"no balancing\", auc(fpr, tpr)))\n",
    "print('-------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# Plot the ROC curve\n",
    "colors = sns.color_palette('Set2')\n",
    "names=['No Balancing']\n",
    "n=0\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color=colors[n],\n",
    "         lw=lw, label='ROC curve {}'.format(names[n]))\n",
    "\n",
    "##plot confusion matrix\n",
    "# Plot confusion matrix of the test set \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_hat_test = model_log.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_hat_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##tune the model\n",
    "# Now let's compare a few different regularization performances on the dataset:\n",
    "C_param_range = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "names = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for n, c in enumerate(C_param_range):\n",
    "    # Fit a model\n",
    "    #print(n, c)\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=c, solver='liblinear')\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    print(model_log) # Preview model params\n",
    "\n",
    "    # Predict\n",
    "    y_hat_test = model_log.predict(X_test)\n",
    "\n",
    "    y_score = model_log.decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    \n",
    "    print('AUC for {}: {}'.format(names[n], auc(fpr, tpr)))\n",
    "    print('-------------------------------------------------------')\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=colors[n],\n",
    "             lw=lw, label='ROC curve Normalization Weight: {}'.format(names[n]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "##use SMOTE to improve on miniority class\n",
    "# Previous original class distribution\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Fit SMOTE to training data\n",
    "X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train) \n",
    "\n",
    "# Preview synthetic sample class distribution\n",
    "print('\\n')\n",
    "print(pd.Series(y_train_resampled).value_counts()) \n",
    "\n",
    "#now rebuild models on SMOTEd sets\n",
    "# Previous original class distribution\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Fit SMOTE to training data\n",
    "X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train) \n",
    "\n",
    "# Preview synthetic sample class distribution\n",
    "print('\\n')\n",
    "print(pd.Series(y_train_resampled).value_counts()) \n",
    "\n",
    "###REMEMBER!  DON\"T SMOTE then TRAIN-TEST - data leakage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-logistic-regression-section-recap\n",
    "\n",
    "#https://github.com/ericthansen/dsc-mle-logistic-reg-intro\n",
    "\n",
    "#https://github.com/ericthansen/dsc-mle-review\n",
    "\n",
    "#https://github.com/ericthansen/dsc-mle-logistic-regression\n",
    "##Super important links!\n",
    "#https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
    "#https://www.bloomberg.com/opinion/articles/2018-10-16/amazon-s-gender-biased-algorithm-is-not-alone\n",
    "# https://www.bostonglobe.com/business/2017/12/21/the-software-that-runs-our-lives-can-bigoted-and-unfair-but-can-fix/RK4xG4gYxcVNVTIubeC1JI/story.html\n",
    "# https://www.bostonglobe.com/ideas/2017/07/07/why-artificial-intelligence-far-too-human/jvG77QR5xPbpwBL2ApAFAN/story.html\n",
    "# https://www.npr.org/2016/03/14/470427605/can-computers-be-racist-the-human-like-bias-of-algorithms\n",
    "# https://onlinecourses.science.psu.edu/stat504/node/150/\n",
    "# https://web.stanford.edu/~hastie/ElemStatLearn//\n",
    "\n",
    "\n",
    "# https://github.com/ericthansen/dsc-gradient-descent-review\n",
    "\n",
    "# https://github.com/ericthansen/dsc-gradient-descent-lab\n",
    "\n",
    "# rss function\n",
    "def rss(m, X=df['budget'], y=df['domgross']):\n",
    "    model = m * X\n",
    "    residuals = model - y\n",
    "    total_rss = residuals.map(lambda x: x**2).sum()\n",
    "    return total_rss\n",
    "\n",
    "#gradient descent - first look at curve\n",
    "x = np.linspace(start=-3, stop=5, num=10**3)\n",
    "y = [rss(xi) for xi in x]\n",
    "plt.plot(x, y)\n",
    "plt.title('RSS Loss Function for Various Values of m')\n",
    "plt.show()\n",
    "\n",
    "##the instructor solution:\n",
    "# The algorithm starts at x=1.5\n",
    "cur_x = 1.5 \n",
    "\n",
    "# Initialize a step size\n",
    "alpha = 1*10**(-7)\n",
    "\n",
    "# Initialize a precision\n",
    "precision = 0.0000000001\n",
    "\n",
    "# Helpful initialization\n",
    "previous_step_size = 1 \n",
    "\n",
    "# Maximum number of iterations\n",
    "max_iters = 10000 \n",
    "\n",
    "# Iteration counter\n",
    "iters = 0 \n",
    "\n",
    "# Create a loop to iterate through the algorithm until either the max_iteration or precision conditions is met\n",
    "while (previous_step_size > precision) & (iters < max_iters):\n",
    "    print('Current value: {} RSS Produced: {}'.format(cur_x, rss(cur_x)))\n",
    "    prev_x = cur_x\n",
    "    # Calculate the gradient. This is often done by hand to reduce computational complexity.\n",
    "    # For here, generate points surrounding your current state, then calculate the rss of these points\n",
    "    # Finally, use the np.gradient() method on this survey region. \n",
    "    # This code is provided here to ease this portion of the algorithm implementation\n",
    "    x_survey_region = np.linspace(start = cur_x - previous_step_size , stop = cur_x + previous_step_size , num = 101)\n",
    "    rss_survey_region = [np.sqrt(rss(m)) for m in x_survey_region]\n",
    "    gradient = np.gradient(rss_survey_region)[50] \n",
    "    cur_x -= alpha * gradient # Move opposite the gradient\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "    iters+=1\n",
    "\n",
    "    \n",
    "# The output for the above will be: ('The local minimum occurs at', 1.1124498053361267)    \n",
    "print(\"The local minimum occurs at\", cur_x)\n",
    "\n",
    "##my solution:\n",
    "# Set a starting point\n",
    "cur_x = -3\n",
    "\n",
    "# Initialize a step size\n",
    "alpha = 10**-8\n",
    "\n",
    "# Initialize a precision\n",
    "precision = 0.0000001 \n",
    "\n",
    "# Helpful initialization\n",
    "previous_step_size = 1 \n",
    "\n",
    "# Maximum number of iterations\n",
    "max_iters = 10000 \n",
    "\n",
    "# Iteration counter\n",
    "iters = 0 \n",
    "\n",
    "# Create a loop to iterate through the algorithm until either the max_iteration or precision conditions is met\n",
    "# Your code here; create a loop as described above\n",
    "for i in range(max_iters):\n",
    "    # Calculate the gradient. This is often done by hand to reduce computational complexity.\n",
    "    # For here, generate points surrounding your current state, then calculate the rss of these points\n",
    "    # Finally, use the np.gradient() method on this survey region. \n",
    "    # This code is provided here to ease this portion of the algorithm implementation\n",
    "    x_survey_region = np.linspace(start = cur_x - previous_step_size , stop = cur_x + previous_step_size , num = 101)\n",
    "    rss_survey_region = [np.sqrt(rss(m)) for m in x_survey_region]\n",
    "    gradient = np.gradient(rss_survey_region)[50] \n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print('iter:{}, gradient: {}, alpha*grad:{}, xcur:{}'.format(i, gradient, -gradient*alpha, cur_x))\n",
    "    \n",
    "    # Update the current x, by taking an \"alpha sized\" step in the direction of the gradient\n",
    "    cur_x +=  - alpha * gradient\n",
    "    # Update the iteration number\n",
    "    iters +=1\n",
    "    if abs(alpha * gradient) < precision:\n",
    "        break\n",
    "# The output for the above will be: ('The local minimum occurs at', 1.1124498053361267)    \n",
    "print(\"The local minimum occurs at\", cur_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-coding-logistic-regression-from-scratch\n",
    "# https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "#linear regression setup\n",
    "import numpy as np\n",
    "\n",
    "def predict_y(X, w):\n",
    "    return np.dot(X,w)\n",
    "#sigmoid setup\n",
    "def sigmoid(x):\n",
    "    x = np.array(x)\n",
    "    return 1/(1 + np.e**(-1*x))\n",
    "# plot sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot sigmoid\n",
    "x = np.linspace(start=-20, stop=20, num=10**4)\n",
    "y = [sigmoid(xi) for xi in x]\n",
    "plt.scatter(x, y)\n",
    "plt.title('The Sigmoid Function')\n",
    "plt.show()\n",
    "\n",
    "# design gradient descnet with sigmoid\n",
    "# Your code here\n",
    "def grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n",
    "    \"\"\"Be sure to set default behavior for the initial_weights parameter.\"\"\"\n",
    "    if initial_weights == None:\n",
    "        initial_weights = np.ones((X.shape[1],1)).flatten()\n",
    "    weights_col= pd.DataFrame(initial_weights)\n",
    "    weights = initial_weights\n",
    "    # Create a for loop of iterations\n",
    "    for iteration in range(max_iterations):\n",
    "        # Generate predictions using the current feature weights\n",
    "        predictions = sigmoid(np.dot(X,weights))\n",
    "        # Calculate an error vector based on these initial predictions and the correct labels\n",
    "        error_vector = y - predictions\n",
    "        # Calculate the gradient \n",
    "        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n",
    "        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n",
    "        # For more details on the derivation, see the additional resources section below.\n",
    "        gradient = np.dot(X.transpose(),error_vector)\n",
    "        # Update the weight vector take a step of alpha in direction of gradient \n",
    "        weights += alpha * gradient\n",
    "        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n",
    "    # Return finalized weights\n",
    "    return weights, weights_col\n",
    "\n",
    "#Rrun the algorithm\n",
    "import pandas as pd\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Create the predictor and target variables\n",
    "y = df['target']\n",
    "X = df.drop(columns=['target'], axis=1)\n",
    "\n",
    "print(y.value_counts())\n",
    "X.head()\n",
    "\n",
    "#run grad descent\n",
    "weights, weight_col = grad_desc(X, y, 10000, 0.001)\n",
    "weight_col.columns = np.arange(len(weight_col.columns))\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for (i, j) in enumerate(weights):\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.title(list(X)[i], size='medium')\n",
    "    plt.plot(weight_col.iloc[i].T)\n",
    "    plt.axis('tight')\n",
    "    \n",
    "#same thing, but with skikitlearn\n",
    "# Your code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e16, random_state=2, solver='liblinear')\n",
    "logreg.fit(X, y)\n",
    "#compare the models\n",
    "# Your code here\n",
    "print(\"Scikit-learn's weights:\", logreg.coef_[0])\n",
    "print(\"Our manual regression weights:\", weights)\n",
    "\n",
    "# refactor the grad descent above to include the cost function - then plot it\n",
    "# Your code here\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n",
    "    \"\"\"Be sure to set default behavior for the initial_weights parameter.\"\"\"\n",
    "    if initial_weights == None:\n",
    "        initial_weights = np.ones((X.shape[1],1)).flatten()\n",
    "    weights = initial_weights\n",
    "    costs = []\n",
    "    # Create a for loop of iterations\n",
    "    for iteration in range(max_iterations):\n",
    "        # Generate predictions using the current feature weights\n",
    "        predictions = sigmoid(np.dot(X,weights))\n",
    "        # Calculate an error vector based on these initial predictions and the correct labels\n",
    "        error_vector = y - predictions\n",
    "        # Calculate the gradient (transpose of X times error is the gradient)\n",
    "        gradient = np.dot(X.transpose(),error_vector)\n",
    "        # Update the weight vector take a step of alpha in direction of gradient \n",
    "        weights += alpha * gradient\n",
    "        # Calculate the cost\n",
    "        cost = ((-y * np.log(predictions))-((1-y)* np.log(1-predictions))).mean()\n",
    "        costs.append(cost)\n",
    "    # Return finalized Weights\n",
    "    return weights, costs\n",
    "\n",
    "max_iterations = 50000\n",
    "weights, costs = grad_desc(X, y, max_iterations, 0.001)\n",
    "print('Coefficient weights:\\n', weights)\n",
    "plt.plot(range(max_iterations), costs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-logistic-regression-model-comparisons-lab\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
