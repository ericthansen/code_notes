{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://math.mit.edu/~gs/linearalgebra/ila0403.pdf #neat lin alg take on lin regression\n",
    "    https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf #academic approach to lin reg in matrix format\n",
    "    https://setosa.io/ev/ordinary-least-squares-regression/ #neat visualization page on lin regress\n",
    "\n",
    "https://nbviewer.jupyter.org/\n",
    "http://www.csszengarden.com/\n",
    "# bash reference manual: https://tiswww.case.edu/php/chet/bash/bashref.html\n",
    "#codepen thing: https://codepen.io/curiositypaths/pen/WddzQM?editors=1100\n",
    "#pep8 https://github.com/ericthansen/dsc-PEP8-online-ds-sp-000\n",
    "#fast fourier transform:  \n",
    "np.fft.fft(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#sorting a dictionary by values:\n",
    "dict1 = {1: 1, 2: 9, 3: 4}\n",
    "sorted_tuples = sorted(dict1.items(), key=lambda item: item[1])\n",
    "print(sorted_tuples)  # [(1, 1), (3, 4), (2, 9)]\n",
    "sorted_dict = {k: v for k, v in sorted_tuples}\n",
    "\n",
    "print(sorted_dict)  # {1: 1, 3: 4, 2: 9}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pandas stuff: \n",
    "#pd.head, tail, info, index, columns, dtypes, shape, .iloc[index or slice], .loc['col name']\n",
    "#create a pandas \"series\" out of a dataframe\n",
    "#Filter on more than one column: #df.loc[(df['Salary_in_1000']>=100) & (df['Age']< 60) & (df['FT_Team'].str.startswith('S')),['Name','FT_Team']]\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df.info()\n",
    "df.index\n",
    "df.columns\n",
    "df.dtypes\n",
    "df.shape\n",
    "df.iloc[3]\n",
    "df.iloc[:, 3:7]\n",
    "df.loc[:, 'magnesium']\n",
    "df.loc[df['alcohol'] < 12]\n",
    "df.loc[df['alcohol'] < 12, ['color_intensity']]\n",
    "col_intensity = df['color_intensity']\n",
    "col_intensity[col_intensity > 8] \n",
    "df.loc[df['color_intensity'] > 10, 'color_intensity'] = 10\n",
    "df.loc[df['color_intensity'] > 7, 'shade'] = 'dark'\n",
    "df.loc[df['color_intensity'] <= 7, 'shade'] = 'light'\n",
    "pd.read_csv()\n",
    "pd.read_excel()\n",
    "pd.read_json()\n",
    "pd.DataFrame.from_dict()\n",
    "df.to_csv()\n",
    "df.to_excel()\n",
    "df.to_json()\n",
    "df.to_dict()\n",
    "df = df.drop(0)\n",
    "df.head(2)\n",
    "df = pd.read_csv('Data/ACS_16_5YR_B24011_with_ann.csv', header=1)\n",
    "df = pd.read_csv('Data/ACS_16_5YR_B24011_with_ann.csv', \n",
    "                 usecols=[0, 1, 2, 5, 6], encoding='latin-1')\n",
    "df = pd.read_csv('Data/ACS_16_5YR_B24011_with_ann.csv', usecols=['GEO.id', 'GEO.id2'], encoding='latin-1')\n",
    "df1 = pd.read_excel('Data/Yelp_Selected_Businesses.xlsx', header=2)\n",
    "df2 = pd.read_excel('Data/Yelp_Selected_Businesses.xlsx', sheet_name=2, header=2)\n",
    "df = pd.read_excel('Data/Yelp_Selected_Businesses.xlsx', sheet_name='Biz_id_RESDU', header=2)\n",
    "workbook = pd.ExcelFile('Data/Yelp_Selected_Businesses.xlsx')\n",
    "workbook.sheet_names\n",
    "df = workbook.parse(sheet_name=1, header=2)\n",
    "#Writedata out\n",
    "df.to_csv('NewSavedView.csv', index=False) \n",
    "df.to_excel('NewSavedView.xlsx')\n",
    "\n",
    "\n",
    "rankings_pd.rename(columns = {'test':'TEST'}, inplace = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Group the data by weekend/weekday and plot the sum of the numeric columns\n",
    "wkend = grouped.groupby('is_weekend').sum()\n",
    "#wkend\n",
    "wkend[['entries', 'exits']].plot(kind='barh')\n",
    "plt.show()#mapping\n",
    "df['On_N_Line'] = df['LINENAME'].map(contains_n)\n",
    "df['On_N_Line'].value_counts(normalize=True)\n",
    "df['On_N_Line'] = df['LINENAME'].map(lambda x: 'N' in x)\n",
    "\n",
    "# We can also check an individual column type rather then all \n",
    "print(df['ENTRIES'].dtype) \n",
    "\n",
    "# Changing the column to float\n",
    "df['ENTRIES'] = df['ENTRIES'].astype(float) \n",
    "# Converting Back\n",
    "print(df['ENTRIES'].dtype) \n",
    "df['ENTRIES'] = df['ENTRIES'].astype(int)\n",
    "print(df['ENTRIES'].dtype)\n",
    "\n",
    "df['LINENAME'] = df['LINENAME'].astype(int)\n",
    "\n",
    "pd.to_datetime(df['DATE']).head() \n",
    "# Notice we include delimiters (in this case /) between the codes \n",
    "pd.to_datetime(df['DATE'], format='%m/%d/%Y').head()\n",
    "\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "df['DATE'].dt.day_name().head()\n",
    "# If you don't pass the axis=1 parameter, pandas will try and drop a row with the specified index\n",
    "df = df.drop('C/A', axis=1) \n",
    "\n",
    "df = df.set_index('date')#but can be buggy with losing the new index\n",
    "df = df.reset_index()\n",
    "\n",
    "# Change the index to 'linename'\n",
    "df = df.set_index('linename', drop = False)\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "#\n",
    "# Group the data by day of week and plot the sum of the numeric columns\n",
    "grouped = df.groupby('day_of_week').sum()\n",
    "grouped.plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Use this dictionary to create a new column \n",
    "weekend_map = {'Monday':False, 'Tuesday':False, 'Wednesday':False, 'Thursday':False, \n",
    "               'Friday':False, 'Saturday':True, 'Sunday':True}\n",
    "\n",
    "# Add a new column 'is_weekend' that maps the 'day_of_week' column using weekend_map\n",
    "grouped['is_weekend'] = grouped['day_of_week'].map(weekend_map)\n",
    "#grouped\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.describe()\n",
    "df.mean()\n",
    "df['Fare'].mean()\n",
    "df['Age'].quantile(.9)\n",
    "df['Age'].median()\n",
    ".mode() -- the mode of the column\n",
    ".count() -- the count of the total number of entries in a column\n",
    ".std() -- the standard deviation for the column\n",
    ".var() -- the variance for the column\n",
    ".sum() -- the sum of all values in the column\n",
    ".cumsum() -- the cumulative sum, where each cell index contains the sum of all indices lower than, and including, itself.\n",
    "\n",
    "Summary Statistics for Categorical Columns\n",
    "df['Embarked'].unique()\n",
    "df['Embarked'].value_counts()\n",
    "\n",
    "Calculating on the Fly with .apply() and .applymap()\n",
    "# Quick function to convert every value in the DataFrame to a string\n",
    "string_df = df.applymap(lambda x: str(x))\n",
    "string_df.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = pd.DataFrame({'A':np.random.randn(365).cumsum(),\n",
    "                    'B':np.random.randn(365).cumsum() + 25,\n",
    "                    'C':np.random.randn(365).cumsum() - 25}, \n",
    "                     index = pd.date_range('1/1/2018', periods = 365))\n",
    "Scatter Plots\n",
    "The DataFrame.plot() allows us to plot a number of different kinds of plots. We can select which plot we want to use by specifying the kind parameter. Here is a complete list from the documentation:\n",
    "\n",
    "kind : str\n",
    "\n",
    "‘line’ : line plot (default)\n",
    "‘bar’ : vertical bar plot\n",
    "‘barh’ : horizontal bar plot\n",
    "‘hist’ : histogram\n",
    "‘box’ : boxplot\n",
    "‘kde’ : Kernel Density Estimation plot\n",
    "‘density’ : same as ‘kde’\n",
    "‘area’ : area plot\n",
    "‘pie’ : pie plot\n",
    "‘scatter’ : scatter plot\n",
    "‘hexbin’ : hexbin plot\n",
    "    \n",
    "data.plot('A', 'B', kind='scatter');\n",
    "\n",
    "data.plot.scatter('A', 'C', \n",
    "                  c = 'B',\n",
    "                  s = data['B'],\n",
    "                  colormap = 'viridis');\n",
    "\n",
    "ax = data.plot.scatter('A', 'C', \n",
    "                        c = 'B',\n",
    "                        s = data['B'],\n",
    "                        colormap = 'viridis');\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Manipulating Pandas plot objects in matplotlib')\n",
    "\n",
    "# Box Plots\n",
    "data.plot.box();\n",
    "\n",
    "# Histograms \n",
    "# Setting alpha level to inspect distribution overlap\n",
    "data.plot.hist(alpha = 0.7); \n",
    "\n",
    "# Kernel Density Estimate plots\n",
    "# Useful for visualizing an estimate of a variable's probability density function. \n",
    "# Kernel density estimation applications will be covered later\n",
    "data.plot.kde();\n",
    "\n",
    "pd.plotting.scatter_matrix(iris);\n",
    "\n",
    "\n",
    "# Set a colormap with 3 colors to show species\n",
    "colormap = ('skyblue', 'salmon', 'lightgreen')\n",
    "plt.figure()\n",
    "pd.plotting.parallel_coordinates(iris, 'species', color=colormap);\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cdf#stuff from project solution:\n",
    "pd.plotting.scatter_matrix(df[['LotArea', 'SalePrice', 'YrSold', 'YearBuilt']], figsize=(10,10));\n",
    "\n",
    "#nicer plots\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.hist(df['SalePrice'], bins='auto')\n",
    "ax.set_title('Distribution of Sale Prices')\n",
    "ax.set_xlabel('Sale Price')\n",
    "ax.set_ylabel('Number of houses')\n",
    "ax.axvline(df['SalePrice'].mean(), color='black');\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.hist(df['LotArea'], bins='auto');\n",
    "ax.set_title('Distribution of Sizes of Lot')\n",
    "ax.set_xlabel('Size of Lot')\n",
    "ax.set_ylabel('Number of Houses');\n",
    "\n",
    "\n",
    "df.corr()['SalePrice'].sort_values()\n",
    "\n",
    "# Perform an Exploration of home values by age\n",
    "df['age'] = df['YrSold'] - df['YearBuilt']\n",
    "df['decades'] = df.age // 10\n",
    "to_plot = df.groupby('decades').SalePrice.mean()\n",
    "to_plot.plot(kind='barh', figsize=(10,8))\n",
    "plt.ylabel('House Age in Decades')\n",
    "plt.xlabel('Average Sale Price of Homes')\n",
    "plt.title('Average Home Values by Home Age');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#data cleaning\n",
    "df.groupby('business_id')['stars'].mean().head()\n",
    "\n",
    "#check for dups\n",
    "df[df.duplicated(keep=False)].sort_values(by='business_id')\n",
    "#Remove dups:\n",
    "df = df[df.duplicated()]\n",
    "# Duplicates should no longer exist\n",
    "df[df.duplicated(keep=False)].sort_values(by='business_id')\n",
    "\n",
    "#PIVOT TABLESSSSS\n",
    "# This transforms the data into a person by person spreadsheet and what stars they gave various restaurants\n",
    "# Most values are NaN (null or missing) because people only review a few restaurants of those that exist\n",
    "usr_reviews = df.pivot(index='user_id', columns='business_id', values='stars')\n",
    "usr_reviews.head()\n",
    "\n",
    "\n",
    "\n",
    "#Count # of words in a review:\n",
    "df['text'].map(lambda x: len(x.split())).head()\n",
    "\n",
    "\n",
    "#THis line is a mess but combines a lot of good things\n",
    "df['text'].map(lambda x: 'Good' if any([word in x.lower() for word in ['awesome', 'love', 'good', 'great']]) else 'Bad').head()\n",
    "\n",
    "\n",
    "#sorting by last name - clever\n",
    "# Without a key\n",
    "names = ['Miriam Marks','Sidney Baird','Elaine Barrera','Eddie Reeves','Marley Beard',\n",
    "         'Jaiden Liu','Bethany Martin','Stephen Rios','Audrey Mayer','Kameron Davidson',\n",
    "         'Carter Wong','Teagan Bennett']\n",
    "sorted(names)\n",
    "# Sorting by last name\n",
    "names = ['Miriam Marks','Sidney Baird','Elaine Barrera','Eddie Reeves','Marley Beard',\n",
    "         'Jaiden Liu','Bethany Martin','Stephen Rios','Audrey Mayer','Kameron Davidson',\n",
    "'Teagan Bennett']\n",
    "sorted(names, key=lambda x: x.split()[1])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(10,10))\n",
    "x = np.linspace(start=-10, stop=10, num=10*83)\n",
    "for i in range(12):\n",
    "    row = i//4\n",
    "    col = i%4\n",
    "    ax = axes[row, col]\n",
    "    ax.scatter(x, x**i)\n",
    "    ax.set_title('Plot of x^{}'.format(i))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#aggregation/groupby\n",
    "df.groupby('Sex')\n",
    "df.groupby('Sex').sum()\n",
    "#can do sum, mean, median, min, max, etc, to see all the options:\n",
    "grouped_df = df.groupby('Sex')\n",
    "grouped_df.<TAB>\n",
    "\n",
    "#on multiple groups:\n",
    "df.groupby(['Sex', 'Pclass']).mean()\n",
    "\n",
    "#for specific columns\n",
    "df.groupby(['Sex', 'Pclass'])['Survived'].mean()\n",
    "\n",
    "#and to get specific values out of that output:\n",
    "grouped = df.groupby(['Sex', 'Pclass'])['Survived'].mean()\n",
    "print(grouped['female'])\n",
    "\n",
    "# Output:\n",
    "# Pclass\n",
    "# 1    0.968085\n",
    "# 2    0.921053\n",
    "# 3    0.500000\n",
    "# Name: Survived, dtype: float64\n",
    "\n",
    "print(grouped['female'][1])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#some join stuff\n",
    "some_dataframe.set_index('name_of_index_column', inplace=True)\n",
    "to_concat = [df1, df2, df3]\n",
    "big_df = pd.concat(to_concat)\n",
    "\n",
    "joined_df = df1.join(df2, how='inner')\n",
    "#NOTE: If both tables contain columns with the same name, the join will throw an error due to a naming collision, \n",
    "#since the resulting table would have multiple columns with the same name. To solve this, pass in a value to \n",
    "#lsuffix= or rsuffix=, which will append this suffix to the offending columns to resolve the naming collisions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#remember Wide format vs long format\n",
    "\n",
    "dataframe.groupby(['Embarked', 'Pclass']).mean()\n",
    "\n",
    "#pivot tables:\n",
    "some_dataframe.pivot(index='State', columns='Gender', values='Deaths_mean')\n",
    "#One of the quickest ways to manipulate the format of a dataset in python is to use the \n",
    "# .stack() and unstack() methods built into pandas DataFrames. - sometimes necessary to unstack \n",
    "multiple times!\n",
    "pivot.plot(kind='barh', figsize=(15,8), stacked=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Drop and reformat type\n",
    "# Your code here\n",
    "to_drop = df.loc[df['Population'] == 'Not Applicable']\n",
    "#joined_df = df.join(to_drop, how='left', rsuffix='_right')\n",
    "df.drop(to_drop.index, axis=0, inplace=True)\n",
    "df['Population']=df['Population'].astype('int64')\n",
    "df['Population'].dtype\n",
    "\n",
    "#slicing/pivot table\n",
    "grouped = df.groupby(['State','Gender'])['Deaths','Population'].agg(['mean','min','max','std'])\n",
    "\n",
    "# We could also flatten these:\n",
    "cols0 = grouped.columns.get_level_values(0)\n",
    "cols1 = grouped.columns.get_level_values(1)\n",
    "grouped.columns = [col0 + '_' + col1 if col1 != '' else col0 for col0, col1 in list(zip(cols0, cols1))]\n",
    "# The list comprehension above is more complicated then what we need but creates a nicer formatting and\n",
    "# demonstrates using a conditional within a list comprehension.\n",
    "# This simpler version works but has some tail underscores where col1 is blank:\n",
    "# grouped.columns = [col0 + '_' + col1 for col0, col1 in list(zip(cols0, cols1))]\n",
    "grouped.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "norm#detecting missing data\n",
    "df.isna()\n",
    "df.isna().sum()\n",
    "#for normally distribution, outliers: more than 3 std from the median\n",
    "#for nonnormal, things that are more than 1.5 IQR from q1 or q3\n",
    "\n",
    "\n",
    "#To detect placeholder values in categorical data, get the unique values in the column and see if there are any values that don't match up with your expectations. Pandas provides a built-in method for this. For instance, in the titanic dataset, we can check the unique values of the Embarked column by typing:\n",
    "\n",
    "df['Embarked'].unique()\n",
    "\n",
    "To drop all rows containing missing values in a DataFrame, use \n",
    "dataframe.dropna()\n",
    " Note that this returns a copy of the dataframe with the rows in question dropped -- however, you can mutate the DataFrame in place by passing in inplace=True as a parameter to the method call.\n",
    " \n",
    "Pandas provides an easy way for us to replace null values. For instance, if we wanted to replace all null values in the Fare column with the column median, we would type:\n",
    "\n",
    "df['Fare'].fillna(df['Fare'].median())\n",
    "\n",
    "Numerical data\n",
    "Often, missing values inside a continuously-valued column will cause all sorts of havoc in your models, so leaving the NaNs alone isn't usually an option here. Instead, consider using Coarse Classification, also referred to as Binning. This allows us to convert the entire column from a numerical column to a categorical column by binning our data into categories. For instance, we could deal with the missing values in the Age column by creating a categorical column that separates each person into 10-year age ranges. Anybody between the ages of 0 and 10 would be a 1, 11 to 20 would be a 2, and so on.\n",
    "\n",
    "Once we have binned the data in a new column, we can throw out the numerical version of the column, and just leave the missing values as one more valid category inside our new categorical column!\n",
    "\n",
    "pivoted = grouped.pivot(index='Pclass', columns = 'Sex', values='Age')\n",
    "pivoted\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pivoted.plot(kind='barh')\n",
    "\n",
    "print('Top 5 Values before:\\n', df['Cabin'].value_counts(normalize=True).reset_index()[:5])\n",
    "# Not a useful means of imputing in most cases, but a simple example to recap\n",
    "df.Cabin = df['Cabin'].fillna(value='?')\n",
    "print('Top 5 Values after:\\n', df.Cabin.value_counts(normalize=True).reset_index()[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Set plot space as inline for inline plots and qt for external plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Use plot() function to create a plot using above values on both x and y coordinates. Add a label.\n",
    "plt.plot(data, data, label='Sample Data')\n",
    "\n",
    "# Add a legend to the plot with legend()\n",
    "plt.legend()\n",
    "\n",
    "# Output the final plot\n",
    "plt.show()\n",
    "# Set plot space as inline for inline plots and qt for external plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Use plot() function to create a plot using above values on both x and y coordinates. Add a label.\n",
    "plt.plot(data, data, label='Sample Data')\n",
    "\n",
    "# Add labels for x and y axes\n",
    "plt.xlabel('X Axis Label')\n",
    "plt.ylabel('Y Axis Label')\n",
    "\n",
    "# Add a title for the plot\n",
    "plt.title('PLOT TITLE')\n",
    "\n",
    "# Add a legend to the plot with legend() in lower right corner\n",
    "plt.legend(loc=4)\n",
    "\n",
    "# Output the final plot\n",
    "plt.show()\n",
    "\n",
    "# Define a new figure with matplotlib's .figure() function. \n",
    "new_figure = plt.figure()\n",
    "\n",
    "# Add a subplot to the figure - a new axes\n",
    "ax = new_figure.add_subplot(111)\n",
    "\n",
    "# Generate a line plot \n",
    "ax.plot([1, 4, 6, 8], [10, 15, 27, 32], color='lightblue', linewidth=3, linestyle = '-.')\n",
    "\n",
    "# Draw a scatter plot on same axes\n",
    "ax.scatter([0.5, 2.2, 4.2, 6.5], [21, 19, 9, 26], color='red', marker='o')\n",
    "\n",
    "# Set the limits of x and y for axes\n",
    "ax.set_xlim(0, 9), ax.set_ylim(5,35)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#A standard plot\n",
    "x = np.linspace(start=0, stop=100, num=10**3)\n",
    "y = [xi**2 for xi in x]\n",
    "plt.scatter(x,y)\n",
    "\n",
    "#The same plot with new axes ticks\n",
    "x = np.linspace(start=0, stop=100, num=10**3)\n",
    "y = [xi**2 for xi in x]\n",
    "plt.scatter(x,y)\n",
    "\n",
    "xticks = np.linspace(start=0, stop=100, num=11)\n",
    "yticks = np.linspace(start=0, stop=100**2, num=11)\n",
    "plt.xticks(xticks); #Adding a semicolon after the call will prevent extraneous input from being displayed\n",
    "plt.yticks(yticks);\n",
    "\n",
    "x = np.linspace(start=0, stop=100, num=10**3)\n",
    "y = [xi**2 for xi in x]\n",
    "plt.scatter(x,y)\n",
    "\n",
    "xticks = np.linspace(start=0, stop=200, num=11)\n",
    "yticks = np.linspace(start=0, stop=10**5, num=11)\n",
    "plt.xticks(xticks); #Adding a semicolon after the call will prevent extraneous input from being displayed\n",
    "plt.yticks(yticks);\n",
    "plt.title('Displaying Terrible Use of plt.xticks() and plt.yticks()');\n",
    "\n",
    "x = np.linspace(start=0, stop=100, num=10**3)\n",
    "y = [xi**2 for xi in x]\n",
    "plt.scatter(x,y)\n",
    "\n",
    "xticks = np.linspace(start=0, stop=50, num=11)\n",
    "yticks = np.linspace(start=0, stop=.5*10**4, num=11)\n",
    "plt.title('More things to avoid')\n",
    "plt.xticks(xticks); #Adding a semicolon after the call will prevent extraneous input from being displayed\n",
    "plt.yticks(yticks);\n",
    "\n",
    "# Define a new figure with matplotlib's .plot() function. Set the size of figure space\n",
    "new_figure = plt.figure(figsize=(10,4))\n",
    "\n",
    "# Add a subplot to the figure - a new axes\n",
    "ax = new_figure.add_subplot(121)\n",
    "\n",
    "# Add a second subplot to the figure - a new axes\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "# Generate a line plot on first axes\n",
    "ax.plot([1, 4, 6, 8], [10, 15, 27, 32], color='lightblue', linewidth=3, linestyle = '-.')\n",
    "\n",
    "# Draw a scatter plot on 2nd axes\n",
    "ax2.scatter([0.5, 2.2, 4.2, 6.5], [21, 19, 9, 26], color='red', marker='o')\n",
    "\n",
    "# Set the limits of x and y for first axes\n",
    "ax.set_xlim(0, 9), ax.set_ylim(5,35)\n",
    "\n",
    "# Set the limits of x and y for 2nd axes\n",
    "ax2.set_xlim(0, 9), ax2.set_ylim(5,35)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##more subplots\n",
    "x = np.linspace(-10, 10, 101)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10,10))\n",
    "plt.title('Graphs of Various Polynomials')\n",
    "for n in range(1,9):\n",
    "    row = (n-1)//2\n",
    "    col = n%2-1\n",
    "    ax = axes[row][col]\n",
    "    y = [xi**n for xi in x]\n",
    "    ax.plot(x,y)\n",
    "    ax.set_title('x^{}'.format(n))\n",
    "    \n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "ax.plot(x, x+1, color=\"red\", linewidth=0.25)\n",
    "ax.plot(x, x+2, color=\"red\", linewidth=0.50)\n",
    "ax.plot(x, x+3, color=\"red\", linewidth=1.00)\n",
    "ax.plot(x, x+4, color=\"red\", linewidth=2.00)\n",
    "\n",
    "# possible linestype options ‘-‘, ‘–’, ‘-.’, ‘:’, ‘steps’\n",
    "ax.plot(x, x+5, color=\"green\", lw=3, linestyle='-')\n",
    "ax.plot(x, x+6, color=\"green\", lw=3, ls='-.')\n",
    "ax.plot(x, x+7, color=\"green\", lw=3, ls=':')\n",
    "\n",
    "# custom dash\n",
    "line, = ax.plot(x, x+8, color=\"black\", lw=1.50)\n",
    "line.set_dashes([5, 10, 15, 10]) # format: line length, space length, ...\n",
    "\n",
    "# possible marker symbols: marker = '+', 'o', '*', 's', ',', '.', '1', '2', '3', '4', ...\n",
    "ax.plot(x, x+9, color=\"blue\", lw=3, ls='-', marker='+')\n",
    "ax.plot(x, x+10, color=\"blue\", lw=3, ls='--', marker='o')\n",
    "ax.plot(x, x+11, color=\"blue\", lw=3, ls='-', marker='s')\n",
    "ax.plot(x, x+12, color=\"blue\", lw=3, ls='--', marker='1')\n",
    "\n",
    "# marker size and color\n",
    "ax.plot(x, x+13, color=\"purple\", lw=1, ls='-', marker='o', markersize=2)\n",
    "ax.plot(x, x+14, color=\"purple\", lw=1, ls='-', marker='o', markersize=4)\n",
    "ax.plot(x, x+15, color=\"purple\", lw=1, ls='-', marker='o', markersize=8, markerfacecolor=\"red\")\n",
    "ax.plot(x, x+16, color=\"purple\", lw=1, ls='-', marker='s', markersize=8, markerfacecolor=\"yellow\", markeredgewidth=3, markeredgecolor=\"green\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*lost a bunch of stuff*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "tips = sns.load_dataset('tips') # Seaborn comes prepackaged with several different datasets that are great for visualizing!\n",
    "\n",
    "boxplot = sns.boxplot(data=tips[\"total_bill\"])\n",
    "\n",
    "sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n",
    "\n",
    "sns.boxplot(x=\"day\", y=\"total_bill\", hue=\"smoker\", data=tips, palette=\"Set3\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#*SQL\n",
    "\n",
    "cur.execute(\"\"\"SELECT customerName,\n",
    "               COUNT(customerName) AS number_purchases,\n",
    "               MIN(amount) AS min_purchase,\n",
    "               MAX(amount) AS max_purchase,\n",
    "               AVG(amount) AS avg_purchase,\n",
    "               SUM(amount) AS total_spent\n",
    "               FROM customers\n",
    "               JOIN payments\n",
    "               USING(customerNumber)\n",
    "               GROUP BY customerName\n",
    "               ORDER BY SUM(amount) DESC;\"\"\")\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df. columns = [i[0] for i in cur.description]\n",
    "print(len(df))\n",
    "df.head()\n",
    "\n",
    "cur.execute(\"\"\"SELECT city, COUNT(customerNumber) AS number_customers\n",
    "               FROM customers\n",
    "               GROUP BY 1\n",
    "               HAVING COUNT(customerNumber)>=5;\"\"\")\n",
    "               \n",
    "cur.execute(\"\"\"SELECT customerName,\n",
    "               COUNT(amount) AS number_purchases_over_50K\n",
    "               FROM customers\n",
    "               JOIN payments\n",
    "               USING(customerNumber)\n",
    "               WHERE amount >= 50000\n",
    "               GROUP BY customerName\n",
    "               HAVING count(amount) >= 2\n",
    "               ORDER BY count(amount) DESC;\"\"\")\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df. columns = [i[0] for i in cur.description]\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independent stuff inspired by project:  Starting with a dataframe (that perhaps came from csv) then importing into\n",
    "#sql table to do joins\n",
    "import sqlite3\n",
    "from pandas import DataFrame\n",
    "\n",
    "conn = sqlite3.connect('TestDB1.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('CREATE TABLE CARS (Brand text, Price number)')\n",
    "conn.commit()\n",
    "\n",
    "Cars = {'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4'],\n",
    "        'Price': [22000,25000,27000,35000]\n",
    "        }\n",
    "\n",
    "df = DataFrame(Cars, columns= ['Brand', 'Price'])\n",
    "df.to_sql('CARS', conn, if_exists='replace', index = False)\n",
    " \n",
    "c.execute('''  \n",
    "SELECT * FROM CARS\n",
    "          ''')\n",
    "\n",
    "for row in c.fetchall():\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#SQL Joins - default SQL JOIN is an INNER JOIN\n",
    "#sqlite doesn't support OUTER JOINS!  \n",
    "#POSTGRESQL does.\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"SELECT * \n",
    "               FROM orderdetails\n",
    "               JOIN products\n",
    "               ON orderdetails.productCode = products.productCode\n",
    "               LIMIT 10;\n",
    "               \"\"\")\n",
    "\n",
    "# Take results and create DataFrame\n",
    "df = pd.DataFrame(cur.fetchall()) \n",
    "df.columns = [i[0] for i in cur.description]\n",
    "df.head()\n",
    "\n",
    "#more concise: - SQL will know you mean productCode in each table\n",
    "cur.execute(\"\"\"SELECT * FROM orderdetails\n",
    "               JOIN products\n",
    "               USING(productCode)\n",
    "               LIMIT 10;\n",
    "               \"\"\")\n",
    "df = pd.DataFrame(cur.fetchall()) \n",
    "df.columns = [i[0] for i in cur.description]\n",
    "df.head()\n",
    "\n",
    "\n",
    "###aliasing - not using AS\n",
    "cur.execute(\"\"\"SELECT * FROM orderdetails o\n",
    "               JOIN products p\n",
    "               ON o.productCode = p.productCode\n",
    "               LIMIT 10;\n",
    "               \"\"\")\n",
    "df = pd.DataFrame(cur.fetchall()) \n",
    "df.columns = [i[0] for i in cur.description]\n",
    "df.head()\n",
    "\n",
    "\n",
    "#Left Join\n",
    "cur.execute(\"\"\"SELECT * \n",
    "               FROM products\n",
    "               LEFT JOIN orderdetails\n",
    "               USING(productCode);\n",
    "               \"\"\")\n",
    "df = pd.DataFrame(cur.fetchall()) \n",
    "df.columns = [i[0] for i in cur.description]\n",
    "print(len(df))\n",
    "print(len(df[df.orderNumber.isnull()]))\n",
    "df[df.orderNumber.isnull()].head()\n",
    "\n",
    "\n",
    "#Multi-JOIN\n",
    "cur.execute('''SELECT contactFirstName, contactLastName, productName, quantityOrdered, orderDate\n",
    "                FROM orders\n",
    "                JOIN customers\n",
    "                USING(customerNumber)\n",
    "                JOIN orderdetails\n",
    "                USING(orderNumber)\n",
    "                JOIN products\n",
    "                USING(productCode)\n",
    "                ORDER BY orderDate DESC\n",
    "                \n",
    "                ;''')\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df.columns = [i[0] for i in cur.description]\n",
    "print('Number of results:', len(df))\n",
    "df.head()\n",
    "\n",
    "\n",
    "#SUBQUERIES\n",
    "cur.execute(\"\"\"SELECT lastName, firstName, officeCode\n",
    "               FROM employees\n",
    "               WHERE officeCode IN (SELECT officeCode \n",
    "                                    FROM offices \n",
    "                                    JOIN employees\n",
    "                                    USING(officeCode)\n",
    "                                    GROUP BY 1\n",
    "                                    HAVING COUNT(employeeNumber) >= 5);\n",
    "                                    \"\"\")\n",
    "cur.execute(\"\"\"SELECT AVG(customerAvgPayment) AS averagePayment\n",
    "               FROM (SELECT AVG(amount) AS customerAvgPayment\n",
    "                     FROM payments\n",
    "                     JOIN customers USING(customerNumber)\n",
    "                     GROUP BY customerNumber);\"\"\")                                    \n",
    "##SUBQUERIES From Lab:\n",
    "# Your code here\n",
    "cur.execute(''' SELECT employeeNumber, firstName, lastName, count(DISTINCT customerNumber)\n",
    "                FROM employees\n",
    "                JOIN customers\n",
    "                ON employeeNumber = salesRepEmployeeNumber\n",
    "                WHERE customerNumber IN          \n",
    "                    (\n",
    "                    SELECT customerNumber\n",
    "                    FROM customers\n",
    "                    GROUP BY customerNumber \n",
    "                    HAVING AVG(creditLimit) > 15000\n",
    "                    )\n",
    "                GROUP BY employeeNumber\n",
    "                ;''')\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df.columns = [i[0] for i in cur.description]\n",
    "df.head(20)\n",
    "\n",
    "cur.execute(''' \n",
    "                SELECT DISTINCT e.employeeNumber, e.firstName, e.lastName, office.city, e.officeCode\n",
    "                FROM employees e \n",
    "                JOIN offices office\n",
    "                USING(officeCode)\n",
    "                WHERE e.employeeNumber IN\n",
    "                    (--select employees who sold products\n",
    "                    SELECT employeeNumber \n",
    "                    FROM employees \n",
    "                    JOIN customers\n",
    "                    ON employees.employeeNumber = customers.salesRepEmployeeNumber\n",
    "                    JOIN orders\n",
    "                    USING(customerNumber)\n",
    "                    JOIN orderdetails\n",
    "                    USING(orderNumber)\n",
    "                    JOIN products\n",
    "                    USING(productCode)\n",
    "                    WHERE products.productCode IN\n",
    "                        (--select products that qualify\n",
    "                        SELECT products.productCode--,  count(DISTINCT orders.customerNumber) as numCust\n",
    "                        FROM products\n",
    "                        JOIN orderdetails\n",
    "                        USING(productCode)\n",
    "                        JOIN orders\n",
    "                        USING(orderNumber)\n",
    "                        --JOIN customers\n",
    "                        --USING(customerNumber)\n",
    "                        GROUP BY productCode\n",
    "                        HAVING count(DISTINCT orders.customerNumber) < 20\n",
    "                        )\n",
    "                    \n",
    "                    )\n",
    "                              \n",
    "            \n",
    "                ;''')\n",
    "\n",
    "\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df.columns = [i[0] for i in cur.description]\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Using SQL with Pandas\n",
    "\n",
    "# Getting Data using slicing syntax\n",
    "foo_df = bar_df[bar_df[bar_df['Col_1'] > bar_df['Col_2']]]\n",
    "\n",
    "# Using The query method\n",
    "foo_df = bar_df.query(\"Col_1 > Col_2\")\n",
    "\n",
    "# These two lines are equivalent!\n",
    "Note that if you want to use and and or statements with the .query() method, you'll need to use \"&\" and \"|\" instead.\n",
    "\n",
    "foo_df = bar_df.query(\"Col_1 > Col_2 & Col_2 <= Col_3\")\n",
    "\n",
    "\n",
    "\n",
    "from pandasql import sqldf\n",
    "\n",
    "Next, it's helpful to write a lambda function that will make it quicker and easier to write queries. Normally, you would have to pass in the global variables every time we use an object. In order to avoid doing this every time, here's how to write a lambda that does this for you:\n",
    "\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "To write a query, you just format it as a multi-line string!\n",
    "\n",
    "q = \"\"\"SELECT\n",
    "        m.date, m.beef, b.births\n",
    "     FROM\n",
    "        meats m\n",
    "     INNER JOIN\n",
    "        births b\n",
    "           ON m.date = b.date;\"\"\"\n",
    "In order to query DataFrames, you can just pass in the query string you've created to our sqldf object that you stored in pysqldf. This will return a DataFrame.\n",
    "\n",
    "results = pysqldf(q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#From Panda/SQL Query lab\n",
    "\n",
    "#Slicing using .query\n",
    "\n",
    "poor_male_survivors_df = df.query(\"Sex == 'male' & (Pclass == '2' | Pclass == '3')\")\n",
    "\n",
    "first_class_df = df.query('Pclass == \"1\"')\n",
    "second_third_class_df = df.query('Pclass != \"1\"')\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize=(14, 7))\n",
    "plt.title('Titanic Survivors by Ship Class')\n",
    "\n",
    "ax = axes[0][0]\n",
    "ax.hist(first_class_df.query('Survived == 1').Age)\n",
    "ax.set_title('First Class Survivors')\n",
    "\n",
    "ax = axes[1][0]\n",
    "ax.hist(first_class_df.query('Survived == 0').Age)\n",
    "ax.set_title('First Class Non-Survivors')\n",
    "\n",
    "ax = axes[0][1]\n",
    "ax.hist(second_third_class_df.query('Survived == 1').Age)\n",
    "ax.set_title('2/3 Class Survivors')\n",
    "\n",
    "ax = axes[1][1]\n",
    "ax.hist(second_third_class_df.query('Survived == 0').Age)\n",
    "ax.set_title('2/3 Class Non-Survivors')\n",
    "\n",
    "female_children_df = df.query('Sex == \"female\" & Age <= 15')\n",
    "df = df.eval('Age_x_Fare = Age*Fare')\n",
    "\n",
    "\n",
    "#########\n",
    "#SQL in pandas!\n",
    "\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "q3 = \"\"\"SELECT Pclass\n",
    "      FROM df\n",
    "      WHERE Sex = 'female' AND Survived = 1\n",
    "      ;\"\"\"\n",
    "\n",
    "q4 = \"\"\"SELECT Pclass --, Sex, Survived\n",
    "      FROM df\n",
    "      WHERE Sex = 'female' AND Survived != 1\n",
    "      ;\"\"\"\n",
    "#display(pysqldf(q3) )\n",
    "#display(pysqldf(q4) )\n",
    "survived_females_by_pclass_df = pysqldf(q3)\n",
    "died_females_by_pclass_df = pysqldf(q4)\n",
    "\n",
    "# Create and label the histograms for each below!\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize=(14, 7))\n",
    "plt.title('Female Titanic Passengers by Survival')\n",
    "\n",
    "ax = axes[0]\n",
    "y_ax = ['1','2','3','?']\n",
    "x_ax = survived_females_by_pclass_df.Pclass.value_counts().loc[y_ax]\n",
    "ax.barh(y_ax,x_ax)\n",
    "ax.set_title('Female Survivors')\n",
    "\n",
    "\n",
    "ax = axes[1]\n",
    "x_ax = died_females_by_pclass_df.Pclass.value_counts().loc[y_ax]\n",
    "ax.barh(y_ax, x_ax)\n",
    "ax.set_title('Female Non-Survivors')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SQL Data Types:\n",
    "In sqlite, \n",
    "text, integer, real, blob\n",
    "\n",
    "\n",
    "##\n",
    "Database Admin (in sqlite)\n",
    "Remember that you can use the bash ls command to preview files and folders in the current working directory.\n",
    "\n",
    "#Creating a db - just connect to a non-existing db\n",
    "import sqlite3 \n",
    "conn = sqlite3.connect('pets_database.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "#\n",
    "#Creating the cats table\n",
    "cur.execute(\"\"\"CREATE TABLE cats (\n",
    "                                id INTEGER PRIMARY KEY,\n",
    "                                name TEXT,\n",
    "                                age INTEGER,\n",
    "                                breed TEXT )          \n",
    "            \"\"\")\n",
    "       \n",
    "# insert Maru into the pet_database.db here\n",
    "cur.execute('''INSERT INTO cats (name, age, breed) \n",
    "                  VALUES ('Maru', 3, 'Scottish Fold');\n",
    "            ''')\n",
    "            \n",
    "#You can also update a table like this: cursor.execute('''ALTER TABLE cats ADD COLUMN notes text;''')\n",
    "\n",
    "#altering a table\n",
    "#The general pattern is ALTER TABLE table_name ADD COLUMN column_name column_type;\n",
    "\n",
    "\n",
    "#updating data\n",
    "cur.execute('''UPDATE [table name] \n",
    "                  SET [column name] = [new value]\n",
    "                  WHERE [column name] = [value];\n",
    "            ''')\n",
    "#eg:\n",
    "cur.execute('''UPDATE cats SET name = \"Hana\" WHERE name = \"Hannah\";''')\n",
    "\n",
    "\n",
    "You use the DELETE keyword to delete table rows.\n",
    "\n",
    "Similar to the UPDATE keyword, the DELETE keyword uses a WHERE clause to select rows.\n",
    "\n",
    "A boilerplate DELETE statement looks like this:\n",
    "\n",
    "cur.execute('''DELETE FROM [table name] WHERE [column name] = [value];''')\n",
    "Code Along III: DELETE\n",
    "Let's go ahead and delete Lil' Bub from our cats table (sorry Lil' Bub):\n",
    "\n",
    "cur.execute('''DELETE FROM cats WHERE id = 2;''')\n",
    "\n",
    "##Saving Changes\n",
    "While everything may look well and good, if you were to connect to the database from another Jupyter notebook (or elsewhere) the database would appear blank! That is, while the changes are reflected in your current session connection to the database you have yet to commit those changes to the master database so that other users and connections can also view the updates.\n",
    "\n",
    "Before you commit the changes, let's demonstrate this concept.\n",
    "\n",
    "First, preview the results of the table:\n",
    "\n",
    "cur.execute(\"\"\"SELECT * FROM cats;\"\"\").fetchall()\n",
    "\n",
    "#Preview the table via a second current cursor/connection \n",
    "#Don't overwrite the previous connection: you'll lose all of your work!\n",
    "conn2 = sqlite3.connect('pets_database.db')\n",
    "cur2 = conn2.cursor()\n",
    "cur2.execute(\"\"\"SELECT * FROM cats;\"\"\").fetchall()\n",
    "\n",
    "#As you can see, the second connection doesn't currently display any data in the cats table! To make the changes universally accessible commit the changes.\n",
    "\n",
    "#In this case:\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "#and verify it shows up\n",
    "#Preview the table via a reloaded second current cursor/connection \n",
    "conn2 = sqlite3.connect('pets_database.db')\n",
    "cur2 = conn2.cursor()\n",
    "cur2.execute(\"\"\"SELECT * FROM cats;\"\"\").fetchall()\n",
    "\n",
    "\n",
    "\n",
    "##dual key\n",
    "Create a Table for Student Grades\n",
    "Create a new table in the database called \"grades\". In the table, include the following fields: userId, courseId, grade.\n",
    "\n",
    "** This problem is a bit more tricky and will require a dual key. (A nuance you have yet to see.) Here's how to do that:\n",
    "\n",
    "CREATE TABLE table_name(\n",
    "   column_1 INTEGER NOT NULL,\n",
    "   column_2 INTEGER NOT NULL,\n",
    "   ...\n",
    "   PRIMARY KEY(column_1,column_2,...)\n",
    ");"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### SQL INterview QUESTIONS\n",
    "\n",
    "# SQL Interview Questions  - Quiz\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This quiz contains questions on topics you can expect to see in an interview pertaining to SQL and Relational Databases. Some of them are multiple choice, while some are short answer. For these short answer questions, double click on the Jupyter Notebook and type your answer below the line. \n",
    "\n",
    "## Question 1\n",
    "\n",
    "What are the 4 main datatypes in SQLite3? Can we use other common types from other kinds of SQL?\n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    "Integer, real, text, blob\n",
    "\n",
    "\n",
    "\n",
    "## Question 2\n",
    "\n",
    "Explain the relationship between **Primary Keys** and **Foreign Keys**.\n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    "A primary key of a table is a unique identifier for a given row of that table.  Foreign keys are columns of other tables that may not be unique but establishes a relationship between a given table and another table.\n",
    "\n",
    "\n",
    "\n",
    "## Question 3\n",
    "\n",
    "Explain the different types of relationships entities can have in a SQL database. \n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "One to one, one to many, many to many.  \n",
    "\n",
    "\n",
    "## Question 4\n",
    "\n",
    "Explain the various types of JOINs possible with SQL. \n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "In SQL in general, right, left, combined with inner/outer.  Depending on needs, can pull all rows of both tables, all rows in just one, or only rows that exist in both.\n",
    "\n",
    "\n",
    "\n",
    "## Question 5\n",
    "\n",
    "Explain the relationship between Aggregate functions and GROUP BY statements.\n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "Any variable that is being aggregrated should have an accompanying variable by which it is grouped.  I.e. the group by variable(s) determine the collections by which the aggregations are grouped.  \n",
    "E.g., one could aggregate total $ spent on orders by customer, or by salesperson, or all $ spent for entire company; each would entail a different way of grouping $ spent.\n",
    "\n",
    "\n",
    "## Question 6\n",
    "\n",
    "What role do Associative Entities play (JOIN Tables) in many-to-many JOINs?\n",
    "\n",
    "\n",
    "Type your answer below this line:\n",
    "_______________________________________________________________________________________________________________________________\n",
    "Associative Entities act to connect tables that don't have any keys in common necessarily, but may be connected through an intermediate table.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, we practiced answering open-ended interview questions for SQL and Relational Databases. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#MONGO DB \n",
    "Installing and Running MongoDB\n",
    "This part is easy -- in order to install mongoDB, we'll use our favorite package manager, conda! This part works the same, regardless of what operating system you're running.\n",
    "\n",
    "To install MongoDB on your machine, open a terminal or conda command prompt and just type:\n",
    "\n",
    "conda install mongodb\n",
    "\n",
    "Next, we have to create a directory to store our Mongo data files:\n",
    "\n",
    "sudo mkdir -p /data/db\n",
    "\n",
    "Give the directory the correct permission:\n",
    "\n",
    "sudo chown -R `id -un` /data/db\n",
    "\n",
    "###\n",
    "Create / Read / Update / Delete (CRUD) \n",
    "##\n",
    "######### MUST DO THIS - RUNNING MONGO in background\n",
    "start by running mondgod in conda prompt\n",
    "then mongo in gitbash\n",
    "#######\n",
    "then you can run mongo commands like...\n",
    "db.help()\n",
    "db.test.help()\n",
    "\n",
    "Typically, the main way you'll be working with MongoDB is through a Python library called pymongo that allows us to connect to and manipulate mongo databases in our code, just like sqlite3 allowed us to connect to and work with SQLite databases in the last section.\n",
    "\n",
    "#put in the terminal (gitbash) window running mongo: \n",
    "db.test.save( { a: 1 } )\n",
    "\n",
    "\n",
    "IMPORTANT SETUP\n",
    "import pymongo\n",
    "myclient = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "mydb = myclient['example_database']\n",
    "\n",
    "print(myclient.list_database_names())\n",
    "\n",
    "mycollection = mydb['example_collection']\n",
    "\n",
    "#CRUD ops\n",
    "example_customer_data = {'name': 'John Doe', 'address': '123 elm street', 'age': 28}\n",
    "\n",
    "results = mycollection.insert_one(example_customer_data)\n",
    "results\n",
    "results.inserted_id\n",
    "\n",
    "\n",
    "#More than one add at once:\n",
    "customer_2 = {'name': 'Jane Doe', 'address': '234 elm street', 'age': 7}\n",
    "customer_3 = {'name': 'Santa Claus', 'address': 'The North Pole', 'age': 547}\n",
    "customer_4 = {'name': 'John Doe jr.', 'address': '', 'age': 0.5}\n",
    "\n",
    "list_of_customers = [customer_2, customer_3, customer_4]\n",
    "\n",
    "results_2 = mycollection.insert_many(list_of_customers)\n",
    "\n",
    "#Note that we are allowed to assign the unique id for each new document ourselves by just including the key _id and the value we want to assign as that document's id. However, in general, it is a best practice to let the database create the unique keys for each document itself, and to leave that part alone.\n",
    "\n",
    "#finding data in the collection:\n",
    "query_1 = mycollection.find({})\n",
    "for x in query_1:\n",
    "    print(x)\n",
    "    \n",
    "#getting selections of the data:  (one represents which columns you want)\n",
    "query_2 = mycollection.find({}, {'_id': 1, 'name': 1, 'address': 1})\n",
    "for item in query_2:\n",
    "    print(item)\n",
    "\n",
    "(0 reps which columns you don't want)\n",
    "query_3 = mycollection.find({}, {'age': 0})\n",
    "for item in query_3:\n",
    "    print(item)\n",
    "    \n",
    "### filtering for certain records:\n",
    "query_4 = mycollection.find({'name': 'Santa Claus'})\n",
    "for item in query_4:\n",
    "    print(item)\n",
    "    \n",
    "#filtering for recs with certain conditions:\n",
    "query_5 = mycollection.find({\"age\": {\"$gt\": 20}})\n",
    "for item in query_5:\n",
    "    print(item)\n",
    "    \n",
    "#Can also use REGULAR EXPRESSIONS to filter - will learn those later\n",
    "\n",
    "\n",
    "#Updating docs:\n",
    "record_to_update = {'name' : 'John Doe'}\n",
    "update_1 = {'$set': {'age': 29}}\n",
    "update_2 = {'$set': {'birthday': '02/20/1986'}}\n",
    "\n",
    "mycollection.update_one(record_to_update, update_1)\n",
    "mycollection.update_one(record_to_update, update_2)\n",
    "query_6 = mycollection.find({'name': 'John Doe'})\n",
    "for item in query_6:\n",
    "    print(item)\n",
    "    \n",
    "---can also update_many but it's a little messy\n",
    "    \n",
    "##deletion\n",
    "\n",
    "deletion_1 = mycollection.delete_one({'name': 'John Doe'})\n",
    "print(deletion_1.deleted_count)\n",
    "\n",
    "#This will delete everything: (empty set is \"true\" for everything?)\n",
    "mycollection.delete_many({})\n",
    "\n",
    "#############\n",
    "#From lab:\n",
    "#this is neat, it combines a row condition and a selection of columns\n",
    "query_3 = mycollection.find({'Balance': {\"$gt\": 0}},{'_id':0, 'Name': 1, 'Email':1, 'Balance':1})\n",
    "for x in query_3:\n",
    "    print(x)\n",
    "    \n",
    "###A wacky update_one usage:  ###should also consider how to do with update_many\n",
    "names_list = ['John Smith', 'Jane Smith', 'Adam Enbar', 'Avi Flombaum', 'Steven S.']\n",
    "birthdays_list = ['02/20/1986','07/07/1983', '12/02/1982', '04/17/1983', '08/30/1991']\n",
    "\n",
    "def update_birthdays(names, birthdays):\n",
    "    #my_collection.update_many(zip(names_list, birthdays_list))\n",
    "    if(len(names_list) != len(birthdays_list)):\n",
    "        print('Error: incompatible lengths')\n",
    "        return None\n",
    "    for i in range(len(names_list)):\n",
    "\n",
    "        mycollection.update_one({'Name': names_list[i]}, {'$set': {'Birthday': birthdays_list[i]}})\n",
    "        \n",
    "update_birthdays(names_list, birthdays_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##NO-sql recap\n",
    "The four different kinds of NoSQL databases are:\n",
    "\n",
    "Document Stores\n",
    "Key-Value Stores\n",
    "Column Stores\n",
    "Graph Databases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON and APIs - Introduction\n",
    "Introduction\n",
    "In this section, you’ll learn about an additional data type: JSON (which stands for JavaScript Object Notation), as well as APIs (Application Programming Interfaces).\n",
    "\n",
    "JSON is the new standard data format for the web. An older data format that is still used on the web is XML, or Extensible Markup Language, which you’ll have a chance to learn more about in the Appendix. APIs are one of the many ways you’ll access data as a data scientist.\n",
    "\n",
    "Working with JSON files\n",
    "A substantial part of the job of a professional data scientist is to find and access data. You've spent a bunch of time looking at how to pull information from relational databases, but there is lots of information you might need to work with that is either not in a relational database, or that is not exposed to you via a relational database.\n",
    "\n",
    "For example, you might work with a third party website that has a lot of geographical data (perhaps points of interest near state highways). Within their company, they may well store the data within a relational database, but you might have to access it using an API (an Application Programming Interface - a way your computer can talk to their computer to go get some information!). Over the next couple of sections, we'll be looking at accessing data through APIs and enough HTML and CSS to get started with web scraping (downloading information automatically from websites). In this section, you'll look at a key data storage format, JSON, that you may well come across when retrieving data from other web applications or from inside your company.\n",
    "\n",
    "JSON\n",
    "You'll start off this section with a brief introduction to JSON so you know what this file format looks like. You'll then get some hands-on practice loading and parsing data from JSON files into Python.\n",
    "\n",
    "JSON Schemas\n",
    "Once you've learned how to import data that has been stored in the JSON format, you'll look at JSON schemas - a way to describe the expected structure of a given JSON file.\n",
    "\n",
    "Exploring JSON Schemas\n",
    "Finally, you'll get a lot more practice working with JSON schemas, exploring unknown schemas, accessing and manipulating data inside a JSON file and then converting JSON to alternate data formats such as pandas DataFrames. This lab will be a great chance for you to practice your Python programming skills and get comfortable with importing and transforming JSON data - something you may well have to do on a regular basis as a professional data scientist.\n",
    "\n",
    "APIs\n",
    "One of the many ways you'll find yourself accessing data as a professional data scientist is via APIs (Application Programming Interfaces). Typically, you'll send a request and get some data back, often in JSON or XML format. In this section, you'll get some hands-on experience retrieving and working with data provided by a range of different APIs.\n",
    "\n",
    "Introduction to APIs\n",
    "In this section, we'll provide a conceptual introduction to various kinds of APIs and some of the reasons that businesses create them.\n",
    "\n",
    "The Client Server Model\n",
    "We then look at the basic model of \"clients\" and \"servers\" to provide a framework for thinking about how your \"client\" retrieves information from an API \"server\".\n",
    "\n",
    "The Request/Response Cycle\n",
    "Next, we'll look at the fundamental mechanism by which web-based APIs are typically accessed - sending an HTTP request and then processing the response provided by the server. We'll also get a little experience working with HTTP requests using the Python .get() method within the requests package. We also get some hands-on experience retrieving information from NASA using Open Notify.\n",
    "\n",
    "APIs and OAuth\n",
    "Usually, access to a given API is limited to avoid abuse. One of the most common mechanisms for identifying your API requests to make sure they fit within acceptable usage guidelines is OAuth - Open Authorization - a standard for authorizing clients across web requests. In this section, we'll provide an overview of what OAuth is and how it works by looking at how it is implemented by Dropbox.\n",
    "\n",
    "Working with the Yelp API\n",
    "Next, we'll get some practice working with a real API, retrieving information from the Yelp API.\n",
    "\n",
    "Creating interactive maps with Folium\n",
    "We wrap up the section by creating interactive maps with Folium. In the Appendix, we include a Lab where you can build a Geographic Information System using Folium and data obtained from the Yelp API to display it on an interactive map.\n",
    "\n",
    "Summary\n",
    "Whether it’s from an API or a NoSQL store, it's quite possible that some of the data you find yourself working with will be stored using JSON. In this section, you'll build the confidence to be able to import and transform such data.\n",
    "\n",
    "Also, many companies provide access to their data via an API, so being able to connect to and work with data provided via an API is a critical skill as a professional data scientist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree\n",
    "    https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree\n",
    "        https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('nyc_2001_campaign_finance.json') as f:\n",
    "    data = json.load(f)\n",
    "print(type(data))\n",
    "\n",
    "\n",
    "#look at meta data\n",
    "import pandas as pd\n",
    "pd.set_option(\"max_colwidth\", 120)\n",
    "pd.DataFrame(\n",
    "    data=data['meta']['view'].values(),\n",
    "    index=data['meta']['view'].keys(),\n",
    "    columns=[\"value\"]\n",
    ")\n",
    "\n",
    "\n",
    "#look at main data\n",
    "\n",
    "len(data['data'])\n",
    "data['data'][0]\n",
    "data['data'][1]\n",
    "#looks tabular, import into pandas\n",
    "pd.DataFrame(data['data'])\n",
    "\n",
    "#looking at meta again\n",
    "data['meta']['view'].keys()\n",
    "#Ok, description is the 7th one! Let's pull the value associated with the description key:\n",
    "data['meta']['view']['description']\n",
    "\n",
    "\n",
    "\n",
    "############## FROM LAB\n",
    "\n",
    "print(f\"The overall data type is {type(data)}\")\n",
    "print(f\"The keys are {list(data.keys())}\")\n",
    "print()\n",
    "print(\"The value associated with the 'meta' key has metadata, including all of these attributes:\")\n",
    "print(list(data['meta']['view'].keys()))\n",
    "print()\n",
    "print(f\"The value associated with the 'data' key is a list of {len(data['data'])} records\")\n",
    "\n",
    "column_names = []\n",
    "for d in data['meta']['view']['columns']:\n",
    "    column_names.append(d['name'])\n",
    "column_names\n",
    "\n",
    "assert len(column_names) == 19\n",
    "\n",
    "# Print the top 10 candidates by total payments\n",
    "sorted(candidate_total_payments, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(data=data['data'][1:], columns=column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with known json schemas\n",
    "#e.g. new york times API\n",
    "import json\n",
    "with open('ny_times_response.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "data['response']['docs']['headline'] #this causes an error - we treated a list like a dictionary\n",
    "#can check types on these\n",
    "type(data['response'])\n",
    "\n",
    "#Flattening data - reducing depth of dictionary embedding.  combine names of dicts.\n",
    "#So, first let's write a function that takes in that complete dictionary, and returns a copy with only the 'main' and 'kicker' keys and values, now labeled 'headline_main' and 'headline_kicker':\n",
    "def extract_headline_info(headline_dict):\n",
    "    result = {}\n",
    "    result['headline_main'] = headline_dict['main']\n",
    "    result['headline_kicker'] = headline_dict['kicker']\n",
    "    return result\n",
    "\n",
    "#testing it:\n",
    "extract_headline_info(docs[2]['headline'])\n",
    "\n",
    "#another extractor/flattener:\n",
    "def extract_doc_info(doc):\n",
    "    info = extract_headline_info(doc['headline'])\n",
    "    info['pub_date'] = doc['pub_date']\n",
    "    info['word_count'] = doc['word_count']\n",
    "    return info\n",
    "doc_info_list = [extract_doc_info(doc) for doc in docs]\n",
    "doc_info_list\n",
    "\n",
    "#using pandas\n",
    "import pandas as pd\n",
    "pd.DataFrame(data['response']['docs'])\n",
    "\n",
    "#but since doc_info_list is flattened, it looks nicer in pandas\n",
    "pd.DataFrame(doc_info_list)\n",
    "\n",
    "#recreating this from raw data using pandas instead of python\n",
    "# Create dataframe of raw docs info\n",
    "df = pd.DataFrame(data['response']['docs'])\n",
    "\n",
    "# Make new headline_main and headline_kicker columns\n",
    "df['headline_main'] = df['headline'].apply(lambda headline_dict: headline_dict['main'])\n",
    "df['headline_kicker'] = df['headline'].apply(lambda headline_dict: headline_dict['kicker'])\n",
    "\n",
    "# Subset to only the relevant columns\n",
    "df = df[['headline_main', 'headline_kicker', 'pub_date', 'word_count']]\n",
    "df\n",
    "#This is a good general strategy for transforming nested JSON: create a DataFrame and then break out nested features into their own column features.\n",
    "\n",
    "#outputting to json!\n",
    "with open('doc_info_list.json', 'w') as f:\n",
    "    json.dump(doc_info_list, f)\n",
    "    \n",
    "#reopening\n",
    "with open('doc_info_list.json') as f:\n",
    "    doc_info_list_from_disk = json.load(f)\n",
    "\n",
    "doc_info_list_from_disk == doc_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b2067b4b554b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#when json schema is unknown:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output.json'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#check type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output.json'"
     ]
    }
   ],
   "source": [
    "#when json schema is unknown:\n",
    "import json\n",
    "with open('output.json') as f:\n",
    "    data = json.load(f)\n",
    "#check type\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From JSON Lab\n",
    "column_names = []\n",
    "for d in m['view']['columns']:\n",
    "    column_names.append(d['name'])\n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data=data['data'], columns=column_names)\n",
    "df_selected = df[(df['Question'] == 'Current asthma prevalence among adults aged >= 18 years') \n",
    "                 & (df['StratificationCategoryID1']=='OVERALL')\n",
    "                & (df['DataValueTypeID']=='CRDPREV')\n",
    "                 &(df['LocationDesc']!='United States')]\n",
    "print('Number of rows after filtering:',len(df_selected))\n",
    "\n",
    "df_selected=df_selected[['DataValue','LocationDesc']]\n",
    "df_selected['DataValue'] = df_selected['DataValue'].astype('float')\n",
    "df_selected.sort_values('DataValue', ascending=False)[:10]\n",
    "\n",
    "names = list(df_selected.sort_values('DataValue', ascending=False)[:10]['LocationDesc'])\n",
    "values = list(df_selected.sort_values('DataValue', ascending=False)[:10]['DataValue'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(names[::-1], values[::-1]) # Values inverted so highest is at top\n",
    "ax.set_title('Adult Asthma Rates by State in 2016')\n",
    "ax.set_xlabel('Percent 18+ with Asthma');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APIs - application programming interfaces\n",
    "#request/response cycle\n",
    "#client/server model\n",
    "\n",
    "#HTTP request/response cycle:\n",
    "#python has two modules for it: \n",
    "#urllib and urllib2 but these are confusing.\n",
    "#To make these things simpler, one easy-to-use third-party library, known asRequests, is available and most developers prefer to use it instead or urllib/urllib2. It is an Apache2 licensed HTTP library powered by urllib3 and httplib.\n",
    "\n",
    "# Uncomment and install requests if you don't have it already\n",
    "# !pip install requests\n",
    "\n",
    "# Import requests to working environment\n",
    "import requests\n",
    "### Making a request\n",
    "resp = requests.get('https://www.google.com')\n",
    "# Check the returned status code\n",
    "resp.status_code == requests.codes.ok\n",
    "#Once we know that our request was successful and we have a valid response, we can check the returned information using .text property of the response object.\n",
    "print (resp.text)\n",
    "# Read the header of the response - convert to dictionary for displaying k:v pairs neatly\n",
    "dict(resp.headers)\n",
    "#The content of the headers is our required element. You can see the key-value pairs holding various pieces of information about the resource and request. Let's try to parse some of these values using the requests library:\n",
    "print(resp.headers['Date'])  # Date the response was sent\n",
    "print(resp.headers['server'])   # Server type (google web service - GWS)\n",
    "\n",
    "#httpbin.org is a popular website to test different HTTP operations and practice with request-response cycles. Let's use httpbin/get to analyze the response to a GET request. First of all, let's find out the response header and inspect how it looks.\n",
    "\n",
    "r = requests.get('http://httpbin.org/get')\n",
    "\n",
    "response = r.json()  \n",
    "print(r.json())  \n",
    "print(response['args'])  \n",
    "print(response['headers'])  \n",
    "print(response['headers']['Accept'])  \n",
    "print(response['headers']['Accept-Encoding'])  \n",
    "print(response['headers']['Host'])  \n",
    "print(response['headers']['User-Agent'])  \n",
    "print(response['origin'])  \n",
    "print(response['url'])  \n",
    "\n",
    "#Let's use requests object structure to parse the values of headers as we did above.\n",
    "\n",
    "print(r.headers['Access-Control-Allow-Credentials'])  \n",
    "print(r.headers['Access-Control-Allow-Origin'])  \n",
    "print(r.headers['CONNECTION'])  \n",
    "print(r.headers['content-length'])  \n",
    "print(r.headers['Content-Type'])  \n",
    "print(r.headers['Date'])  \n",
    "print(r.headers['server'])  \n",
    "\n",
    "#In some cases, you'll need to pass parameters along with your GET requests. These extra parameters usually take the the form of query strings added to the requested URL. To do this, we need to pass these values in the params parameter. Let's try to access information from httpbin with some user information.\n",
    "credentials = {'user_name': 'FlatironSchool', 'password': 'learnlovecode'}  \n",
    "r = requests.get('http://httpbin.org/get', params=credentials)\n",
    "\n",
    "print(r.url)  \n",
    "print(r.text)  \n",
    "\n",
    "# HTTP POST method\n",
    "# Sometimes we need to send one or more files simultaneously to the server. For example, if a user is submitting a form and the form includes different fields for uploading files, like user profile picture, user resume, etc. Requests can handle multiple files on a single request. This can be achieved by putting the files to a list of tuples in the form (field_name, file_info).\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'http://httpbin.org/post'  \n",
    "file_list = [  \n",
    "    ('image', ('fi.png', open('images/fi.png', 'rb'), 'image/png')),\n",
    "    ('image', ('fi2.jpeg', open('images/fi2.jpeg', 'rb'), 'image/png'))\n",
    "]\n",
    "\n",
    "r = requests.post(url, files=file_list)  \n",
    "print(r.text)  \n",
    "\n",
    "\n",
    "#Open Notify is a cool project that gives good info about NASA stuff\n",
    "\n",
    "lat = 40.71\n",
    "lon = -74\n",
    "params = {'lat': lat, 'lon': lon}\n",
    "resp = requests.get('http://api.open-notify.org/iss-pass.json', params = params)\n",
    "resp.status_code == requests.codes.ok\n",
    "#print ('Response headers:',resp.headers, type(resp.headers))\n",
    "#print('Resp json:',resp.json())\n",
    "print ('Response text:',resp.text, type(resp.text),'\\n')\n",
    "r3=dict(resp.json())\n",
    "print('Responses as dictionary:',r)\n",
    "#print(r['passes'])\n",
    "print(\"Pass info for ISS passes over NYC is given.\")\n",
    "\n",
    "print('Current time:', r['timestamp'])\n",
    "print('Seconds(probably?) until next pass:',resp.json()['response'][0]['risetime']-r['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Auth\n",
    "#see explanation of auth cycle\n",
    "#Dropbox has good documentation of it.\n",
    "#examples using yelp:\n",
    "#Generate access token:\n",
    "# https://www.yelp.com/developers/v3/manage_app\n",
    "\n",
    "#for doing secret password/token stuff, \n",
    "# Move to your home (root) directory:\n",
    "# cd ~\n",
    "# Now make the .secret/ directory:\n",
    "# mkdir .secret\n",
    "# This will create a new folder in your home directory where you can store files for any of the API information you have.\n",
    "\n",
    "# Can you find the file you just made in your terminal? NOTE: dot files won't show up with just ls you must use the show all command as well ls -a\n",
    "\n",
    "# Move into the newly created .secret/ folder and create a file using vscode or any text editor to store your yelp API login info.¶\n",
    "# cd .secret/\n",
    "# code yelp_api.json\n",
    "# In this file, let's create a dictionary of values representing the client id and API key that looks something like this:\n",
    "\n",
    "# {\"api_key\": \"input api key here!\"}\n",
    "\n",
    "import json\n",
    "\n",
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "keys = get_keys(\"/Users/erict/.secret/yelp_api.json\")\n",
    "\n",
    "api_key = keys['api_key']\n",
    "\n",
    "###\n",
    "import requests\n",
    "term = 'Mexican'\n",
    "location = 'Astoria NY'\n",
    "SEARCH_LIMIT = 10\n",
    "\n",
    "url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "headers = {\n",
    "        'Authorization': 'Bearer {}'.format(api_key),\n",
    "    }\n",
    "\n",
    "url_params = {\n",
    "                'term': term.replace(' ', '+'),\n",
    "                'location': location.replace(' ', '+'),\n",
    "                'limit': SEARCH_LIMIT\n",
    "            }\n",
    "response = requests.get(url, headers=headers, params=url_params)\n",
    "print(response)\n",
    "print(type(response.text))\n",
    "print(response.text[:1000])\n",
    "response.json().keys()\n",
    "\n",
    "for key in response.json().keys():\n",
    "    print(key)\n",
    "    value = response.json()[key] #Use standard dictionary formatting\n",
    "    print(type(value)) #What type is it?\n",
    "    print('\\n\\n') #Separate out data\n",
    "    \n",
    "response.json()['businesses'][:2]\n",
    "response.json()['total']\n",
    "response.json()['region']\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(response.json()['businesses'])\n",
    "print(len(df)) #Print how many rows\n",
    "print(df.columns) #Print column names\n",
    "df.head() #Previews the first five rows. \n",
    "#You could also write df.head(10) to preview 10 rows or df.tail() to see the bottom\n",
    "\n",
    "#of course, every API has different parameters - so check out their documentation\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "#Our previous function for loading our api key file\n",
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "keys = get_keys(\"/Users/erict/.secret/yelp_api.json\")\n",
    "api_key = keys['api_key']\n",
    "import requests\n",
    "#https://www.yelp.com/developers/documentation/v3/business_search\n",
    "url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "headers = {\n",
    "        'Authorization': 'Bearer {}'.format(api_key),\n",
    "    }\n",
    "\n",
    "url_params = {\n",
    "                'location': 'NYC'\n",
    "            }\n",
    "response = requests.get(url, headers=headers, params=url_params)\n",
    "#can even pass more params\n",
    "url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "headers = {\n",
    "        'Authorization': 'Bearer {}'.format(api_key),\n",
    "    }\n",
    "\n",
    "url_params = {\n",
    "                'location': 'NYC',\n",
    "                'term' : 'pizza',\n",
    "                'limit' : 50,\n",
    "                'price' : \"1,2,3,4\",\n",
    "                'open_now' : True\n",
    "            }\n",
    "response = requests.get(url, headers=headers, params=url_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folium codealong\n",
    "import folium\n",
    "\n",
    "lat = 51.51\n",
    "long = -0.14\n",
    "\n",
    "#Create a map of the area\n",
    "base_map = folium.Map([lat, long], zoom_start=13)\n",
    "base_map\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Generate some random locations to add to our map\n",
    "x = [lat + np.random.uniform(-.1,.1) for i in range(20)]\n",
    "y = [long + np.random.uniform(-.1,.1) for i in range(20)]\n",
    "points = list(zip(x, y))\n",
    "for p in points:\n",
    "    lat = p[0]\n",
    "    long = p[1]\n",
    "    marker = folium.Marker(location=[lat, long])\n",
    "    marker.add_to(base_map)\n",
    "base_map\n",
    "\n",
    "#labels for points\n",
    "for p in points:\n",
    "    lat = p[0]\n",
    "    long = p[1]\n",
    "    popup_text = \"Latitude: {}, Longitude: {}\".format(lat,long)\n",
    "    popup = folium.Popup(popup_text, parse_html=True)\n",
    "    marker = folium.Marker(location=[lat, long], popup=popup)\n",
    "    marker.add_to(base_map)\n",
    "base_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML\n",
    "<p>Hello World</p>\n",
    "<p>This <a href=\"http://www.google.com\">link</a> will be a part of a separate paragraph.</p>\n",
    "\n",
    "#To use HTML5, the current up-to-date version, you can simply declare <!DOCTYPE html>.\n",
    "<!DOCTYPE html>\n",
    "<html>#then html open/close tags\n",
    "#and inside, head and body\n",
    "<head>\n",
    "        <!-- metadata about the HTML document as a whole -->\n",
    "\n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "        <!-- content of our page will be here! -->\n",
    "\n",
    "    </body>\n",
    "\n",
    "</html>\n",
    "#to make comments:\n",
    "<!-- NYC Pizza is world-famous, cheap, and loved by both vermin and human-like! -->\n",
    "\n",
    "#header levels:\n",
    "<h1>Dogs!</h1>\n",
    "<h3>Why Dogs are Great</h3>\n",
    "\n",
    "#images\n",
    "<img src=\"URL_TO_IMAGE\" alt=\"Picture of a Dog\">\n",
    "\n",
    "#lists\n",
    "<h5>My Favorite Things in No Particular Order</h5>\n",
    "<ul>\n",
    "    <li>Coffee</li>\n",
    "    <li>Vinyl Records</li>\n",
    "    <li>Pickling</li>\n",
    "</ul>\n",
    "\n",
    "#numbered lists\n",
    "<h5>Top 5 Pizza Places in NYC</h5>\n",
    "<ol>\n",
    "    <li>DiFara Pizza</li>\n",
    "    <li>Lucali's</li>\n",
    "    <li>Sal and Carmine's</li>\n",
    "    <li>Juliana's</li>\n",
    "    <li>Joe's</li>\n",
    "</ol>\n",
    "\n",
    "GOod resources:\n",
    "    w3schools \n",
    "    MDN\n",
    "    \n",
    "https://web.stanford.edu/group/csp/cs21/htmlcheatsheet.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Intro to CSS\n",
    "https://learn.co/tracks/module-1-data-science-career-2-1/intro-to-data-with-python-and-sql/section-10-html-css-and-web-scraping/intro-to-css\n",
    "\n",
    "How Does Writing CSS Differ From Writing HTML?\n",
    "As we write CSS these are the type of questions we might ask ourselves:\n",
    "\n",
    "Should the layout of the text be in a single or double column?\n",
    "Should we use a different font color for the header?\n",
    "How should the same content appear differently on a desktop vs. a mobile device?\n",
    "All of the questions above deal with the esthetic considerations of the page. These are the concerns of the presentation layer (CSS).\n",
    "\n",
    "As a contrast, let's consider the type of questions we might ask ourselves as we write HTML:\n",
    "\n",
    "Does the order of items within a list matter? Should it be a numbered list?\n",
    "Should we wrap a list of links inside a navigation tag?\n",
    "Is this the most important header in the whole HTML document?\n",
    "The last few questions deal with structure, hierarchy, and meaning. These are the concerns of the content layer (HTML).\n",
    "\n",
    "When we write CSS, we focus on esthetic and display considerations. When we write HTML, we focus on structure, hierarchy, and meaning.\n",
    "\n",
    "    \n",
    "\n",
    "CSS selectors are a way of declaring which HTML elements you wish to style. Selectors can appear a few different ways:\n",
    "\n",
    "The type of HTML element(h1, p, div, etc.)\n",
    "The value of an element's id or class (<p id='idvalue'></p>, <p\n",
    "class='classname'></p>)\n",
    "The value of an element's attributes (value=\"hello\")\n",
    "The element's relationship with surrounding elements (a p within an element with class of .infobox)\n",
    "For example, if you want the body of the page to have a black background, your selector syntax may be html or body. For anchors, your selector would be a. A few more examples are listed below:\n",
    "\n",
    "/*\n",
    "The CSS comment syntax is text between \"slash-star\" and \"star-slash\"\n",
    "*/\n",
    "\n",
    "Type selectors documentation https://developer.mozilla.org/en-US/docs/Web/CSS/Type_selectors\n",
    "    \n",
    "The element type class is a commonly used selector. Class selectors are used to select all elements that share a given class name. The class selector syntax is: .classname. Prefix the class name with a '.'(period).\n",
    "/*\n",
    "select all elements that have the 'important-topic' classname (e.g. <h1 class='important-topic'>\n",
    "and <h1 class='important-topic'>)\n",
    "*/\n",
    ".important-topic\n",
    "/*\n",
    "select all elements that have the 'welcome-message' classname (e.g. <p class='helpful-hint'>\n",
    "and <p class='helpful-hint'>) - #I think they meant helpful hint instead of welcome message\n",
    "*/\n",
    ".helpful-hint \n",
    "\n",
    "\n",
    "You can also use the id selector to style elements. However, there should be only one element with a given id in an HTML document. This can make styling with the ID selector ideal for one-off styles. The id selector syntax is: #idvalue. Prefix the id attribute of an element with a # (which is called \"octothorpe,\" \"pound sign\", or \"hashtag\").\n",
    "\n",
    "/*\n",
    "selects the HTML element with the id 'main-header' (e.g. <h1 id='main-header'>)\n",
    "*/\n",
    "#main-header\n",
    "\n",
    "/*\n",
    "selects the HTML element with the id 'welcome-message' (e.g. <p id='welcome-message'>)\n",
    "*/\n",
    "#welcome-message\n",
    "\n",
    "\n",
    "\n",
    "ID Selectors docn: https://developer.mozilla.org/en-US/docs/Web/CSS/ID_selectors\n",
    "        \n",
    "Declaring css properties:\n",
    "A CSS property name with a CSS property value is a CSS declaration. To apply a CSS declaration like color: blue to a specific HTML element, you need to combine your CSS declaration with a CSS selector. The association between one or more CSS declarations and a CSS selector is called a CSS declaration block. CSS declarations (one or more) that applied to a specific selector are wrapped by curly braces ({ }). Each declaration inside a declaration block must be separated by a semi-colon (;).\n",
    "\n",
    "Below is a sample CSS declaration block.\n",
    "\n",
    "selector {\n",
    "  color: blue;\n",
    "}\n",
    "/*\n",
    "This is a css declaration for a selector\n",
    "'color' is a property name and 'blue' is a css property value\n",
    "!!!!! CSS declarations must end with a semi-colon (;) !!!!!\n",
    "*/\n",
    "Here's a more complete example declaration block.\n",
    "\n",
    "/*\n",
    "The CSS declaration block below:\n",
    "* Will apply to all `h1` elements\n",
    "* Will change the text color to blue\n",
    "* Will set the font family to Georgia\n",
    "*/\n",
    "h1 {\n",
    "  color: blue;\n",
    "  font-family: Georgia;\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "https://codepen.io/\n",
    "    \n",
    "Sample:\n",
    "https://codepen.io/curiositypaths/pen/WddzQM?editors=1100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The css lab seems written by someone else.  Doesn't explain well.\n",
    "Making a Rainbow\n",
    "First off make sure you have forked and cloned this repo. Next, create a new branch, and switch to it; it's git checkout -b your_solution_branch_name in case you forgot.\n",
    "\n",
    "In that directory, you'll see three files. index.html, main.css, and this README.md. Open them in your text editor via your command line. Also, open index.html in your browser; if everything is working correctly you should see a white page. Good job!\n",
    "\n",
    "see lab documents.\n",
    "#css snippet:\n",
    "div {\n",
    "  border: 20px solid #000; /* this is short hand for setting\n",
    "                            the border's width, its type, and color */\n",
    "  display: inline-block; /* a way of positioning elements */\n",
    "  min-width: 20em; /* the two min styles make the shape */\n",
    "  min-height: 25em; /* of the divs oblong*/\n",
    "  border-radius: 50%; /* this makes the normally square div round */\n",
    "  border-left-color: transparent; /* these remove the color from the left side */\n",
    "  border-right-color: transparent; /* the right side */\n",
    "  border-bottom-color: transparent; /* and the bottom of the circle */\n",
    "}\n",
    "#red { /* this selects any elements with the red id */\n",
    "  border-top-color: #f00;\n",
    "}\n",
    "#orange { /* this selects any elements with the red id */\n",
    "  border-top-color: #ffa500;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beautiful Soup - a Python package for web scraping\n",
    "#DOM: Document Object Model.  https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction\n",
    "from bs4 import BeautifulSoup\n",
    "with open('sample_page.html') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "print(soup.prettify())\n",
    "##\n",
    "print(soup.title)\n",
    "# <title>The Dormouse's story</title>\n",
    "\n",
    "print(soup.title.name)\n",
    "# u'title'\n",
    "\n",
    "print(soup.title.string)\n",
    "# u'The Dormouse's story'\n",
    "\n",
    "print(soup.title.parent.name)\n",
    "# u'head'\n",
    "\n",
    "print(soup.p)\n",
    "# <p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "print(soup.p['class'])\n",
    "# u'title'\n",
    "\n",
    "print(soup.a)\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    "\n",
    "print(soup.find_all('a'))\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "print(soup.find(id=\"link3\"))\n",
    "# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
    "###\n",
    "Beautiful soup is the preliminary tool for web scraping. That said, there are more complex examples where you may wish to either scrape larger amounts of data through full-on web crawling or trickier examples involving javascript. For these and other scenarios, alternative tools such as Selenium and Scrapy are worth investigating.\n",
    "\n",
    "#\n",
    "Beautiful Soup - A good go-to tool for parsing the DOM\n",
    "https://www.crummy.com/software/BeautifulSoup/?\n",
    "\n",
    "Selenium - Browser automation (useful when you need to interact with javascript for more complex scraping)\n",
    "https://www.seleniumhq.org/\n",
    "\n",
    "Scrapy - Another package for scraping larger datasets at scale\n",
    "https://scrapy.org/\n",
    "    \n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping in practice\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "html_page = requests.get('http://books.toscrape.com/') # Make a get request to retrieve the page\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser') # Pass the page contents to beautiful soup for parsing\n",
    "\n",
    "soup.prettify\n",
    "\n",
    "warning = soup.find('div', class_=\"alert alert-warning\")\n",
    "warning # Previewing is optional but can help you verify you are selecting what you think you are\n",
    "\n",
    "# This code is a bit brittle but works for now; in general, ask, are you confident that this will work for all pages?\n",
    "book_container = warning.nextSibling.nextSibling \n",
    "book_container\n",
    "\n",
    "titles = book_container.findAll('h3') # Make a selection\n",
    "titles[0] # Preview the first entry it\n",
    "\n",
    "titles[0].find('a')\n",
    "titles[0].find('a').attrs['title']\n",
    "final_titles = [h3.find('a').attrs['title'] for h3 in book_container.findAll('h3')]\n",
    "print(len(final_titles), final_titles[:5])\n",
    "#Passing regular expressions\n",
    "import re\n",
    "regex = re.compile(\"star-rating (.*)\")\n",
    "book_container.findAll('p', {\"class\" : regex}) # Initial Trial in developing the script\n",
    "star_ratings = []\n",
    "for p in book_container.findAll('p', {\"class\" : regex}):\n",
    "    star_ratings.append(p.attrs['class'][-1])\n",
    "star_ratings\n",
    "star_dict = {'One': 1, 'Two': 2, 'Three':3, 'Four': 4, 'Five':5} # Manually create a dictionary to translate to numeric\n",
    "star_ratings = [star_dict[s] for s in star_ratings]\n",
    "star_ratings\n",
    "book_container.findAll('p', class_=\"price_color\") # First preview\n",
    "prices = [p.text for p in book_container.findAll('p', class_=\"price_color\")] # Keep cleaning it up\n",
    "print(len(prices), prices[:5])\n",
    "prices = [float(p[1:]) for p in prices] # Removing the pound sign and converting to float\n",
    "print(len(prices), prices[:5])\n",
    "avails = book_container.findAll('p', class_=\"instock availability\")\n",
    "avails[:5] # Preview our selection\n",
    "avails[0].text # Dig a little deeper into the structure\n",
    "avails = [a.text.strip() for a in book_container.findAll('p', class_=\"instock availability\")] # Finalize the selection\n",
    "print(len(avails), avails[:5])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([final_titles, star_ratings, prices, avails]).transpose()\n",
    "df.columns = ['Title', 'Star_Rating', 'Price_(pounds)', 'Availability']\n",
    "df\n",
    "\n",
    "#pseudocode:\n",
    "df = pd.DataFrame()\n",
    "for i in range(2,51):\n",
    "    url = \"http://books.toscrape.com/catalogue/page-{}.html\".format(i)\n",
    "    html_page = requests.get(url)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    warning = soup.find('div', class_=\"alert alert-warning\")\n",
    "    book_container = warning.nextSibling.nextSibling\n",
    "    new_titles = retrieve_titles(book_container)\n",
    "    new_star_ratings = retrieve_ratings(book_container)\n",
    "    new_prices = retrieve_prices(book_container)\n",
    "    new_avails = retrieve_avails(book_container)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Scraping lab - hardcoded URL hack vs scrape for next page url:\n",
    "\n",
    "#hacked next page:\n",
    "#Your code here\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_info = []\n",
    "book_info = {}\n",
    "final_titles = []\n",
    "star_ratings = []\n",
    "prices = []\n",
    "avails = []\n",
    "df = pd.DataFrame()\n",
    "for i in range(1,51):\n",
    "    url = \"http://books.toscrape.com/catalogue/page-{}.html\".format(i)\n",
    "    html_page = requests.get(url)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    new_titles = retrieve_titles(soup)\n",
    "    new_star_ratings = retrieve_ratings(soup)\n",
    "    new_prices = retrieve_prices(soup)\n",
    "    new_avails = retrieve_availabilities(soup)\n",
    "    page_info.append([new_titles, new_star_ratings, new_prices, new_avails])\n",
    "    final_titles+=new_titles\n",
    "    star_ratings+=new_star_ratings\n",
    "    prices+=new_prices\n",
    "    avails+=new_avails\n",
    "    \n",
    "    for i in range(len(new_titles)):\n",
    "        book_info[new_titles[i]]= [new_star_ratings[i], new_prices[i], new_avails]\n",
    "        \n",
    "df = pd.DataFrame([final_titles, star_ratings, prices, avails]).transpose()\n",
    "df.columns = ['Title', 'Star_Rating', 'Price_(pounds)', 'Availability']\n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Scrape for next page url:\n",
    "#Your code here\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "page_info = []\n",
    "book_info = {}\n",
    "final_titles = []\n",
    "star_ratings = []\n",
    "prices = []\n",
    "avails = []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "url = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "html_page = requests.get(url)\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "\n",
    "def nextUrl(soup):\n",
    "    \n",
    "    localwarning = soup.find('div', class_=\"alert alert-warning\")\n",
    "    localbook_container = localwarning.nextSibling.nextSibling\n",
    "    try:\n",
    "        #print('url',url)\n",
    "        rel_extension = localbook_container.findAll('li')[-1].find('a').attrs['href']#.find('href=')\n",
    "        #print('relex:',rel_extension)\n",
    "        #print('url',url)\n",
    "        return rel_extension\n",
    "    except:\n",
    "        #print(\"Error\")\n",
    "        return None\n",
    "        \n",
    "while(True):\n",
    "    #print('while url:',url)\n",
    "    html_page = requests.get(url)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    new_titles = retrieve_titles(soup)\n",
    "    new_star_ratings = retrieve_ratings(soup)\n",
    "    new_prices = retrieve_prices(soup)\n",
    "    new_avails = retrieve_availabilities(soup)\n",
    "    page_info.append([new_titles, new_star_ratings, new_prices, new_avails])\n",
    "    final_titles+=new_titles\n",
    "    star_ratings+=new_star_ratings\n",
    "    prices+=new_prices\n",
    "    avails+=new_avails\n",
    "    \n",
    "    for i in range(len(new_titles)):\n",
    "        book_info[new_titles[i]]= [new_star_ratings[i], new_prices[i], new_avails]\n",
    "        \n",
    "    #Next URL logic:\n",
    "    try:\n",
    "        rel_extension = nextUrl(soup)\n",
    "        #print('rel_extension:',rel_extension)\n",
    "        if rel_extension == None:\n",
    "            break\n",
    "        rel_path = url[0:url.rfind('/')]+'/'\n",
    "        #print('relpath',rel_path)\n",
    "        url = rel_path+rel_extension\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "df = pd.DataFrame([final_titles, star_ratings, prices, avails]).transpose()\n",
    "df.columns = ['Title', 'Star_Rating', 'Price_(pounds)', 'Availability']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping for images\n",
    "#normal setup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "html_page = requests.get('http://books.toscrape.com/') # Make a get request to retrieve the page\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser') # Pass the page contents to beautiful soup for parsing\n",
    "warning = soup.find('div', class_=\"alert alert-warning\")\n",
    "book_container = warning.nextSibling.nextSibling\n",
    "#look for img\n",
    "images = book_container.findAll('img')\n",
    "ex_img = images[0] # Preview an entry\n",
    "ex_img\n",
    "#use tab complete to look at options\n",
    " ex_img.\n",
    "check out source url\n",
    "ex_img.attrs['src']\n",
    "\n",
    "\n",
    "#download image locally\n",
    "import shutil\n",
    "url_base = \"http://books.toscrape.com/\"\n",
    "url_ext = ex_img.attrs['src']\n",
    "full_url = url_base + url_ext\n",
    "r = requests.get(full_url, stream=True)\n",
    "if r.status_code == 200:\n",
    "    with open(\"images/book1.jpg\", 'wb') as f:\n",
    "        r.raw.decode_content = True\n",
    "        shutil.copyfileobj(r.raw, f)\n",
    "        \n",
    "#Bash to see if its there\n",
    "ls images/\n",
    "\n",
    "#preview it\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('images/book1.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "#this using PIL doesn't work - PIL doesn't seem to integrate with learn-env\n",
    "#can preview using pandas\n",
    "import pandas as pd\n",
    "from IPython.display import Image, HTML\n",
    "row1 = [ex_img.attrs['alt'], '<img src=\"images/book1.jpg\"/>']\n",
    "df = pd.DataFrame(row1).transpose()\n",
    "df.columns = ['title', 'cover']\n",
    "HTML(df.to_html(escape=False))\n",
    "\n",
    "#\"all together now\"\n",
    "data = []\n",
    "for n, img in enumerate(images):\n",
    "    url_base = \"http://books.toscrape.com/\"\n",
    "    url_ext = img.attrs['src']\n",
    "    full_url = url_base + url_ext\n",
    "    r = requests.get(full_url, stream=True)\n",
    "    path = \"images/book{}.jpg\".format(n+1)\n",
    "    title = img.attrs['alt']\n",
    "    if r.status_code == 200:\n",
    "        with open(path, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "        row = [title, '<img src=\"{}\"/>'.format(path)]\n",
    "        data.append(row)\n",
    "df = pd.DataFrame(data)\n",
    "print('Number of rows: ', len(df))\n",
    "df.columns = ['title', 'cover']\n",
    "HTML(df.to_html(escape=False))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping lab 2 - concerts\n",
    "#so far, this lab seems not workable because of cloudflare protections against scraping.  But here's the solution code\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "#Exploration; designing/testing function parts\n",
    "response = requests.get(\"https://www.residentadvisor.net/events/us/newyork\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "event_listings = soup.find('div', id=\"event-listing\")\n",
    "entries = event_listings.findAll('li')\n",
    "print(len(entries), entries[0])\n",
    "#Successive exploration in function development\n",
    "rows = []\n",
    "for entry in entries:\n",
    "    #Is it a date? If so, set current date.\n",
    "    date = entry.find('p', class_=\"eventDate date\")\n",
    "    event = entry.find('h1', class_=\"event-title\")\n",
    "    if event:\n",
    "        details = event.text.split(' at ')\n",
    "        event_name = details[0].strip()\n",
    "        venue = details[1].strip()\n",
    "        try:\n",
    "            n_attendees = int(re.match(\"(\\d*)\", entry.find('p', class_=\"attending\").text)[0])\n",
    "        except:\n",
    "            n_attendees = np.nan\n",
    "        rows.append([event_name, venue, cur_date, n_attendees])\n",
    "    elif date:\n",
    "        cur_date = date.text\n",
    "    else:\n",
    "        continue\n",
    "df = pd.DataFrame(rows)\n",
    "df.head()\n",
    "\n",
    "#Final function\n",
    "def scrape_events(events_page_url):\n",
    "    #Your code here\n",
    "    response = requests.get(events_page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    entries = event_listings.findAll('li')\n",
    "    rows = []\n",
    "    for entry in entries:\n",
    "        #Is it a date? If so, set current date.\n",
    "        date = entry.find('p', class_=\"eventDate date\")\n",
    "        event = entry.find('h1', class_=\"event-title\")\n",
    "        if event:\n",
    "            details = event.text.split(' at ')\n",
    "            event_name = details[0].strip()\n",
    "            venue = details[1].strip()\n",
    "            try:\n",
    "                n_attendees = int(re.match(\"(\\d*)\", entry.find('p', class_=\"attending\").text)[0])\n",
    "            except:\n",
    "                n_attendees = np.nan\n",
    "            rows.append([event_name, venue, cur_date, n_attendees])\n",
    "        elif date:\n",
    "            cur_date = date.text\n",
    "        else:\n",
    "            continue\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.head()\n",
    "    df.columns = [\"Event_Name\", \"Venue\", \"Event_Date\", \"Number_of_Attendees\"]\n",
    "    return df\n",
    "#Write a Function to Retrieve the URL for the Next Page\n",
    "#Function development cell\n",
    "soup.find('a', attrs={'ga-event-action':\"Next \"}).attrs['href']\n",
    "def next_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    url_ext = soup.find('a', attrs={'ga-event-action':\"Next \"}).attrs['href']\n",
    "    next_page_url = \"https://www.residentadvisor.net\" + url_ext\n",
    "    #Your code here\n",
    "    return next_page_url\n",
    "\n",
    "#Scrape the Next 1000 Events for Your Area\n",
    "#Your code here\n",
    "dfs = []\n",
    "total_rows = 0\n",
    "cur_url = \"https://www.residentadvisor.net/events/us/newyork\"\n",
    "while total_rows <= 1000:\n",
    "    df = scrape_events(cur_url)\n",
    "    dfs.append(df)\n",
    "    total_rows += len(df)\n",
    "    cur_url = next_page(cur_url)\n",
    "    time.sleep(.2)\n",
    "df = pd.concat(dfs)\n",
    "df = df.iloc[:1000]\n",
    "print(len(df))\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3resource.com/sqlite/sqlite-dot-commands.php\n",
    "#useful sqlite page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project tricks - taking raw dfs into sql tables to join #I did actually put this in above in the sql section\n",
    "import sqlite3\n",
    "from pandas import DataFrame\n",
    "\n",
    "conn = sqlite3.connect('TestDB1.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('CREATE TABLE CARS (Brand text, Price number)')\n",
    "conn.commit()\n",
    "\n",
    "Cars = {'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4'],\n",
    "        'Price': [22000,25000,27000,35000]\n",
    "        }\n",
    "\n",
    "df = DataFrame(Cars, columns= ['Brand', 'Price'])\n",
    "df.to_sql('CARS', conn, if_exists='replace', index = False)\n",
    " \n",
    "c.execute('''  \n",
    "SELECT * FROM CARS\n",
    "          ''')\n",
    "\n",
    "for row in c.fetchall():\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project Hints from Jeff:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook') #and other options\n",
    "plt.style.available\n",
    "with plt.style.context('dark_background'):\n",
    "    plt.scatter([1,2,3],[4,5,6])\n",
    "    plt.xlabel('Xlabel')\n",
    "    plt.ylabel('Ylabel');\n",
    "    \n",
    "#pandas explode function\n",
    "#Docstrings for functions\n",
    "#dataframes have built in .copy function, don't use copy module"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############Permutations/combinations and Probability\n",
    "#combinations - order doesn't matter; permutations - order does\n",
    "In this section, we dug into a number of foundational concepts:\n",
    "\n",
    "Probability is \"how likely\" it is that an event will happen\n",
    "Sets in Python are unordered collections of unique elements\n",
    "A sample space is a collection of every single possible outcome in a trial\n",
    "The inclusion exclusion principle is a counting technique used to calculate the number of elements in a collection of sets with overlapping elements.\n",
    "Factorials provide the basis for calculating permutations\n",
    "The difference between permutations and combinations is that with combinations, order is not important\n",
    "The \"sum rule\" of probability states that \n",
    "Independent events don't affect each other - e.g. consecutive coin tosses\n",
    "Dependent events do affect each other - e.g. picking consecutive colored marbles from a bag\n",
    "The product rule is useful when the conditional probability is easy to compute, but the probability of intersections of events is not\n",
    "The chain rule (also called the general product rule) permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities.\n",
    "Bayes theorem describes the probability of an event based on prior knowledge of conditions that might be related to the event\n",
    "The law of total probability states that the probability for a sample space is the sum of the probabilities for partitions of that sample space\n",
    "In this section, we introduced the ideas of combinatorics and probability. In the next section, you'll use this knowledge and take it a step further by learning about statistical distributions and their applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-distributions-use-cases/master/images/dists.png\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prob Mass Funct\n",
    "\n",
    "# Count the frequency of values in a given dataset\n",
    "import collections\n",
    "x = [1,1,1,1,2,2,2,2,3,3,4,5,5]\n",
    "counter = collections.Counter(x)\n",
    "print(counter)\n",
    "\n",
    "print(len(x))\n",
    "\n",
    "# Convert frequency to probability - divide each frequency value by total number of values\n",
    "pmf = []\n",
    "for key,val in counter.items():\n",
    "    pmf.append(round(val/len(x), 2))\n",
    "    \n",
    "print(counter.keys(), pmf)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.array(pmf).sum()\n",
    "\n",
    "####\n",
    "def p(x_i):\n",
    "    frequency = counter[x_i]\n",
    "    total_number = len(x)\n",
    "    return frequency / total_number\n",
    "\n",
    "print(\"p(1) =\", p(1))\n",
    "print(\"p(3) =\", p(3))\n",
    "\n",
    "###\n",
    "#normalized bars\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "outcomes = counter.keys()\n",
    "\n",
    "plt.bar(outcomes, [p(x_i) for x_i in outcomes]);\n",
    "plt.title(\"A Probability Mass Function\")\n",
    "plt.xlabel(\"Outcomes\")\n",
    "plt.ylabel(\"Probabilities of Outcomes\");\n",
    "\n",
    "###\n",
    "#or as a histogram\n",
    "plt.hist(x);\n",
    "plt.title(\"Histogram of Outcomes\")\n",
    "plt.xlabel(\"Bins of Outcomes\")\n",
    "plt.ylabel(\"Frequencies of Outcomes\");\n",
    "\n",
    "###\n",
    "#customizing vis\n",
    "xtick_locations = range(1,6)\n",
    "bins = np.arange(6)+0.5\n",
    "plt.hist(x, bins=bins,  rwidth=0.25, density=True)\n",
    "plt.xticks(ticks=xtick_locations)\n",
    "plt.xlabel('Bins of Outcomes')\n",
    "plt.ylabel('Probabilities of Bins of Outcomes')\n",
    "plt.title(\"Adjusted Histogram with `density=True`\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas series fun - \n",
    "#start with a series, like sizes. make a function (e.g. p_perceived) then use .apply \n",
    "#on it to make a new series\n",
    "sum(sizes.apply(p_perceived) * sizes)\n",
    "\n",
    "\n",
    "#\n",
    "pmf_df.plot.bar(x='Class Size')\n",
    "\n",
    "\n",
    "###neat thing to plot two bar plots atop each other with transparency:\n",
    "# Setting up shared axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Your code here\n",
    "pmf_df.plot.bar(ax = ax, x='Class Size', y = 'Overall Probability', color = 'tab:red', alpha = 0.5)\n",
    "pmf_df.plot.bar(ax = ax, x='Class Size', y = 'Perceived Probability', color = 'tab:blue', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric methods use parameters like mean and standard deviation of given data and attempt to work out the shape of the distribution that the data belongs to. These may implement maximum likelihood methods to fit a distribution to the given data. You'll learn more about this later.\n",
    "\n",
    "Kernel density estimation or KDE is a common non-parametric estimation technique to plot a curve (the kernel) at every individual data point. These curves are then added to plot a smooth density estimation. The kernel most often used is a Gaussian (which produces a bell curve at each data point). Other kernels can be used in special cases when the underlying distribution is not normal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('weight-height.csv')\n",
    "\n",
    "print(data.head())\n",
    "data.describe()\n",
    "\n",
    "##\n",
    "welch\n",
    "# Create two vertical subplots sharing 15% and 85% of plot space\n",
    "# sharex allows sharing of axes i.e. building multiple plots on same axes\n",
    "fig, (ax, ax2) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)}, figsize = (10,8) )\n",
    "\n",
    "sns.distplot(data.Height, \n",
    "             hist=True, hist_kws={\n",
    "                                  \"linewidth\": 2,\n",
    "                                  \"edgecolor\" :'red',\n",
    "                                  \"alpha\": 0.4, \n",
    "                                  \"color\":  \"w\",\n",
    "                                  \"label\": \"Histogram\",\n",
    "                                  },\n",
    "             kde=True, kde_kws = {'linewidth': 3,\n",
    "                                  'color': \"blue\",\n",
    "                                  \"alpha\": 0.7,\n",
    "                                  'label':'Kernel Density Estimation Plot'\n",
    "                                 },\n",
    "             fit= stats.norm, fit_kws = {'color' : 'green',\n",
    "                                         'label' : 'parametric fit',\n",
    "                                         \"alpha\": 0.7,\n",
    "                                          'linewidth':3},\n",
    "             ax=ax2)\n",
    "ax2.set_title('Density Estimations')\n",
    "\n",
    "sns.boxplot(x=data.Height, ax = ax,color = 'red')\n",
    "ax.set_title('Box and Whiskers Plot')\n",
    "ax2.set(ylim=(0, .08))\n",
    "plt.ylim(0,0.11)\n",
    "plt.legend();\n",
    "##\n",
    "#interpolation\n",
    "import numpy as np\n",
    "n, bins = np.histogram(data.Height, 20, density=1)\n",
    "n , bins\n",
    "\n",
    "# Initialize numpy arrays according to number of bins with zeros to store interpolated values\n",
    "pdfx = np.zeros(n.size)\n",
    "pdfy = np.zeros(n.size)\n",
    "\n",
    "# Interpolate through histogram bins \n",
    "# identify middle point between two neighbouring bins, in terms of x and y coords\n",
    "for k in range(n.size):\n",
    "    pdfx[k] = 0.5*(bins[k]+bins[k+1])\n",
    "    pdfy[k] = n[k]\n",
    "\n",
    "# plot the calculated curve\n",
    "plt.plot(pdfx, pdfy);\n",
    "###\n",
    "plt.figure(figsize=(7,5))\n",
    "data.Height.plot.hist(bins = 20, density=True, label = 'Normalized histogram', alpha = 0.7)\n",
    "# plot the calculated curve\n",
    "plt.plot(pdfx, pdfy, label = 'Density function')\n",
    "plt.ylabel ('Probabilities')\n",
    "plt.legend()\n",
    "plt.title ('PDF for height data')\n",
    "plt.show()\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CDFs (and PMFs) can be calculated using built-in NumPy and matplotlib methods. So we don't have create custom functions to calculate these. We can draw a histogram styled CDF as shown below using the following steps\n",
    "\n",
    "# You would need to perform these steps\n",
    "\n",
    "# Use np.histogram() to automatically calculate the histogram with probabilities. Here is numpy histogram documentation to help you dig deeper.\n",
    "\n",
    "# Use plt.scatter() method with np.cumsum() to calculate and plot cumulative probabilities (just like we did above).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n, bins = np.histogram(dice_lst, bins=[1,2,3,4,5,6,7], density=True)\n",
    "#display('n:',n, 'bins',bins)\n",
    "plt.scatter(dice_lst, np.cumsum(n))\n",
    "#plt.scatter(s=\n",
    "#display(np.cumsum(n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli and binomial dist\n",
    "A general rule for the Bernoulli distribution is that:  𝐸(𝑋)=𝑝  and  𝜎2=𝑝∗(1−𝑝) .\n",
    "\n",
    "Note how the Bernoulli distribution describes a single coin flip, a single penalty shot, etc. What if we repeat this process multiple times and are interested in the probability of obtaining a certain number of 1s/successes/tails? This process is described by the binomial distribution.\n",
    "\n",
    "#use numpy to generate binomial/bernoulli trials\n",
    "import numpy as np\n",
    "np.random.seed(123) # set a seed to get the same results\n",
    "np.random.binomial(100, 0.8)\n",
    "#and again - for different result\n",
    "np.random.binomial(100, 0.8)\n",
    "\n",
    "#or a bunch of times\n",
    "iteration = []\n",
    "for loop in range(500):\n",
    "    iteration.append(np.random.binomial(100, 0.8))\n",
    "    np_it = np.array(iteration)\n",
    "    \n",
    "## a different experiment - success on 3 penalty shots\n",
    "n = 10000\n",
    "iteration = []\n",
    "for loop in range(n):\n",
    "    iteration.append(np.random.binomial(3, 0.8))\n",
    "    np_it = np.array(iteration)\n",
    "#unique gets counts of the unique values\n",
    "values, counts = np.unique(np_it, return_counts=True)\n",
    "print(values)\n",
    "print(counts)\n",
    "\n",
    "#and visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(values, counts/10000, align='center', alpha=0.9)\n",
    "plt.xticks(values)\n",
    "plt.ylabel('Fraction')\n",
    "plt.title('Number of penalty goals')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal dist = gaussian\n",
    "he Probability Density Function\n",
    "The probability density function equation for the normal distribution is given by the following expression:\n",
    "\n",
    "𝑁(𝑥)= (prob dens function doesnt display nicely)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making normal dist vis\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "mu, sigma = 0.5, 0.1\n",
    "n = 1000\n",
    "s = np.random.normal(mu, sigma, n)\n",
    "sns.distplot(s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting normal dist to std normal dist.\n",
    "#z=(x-mu)/sigma\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "mean1, sd1 = 5, 3 # dist 1 \n",
    "mean2, sd2 = 10, 2 # dist 2 \n",
    "d1 = np.random.normal(mean1, sd1, 1000)\n",
    "d2 = np.random.normal(mean2, sd2, 1000)\n",
    "sns.distplot(d1);\n",
    "sns.distplot(d2);\n",
    "\n",
    "# Stardardizing and visualizing distributions\n",
    "\n",
    "sns.distplot([(x - d1.mean())/d1.std() for x in d1]);\n",
    "sns.distplot([(x - d2.mean())/d2.std() for x in d2]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bring in SciPy¶\n",
    "In the previous lesson, you have seen formulas to calculate skewness and kurtosis for your data. SciPy comes packaged with these functions and provides an easy way to calculate these two quantities, see scipy.stats.kurtosis and scipy.stats.skew. Check out the official SciPy documentation to dig deeper into this. Otherwise, simply pull up the documentation within the Jupyter notebook using shift+tab within the function call or pull up the full documentation with kurtosis? or skew?, once you have imported these methods from the SciPy package.\n",
    "\n",
    "You'll generate two datasets and measure/visualize and compare their skew and kurtosis in this lab.\n",
    "\n",
    "\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "x_random = np.random.normal(0,2,10000)\n",
    "plt.hist(x_random, bins='auto')\n",
    "display('skew',skew(x_random))\n",
    "display('kurtosis',kurtosis(x_random))\n",
    "\n",
    "\n",
    "#manually doing a normal dist\n",
    "x = np.linspace( -5, 5, 10000 )\n",
    "y = 1./(np.sqrt(2.*np.pi)) * np.exp( -.5*(x)**2  )  # normal distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlighting some specific distributions:\n",
    "Bernoulli Trials deal with a series of boolean events, which is a type of discrete distribution\n",
    "The normal distribution is the classic \"bell curve\" with 68% of the probability mass within 1 SD (standard deviation) of the mean, 95% within 2 SDs, and 99.7% within 3 SDs.\n",
    "The standard normal distribution is a standardized version of the normal distribution, where the mean is 0 and the SD is 1\n",
    "\n",
    "The z-score can be used to understand how extreme a certain result is\n",
    "Skewness and kurtosis (number of outliers)can be used to measure how different a given distribution is from a normal distribution\n",
    "\n",
    "The uniform distribution, which represents processes where each outcome is equally likely, like rolling dice\n",
    "The Poisson distribution, which can be used to represent the likelihood of a given number of successes over a given time period\n",
    "The exponential distribution, which can be used to represent the amount of time it may take before an event occurs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/learn-co-curriculum/dsc-central-limit-theorem/master/images/new_CentralLimitTheorem.png\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Any of the read_ methods in pandas will store 1-dimensional in a Series instead of a DataFrame if passed the optimal parameter squeeze=True.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "np.random.seed(0) #set a random seed for reproducibility\n",
    "\n",
    "#checking for normality\n",
    "data = pd.read_csv('non_normal_dataset.csv', squeeze=True)\n",
    "data.head()\n",
    "sns.distplot(data)\n",
    "\n",
    "st.normaltest(data)\n",
    "\n",
    "#sampling with replacement\n",
    "def get_sample(data, n):\n",
    "    sample=[]\n",
    "    l = len(data)\n",
    "    for i in range(n):\n",
    "        r = np.random.randint(l)\n",
    "        sample.append(data[r])\n",
    "    return sample\n",
    "    \n",
    "\n",
    "test_sample = get_sample(data, 30)\n",
    "print(test_sample[:5]) \n",
    "\n",
    "#generating sample mean\n",
    "def get_sample_mean(sample):\n",
    "    return np.mean(sample)\n",
    "\n",
    "test_sample2 = get_sample(data, 30)\n",
    "test_sample2_mean = get_sample_mean(test_sample2)\n",
    "print(test_sample2_mean) \n",
    "\n",
    "#create sample dist of sample means\n",
    "def create_sample_distribution(data, dist_size=100, n=30):\n",
    "    sample_dist = []\n",
    "    for i in range(dist_size):\n",
    "        s = get_sample(data, n)\n",
    "        sample_dist.append(get_sample_mean(s))\n",
    "    return sample_dist\n",
    "        \n",
    "\n",
    "test_sample_dist = create_sample_distribution(data)\n",
    "print(test_sample_dist[:5]) \n",
    "\n",
    "#creating different samples and visualizing\n",
    "sample_dist = create_sample_distribution(data, 10, 3)\n",
    "sns.distplot(sample_dist)\n",
    "\n",
    "sample_dist = create_sample_distribution(data, 100, 30)\n",
    "sns.distplot(sample_dist)\n",
    "\n",
    "#normal curving\n",
    "mu=0\n",
    "sigma=1\n",
    "x=np.linspace(\n",
    "    stats.norm(mu,sigma).ppf(0.01),\n",
    "    stats.norm(mu,sigma).ppf(0.99),\n",
    "    100\n",
    ")\n",
    "y=stats.norm(mu,sigma).pdf(x)\n",
    "ax.plot(x,y,'r-')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Calculate the frequency of each mean value\n",
    "freq, c = np.unique(means, return_counts=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Standard Error (SE) is very similar to the standard deviation. Both are measures of spread. The higher the number, the more spread out your data is. To put it simply, the two terms are essentially equal — but there is one important difference. While the standard error uses statistics (sample data), standard deviations use parameters (population data). We achieve this by dividing the standard deviation by the square root of the sample size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from conf interval video\n",
    "pop = list(stats.norm.rvs(size=1000, random_state=42))\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax = sns.kdeplot(pop, ax=ax, label='Label')\n",
    "plt.axvline(pop_mean, ls='-.', c='r', label='$\\mu$')\n",
    "\n",
    "#zscore for certain percentages\n",
    "z=stats.norm.ppf(0.95) #z score of 95% values\n",
    "\n",
    "\n",
    "#confidence intervals:\n",
    "sample mean +/- standard error * z score (or t-score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "student's t-dist function - similar to normal but with shorter peak and thicker tails; it has a pdfunction containing the Gamma function.\n",
    "\n",
    "We can use the normal dist when either:\n",
    "population std. dev is known or\n",
    "sample size is greater than 30\n",
    "\n",
    "if you don't know either one, then use the t-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###from Confidence Intervals lab\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Plot styles\n",
    "plt.style.use('fivethirtyeight')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "#The Poisson distribution is the discrete probability distribution of the number of events occurring in a given time period, given the average number of times the event occurs over that time period. We shall use a Poisson distribution to construct a bimodal distribution.\n",
    "np.random.seed(15)\n",
    "population_ages1 = stats.poisson.rvs(loc=18, mu=35, size=150000)\n",
    "population_ages2 = stats.poisson.rvs(loc=18, mu=10, size=100000)\n",
    "population_ages = np.concatenate((population_ages1, population_ages2))\n",
    "\n",
    "pop_ages = pd.DataFrame(population_ages)\n",
    "np.random.seed(15)\n",
    "\n",
    "# Take random sample of size 500\n",
    "sample_size = 500\n",
    "sample = pop_ages.sample(500)\n",
    "\n",
    "np.random.seed(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student t-distribution stuff\n",
    "sample_chol_levels = np.random.normal(loc=54, scale=17, size=1000)\n",
    "# or just given some sample values\n",
    "x_bar = np.mean(sample_chol_levels)\n",
    "s = np.std(sample_chol_levels, ddof = 1)\n",
    "print('Sample mean:', x_bar)\n",
    "print('Sample standard deviation:', s)\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "stats.t.interval(alpha = 0.95,                              # Confidence level\n",
    "                 df= len(sample_chol_levels)-1,             # Degrees of freedom\n",
    "                 loc = x_bar,                               # Sample mean\n",
    "                 scale = s)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Critical Value\n",
    "For the sake of example, let's say that we are calculating the confidence interval solely based on information in the sample. In other words, unlike when we calculated confidence intervals using the z-distribution, we do not have the population standard deviation.\n",
    "\n",
    "We can calculate a confidence interval without the population standard deviation using the t-distribution, represented by the stats.t.ppf(q, df) function. This function takes in a value for the confidence level required (q) with \"degrees of freedom\" (df).\n",
    "\n",
    "Hints:\n",
    "\n",
    "In this case, we want 95% confidence level for a two-tail test. This means the confidence level (q) for this function needs to be  (1−0.95)/2 , i.e.  0.975 \n",
    "In this case, the number of degrees of freedom (df) is equal to the sample size minus 1, or df = sample_size - 1.\n",
    "Calculate the t-critical value for a 95% confidence level based on the sample taken above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a0b61401311e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Get the t-critical value by using 95% confidence level and degree of freedom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mt_critical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.975\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;31m#stats.t.interval(alpha = 0.95,                              # Confidence level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m              \u001b[1;31m#    df= len(sample)-1,             # Degrees of freedom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# Get the t-critical value by using 95% confidence level and degree of freedom\n",
    "t_critical = stats.t.ppf(q=.975, df=sample_size-1)\n",
    "            #stats.t.interval(alpha = 0.95,                              # Confidence level\n",
    "             #    df= len(sample)-1,             # Degrees of freedom\n",
    "             #    loc = sample_mean,                               # Sample mean\n",
    "             #    scale = np.std(sample, ddof = 1))   \n",
    "\n",
    "# Check the t-critical value\n",
    "print(\"t-critical value:\")\n",
    "print(t_critical)     \n",
    "\n",
    "# t-critical value:\n",
    "# 2.0638985616280205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Get the sample standard deviation\n",
    "sample_stdev = sample.std(ddof=1)\n",
    "\n",
    "\n",
    "# Calculate the standard error using the formula described above\n",
    "se = sample_stdev/np.sqrt(sample_size)\n",
    "\n",
    "# Check the SE\n",
    "print(\"Sample Standard Error of the Mean:\")\n",
    "print(se)\n",
    "\n",
    "# Sample Standard Error of the Mean:\n",
    "# 0.697197803193802\n",
    "\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Calculate margin of error using t_critical and se\n",
    "margin_of_error = t_critical*se\n",
    "\n",
    "# Calculate the confidence interval using margin_of_error\n",
    "confidence_interval = (sample_mean-margin_of_error, sample_mean+margin_of_error)\n",
    "\n",
    "# Check the confidence interval\n",
    "print(\"Confidence Interval:\")\n",
    "print(confidence_interval)\n",
    "\n",
    "# Confidence Interval:\n",
    "# (18.431843086289952, 21.309734172653762)\n",
    "\n",
    "\n",
    "####OR USING BUILT IN\n",
    "\n",
    "# Run this cell without changes\n",
    "\n",
    "ci = stats.t.interval(\n",
    "    alpha=0.95,         # Confidence level\n",
    "    df=sample_size - 1, # Degrees of freedom\n",
    "    loc=sample_mean,    # Sample mean\n",
    "    scale=se            # Standard error\n",
    ")\n",
    "\n",
    "print(\"True Population Mean:\")\n",
    "print(population_mean)\n",
    "print(\"95% Confidence Interval of Mean Based on Sample:\")\n",
    "print(ci)\n",
    "\n",
    "# True Population Mean:\n",
    "# 21.00857750766395\n",
    "# 95% Confidence Interval of Mean Based on Sample:\n",
    "# (18.431843086289952, 21.309734172653762)\n",
    "\n",
    "\n",
    "\n",
    "###Now, to refactor and iterate for repeated samples\n",
    "# Replace None with appropriate code\n",
    "\n",
    "def conf_interval(sample):\n",
    "    '''\n",
    "    Input:  Sample data\n",
    "    Output: Confidence interval for the mean of the\n",
    "            population that the sample was drawn from\n",
    "    '''\n",
    "    \n",
    "    # Sample size\n",
    "    n = len(sample)\n",
    "    # Sample mean\n",
    "    x_hat = sample.mean()\n",
    "    \n",
    "    # Standard error of the mean\n",
    "    standard_error = sample.std()/np.sqrt(n)\n",
    "    \n",
    "    # Compute confidence interval with stats.t.interval\n",
    "    conf = stats.t.interval(\n",
    "            alpha=0.95,         # Confidence level\n",
    "            df=sample_size - 1, # Degrees of freedom\n",
    "            loc=x_hat,    # Sample mean\n",
    "            scale=standard_error            # Standard error\n",
    "        )\n",
    "    \n",
    "    return conf\n",
    "\n",
    "# Confirm that this produces the same interval as the previous code\n",
    "conf_interval(sample)\n",
    "\n",
    "# (18.431843086289952, 21.309734172653762)\n",
    "\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(12)\n",
    "\n",
    "# Select the sample size \n",
    "sample_size = 25\n",
    "\n",
    "# Initialize lists to store interval and mean values\n",
    "sample_means = []\n",
    "intervals = []\n",
    "\n",
    "# Run a for loop for sampling 20 times and calculate + store \n",
    "# confidence interval and sample mean values in lists initialized above\n",
    "\n",
    "for sample in range(20):\n",
    "    # Take a random sample of chosen size from population_ages\n",
    "    #population_ages.random_sample(sample_size)\n",
    "    s=np.random.choice(population_ages, sample_size)\n",
    "    \n",
    "    # Calculate sample mean and confidence_interval\n",
    "    sample_mean = s.mean()\n",
    "    ci = conf_interval(s)\n",
    "  \n",
    "    # Append sample means and conf intervals for each iteration\n",
    "    sample_means.append(sample_mean)\n",
    "    intervals.append(ci)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# Run this cell without changes\n",
    "\n",
    "# Set up figure\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "# Draw the means and confidence intervals for each sample\n",
    "ax.errorbar(\n",
    "    x=np.arange(1, 21, 1),\n",
    "    y=sample_means,\n",
    "    yerr=[(upper-lower)/2 for upper, lower in intervals],\n",
    "    fmt='o',\n",
    "    color=\"gray\",\n",
    "    markerfacecolor=\"blue\"\n",
    ")\n",
    "\n",
    "# Draw the population mean as a horizontal line \n",
    "ax.hlines(\n",
    "    xmin=0,\n",
    "    xmax=21,\n",
    "    y=population_ages.mean(), \n",
    "    linewidth=2.0,\n",
    "    color=\"red\"\n",
    ")\n",
    "\n",
    "# Label plot\n",
    "ax.set_xlabel(\"Samples\")\n",
    "ax.set_ylabel(\"Means\")\n",
    "\n",
    "# Customize legend appearance\n",
    "legend_elements = [\n",
    "    # Sample mean (blue circle with gray edge)\n",
    "    Line2D(\n",
    "        [0], # \"Dummy\" line being graphed\n",
    "        [0], # for use in the legend\n",
    "        marker=\"o\",\n",
    "        color=\"w\",\n",
    "        markerfacecolor=\"blue\",\n",
    "        markeredgecolor=\"gray\"\n",
    "    ),\n",
    "    # Confidence interval (gray vertical line)\n",
    "    Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        marker=\"|\",\n",
    "        markersize=15,\n",
    "        color=\"w\",\n",
    "        markeredgewidth=1.5,\n",
    "        markeredgecolor=\"gray\"\n",
    "    ),\n",
    "    # Population mean (red horizontal line)\n",
    "    Line2D([0],[0], color=\"red\")\n",
    "]\n",
    "\n",
    "ax.legend(\n",
    "    handles=legend_elements,\n",
    "    labels=[\"Sample Mean\", \"Sample Confidence Interval for Mean\", \"True Population Mean\"],\n",
    "    loc=\"lower left\", \n",
    "    fontsize=\"large\"\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intro to statistical significance\n",
    "import numpy as np\n",
    "\n",
    "sample = np.array([15.46097664, 15.5930238 , 19.55426936, 16.54231132, 18.40712804,\n",
    "                   13.33092129, 13.79141786, 12.44315636, 15.70385525, 12.48722755,\n",
    "                   11.81594655, 15.22694063, 18.25064431, 15.93900563, 17.38471543,\n",
    "                   15.02334988, 14.55826229, 16.64212199, 16.50657618, 13.44759329,\n",
    "                   13.05467437, 14.151049  , 13.55036322, 13.37386788, 10.25090132,\n",
    "                   16.45380807, 12.63016764, 11.90102614, 15.34426397, 15.2048003 ,\n",
    "                   11.60623705, 16.10720081, 16.42266283, 13.74686281, 14.51850311,\n",
    "                   15.59951107, 18.03269318, 14.35882143, 17.95626942, 14.06849303,\n",
    "                   14.4507767 , 17.27149508, 15.06747021, 13.82402614, 12.40651465,\n",
    "                   15.94104138, 16.8003216 , 18.45973001, 14.24757027, 14.06031845])\n",
    "sample_mean = sample.mean()\n",
    "sample_mean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots()\n",
    "ax = sns.kdeplot(sample, ax=ax, label='Sample PDF')\n",
    "ax.axvline(sample_mean, color=\"red\", label=r'$\\bar{x}$')\n",
    "ax.legend();\n",
    "\n",
    "from scipy import stats\n",
    "stats.t.interval(\n",
    "    alpha=0.9999999999999999,\n",
    "    df=len(sample)-1,\n",
    "    loc=sample_mean,\n",
    "    scale=stats.sem(sample)\n",
    ")\n",
    "\n",
    "stats.t.interval(\n",
    "    alpha=0.999,\n",
    "    df=len(sample)-1,\n",
    "    loc=sample_mean,\n",
    "    scale=stats.sem(sample)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental design:\n",
    "    https://github.com/ericthansen/dsc-experimental-design.git  \n",
    "    \n",
    "Other - p values intro\n",
    "    https://github.com/ericthansen/dsc-z-score-p-value\n",
    "    \n",
    "population/parameters, sample/statistics\n",
    "\n",
    "p-vals and null hyp\n",
    "https://github.com/ericthansen/dsc-p-values-and-null-hypothesis\n",
    "\n",
    "\"null hyp loves you\":\n",
    "https://byrslf.co/the-null-hypothesis-loves-you-and-wants-you-to-be-happy-3189413d8cd0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-table in python\n",
    "# Z-table in Python \n",
    "import scipy.stats as stats\n",
    "\n",
    "# Probabilities up to z-score of 1.5\n",
    "print(stats.norm.cdf(1.5))\n",
    "\n",
    "# Probabilities greater than z-score of 1.34\n",
    "print (1-stats.norm.cdf(1.34))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statsignif\n",
    "import scipy.stats as stats\n",
    "from math import sqrt\n",
    "x_bar = 103 # sample mean \n",
    "n = 40 # number of students\n",
    "sigma = 16 # sd of population\n",
    "mu = 100 # Population mean \n",
    "\n",
    "z = (x_bar - mu)/(sigma/sqrt(n))\n",
    "z\n",
    "\n",
    "#fancy z-plotting (statistical significance) \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.fill_between(x=np.arange(-4,1.19,0.01),\n",
    "                 y1= stats.norm.pdf(np.arange(-4,1.19,0.01)) ,\n",
    "                 facecolor='red',\n",
    "                 alpha=0.35,\n",
    "                 label= 'Area below z-statistic'\n",
    "                 )\n",
    "\n",
    "plt.fill_between(x=np.arange(1.19,4,0.01), \n",
    "                 y1= stats.norm.pdf(np.arange(1.19,4,0.01)) ,\n",
    "                 facecolor='blue',\n",
    "                 alpha=0.35, \n",
    "                 label= 'Area above z-statistic')\n",
    "plt.legend()\n",
    "plt.title ('z-statistic = 1.19');\n",
    "\n",
    "\n",
    "#comparing\n",
    "stats.norm.cdf(z)\n",
    "pval = 1 - stats.norm.cdf(z)\n",
    "pval #pval 0.117\n",
    "#but since pval is > 0.05, we can't reject null hyp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-value and effect size\n",
    "\n",
    "Effect size is used to quantify the size of the difference between two groups under observation. Effect sizes are easy to calculate, understand and apply to any measured outcome and are applicable to a multitude of study domains. It is highly valuable towards quantifying the effectiveness of a particular intervention, relative to some comparison. Measuring effect size allows scientists to go beyond the obvious and simplistic 'Does it work or not?' to the far more sophisticated, 'How well does it work in a range of contexts?'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9788e1b6d9ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m#but relative to which one?  Could do the average,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m#or the place where pdfs cross:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mthresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstd1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmean2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstd2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmean1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstd1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstd2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mmale_below_thresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmale_sample\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'std1' is not defined"
     ]
    }
   ],
   "source": [
    "#scipy imports\n",
    "#documentation: https://docs.scipy.org/doc/scipy/reference/index.html\n",
    "# Import necessary modules \n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "\n",
    "# Import SciPy stats and matplotlib for calculating and visualising effect size\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# seed the random number generator so you get the same results\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "##creating a random variable object\n",
    "#Mean height and sd for males\n",
    "male_mean = 178\n",
    "male_sd = 7.7\n",
    "\n",
    "# Generate a normal distribution for male heights \n",
    "male_height = scipy.stats.norm(male_mean, male_sd)\n",
    "\n",
    "\n",
    "\n",
    "#function to eval a gaussian pdf \n",
    "def evaluate_PDF(rv, x=4):\n",
    "    '''Input: a random variable object, standard deviation\n",
    "    output : x and y values for the normal distribution\n",
    "    '''\n",
    "    \n",
    "    # Identify the mean and standard deviation of random variable \n",
    "    mean = rv.mean()\n",
    "    std = rv.std()\n",
    "\n",
    "    # Use numpy to calculate evenly spaced numbers over the specified interval (4 sd) and generate 100 samples.\n",
    "    xs = np.linspace(mean - x*std, mean + x*std, 100)\n",
    "    \n",
    "    # Calculate the peak of normal distribution i.e. probability density. \n",
    "    ys = rv.pdf(xs)\n",
    "\n",
    "    return xs, ys # Return calculated values\n",
    "\n",
    "\n",
    "#Use scipy stats rvs method to generate a random sample from a pop dist\n",
    "male_sample = male_height.rvs(1000)\n",
    "#the result is a numpy array\n",
    "\n",
    "#one way to express difference between 2 means is by \n",
    "#percentage difference of the means.\n",
    "#but relative to which one?  Could do the average,\n",
    "#or the place where pdfs cross:\n",
    "thresh = (std1 * mean2 + std2 * mean1) / (std1 + std2)\n",
    "\n",
    "male_below_thresh = sum(male_sample < thresh)\n",
    "male_below_thresh\n",
    "female_above_thresh = sum(female_sample > thresh)\n",
    "female_above_thresh\n",
    "#now plot the overlap:\n",
    "# Male height\n",
    "m_xs, male_ys = evaluate_PDF(male_height)\n",
    "plt.plot(m_xs, male_ys, label='male', linewidth=4, color='#beaed4') \n",
    "\n",
    "#Female height \n",
    "f_xs, female_ys = evaluate_PDF(female_height)\n",
    "plt.plot(f_xs, female_ys, label='female', linewidth=4, color='#fdc086')\n",
    "plt.vlines(thresh,ymin=0,ymax=0.06)\n",
    "plt.fill_betweenx(male_ys,x1 = m_xs,x2=thresh, where = m_xs < thresh,color='b')\n",
    "plt.fill_betweenx(female_ys,x1=f_xs,x2=thresh, where = f_xs > thresh,color='b')\n",
    "plt.xlabel('height (cm)')\n",
    "# Calculate the overlap \n",
    "overlap = male_below_thresh / len(male_sample) + female_above_thresh / len(female_sample)\n",
    "overlap\n",
    "#Or in more practical terms, you might report the fraction of people who would be misclassified if you tried to use height to guess sex:\n",
    "misclassification_rate = overlap / 2\n",
    "misclassification_rate\n",
    "\n",
    "#short way to find: if chose m and f samples at random, what is prob that males taller than females\n",
    "# Python zip() The zip() function take iterables (can be zero or more), \n",
    "# makes iterator that aggregates elements based on the iterables passed, \n",
    "# and returns an iterator of tuples.\n",
    "\n",
    "sum(x > y for x, y in zip(male_sample, female_sample)) / len(male_sample)\n",
    "\n",
    "\n",
    "#Cohen's d\n",
    "Cohen’s d is one of the most common ways to measure effect size. As an effect size, Cohen's d is typically used to represent the magnitude of differences between two (or more) groups on a given variable, with larger values representing a greater differentiation between the two groups on that variable.\n",
    "\n",
    "The basic formula to calculate Cohen’s  𝑑  is:\n",
    "\n",
    "𝑑  = effect size (difference of means) / pooled standard deviation\n",
    "\n",
    "The denominator is the standardiser, and it is important to select the most appropriate one for a given dataset. The pooled standard deviation is the average spread of all data points around their group mean (not the overall mean).\n",
    "#there is some discussion about different standardizers\n",
    "def Cohen_d(group1, group2):\n",
    "\n",
    "    # Compute Cohen's d.\n",
    "\n",
    "    # group1: Series or NumPy array\n",
    "    # group2: Series or NumPy array\n",
    "\n",
    "    # returns a floating point number \n",
    "\n",
    "    diff = group1.mean() - group2.mean()\n",
    "\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1 = group1.var()\n",
    "    var2 = group2.var()\n",
    "\n",
    "    # Calculate the pooled threshold as shown earlier\n",
    "    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "    \n",
    "    # Calculate Cohen's d statistic\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    \n",
    "    return d\n",
    "\n",
    "#Interpreting d: 0.2 small effect, .5 medium effect, .8 large\n",
    "\n",
    "\n",
    "#Plotting pdfs\n",
    "def plot_pdfs(cohen_d=2):\n",
    "    \"\"\"Plot PDFs for distributions that differ by some number of stds.\n",
    "    \n",
    "    cohen_d: number of standard deviations between the means\n",
    "    \"\"\"\n",
    "    group1 = scipy.stats.norm(0, 1)\n",
    "    group2 = scipy.stats.norm(cohen_d, 1)\n",
    "    xs, ys = evaluate_PDF(group1)\n",
    "    plt.fill_between(xs, ys, label='Group1', color='#ff2289', alpha=0.7)\n",
    "\n",
    "    xs, ys = evaluate_PDF(group2)\n",
    "    plt.fill_between(xs, ys, label='Group2', color='#376cb0', alpha=0.7)\n",
    "    \n",
    "    o, s = overlap_superiority(group1, group2)\n",
    "    print('overlap', o)\n",
    "    print('superiority', s)\n",
    "    \n",
    "https://github.com/ericthansen/dsc-t-tests    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ericthansen/dsc-t-tests\n",
    "\n",
    "Two Sample t-tests\n",
    "The two-sample t-test is used to determine if two population means are equal. The main types of two-sampled t-tests are paired and independent tests. Paired tests are useful for determining how different a sample is affected by a certain treatment. In other words, the individual items/people in the sample will remain the same and researchers are comparing how they change after treatment. Here is an example of a scenario where a two-sample paired t-test could be applied:\n",
    "\n",
    "The US Olympic weightlifting team is trying out a new workout technique to in an attempt to improve everyone's powerlifting abilities. Did the program have an effect at a 95% significance level?\n",
    "\n",
    "Because we are looking at how specific individuals were affected by a treatment, we would use the paired t-test.\n",
    "\n",
    "Independent two-sample t-tests are for when we are comparing two different, unrelated samples to one another. Unlike paired t-tests, we are not taking paired differences because there is no way to pair two unrelated samples! Here is an example of a scenario where a two-sample independent t-test could be applied:\n",
    "\n",
    "Agricultural scientists are trying to compare the difference in soybean yields in two different counties in Mississippi.\n",
    "\n",
    "\n",
    "Samples vs. Tails\n",
    "Note that we now have two different labels where \"one\" and \"two\" appear repeatedly. Let's make sure the difference is clear!\n",
    "\n",
    "Previously we learned about one-tail and two-tail tests. A one-tail test means that the alternative hypothesis contains something like  >  or  < , which means that we are only looking at the area under the curve on one side. Whereas a two-tail test means that the alternative hypothesis contains something like  ≠  (which could mean greater than or less than), which means that we are looking at the area under the curve in two places, one on each side.\n",
    "\n",
    "One-sample tests can be one-tail or two-tail tests, as can two-sample tests. Some quick examples:\n",
    "\n",
    "One-sample one-tail:  𝐻𝑎:𝜇<3 \n",
    "One-sample two-tail:  𝐻𝑎:𝜇≠3 \n",
    "Two-sample one-tail:  𝐻𝑎:𝜇1<𝜇2 \n",
    "Two-sample two-tail:  𝐻𝑎:𝜇1≠𝜇2\n",
    "\n",
    "Assumptions for Performing t-tests\n",
    "When performing various kinds of t-tests, you assume that the sample observations have numeric and continuous values. You also assume that the sample observations are independent from each other (that is, that you have a simple random sample) and that the samples have been drawn from normal distributions. You can visually inspect the distribution of your sample using a histogram, for example.\n",
    "\n",
    "In the case of unpaired two-sample t-tests, you also assume that the populations the samples have been drawn from have the same variance. For paired two-sample t-tests, you assume that the difference between the two sets of samples are normally distributed.\n",
    "\n",
    "Regardless of the type of t-test you are performing, there are 5 main steps to executing them:\n",
    "1) Set up null and alternative hypotheses\n",
    "\n",
    "2) Choose a significance level\n",
    "\n",
    "3) Calculate the test statistic (t-value)\n",
    "\n",
    "4) Determine the critical t-value (find the rejection region)\n",
    "\n",
    "5) Compare t-value with critical t-value to determine if we can reject the null hypothesis.\n",
    "\n",
    "Hypotheses are written in terms of population parameters like mu\n",
    "rather than sample stats like xbar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEy look, you can put latex in here too!\n",
    "### Step 3: Calculate the t-statistic\n",
    "\n",
    "Assuming that we are fulfilling the three requirements for a t-test mentioned above (i.e. normality, independence, and randomness), we are ready to calculate our t statistic using the formula for one-sample t-test given as:\n",
    "\n",
    "# $$t = \\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}}$$\n",
    " \n",
    "(The *t-statistic* is also known as the *t-value*.)\n",
    "\n",
    "Using the formula given above, calculate the t-statistic in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From lab\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats \n",
    "import math\n",
    "\n",
    "# For visualizing distributions - optional \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def one_sample_ttest(sample, popmean, alpha):\n",
    "    '''print out stats for sample and identify if significant, also visualize'''\n",
    "    \n",
    "\n",
    "    # Visualize sample distribution for normality \n",
    "    sns.set(color_codes=True)\n",
    "    sns.histplot(sample, kde=True, bins=5);\n",
    "    \n",
    "    # Population mean \n",
    "    mu = popmean#not sure what else supposed to do here\n",
    "\n",
    "    # Sample mean (x̄) using NumPy mean()\n",
    "    x_bar = sample.mean()\n",
    "\n",
    "    # Sample Standard Deviation (sigma) using Numpy\n",
    "    sigma = np.std(sample, ddof=1)\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    n = len(sample)\n",
    "    dof = n-1\n",
    "    \n",
    "    # Calculate the critical t-value\n",
    "    t_crit = stats.t.ppf(1 - alpha, df=dof)\n",
    "    print('Critical t-value: {}'.format(t_crit))\n",
    "    \n",
    "    # Calculate the t-value and p-value      \n",
    "    t = (x_bar -  mu)/(sigma/np.sqrt(n))\n",
    "    print('T-value: {}'.format(t))\n",
    "    \n",
    "    p= stats.t.sf(t, df=dof)\n",
    "    print('P-value: {}'.format(p))\n",
    "    ###alternately,  and to check\n",
    "    results = stats.ttest_1samp(\n",
    "    a=sample,   # the entire array-like sample\n",
    "    popmean=mu # the mean you are testing the sample against\n",
    "    )\n",
    "    print(\"Checking t and p value: t:{}, p:{}, results:{} & {}\".format(t, p, results.statistic, results.pvalue/2))\n",
    "    \n",
    "    # return results\n",
    "    if t>t_crit:\n",
    "        results='Significant'\n",
    "    else:\n",
    "        results=\"Not Significant\"\n",
    "    return results\n",
    "\n",
    "##\n",
    "\n",
    "sample = np.array([84.0, 92.4, 74.3, 79.4, 86.7, 75.3, 90.9, 86.1, 81.0, 85.1, \n",
    "      78.7, 73.5, 86.9, 87.4, 82.7, 81.9, 69.9, 77.2, 79.3, 83.3])\n",
    "popmean = 65\n",
    "alpha = 0.05\n",
    "results=one_sample_ttest(sample, popmean, alpha)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating pooled sample variance\n",
    "\n",
    "def sample_variance(sample):\n",
    "    n=len(sample)\n",
    "    x_bar=sample.mean()\n",
    "    num=0\n",
    "    for s in sample:\n",
    "        num += (s-x_bar)**2\n",
    "    den=n-1\n",
    "    \n",
    "    return num/den\n",
    "\n",
    "def pooled_variance(sample1, sample2):\n",
    "    n1=len(sample1)\n",
    "    n2=len(sample2)\n",
    "    num=(n1-1)*sample_variance(sample1) + (n2-1)*sample_variance(sample2)\n",
    "    den=n1+n2-2\n",
    "    return num/den\n",
    "\n",
    "def twosample_tstatistic(expr, ctrl):\n",
    "    num=expr.mean()-ctrl.mean()\n",
    "    den=np.sqrt(pooled_variance(expr, ctrl)*(1/len(expr)+1/len(ctrl)))\n",
    "    return num/den\n",
    "\n",
    "t_stat = twosample_tstatistic(experimental, control)\n",
    "t_stat\n",
    "# -1.8915462966190268\n",
    "\n",
    "\n",
    "\n",
    "#All built in t-test:\n",
    "stats.ttest_ind(experimental, control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick p-valu check\n",
    "import scipy.stats as st\n",
    "\n",
    "n = 20 #Number of flips\n",
    "p = .75 #Simulating an unfair coin\n",
    "coin1 = np.random.binomial(n, p)\n",
    "sigma = np.sqrt(n*.5*(1-.5))#For a binomial\n",
    "st.norm.sf(np.abs(z))\n",
    "z = (coin1 - 10) / (sigma / np.sqrt(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c18557f0d09c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.75\u001b[0m \u001b[1;31m# Simulating an unfair coin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mn_heads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#more p-value stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "#checking pvalue change over time with more flips\n",
    "\n",
    "#How many times would you have to flip a 75% heads coin to determine it was false?\n",
    "p_vals = []\n",
    "#Iterate through various numbers of trials\n",
    "for n in range(1,50):\n",
    "    #Do multiple runs for that number of samples to compare\n",
    "    p_val = []\n",
    "    for i in range(200):\n",
    "        p = .75 # Simulating an unfair coin\n",
    "        n_heads = np.random.binomial(n, p)\n",
    "        mu = n / 2\n",
    "        sigma = np.sqrt(n*.5*(1-.5))\n",
    "        z  = (n_heads - mu) / (sigma / np.sqrt(n))\n",
    "        p_val.append(st.norm.sf(np.abs(z)))\n",
    "    p_vals.append(np.mean(p_val))\n",
    "plt.plot(list(range(1,50)), p_vals)\n",
    "plt.title('Average P-Values Associated with Hypothesis Testing of a .75 Unfair Coin by Number of Trials')\n",
    "plt.ylabel('Average P-Value of Simulations')\n",
    "plt.xlabel('Number of Coin Flips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t test with power, etc using 2 random vars\n",
    "\n",
    "import scipy.stats as stats\n",
    "def run_ttest_sim(p1, p2, std, nobs, alpha=0.05, n_sim=10**5):\n",
    "    \"\"\"p1 and p2 are the underlying means probabilities for 2 normal variables\n",
    "    Samples will be generated using these parameters.\"\"\"\n",
    "    # Calculate Normalized Effect Size\n",
    "    effect_size = np.abs(p1-p2)/std\n",
    "    \n",
    "    # Run a Simulation\n",
    "    # Initialize array to store results\n",
    "    p = (np.empty(n_sim))\n",
    "    p.fill(np.nan)\n",
    "\n",
    "    #  Run a for loop for range of values in n_sim\n",
    "    for s in range(n_sim):\n",
    "        control = np.random.normal(loc= p1, scale=std, size=nobs)\n",
    "        experimental = np.random.normal(loc= p2, scale=std, size=nobs)\n",
    "        t_test = stats.ttest_ind(control, experimental)\n",
    "        p[s] = t_test[1]\n",
    "    \n",
    "    num_null_rejects = np.sum(p < alpha)\n",
    "    power = num_null_rejects/n_sim\n",
    "    # Store results\n",
    "    stat_dict = {'alpha':alpha,\n",
    "                 'nobs':nobs,\n",
    "                 'effect_size':effect_size,\n",
    "                 'power': power}\n",
    "    return stat_dict\n",
    "\n",
    "run_ttest_sim(.5, .7, 1, 50)\n",
    "\n",
    "###power/alpha/samplesize/effectsize\n",
    "from statsmodels.stats.power import TTestIndPower, TTestPower\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid') # Nice background styling on plots\n",
    "power_analysis = TTestIndPower()\n",
    "power_analysis.plot_power(dep_var='nobs',\n",
    "                          nobs = np.array(range(5,1500)),\n",
    "                          effect_size=np.array([.05, .1, .2,.3,.4,.5]),\n",
    "                          alpha=0.05)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate power\n",
    "power_analysis.solve_power(effect_size=.2, nobs1=80, alpha=.05)\n",
    "# Calculate minimum effect size to satisfy desired alpha and power as well as respect sample size limitations\n",
    "power_analysis.solve_power(nobs1=25, alpha=.05, power=.8)\n",
    "#can solve for any of the 4\n",
    "\n",
    "\n",
    "#initialize empty numpy array\n",
    "p = (np.empty(n_sim))\n",
    "p.fill(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welch's t-test  \n",
    "https://github.com/ericthansen/dsc-welchs-ttest.git\n",
    "\n",
    "Welch's t score:\n",
    "![text](https://render.githubusercontent.com/render/math?math=%5CLarge%20t%20=%20%5Cfrac%7B%5Cbar%7BX_1%7D-%5Cbar%7BX_2%7D%7D%7B%5Csqrt%7B%5Cfrac%7Bs_1%5E2%7D%7BN_1%7D%20%2b%20%5Cfrac%7Bs_2%5E2%7D%7BN_2%7D%7D%7D%20=%20%5Cfrac%7B%5Cbar%7BX_1%7D-%5Cbar%7BX_2%7D%7D%7B%5Csqrt%7Bse_1%5E2%2bse_2%5E2%7D%7D)\n",
    "Calculate the degrees of freedom\n",
    "Once the t-score has been calculated for the experiment using the above formula, you then must calculate the degrees of freedom for the t-distribution. Under the two-sample Student's t-test, this is simply the total number of observations in the samples size minus two, but given that the sample sizes may vary using the Welch's t-test, the calculation is a bit more complex:\n",
    "![Alt text](https://render.githubusercontent.com/render/math?math=%5CLarge%20v%20%5Capprox%20%5Cfrac%7B%5Cleft(%20%5Cfrac%7Bs_1%5E2%7D%7BN_1%7D%20%2b%20%5Cfrac%7Bs_2%5E2%7D%7BN_2%7D%5Cright)%5E2%7D%7B%5Cfrac%7Bs_1%5E4%7D%7BN_1%5E2v_1%7D%20%2b%20%5Cfrac%7Bs_2%5E4%7D%7BN_2%5E2v_2%7D%7D \"a title\")\n",
    "\n",
    "Then calculate p-stat for a t dist with that tscore and those deg of freedom\n",
    "One good way is with  \n",
    "p = 1 - stats.t.cdf(t, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the welch t test lab\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(82)\n",
    "control = np.random.normal(loc=10, scale=1, size=8)\n",
    "treatment = np.random.normal(loc=10.5, scale=1.2, size=12)\n",
    "def welch_t(a, b):\n",
    "    \n",
    "    \"\"\" Calculate Welch's t-statistic for two samples. \"\"\"\n",
    "    num = abs(np.mean(control) - np.mean(treatment))\n",
    "    denom = np.sqrt((np.var(control))/len(control) + (np.var(treatment)/len(treatment)))\n",
    "    \n",
    "\n",
    "    \n",
    "    return num/denom# Return the t-score!\n",
    "\n",
    "welch_t(control, treatment)\n",
    "# 2.0997990691576858\n",
    "def welch_df(a, b):\n",
    "    \n",
    "    \"\"\" Calculate the effective degrees of freedom for two samples. \"\"\"\n",
    "    na = len(a)\n",
    "    nb = len(b)\n",
    "    vara = np.var(a)\n",
    "    varb = np.var(b)\n",
    "    nua = na-1\n",
    "    nub = nb-1\n",
    "    num = (vara/na + varb/nb)**2\n",
    "    den = (vara**2)/(na**2 * nua) + (varb**2)/(nb**2 * nub)\n",
    "    return num/den # Return the degrees of freedom\n",
    "\n",
    "welch_df(control, treatment)\n",
    "# 17.673079085111\n",
    "\n",
    "# Your code here; calculate t-score and the degrees of freedom for the two samples, a and b\n",
    "t = welch_t(control, treatment)\n",
    "df = welch_df(control, treatment)\n",
    "print(t, df)\n",
    "# 2.0997990691576858 17.673079085111\n",
    "\n",
    "# Your code here; calculate the p-value for the two samples defined above\n",
    "\n",
    "from scipy import stats\n",
    "p = 1 - stats.t.cdf(t, df)\n",
    "print(p)\n",
    "# 0.025191666225846454\n",
    "\n",
    "def p_value(a, b, two_sided=False):\n",
    "    # Your code here\n",
    "    from scipy import stats\n",
    "    t = welch_t(a, b)\n",
    "    df = welch_df(a, b)\n",
    "    p = 1 - stats.t.cdf(t, df)\n",
    "    if two_sided:\n",
    "        return p*2\n",
    "    else:\n",
    "        return p\n",
    "     # Return the p-value!\n",
    "p_value(treatment, control)\n",
    "# 0.025191666225846454\n",
    "\n",
    "p_value(treatment, control, two_sided=True)\n",
    "# 0.05038333245169291\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolmogorov/Smirnof Stat - representation of difference from normality (or difference between two distributions) based on comparing the max of the vertical distance between their CDFs\n",
    "\n",
    "For one sample vs a known distribution (eg. Normal aka 'norm')\n",
    "scipy.stats.kstest(rvs, cdf, args=(), N=20, alternative='two-sided', mode='approx')\n",
    "\n",
    "A two-sample K-S test is available in SciPy using following function:\n",
    "\n",
    "scipy.stats.ks_2samp(data1, data2)[source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANOVA\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "df = pd.read_csv('IT_salaries.csv')\n",
    "df.head()\n",
    "\n",
    "\n",
    "#in this example:\n",
    "# S - the individuals salary\n",
    "# X - years of experience\n",
    "# E - education level (1-Bachelors, 2-Masters, 3-PHD)\n",
    "# M - management (0-no management, 1-yes management)\n",
    "#In order to generate the ANOVA table, you first fit a linear model and then generate the table from this object. Our formula will be written as:\n",
    "\n",
    "#Control_Column ~ C(factor_col1) + factor_col2 + C(factor_col3) + ... + X\n",
    "\n",
    "#We indicate categorical variables by wrapping them with C().\n",
    "formula = 'S ~ C(E) + C(M) + X'\n",
    "lm = ols(formula, df).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2)\n",
    "print(table)\n",
    "\n",
    "#anova video: \n",
    "https://learning.flatironschool.com/courses/1888/pages/video-anova?module_item_id=261345\n",
    "    \n",
    "check out seaborn swarmplot\n",
    "boxplot\n",
    "violinplot\n",
    "\n",
    "#also, if we have a bunch of different groups and want\n",
    "#to see if they significantly differ\n",
    "groups = {'1':data1, ...}\n",
    "f_stat, p = stats.f_oneway(*groups.values()) \n",
    "#this uses the Fstat (at the heart of ANOVA testing)\n",
    "\n",
    "#remember, it's an Omnibus test - it only tells us\n",
    "#if there's a sig diff, but doesn't tell us where\n",
    "#aha!  so the ols(ordinary least squares) code above breaks out by group - so it\n",
    "#is a little more comprehensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi-Squared!\n",
    "#chi sq dist can be used for lots of things\n",
    "\n",
    "#one of them is comparing if some observations differ\n",
    "#from expectations\n",
    "x=np.linspace(stats.chi2.ppf(0.00001, dof), stats.chi2.ppf(0.9999, dof), 500)\n",
    "f,ax=plt.subplots()\n",
    "ax.set_title('chi^2, dof deg of freedom')\n",
    "ax.plot(x, stats.chi2.pdf(x, dof, 'r-', lw=5))\n",
    "plt.tight_layout()\n",
    "\n",
    "#can get p score from\n",
    "#calculating the chisq_stat yourself, then\n",
    "p=1-stats.chi2.cdf(chisq_stat, df=dof)\n",
    "#or\n",
    "stats.chi2.sf(chisq)\n",
    "\n",
    "#or go straight to the built in:\n",
    "obs=[50,100,180,70]\n",
    "exp = [60,80,180,80]\n",
    "result = stats.chisquare(f_obs=obs, f_exp = exp)\n",
    "\n",
    "#the other thing is test for independence.\n",
    "#e.g. if you had pc/mac users who create profiles abcd..\n",
    "#ie. two categories, and each profile type we expect to be\n",
    "#independent of pc/mac, you can test if they are indep.\n",
    "\n",
    "stats.chi2.pdf()\n",
    "#Lots of good functions to use, but one for 2 category\n",
    "#e.g. 2 rows, 3 cols\n",
    "chi, p, dof, exp = stats.contingency.chi2_contingency(observations)\n",
    "#where observations are, e.g., [[category 1 yes, category 1 no], [cat 2 yes, cat 2 no]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AB Testing - i.e. what sample size do we need to observe\n",
    "#A given alpha/power/effectsize\n",
    "#developed for the lab\n",
    "\n",
    "# Calculate the required sample size\n",
    "from statsmodels.stats.power import TTestIndPower, TTestPower\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid') # Nice background styling on plots\n",
    "power_analysis = TTestIndPower()\n",
    "\n",
    "\n",
    "alpha=0.05\n",
    "power=0.8\n",
    "effect_size=0.01\n",
    "std=0.0475\n",
    "#Not sure how to make normalized effect size using Cohen's d since we don't have current population to make pooled variance\n",
    "#could potentially just use \"number of stdevs between the means\" idea.  If both pops have same std, can do...\n",
    "effect_size = effect_size/std\n",
    "\n",
    "res = power_analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
    "res\n",
    "\n",
    "#now do some plotting of different scenarios\n",
    "#Your code; plot power curves for the various alpha and effect size combinations\n",
    "alphas=[0.01, 0.05, 0.1]\n",
    "power=0.8\n",
    "effect_sizes=np.array([0.005, .01, .02, .03])/std\n",
    "\n",
    "# res = power_analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
    "# res\n",
    "for i in range(len(alphas)):\n",
    "    power_analysis.plot_power(dep_var='nobs',\n",
    "                              nobs = np.array(range(10,int(res*5),10)),\n",
    "                              effect_size=effect_sizes,\n",
    "                              alpha=alphas[i]\n",
    "                             )\n",
    "    plt.title('Power vs. Sample Size; Alpha: {}'.format(alphas[i]))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####flatiron_stats.py\n",
    "#flatiron_stats\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def welch_t(a, b):\n",
    "    \n",
    "    \"\"\" Calculate Welch's t statistic for two samples. \"\"\"\n",
    "\n",
    "    numerator = a.mean() - b.mean()\n",
    "    \n",
    "    # “ddof = Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, \n",
    "    #  where N represents the number of elements. By default ddof is zero.\n",
    "    \n",
    "    denominator = np.sqrt(a.var(ddof=1)/a.size + b.var(ddof=1)/b.size)\n",
    "    \n",
    "    return np.abs(numerator/denominator)\n",
    "\n",
    "def welch_df(a, b):\n",
    "    \n",
    "    \"\"\" Calculate the effective degrees of freedom for two samples. This function returns the degrees of freedom \"\"\"\n",
    "    \n",
    "    s1 = a.var(ddof=1) \n",
    "    s2 = b.var(ddof=1)\n",
    "    n1 = a.size\n",
    "    n2 = b.size\n",
    "    \n",
    "    numerator = (s1/n1 + s2/n2)**2\n",
    "    denominator = (s1/ n1)**2/(n1 - 1) + (s2/ n2)**2/(n2 - 1)\n",
    "    \n",
    "    return numerator/denominator\n",
    "\n",
    "\n",
    "def p_value_welch_ttest(a, b, two_sided=False):\n",
    "    \"\"\"Calculates the p-value for Welch's t-test given two samples.\n",
    "    By default, the returned p-value is for a one-sided t-test. \n",
    "    Set the two-sided parameter to True if you wish to perform a two-sided t-test instead.\n",
    "    \"\"\"\n",
    "    t = welch_t(a, b)\n",
    "    df = welch_df(a, b)\n",
    "    \n",
    "    p = 1-stats.t.cdf(np.abs(t), df)\n",
    "    \n",
    "    if two_sided:\n",
    "        return 2*p\n",
    "    else:\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Resources\n",
    "Choosing Statistical Tests (Links to an external site.)\n",
    "https://stats.idre.ucla.edu/other/mult-pkg/whatstat/\n",
    "\n",
    "How to choose the right statistical test?\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3116565/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random musing:\n",
    "What about something like the reverse monty hall problem - i.e. a situation where you get more info but still would not want to switch.  idea - what if, instead of a car, it's a hungry tiger?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes Theorem!\n",
    "# Bayes vs frequentist.  difference in philosophy as well as  b's theorem\n",
    "def bayes(P_a, P_b, P_b_given_a):\n",
    "    # Your code here\n",
    "    P_a_given_b = P_b_given_a * P_a/P_b\n",
    "    return P_a_given_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usual imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max likelihood estimation\n",
    "establish equation for parameters, use derivative to maximize - can take monotonic funciton (e.g. log) of this to make taking the derivtive easier - extrema will still happen at same place.\n",
    "other resources:\n",
    "https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1\n",
    "\n",
    "https://www.statisticshowto.datasciencecentral.com/iid-statistics/\n",
    "\n",
    "https://opencurriculum.org/5512/monotonically-increasing-and-decreasing-functions-an-algebraic-approach/\n",
    "\n",
    "https://mathbitsnotebook.com/Algebra2/Exponential/EXLogFunctions.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maximum aposteriori \n",
    "https://github.com/ericthansen/dsc-map-multinomial-bayes.git  \n",
    "Maximum A Posteriori Estimation\n",
    "Maximum A Posteriori Estimation (MAP) is similar to Maximum Likelihood Estimation but extends this concept by allowing one to also account for prior beliefs regarding the distribution of the variable in question. Recall Bayes' theorem:\n",
    "\n",
    "\n",
    "\n",
    "The Bayesian interpretation of this formula is\n",
    "likelihood*prior/evidence\n",
    "\n",
    "\n",
    "\n",
    "With MAP, you then attempt to optimize a parameter  for the assumed distribution in order to maximize the posterior probability.\n",
    "\n",
    "Multinomial Bayes\n",
    "Multinomial Bayes also extends the notions within Bayes' theorem, allowing one to chain inferences. The primary assumption for this is assuming that your variables are independent of one another. Recall that if you assume two events A and B are independent of one another, then  . Similarly, if independence is assumed when extending Bayes theorem to a multivariate case, one can multiply the successive probability estimates. Mathematically, this can be summarized as:\n",
    "\n",
    "\n",
    "\n",
    "Summary\n",
    "This lesson briefly introduced the concept of Maximum A Posteriori Estimation and extending Bayes' theorem to multivariate cases. In later sections, you'll investigate these ideas in practice, working with practical examples and coding your own implementations to gain a full understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical learning theory\n",
    "nice representation of fitting a model\n",
    "https://learning.flatironschool.com/courses/1888/pages/statistical-learning-theory?module_item_id=261365  \n",
    "https://github.com/ericthansen/dsc-stat-learning-theory?organization=ericthansen&organization=ericthansen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "#m = sy/sx\n",
    "#or\n",
    "def calc_slope(xs,ys):\n",
    "    \n",
    "    return ((X.mean()*Y.mean() - (X*Y).mean())/((X.mean())**2 - (X**2).mean()))\n",
    "\n",
    "calc_slope(X,Y)\n",
    "\n",
    "\n",
    "###########from lab\n",
    "# import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Initialize arrays X and Y with given values\n",
    "# X = Independent Variable\n",
    "X = np.array([1,2,3,4,5,6,8,8,9,10], dtype=np.float64)\n",
    "# Y = Dependent Variable\n",
    "Y = np.array([7,7,8,9,9,10,10,11,11,12], dtype=np.float64)\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(X, Y)\n",
    "\n",
    "# Write the function to calculate slope as: \n",
    "# (mean(x) * mean(y) – mean(x*y)) / ( mean (x)^2 – mean( x^2))\n",
    "def calc_slope(xs,ys):\n",
    "    \n",
    "    return ((X.mean()*Y.mean() - (X*Y).mean())/((X.mean())**2 - (X**2).mean()))\n",
    "\n",
    "calc_slope(X,Y)\n",
    "\n",
    "# 0.5393518518518512\n",
    "\n",
    "# use the slope function with intercept formula to return calculate slope and intercept from data points\n",
    "\n",
    "def best_fit(xs,ys):\n",
    "    \n",
    "    m = calc_slope(xs, ys)\n",
    "    #y = mx + c => c = y-mx\n",
    "    c = ys.mean() - m * xs.mean()\n",
    "    return (m, c)\n",
    "\n",
    "# Uncomment below to test your function\n",
    "\n",
    "m, c = best_fit(X,Y)\n",
    "m, c\n",
    "\n",
    "# (0.5393518518518512, 6.379629629629633)\n",
    "\n",
    "def reg_line (m, c, xs):\n",
    "    \n",
    "    ys = np.zeros(len(xs), dtype=np.float64)\n",
    "    \n",
    "    for i in range(len(xs)):\n",
    "        ys[i] = m * xs[i] + c\n",
    "    return ys\n",
    "    \n",
    "\n",
    "# Uncomment below\n",
    "regression_line = reg_line(m,c,X)\n",
    "regression_line\n",
    "list(zip(X, regression_line))\n",
    "print(regression_line)\n",
    "print(X)\n",
    "\n",
    "# Plot data and regression line\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.scatter(X,Y)\n",
    "ax.plot(X, regression_line)\n",
    "\n",
    "x_new = 7\n",
    "y_predicted = m*x_new + c\n",
    "y_predicted\n",
    "\n",
    "# 10.155092592592592\n",
    "\n",
    "# Plot as above and show the predicted value\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.scatter(X,Y)\n",
    "ax.plot(X, regression_line)\n",
    "ax.scatter(x_new, y_predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R squared = coefficient of determination\n",
    "# r^2 = 1 - ss_res/ss_tot = ss_exp/ss_tot \n",
    "#ss_res = sum (y_i-yhat_i)^2\n",
    "#ss_tot = sum( y_i - ybar)^2\n",
    "#respectively, residual, explained, total\n",
    "\n",
    "###From Lab\n",
    "\n",
    "# Calculate sum of squared errors between regression and mean line \n",
    "import numpy as np\n",
    "\n",
    "def sq_err(y_real, y_predicted):\n",
    "    \"\"\"\n",
    "    input\n",
    "    y_real : true y values\n",
    "    y_predicted : regression line\n",
    "\n",
    "    \n",
    "    return\n",
    "    squared error between regression and true line (ss_tot)\n",
    "    \"\"\"\n",
    "    if len(y_real) != len(y_predicted):\n",
    "        return \"Error\"\n",
    "    else:\n",
    "        sq_e = 0\n",
    "        for r, p in zip(y_real, y_predicted):\n",
    "            sq_e += (r-p)**2\n",
    "    return sq_e\n",
    "\n",
    "\n",
    "# Calculate Y_mean , squared error for regression and mean line , and calculate r-squared\n",
    "\n",
    "def r_squared(y_real, y_predicted):\n",
    "    \"\"\"\n",
    "    input\n",
    "    y_real: real values\n",
    "    y_predicted: regression values\n",
    "    \n",
    "    return\n",
    "    r_squared value\n",
    "    \"\"\"\n",
    "    if len(y_real) != len(y_predicted):\n",
    "        return \"Error\"\n",
    "    else:\n",
    "        y_bar = y_real.mean()\n",
    "        y_mean = np.zeros(len(y_real))\n",
    "        y_mean += y_bar\n",
    "        \n",
    "        ssr = sq_err(y_real, y_predicted)\n",
    "        sst = sq_err(y_real, y_mean)\n",
    "        #print(sst, y_real.var()*len(y_real))\n",
    "        r_squared = 1-ssr/sst\n",
    "        return r_squared\n",
    "\n",
    "# Check the output with some example data\n",
    "Y = np.array([1, 3, 5, 7])\n",
    "Y_pred = np.array([4.1466666666666665, 2.386666666666667, 3.56, 5.906666666666666])\n",
    "\n",
    "r_squared(Y, Y_pred)\n",
    "print(r_squared(Y, Y_pred))\n",
    "\n",
    "# 0.32\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ericthansen/dsc-regression-assumptions\n",
    "\n",
    "Heteroscedasticity (also spelled heteroskedasticity) refers to the circumstance in which the dependent variable is unequal across the range of values of the predictor(s).\n",
    "\n",
    "When there is heteroscedasticity in the data, a scatterplot of these variables will often create a cone-like shape. The scatter of the dependent variable widens or narrows as the value of the independent variable increases.  \n",
    "\n",
    "A scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line). The scatter plots shown here are examples of data that are heteroscedastic (except the plot far right). You can also use significance tests like Breusch-Pagan / Cook-Weisberg test or White general test to detect this phenomenon. You will learn about p-values later, but for now, you can remember that, if these tests give you a p-value < 0.05, the null hypothesis can rejected, and you can assume the data is heteroscedastic.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary least squares in Statsmodels\n",
    "https://github.com/ericthansen/dsc-ols-statsmodels  \n",
    "https://github.com/ericthansen/dsc-ols-statsmodels-lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#check linearity\n",
    "df = pd.read_csv('heightWeight.csv')\n",
    "plt.scatter(df.height, df.weight)\n",
    "plt.title(\"Linearity check\")\n",
    "plt.show()\n",
    "\n",
    "#get a general sense of distribution- not sufficient to assume normality here!!!\n",
    "df.plot.kde()\n",
    "plt.title(\"distribution check for dependent and independent variable\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Statsmodels allows users to fit statistical models using R-style formulas. The formula framework is quite powerful and for simple regression it is written using a ~ as Y ~ X.\n",
    "#The formula gives instruction for a general structure for a regression call. For a statsmodels ols calls, you'll need a Pandas dataframe with column names that you will add to your formula.\n",
    "\n",
    "#regression formula\n",
    "f = 'weight~height'\n",
    "\n",
    "#pass to OLS with fit method\n",
    "model = ols(formula=f, data=df).fit()\n",
    "\n",
    "model.summary()\n",
    "dir(model)# to see specific parts!! very useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a lot of information. Statsmodels performs a ton of tests and calculates measures to identify goodness of fit. \n",
    "\n",
    "* You can find the R-Squared, which is 0.95 i.e. the data are very linearly related\n",
    "* You can also look at the coefficients of the model for intercept and slope (next to \"height\")\n",
    "* Kurtosis and Skew values are shown here\n",
    "* A lot of significance testing is being done here\n",
    "\n",
    "\n",
    "**Here is a brief description of these measures:**\n",
    "\n",
    "The left part of the first table gives some specifics on the data and the model:\n",
    "\n",
    "* **Dep. Variable**: Singular. Which variable is the point of interest of the model\n",
    "* **Model**: Technique used, an abbreviated version of Method (see methods for more).\n",
    "* **Method**: The loss function optimized in the parameter selection process. Least Squares since it picks the parameters that reduce the training error. This is also known as Mean Square Error [MSE].\n",
    "* **No. Observations**: The number of observations used by the model, or size of the training data.\n",
    "* **Degrees of Freedom Residuals**: Degrees of freedom of the residuals, which is the number of observations – number of parameters. Intercept is a parameter. The purpose of Degrees of Freedom is to reflect the impact of descriptive/summarizing statistics in the model, which in regression is the coefficient. Since the observations must \"live up\" to these parameters, they only have so many free observations, and the rest must be reserved to \"live up\" to the parameters' prophecy. This internal mechanism ensures that there are enough observations to match the parameters.\n",
    "* **Degrees of Freedom Model**: The number of parameters in the model (not including the constant/intercept term if present)\n",
    "* **Covariance Type**: Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process. Since this model is Ordinary Least Squares, it is non-robust and therefore highly sensitive to outliers.\n",
    "\n",
    "The right part of the first table shows the goodness of fit: \n",
    "\n",
    "* **R-squared**: The coefficient of determination, the Sum Squares of Regression divided by Total Sum Squares. This translates to the percent of variance explained by the model. The remaining percentage represents the variance explained by error, the E term, the part that model and predictors fail to grasp.\n",
    "* **Adj. R-squared**: Version of the R-Squared that penalizes additional independent variables. \n",
    "* **F-statistic**: A measure of how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals. Feeds into the calculation of the P-Value.\n",
    "* **Prob (F-statistic) or P-Value**: The probability that a sample like this would yield the above statistic, and whether the model's verdict on the null hypothesis will consistently represent the population. Does not measure effect magnitude, instead measures the integrity and consistency of this test on this group of data.\n",
    "* **Log-likelihood**: The log of the likelihood function.\n",
    "* **AIC**: The Akaike Information Criterion. Adjusts the log-likelihood based on the number of observations and the complexity of the model. Penalizes the model selection metrics when more independent variables are added.\n",
    "* **BIC**: The Bayesian Information Criterion. Similar to the AIC, but has a higher penalty for models with more parameters. Penalizes the model selection metrics when more independent variables are added.\n",
    "\n",
    "The second table shows the coefficient report: \n",
    "\n",
    "* **coef**: The estimated value of the coefficient. By how much the model multiplies the independent value by.\n",
    "* **std err**: The basic standard error of the estimate of the coefficient. Average distance deviation of the points from the model, which offers a unit relevant way to gauge model accuracy.\n",
    "* **t**: The t-statistic value. This is a measure of how statistically significant the coefficient is.\n",
    "* **P > |t|**: P-value that the null-hypothesis that the coefficient = 0 is true. If it is less than the confidence level, often 0.05, it indicates that there is a statistically significant relationship between the term and the response.\n",
    "* **[95.0% Conf. Interval]**: The lower and upper values of the 95% confidence interval. Specific range of the possible coefficient values.\n",
    "\n",
    "The third table shows information about the residuals, autocorrelation, and multicollinearity: \n",
    "\n",
    "* **Skewness**: A measure of the symmetry of the data about the mean. Normally-distributed errors should be symmetrically distributed about the mean (equal amounts above and below the line). The normal distribution has 0 skew.\n",
    "* **Kurtosis**: A measure of the shape of the distribution. Compares the amount of data close to the mean with those far away from the mean (in the tails), so model \"peakiness\". The normal distribution has a Kurtosis of 3, and the greater the number, the more the curve peaks.\n",
    "* **Omnibus D’Angostino’s test**: Provides a combined statistical test for the presence of skewness and kurtosis.\n",
    "* **Prob(Omnibus)**: The above statistic turned into a probability\n",
    "* **Jarque-Bera**: A different test of the skewness and kurtosis\n",
    "* **Prob (JB)**: The above statistic turned into a probability\n",
    "* **Durbin-Watson**: A test for the presence of autocorrelation (that the errors are not independent), which is often important in time-series analysis\n",
    "* **Cond. No**: A test for multicollinearity (if in a fit with multiple parameters, the parameters are related to each other).\n",
    "\n",
    "The interpretation of some of these measures will be explained in the next lessons. For others, you'll get a better insight into them in the lessons on statistics. \n",
    "\n",
    "\n",
    "## Visualize error terms\n",
    "\n",
    "You can also plot some visualizations to check the regression assumptions with respect to the error terms. You'll use `sm.graphics.plot_regress_exog()` for some built-in visualization capabilities of statsmodels. Here is how to do it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-761c9fefae30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_regress_exog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"height\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"height\", fig=fig)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the four graphs we see above:\n",
    "\n",
    "* The **Y and Fitted vs. X** graph plots the dependent variable against our predicted values with a confidence interval. The positive relationship shows that height and weight are correlated, i.e., when one variable increases the other increases.\n",
    "\n",
    "* The **Residuals versus height** graph shows our model's errors versus the specified predictor variable. Each dot is an observed value; the line represents the mean of those observed values. Since there's no pattern in the distance between the dots and the mean value, the OLS assumption of homoskedasticity holds.\n",
    "\n",
    "* The **Partial regression plot** shows the relationship between height and weight, taking in to account the impact of adding other independent variables on our existing height coefficient. You'll later learn how this same graph changes when you add more variables.\n",
    "\n",
    "* The **Component and Component Plus Residual (CCPR)** plot is an extension of the partial regression plot. It shows where the trend line would lie after adding the impact of adding our other independent variables on the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QQ Plots\n",
    "import scipy.stats as stats\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more with regression stats and diagnostics\n",
    "#  https://github.com/ericthansen/dsc-ols-regression-diagnostics\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "data = pd.read_csv('advertising.csv', index_col=0)\n",
    "f = 'sales~TV'\n",
    "f2 = 'sales~radio'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "model2 = smf.ols(formula=f2, data=data).fit()\n",
    "\n",
    "resid1 = model.resid\n",
    "resid2 = model2.resid\n",
    "fig = sm.graphics.qqplot(resid1, dist=stats.norm, line='45', fit=True)\n",
    "fig = sm.graphics.qqplot(resid2, dist=stats.norm, line='45', fit=True)\n",
    "\n",
    "#interpreting qq plots - ccu: peak is to the left (left skew- long right tail)\n",
    "#ccd: peak os to the right - long left tail\n",
    "#like an x^3 graph: too clustered around center than normal or with weird plateau in center\n",
    "#like an x^1/3 graph: too flat / triangular\n",
    "\n",
    "#jarque-bera test for normality - it inspects the skewness and kurtosis.  JB of >=6 indicates difference from normality.\n",
    "# JB test for TV\n",
    "name = ['Jarque-Bera','Prob','Skew', 'Kurtosis']\n",
    "test = sms.jarque_bera(model.resid)\n",
    "list(zip(name, test))\n",
    "# JB test for radio\n",
    "name = ['Jarque-Bera','Prob','Skew', 'Kurtosis']\n",
    "test2 = sms.jarque_bera(model2.resid)\n",
    "list(zip(name, test2))\n",
    "\n",
    "#The Goldfeld Quandt (GQ) test is used in regression analysis to check for homoscedasticity in the error terms. \n",
    "#The GQ test checks if you can define a point that can be used to differentiate the variance of the error term. It is a parametric test and uses the assumption that the data is normally distributed. So it is general practice to check for normality before going over to the GQ test!\n",
    "#In the image below, you can see how observations are split into two groups. Next, a test statistic is run through taking the ratio of mean square residual errors for the regressions on the two subsets. Evidence of heteroscedasticity is based on performing a hypothesis test (more on this later) as shown in the image.\n",
    "lwr_thresh = data.TV.quantile(q=.45)\n",
    "upr_thresh = data.TV.quantile(q=.55)\n",
    "middle_10percent_indices = data[(data.TV >= lwr_thresh) & (data.TV<=upr_thresh)].index\n",
    "# len(middle_10percent_indices)\n",
    "\n",
    "indices = [x-1 for x in data.index if x not in middle_10percent_indices]\n",
    "plt.scatter(data.TV.iloc[indices], model.resid.iloc[indices])\n",
    "plt.xlabel('TV')\n",
    "plt.ylabel('Model Residuals')\n",
    "plt.title(\"Residuals versus TV Feature\")\n",
    "plt.vlines(lwr_thresh, ymax=8, ymin=-8, linestyles='dashed',linewidth=2)\n",
    "plt.vlines(upr_thresh, ymax=8, ymin=-8, linestyles='dashed',linewidth=2);\n",
    "\n",
    "#Here is a brief description of the steps involved:\n",
    "# Order the data in ascending order\n",
    "# Split your data into three parts and drop values in the middle part.\n",
    "# Run separate regression analyses on two parts. After each regression, find the Residual Sum of Squares.\n",
    "# Calculate the ratio of the Residual sum of squares of two parts.\n",
    "# Apply the F-test. (more on F-Test later)\n",
    "# For now, you should just remember that high F values typically indicate that the variances are different. If the error term is homoscedastic, there should be no systematic difference between residuals and F values will be small. However, if the standard deviation of the distribution of the error term is proportional to the x variable, one part will generate a higher sum of square values than the other.\n",
    "\n",
    "# Run Goldfeld Quandt test\n",
    "name = ['F statistic', 'p-value']\n",
    "test = sms.het_goldfeldquandt(model.resid.iloc[indices], model.model.exog[indices])\n",
    "list(zip(name, test))\n",
    "# Run Goldfeld Quandt test\n",
    "import statsmodels.stats.api as sms\n",
    "name = ['F statistic', 'p-value']\n",
    "test = sms.het_goldfeldquandt(model2.resid.iloc[indices], model2.model.exog[indices])\n",
    "list(zip(name, test))\n",
    "\n",
    "# The null hypothesis for the GQ test is homoscedasticity. The larger the F-statistic, the more evidence we will have against the homoscedasticity assumption and the more likely we have heteroscedasticity (different variance for the two groups).\n",
    "\n",
    "# The p-value for our tests above tells us whether or not to reject the null-hypothesis of homoscedasticity. Taking a confidence level of alpha = 0.05, we cannot reject the null hypothesis because for both TV and radio, p-values are larger than 0.05. So even though we visually inspected some heteroscedasticity previously, this cannot be confirmed by the GQ test.\n",
    "\n",
    "# Statsmodel also offers newer tests for heteroscadasticity including the Breush-Pagan Test and White’s Test which may be advantageous in certain cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More on p-value significance?\n",
    "#https://github.com/ericthansen/dsc-significance-p-value\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "data = pd.read_csv('Advertising.csv', index_col=0)\n",
    "f = 'sales~TV'\n",
    "f2 = 'sales~radio'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "\n",
    "model\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Have a look at the probability for the F-statistic in the upper right plane. This is a p-value for the overall model fit. You can see how this probability value is the exact same as the p-value for TV. You'll see that this is always the case if there is only one independent variable. Here, TV drives the entire model, so the model p-value is the same as the TV coefficient p-value.\n",
    "\n",
    "# In the plane with the coefficient estimates, p-values are in the column P > |t| and rounded to 3 digits (hence 0.000 for Intercept and TV). You can find the confidence intervals in the two last columns: The confidence interval for intercept is [6,13, 7.935] meaning that there is a 95% chance that the actual coefficient value is in that range. For TV, there is a 95% chance that the actual coefficient value is in the interval [0.042, 0.053]. Note that 0 is in none of these intervals as expected given the very low p-value.\n",
    "    \n",
    "#see also this lab which was somewhat comprehensive:\n",
    "#    https://github.com/ericthansen/dsc-regression-boston-lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Takeaways\n",
    "In this section, the nominal focus was on how to perform a linear regression, but the real value was learning how to think about the application of machine learning models to data sets.\n",
    "\n",
    "Key takeaways include:\n",
    "\n",
    "Statistical learning theory deals with the problem of finding a predictive function based on data  \n",
    "A loss function calculates how well a given model represents the relationship between data values  \n",
    "A linear regression is simply a (straight) line of best fit for predicting a continuous value (y = mx + c)  \n",
    "The Coefficient of Determination (R Squared) can be used to determine how well a given line fits a given data set  \n",
    "Certain assumptions must hold true for a least squares linear regression to be useful - linearity, normality and heteroscedasticity  \n",
    "Q-Q plots can check for normality in residual errors  \n",
    "The Jarque-Bera test can be used to test for normality - especially when the number of data points is large  \n",
    "The Goldfeld-Quant test can be used to check for homoscedasticity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lecture: https://learning.flatironschool.com/courses/1888/pages/video-simple-linear-regression-part-2?module_item_id=261379\n",
    "# repo: https://github.com/ericthansen/ds-simple_linear_regression-nbz32.git\n",
    "## This has some good explanations of OLS report terms!! #\n",
    "#desmos tool link: https://www.desmos.com/calculator/jwquvmikhr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1234)\n",
    "sen = np.random.uniform(18, 65, 100)\n",
    "income = np.random.normal((sen/10), 0.5)\n",
    "sen = sen.reshape(-1, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "fig.suptitle('seniority vs. income', fontsize=16)\n",
    "plt.scatter(sen, income)\n",
    "plt.plot(sen, sen/10, c='black')\n",
    "plt.xlabel('seniority', fontsize=14)\n",
    "plt.ylabel('monthly income', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with categorical variables\n",
    "#https://github.com/ericthansen/dsc-dealing-with-categorical-variables\n",
    "#Namely, one-hot variables\n",
    "\n",
    "#Sample code:\n",
    "import pandas as pd\n",
    "data = pd.read_csv('auto-mpg.csv')\n",
    "# First convert horsepower into a string and then to int\n",
    "data['horsepower'].astype(str).astype(int)\n",
    "data.head()\n",
    "data.info()\n",
    "\n",
    "print(data['origin'].describe())\n",
    "print(data['origin'].nunique())\n",
    "#Values range from 1 to 3, moreover, actually the only values that are in the dataset are 1, 2 and 3! it turns out that \"origin\" is a so-called categorical variable. It does not represent a continuous number but refers to a location - say 1 may stand for US, 2 for Europe, 3 for Asia (note: for this dataset the actual meaning is not disclosed).\n",
    "\n",
    "#id-ing cat vars\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(16,3))\n",
    "\n",
    "for xcol, ax in zip(['acceleration', 'displacement', 'horsepower', 'weight'], axes):\n",
    "    data.plot(kind='scatter', x=xcol, y='mpg', ax=ax, alpha=0.4, color='b')\n",
    "    \n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,3))\n",
    "\n",
    "for xcol, ax in zip([ 'cylinders', 'model year', 'origin'], axes):\n",
    "    data.plot(kind='scatter', x=xcol, y='mpg', ax=ax, alpha=0.4, color='b')\n",
    "\n",
    "#To id categoricals, its helpful to look at histogram or the number of unique vals\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.gca()\n",
    "data.hist(ax = ax);\n",
    "\n",
    "data[['cylinders', 'model year', 'origin']].nunique()\n",
    "\n",
    "# #Transforming categorical variables\n",
    "# When you want to use categorical variables in regression models, they need to be transformed. There are two approaches to this:\n",
    "\n",
    "# 1) Perform label encoding\n",
    "# 2) Create dummy variables / one-hot-encoding\n",
    "\n",
    "\n",
    "# Label encoding\n",
    "# Let's illustrate label encoding and dummy creation with the following Pandas Series with 3 categories: \"USA\", \"EU\" and \"ASIA\".\n",
    "origin = ['USA', 'EU', 'EU', 'ASIA','USA', 'EU', 'EU', 'ASIA', 'ASIA', 'USA']\n",
    "origin_series = pd.Series(origin)\n",
    "\n",
    "#Now you'll want to make sure Python recognizes there strings as categories. This can be done as follows:\n",
    "cat_origin = origin_series.astype('category')\n",
    "cat_origin\n",
    "\n",
    "# Sometimes you'll want to represent your labels as numbers. This is called label encoding.\n",
    "\n",
    "# You'll perform label encoding in a way that numerical labels are always between 0 and (number_of_categories)-1. There are several ways to do this, one way is using .cat.codes\n",
    "\n",
    "cat_origin.cat.codes\n",
    "\n",
    "#Another way is to use scikit-learn's LabelEncoder:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "origintrap_df = pd.get_dummies(cat_origin)\n",
    "trap_df_encoded = lb_make.fit_transform(cat_origin)\n",
    "\n",
    "#Note that while .cat.codes can only be used on variables that are transformed using .astype(category), this is not a requirement to use LabelEncoder.\n",
    "\n",
    "#Creating Dummy Variables\n",
    "#Another way to transform categorical variables is through using one-hot encoding or \"dummy variables\". The idea is to convert each category into a new column, and assign a 1 or 0 to the column. There are several libraries that support one-hot encoding, let's take a look at two:\n",
    "\n",
    "pd.get_dummies(cat_origin)\n",
    "#See how the label name has become the column name! Another method is through using the LabelBinarizer in scikit-learn.\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "origin_dummies = lb.fit_transform(cat_origin)\n",
    "# You need to convert this back to a dataframe\n",
    "origin_dum_df = pd.DataFrame(origin_dummies,columns=lb.classes_)\n",
    "origin_dum_df\n",
    "#The advantage of using dummies is that, whatever algorithm you'll be using, your numerical values cannot be misinterpreted as being continuous. Going forward, it's important to know that for linear regression (and most other algorithms in scikit-learn), one-hot encoding is required when adding categorical variables in a regression model!\n",
    "\n",
    "#The Dummy Variable Trap\n",
    "#Due to the nature of how dummy variables are created, one variable can be predicted from all of the others. This is known as perfect multicollinearity and it can be a problem for regression. Multicollinearity will be covered in depth later but the basic idea behind perfect multicollinearity is that you can perfectly predict what one variable will be using some combination of the other variables. If this isn't super clear, go back to the one-hot encoded origin data above:\n",
    "trap_df = pd.get_dummies(cat_origin)\n",
    "trap_df\n",
    "# Predict ASIA column from EU and USA\n",
    "predicted_asia = 1 - (trap_df['EU'] + trap_df['USA'])\n",
    "predicted_asia.to_frame(name='Predicted_ASIA')\n",
    "#You are probably wondering why this is a problem for regression. Recall that the coefficients derived from a regression model are used to make predictions. In a multiple linear regression, the coefficients represent the average change in the dependent variable for each 1 unit change in a predictor variable, assuming that all the other predictor variables are kept constant. This is no longer the case when predictor variables are related which, as you've just seen, happens automatically when you create dummy variables. This is what is known as the Dummy Variable Trap.\n",
    "#Fortunately, the dummy variable trap can be avoided by simply dropping one of the dummy variables. You can do this by subsetting the dataframe manually or, more conveniently, by passing drop_first=True to get_dummies():\n",
    "pd.get_dummies(cat_origin, drop_first=True)\n",
    "\n",
    "\n",
    "#Back to our auto-mpg data\n",
    "#Let's go ahead and change our \"cylinders\", \"model year\", and \"origin\" columns over to dummies and drop the first variable.\n",
    "cyl_dummies = pd.get_dummies(data['cylinders'], prefix='cyl', drop_first=True)\n",
    "yr_dummies = pd.get_dummies(data['model year'], prefix='yr', drop_first=True)\n",
    "orig_dummies = pd.get_dummies(data['origin'], prefix='orig', drop_first=True)\n",
    "#Next, let's remove the original columns from our data and add the dummy columns instead\n",
    "data = data.drop(['cylinders','model year','origin'], axis=1)\n",
    "data = pd.concat([data, cyl_dummies, yr_dummies, orig_dummies], axis=1)\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity example (more)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "data = pd.read_csv('auto-mpg.csv')\n",
    "data['horsepower'].astype(str).astype(int) # don't worry about this for now\n",
    "data.head()\n",
    "#removing the target variable (and car name)\n",
    "data_pred = data.iloc[:,1:8]\n",
    "data_pred.head()\n",
    "#initial look at how predictors relate\n",
    "pd.plotting.scatter_matrix(data_pred,figsize  = [9, 9]);\n",
    "plt.show()\n",
    "#and the corresponding correlation numbers\n",
    "data_pred.corr()\n",
    "#Generally, a correlation with an absolute value around 0.7-0.8 or higher is considered a high correlation. If we take 0.75 as a cut-off, how many high correlations do we have?\n",
    "abs(data_pred.corr()) > 0.75\n",
    "\n",
    "#what if the set was too large to manage manually?\n",
    "####\n",
    "####THIS IS HIGHLY SLICK CODE THAT CREATES HIGHLY CORRELATED PAIRS.  KEEP IT HANDY!###\n",
    "# save absolute value of correlation matrix as a data frame\n",
    "# converts all values to absolute value\n",
    "# stacks the row:column pairs into a multindex\n",
    "# reset the index to set the multindex to seperate columns\n",
    "# sort values. 0 is the column automatically generated by the stacking\n",
    "\n",
    "\n",
    "df=data_pred.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "# zip the variable name columns (Which were only named level_0 and level_1 by default) in a new column named \"pairs\"\n",
    "df['pairs'] = list(zip(df.level_0, df.level_1))\n",
    "\n",
    "# set index to pairs\n",
    "df.set_index(['pairs'], inplace = True)\n",
    "\n",
    "#d rop level columns\n",
    "df.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# rename correlation column as cc rather than 0\n",
    "df.columns = ['cc']\n",
    "\n",
    "# drop duplicates. This could be dangerous if you have variables perfectly correlated with variables other than themselves.\n",
    "# for the sake of exercise, kept it in.\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "#Then subset as before\n",
    "df[(df.cc>.75) & (df.cc <1)]\n",
    "\n",
    "#####\n",
    "#With the variables 'cylinder', 'displacement', 'horsepower', and 'weight' so highly correlated, you would typically remove three of them in order to remove collinear features.\n",
    "\n",
    "#Another option is to use a heatmap to render the correlation matrix as a visualization.\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create a df with the target as the first column,\n",
    "# then compute the correlation matrix\n",
    "heatmap_data = pd.concat([y_train, X_train], axis=1)\n",
    "corr = heatmap_data.corr()\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, ax = plt.subplots(figsize=(5, 8))\n",
    "\n",
    "# Plot a heatmap of the correlation matrix, with both\n",
    "# numbers and colors indicating the correlations\n",
    "sns.heatmap(\n",
    "    # Specifies the data to be plotted\n",
    "    data=corr,\n",
    "    # The mask means we only show half the values,\n",
    "    # instead of showing duplicates. It's optional.\n",
    "    mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "    # Specifies that we should use the existing axes\n",
    "    ax=ax,\n",
    "    # Specifies that we want labels, not just colors\n",
    "    annot=True,\n",
    "    # Customizes colorbar appearance\n",
    "    cbar_kws={\"label\": \"Correlation\", \"orientation\": \"horizontal\", \"pad\": .2, \"extend\": \"both\"}\n",
    ")\n",
    "\n",
    "# Customize the plot appearance\n",
    "ax.set_title(\"Heatmap of Correlation Between Attributes (Including Target)\");\n",
    "#crap - must have overwritten this\n",
    "\n",
    "\n",
    "##lab on multicollinearity/vis\n",
    "\n",
    "#trick for making readable huge scatterplot\n",
    "sm = pd.plotting.scatter_matrix(ames_preprocessed, figsize=[20, 20]);\n",
    "\n",
    "# Rotates the text\n",
    "[s.xaxis.label.set_rotation(90) for s in sm.reshape(-1)]\n",
    "[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n",
    "\n",
    "#May need to offset label when rotating to prevent overlap of figure\n",
    "[s.get_yaxis().set_label_coords(-1,0.5) for s in sm.reshape(-1)]\n",
    "\n",
    "#Hide all ticks\n",
    "[s.set_xticks(()) for s in sm.reshape(-1)]\n",
    "[s.set_yticks(()) for s in sm.reshape(-1)]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transformations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "'''Linear Regression Assumptions\n",
    "Remember that linear regression operates under various assumptions including that the dependent variable can be decomposed into a linear combination of the independent features. Additionally, data should be homoscedastic and the residuals should follow a normal distribution.\n",
    "\n",
    "One thing we briefly touched upon previously is the distributions of the predictors. In previous labs, you have looked at these distributions to have an understanding of what the distributions look like. In fact, you'll often find that having the data more normally distributed will benefit your model and model performance in general. So while normality of the predictors is not a mandatory assumption, having (approximately) normal features may be helpful for your model!'''\n",
    "\n",
    "#using raw features that aren't normal\n",
    "data = pd.read_csv('auto-mpg.csv')\n",
    "data.head()\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "outcome = 'mpg'\n",
    "x_cols = ['displacement', 'horsepower', 'weight', 'acceleration']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=data).fit()\n",
    "model.summary()\n",
    "pd.plotting.scatter_matrix(data[x_cols], figsize=(10,12));\n",
    "\n",
    "#reminder about log\n",
    "x = np.linspace(start=-100, stop=100, num=10**3)\n",
    "y = np.log(x)\n",
    "plt.plot(x, y);\n",
    "\n",
    "#Transforming nonnormal features\n",
    "non_normal = ['displacement', 'horsepower', 'weight']\n",
    "for feat in non_normal:\n",
    "    data[feat] = data[feat].map(lambda x: np.log(x))\n",
    "pd.plotting.scatter_matrix(data[x_cols], figsize=(10,12));\n",
    "#after transforming, model\n",
    "outcome = 'mpg'\n",
    "x_cols = ['displacement', 'horsepower', 'weight', 'acceleration']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=data).fit()\n",
    "model.summary()\n",
    "'''While not dramatic, you can observe that simply by transforming non-normally distributed features using log transformations, we have increased our  𝑅2  value of the model from 0.707 to 0.748.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other kinds of scaling\n",
    "'''Min-max scaling\n",
    "standardization by mean and std\n",
    "mean norm(subtract mean, divide by diff of max and min)\n",
    "unit vector normalization (dvide by magnitude of x)\n",
    "'''\n",
    "#applying transform to auto-mpg data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "data = pd.read_csv('auto-mpg.csv')\n",
    "data['horsepower'].astype(str).astype(int) # don't worry about this for now\n",
    "data_pred = data.iloc[:,1:8]\n",
    "data_pred.head()\n",
    "data_pred[['acceleration', 'displacement', 'horsepower', 'weight']].hist(figsize  = [6, 6]); \n",
    "#skewness is an issue\n",
    "#first make it more normal, then feature scale to manage the diff in magnitude\n",
    "import numpy as np\n",
    "data_log = pd.DataFrame([])\n",
    "data_log['logdisp'] = np.log(data_pred['displacement'])\n",
    "data_log['loghorse'] = np.log(data_pred['horsepower'])\n",
    "data_log['logweight'] = np.log(data_pred['weight'])\n",
    "data_log.hist(figsize  = [6, 6]);\n",
    "'''Now, let's perform min-max scaling (on 'acceleration'), standardization (on 'logdisp' and 'logweight'), and mean normalization (on 'loghorse').'''\n",
    "acc = data_pred['acceleration']\n",
    "logdisp = data_log['logdisp']\n",
    "loghorse = data_log['loghorse']\n",
    "logweight = data_log['logweight']\n",
    "\n",
    "scaled_acc = (acc - min(acc)) / (max(acc) - min(acc))\n",
    "scaled_disp = (logdisp - np.mean(logdisp)) / np.sqrt(np.var(logdisp))\n",
    "scaled_weight = (logweight - np.mean(logweight)) / np.sqrt(np.var(logweight))\n",
    "scaled_horse = (loghorse - np.mean(loghorse)) / (max(loghorse) - min(loghorse))\n",
    "\n",
    "data_cont_scaled = pd.DataFrame([])\n",
    "data_cont_scaled['acc'] = scaled_acc\n",
    "data_cont_scaled['disp'] = scaled_disp\n",
    "data_cont_scaled['horse'] = scaled_horse\n",
    "data_cont_scaled['weight'] = scaled_weight\n",
    "\n",
    "data_cont_scaled.hist(figsize = [6, 6]);\n",
    "\n",
    "'''Scikit-learn provides automatic tools to scale features, see, among others, MinMaxScaler, StandardScaler, and Normalizer. Have a look at these built-in functions and some code examples here: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing!\n",
    "\n",
    "To learn more about feature scaling in general, you can have a look at this blogpost: https://sebastianraschka.com/Articles/2014_about_feature_scaling.html (up until \"bottom-up approaches\").'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple Lin Regression Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('auto-mpg.csv') \n",
    "data['horsepower'].astype(str).astype(int)\n",
    "\n",
    "acc = data['acceleration']\n",
    "logdisp = np.log(data['displacement'])\n",
    "loghorse = np.log(data['horsepower'])\n",
    "logweight= np.log(data['weight'])\n",
    "\n",
    "scaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\n",
    "scaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\n",
    "scaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\n",
    "scaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n",
    "\n",
    "data_fin = pd.DataFrame([])\n",
    "data_fin['acc'] = scaled_acc\n",
    "data_fin['disp'] = scaled_disp\n",
    "data_fin['horse'] = scaled_horse\n",
    "data_fin['weight'] = scaled_weight\n",
    "cyl_dummies = pd.get_dummies(data['cylinders'], prefix='cyl', drop_first=True)\n",
    "yr_dummies = pd.get_dummies(data['model year'], prefix='yr', drop_first=True)\n",
    "orig_dummies = pd.get_dummies(data['origin'], prefix='orig', drop_first=True)\n",
    "mpg = data['mpg']\n",
    "data_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\n",
    "\n",
    "data_fin.info()\n",
    "\n",
    "#simplifying the model to fewer things\n",
    "data_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\n",
    "data_ols.head()\n",
    "\n",
    "#now use statsmodels api\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "formula = 'mpg ~ acceleration+weight+orig_2+orig_3'\n",
    "model = ols(formula=formula, data=data_ols).fit()\n",
    "\n",
    "'''Having to type out all the predictors isn't practical when you have many. Another better way than to type them all out is to seperate out the outcome variable 'mpg' out of your DataFrame, and use the a '+'.join() command on the predictors, as done below:'''\n",
    "outcome = 'mpg'\n",
    "predictors = data_ols.drop('mpg', axis=1)\n",
    "pred_sum = '+'.join(predictors.columns)\n",
    "formula = outcome + '~' + pred_sum\n",
    "model = ols(formula=formula, data=data_ols).fit()\n",
    "model.summary()\n",
    "#to avoid making the summation string,  use the ols function\n",
    "'''The advantage is that you don't have to create the summation string. Important to note, however, is that the intercept term is not included by default, so you have to make sure you manipulate your predictors DataFrame so it includes a constant term. You can do this using .add_constant.'''\n",
    "import statsmodels.api as sm\n",
    "predictors_int = sm.add_constant(predictors)\n",
    "model = sm.OLS(data['mpg'],predictors_int).fit()\n",
    "model.summary()\n",
    "'''Just like for single multiple regression, the coefficients for the model should be interpreted as \"how does  𝑦  change for each additional unit  𝑋 \"? However, do note that since  𝑋  was transformed, the interpretation can sometimes require a little more attention. In fact, as the model is built on the transformed  𝑋 , the actual relationship is \"how does  𝑦  change for each additional unit  𝑋′ \", where  𝑋′  is the (log- and min-max, standardized,...) transformed data matrix.'''\n",
    "'''Linear regression using scikit-learn\n",
    "You can also repeat this process using scikit-learn. The code to do this can be found below. The scikit-learn package is known for its machine learning functionalities and generally very popular when it comes to building a clear data science workflow. It is also commonly used by data scientists for regression. The disadvantage of scikit-learn compared to statsmodels is that it doesn't have some statistical metrics like the p-values of the parameter estimates readily available. For a more ad-hoc comparison of scikit-learn and statsmodels, you can read this blogpost: https://blog.thedataincubator.com/2017/11/scikit-learn-vs-statsmodels/.'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "y = data_ols['mpg']\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(predictors, y)\n",
    "# coefficients\n",
    "linreg.coef_\n",
    "# intercept\n",
    "linreg.intercept_\n",
    "\n",
    "\n",
    "#Predicting - from lab\n",
    "#I just took dot product of the inputs and the coefficients (after manually one-hot-ting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelling for inference (understanding) vs. Prediction (application).  The latter often becomes\n",
    "#a black box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model fit in linear regression\n",
    "\n",
    "# https://github.com/ericthansen/dsc-model-fit-linear-regression.git\n",
    "'''You will be able to:\n",
    "\n",
    "Use stepwise selection methods to determine the most important features for a model\n",
    "Use recursive feature elimination to determine the most important features for a model'''\n",
    "\n",
    "\n",
    "##summary of previous work, including log mapping and categorical handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('auto-mpg.csv')\n",
    "\n",
    "acc = data['acceleration']\n",
    "logdisp = np.log(data['displacement'])\n",
    "loghorse = np.log(data['horsepower'])\n",
    "logweight = np.log(data['weight'])\n",
    "\n",
    "scaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\n",
    "scaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\n",
    "scaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\n",
    "scaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n",
    "\n",
    "data_fin = pd.DataFrame([])\n",
    "data_fin['acc'] = scaled_acc\n",
    "data_fin['disp'] = scaled_disp\n",
    "data_fin['horse'] = scaled_horse\n",
    "data_fin['weight'] = scaled_weight\n",
    "cyl_dummies = pd.get_dummies(data['cylinders'], prefix='cyl', drop_first=True)\n",
    "yr_dummies = pd.get_dummies(data['model year'], prefix='yr', drop_first=True)\n",
    "orig_dummies = pd.get_dummies(data['origin'], prefix='orig', drop_first=True)\n",
    "mpg = data['mpg']\n",
    "data_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\n",
    "\n",
    "###\n",
    "data_ols = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\n",
    "data_ols.head(3)\n",
    "# Import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "#predictors\n",
    "outcome = 'mpg'\n",
    "predictors = data_ols.drop('mpg', axis=1)\n",
    "pred_sum = '+'.join(predictors.columns)\n",
    "formula = outcome + '~' + pred_sum\n",
    "\n",
    "#model fit\n",
    "model = ols(formula=formula, data=data_ols).fit()\n",
    "model.summary()\n",
    "\n",
    "'''Note that with 6 variables, we can sub-select some of those variables, each to create a different model.'''\n",
    "'''This means that with just 6 variables, a so-called exhaustive search of all submodels results in having to evaluate 64 models. Below, we'll describe two methods that can be used to select a submodel with the most appropriate features: stepwise selection on the one hand, and feature ranking with recursive feature elimination on the other hand.'''\n",
    "\n",
    "#stepwise selection links:\n",
    "https://en.wikipedia.org/wiki/Stepwise_regression\n",
    "https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm\n",
    "#stepwise selection - not yet built in to python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "\n",
    "result = stepwise_selection(predictors, data_fin['mpg'], verbose=True)\n",
    "print('resulting features:')\n",
    "print(result)\n",
    "\n",
    "#link to scikit learn for feature ranking\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection\n",
    "    \n",
    "#from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "selector = RFE(linreg, n_features_to_select=3)\n",
    "selector = selector.fit(predictors, data_fin['mpg'])\n",
    "\n",
    "#this tells you which variables are selected\n",
    "selector.support_ \n",
    "#and the ranking\n",
    "selector.ranking_\n",
    "\n",
    "#you can get access to the parameter estimates\n",
    "estimators = selector.estimator_\n",
    "print(estimators.coef_)\n",
    "print(estimators.intercept_)\n",
    "\n",
    "'''Note that the regression coefficients and intercept are slightly different. This is because only the three most important features were used in the model instead of the original four features. If you pass n_features_to_select=4, you should get the original coefficients.'''\n",
    "n_features_to_select=4\n",
    "\n",
    "https://planspace.org/20150423-forward_selection_with_statsmodels/\n",
    "    provides code for a forward selection procedure (much like stepwise selection, but without a backward pass), but this time looking at the adjusted R-squared to make decisions on which variable to add to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test sets\n",
    "# default 70/30 split\n",
    "#a big diff between test and training (Root)mean square error is a sign of overfitting\n",
    "\n",
    "#applying again to auto-mpg data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('auto-mpg.csv') \n",
    "data['horsepower'].astype(str).astype(int)\n",
    "\n",
    "acc = data['acceleration']\n",
    "logdisp = np.log(data['displacement'])\n",
    "loghorse = np.log(data['horsepower'])\n",
    "logweight = np.log(data['weight'])\n",
    "\n",
    "scaled_acc = (acc-min(acc))/(max(acc)-min(acc))\n",
    "scaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\n",
    "scaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\n",
    "scaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n",
    "\n",
    "data_fin = pd.DataFrame([])\n",
    "data_fin['acc'] = scaled_acc\n",
    "data_fin['disp'] = scaled_disp\n",
    "data_fin['horse'] = scaled_horse\n",
    "data_fin['weight'] = scaled_weight\n",
    "cyl_dummies = pd.get_dummies(data['cylinders'], prefix='cyl', drop_first=True)\n",
    "yr_dummies = pd.get_dummies(data['model year'], prefix='yr', drop_first=True)\n",
    "orig_dummies = pd.get_dummies(data['origin'], prefix='orig', drop_first=True)\n",
    "mpg = data['mpg']\n",
    "data_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)\n",
    "\n",
    "data = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\n",
    "y = data[['mpg']]\n",
    "X = data.drop(['mpg'], axis=1)\n",
    "\n",
    "#Scikitlearn has a handy function train_test_split\n",
    "imp\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "\n",
    "#doing regression on the training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "y_hat_train = linreg.predict(X_train)\n",
    "y_hat_test = linreg.predict(X_test)\n",
    "\n",
    "#look at residuals, calc MSE (mean square error) for each\n",
    "train_residuals = y_hat_train - y_train\n",
    "test_residuals = y_hat_test - y_test\n",
    "mse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\n",
    "mse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\n",
    "print('Train Mean Squarred Error:', mse_train)\n",
    "print('Test Mean Squarred Error:', mse_test)\n",
    "\n",
    "#or calc directly from scikitlearn function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_mse = mean_squared_errorcross_val\n",
    "test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "print('Train Mean Squarred Error:', train_mse)\n",
    "print('Test Mean Squarred Error:', test_mse)\n",
    "\n",
    "#for this sample, not a big difference\n",
    "\n",
    "#A nice blog post with some handy visualizations and workflow with model building, test sets, etc\n",
    "https://towardsdatascience.com/linear-regression-in-python-9a1f5f000606\n",
    "'''summary from that blog: Replace feature_cols & X\n",
    "Train_test_split your data\n",
    "Fit the model to linreg again using linreg.fit\n",
    "Make predictions using (y_pred = linreg.predict(X_test))\n",
    "Compute RMSE\n",
    "Repeat until RMSE satisfactory'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lab.  including sub loop to smooth out, averaging over several different traintest splits\n",
    "#https://github.com/ericthansen/dsc-regression-model-validation-lab\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Your code here\n",
    "# Your code here\n",
    "iters = 10\n",
    "results = pd.DataFrame(columns = ['Training Size', 'Train Error', 'Test Error'])\n",
    "#pd.DataFrame(data, columns = ['Name', 'Age'])\n",
    "for split in range(5, 96, 5):\n",
    "    s = split/100\n",
    "    avg_train_mse = 0\n",
    "    avg_test_mse = 0\n",
    "    for _ in range(iters):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=s)#, random_state=42)\n",
    "        linreg = LinearRegression()\n",
    "        linreg.fit(X_train, y_train)\n",
    "        y_hat_train = linreg.predict(X_train)\n",
    "        y_hat_test = linreg.predict(X_test)\n",
    "        train_residuals = y_hat_train - y_train\n",
    "        test_residuals = y_hat_test - y_test\n",
    "    #     mse_train = np.sum((y_train-y_hat_train)**2)/len(y_train)\n",
    "    #     mse_test = np.sum((y_test-y_hat_test)**2)/len(y_test)\n",
    "        train_mse = mean_squared_error(y_train, y_hat_train)\n",
    "        test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "        avg_train_mse += train_mse\n",
    "        avg_test_mse += test_mse\n",
    "    train_mse = avg_train_mse / iters\n",
    "    test_mse = avg_test_mse / iters\n",
    "    \n",
    "    results = results.append({'Training Size':s, 'Train Error': train_mse, 'Test Error': test_mse},ignore_index=True)\n",
    "#results\n",
    "results.plot(x='Training Size', y = ['Train Error', 'Test Error'])\n",
    "    #['Train Error', 'Test Error'], x=['Training Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We can see from this that different random test splits can results in dramatically different\n",
    "results.  One way to address this: K-Fold Cross Validation\n",
    "In order to deal with the issues that random sampling can introduce into interpreting the quality of our models, we'll use a more advanced technique called K-Fold Cross Validation.\n",
    "'''\n",
    "'''K-Fold Cross Validation expands on the idea of training and test splits by splitting the entire dataset into {K} equal sections of data. We'll then iteratively train {K} linear regression models on the data, with each linear model using a different section of data as the test set, and all other sections combined as the training set.\n",
    "We can then average the individual results frome each of these linear models to get a Cross-Validation MSE. This will be closer to the model's actual MSE, since \"noisy\" results that are higher than average will cancel out the \"noisy\" results that are lower than average.'''\n",
    "\n",
    "'''You can easily do this in scikit-learn using cross_val_score(). If you want the mean squared error as an output, you need to set the scoring argument to 'neg_mean_squared_error'. Note that this negates your mean squared error, so larger means better!'''\n",
    "cross_val_score()\n",
    "\n",
    "#for example\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_5_results  = np.mean(polynom)\n",
    "cv_10_results = np.mean(cross_val_score(linreg, X, y, cv=10, scoring='neg_mean_squared_error'))\n",
    "cv_20_results = np.mean(cross_val_score(linreg, X, y, cv=20, scoring='neg_mean_squared_error'))\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/cross_validation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle!\n",
    "#basically a way to save objects (or virtually anything in the Python interpreter) to a file\n",
    "import pickle\n",
    "data_object = {\n",
    "    'a': [1, 2.0, 3, 4+6j],\n",
    "    'b': ('character string', b'byte string'),\n",
    "    'c': {None, True, False}\n",
    "}\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(data_object, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#with scikit learn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# y = 1 * x_0 + 2 * x_1 + 3\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "import pickle\n",
    "# Save\n",
    "with open('regression_model.pickle', 'wb') as f:\n",
    "    pickle.dump(reg, f)\n",
    "\n",
    "# Load\n",
    "with open('regression_model.pickle', 'rb') as file:\n",
    "    reg2 = pickle.load(file)\n",
    "reg2.predict(X)\n",
    "\n",
    "\n",
    "#links\n",
    "https://docs.python.org/3/library/pickle.html\n",
    "https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science workflow\n",
    "Lots of visuals here, worthwhile to see\n",
    "https://github.com/ericthansen/dsc-data-science-processes\n",
    "\n",
    "Data Science projects are often complex, with many stakeholders, data sources, and goals. Due to this, the Data Science community has created several methodologies for helping organize and structure Data Science Projects. In this lesson, you'll explore three of the most popular methodologies -- CRISP-DM, KDD, and OSEMN, and explore how you can make use of them to keep your projects well-structured and organized.\n",
    "\n",
    "CRoss-Industry Standard Process for Data Mining (CRISP-DM)\n",
    "-business understanding\n",
    "Good questions for this stage include:\n",
    "\n",
    "Who are the stakeholders in this project? Who will be directly affected by the creation of this project?\n",
    "What business problem(s) will this Data Science project solve for the organization?\n",
    "What problems are inside the scope of this project?\n",
    "What problems are outside the scope of this project?\n",
    "What data sources are available to us?\n",
    "What is the expected timeline for this project? Are there hard deadlines (e.g. \"must be live before holiday season shopping\") or is this an ongoing project?\n",
    "Do stakeholders from different parts of the company or organization all have the exact same understanding about what this project is and isn't?\n",
    "\n",
    "Data Understanding:\n",
    "What data is available to us? Where does it live? Do we have the data, or can we scrape/buy/source the data from somewhere else?\n",
    "Who controls the data sources, and what steps are needed to get access to the data?\n",
    "What is our target?\n",
    "What predictors are available to us?\n",
    "What data types are the predictors we'll be working with?\n",
    "What is the distribution of our data?\n",
    "How many observations does our dataset contain? Do we have a lot of data? Only a little?\n",
    "Do we have enough data to build a model? Will we need to use resampling methods?\n",
    "How do we know the data is correct? How is the data collected? Is there a chance the data could be wrong?\n",
    "\n",
    "\n",
    "Data Preparation:\n",
    "\n",
    "Once we have a strong understanding of our data, we can move onto preparing the data for our modeling steps.\n",
    "\n",
    "During this stage, we'll want to handle the following issues:\n",
    "\n",
    "Detecting and dealing with missing values\n",
    "Data type conversions (e.g. numeric data mistakenly encoded as strings)\n",
    "Checking for and removing multicollinearity (correlated predictors)\n",
    "Normalizing our numeric data\n",
    "Converting categorical data to numeric format through one-hot encoding\n",
    "\n",
    "Modeling:\n",
    "\n",
    "Once we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task.\n",
    "\n",
    "Consider the following questions during the modeling step:\n",
    "\n",
    "Is this a classification task? A regression task? Something else?\n",
    "What models will we try?\n",
    "How do we deal with overfitting?\n",
    "Do we need to use regularization or not?\n",
    "What sort of validation strategy will we be using to check that our model works well on unseen data?\n",
    "What loss functions will we use?\n",
    "What threshold of performance do we consider as successful?\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "During this step, we'll evaluate the results of our modeling efforts. Does our model solve the problems that we outlined all the way back during step 1? Why or why not? Often times, evaluating the results of our modeling step will raise new questions, or will cause us to consider changing our approach to the problem. Notice from the CRISP-DM diagram above, that the \"Evaluation\" step is unique in that it points to both Business Understanding and Deployment. As we mentioned before, Data Science is an iterative process -- that means that given the new information our model has provided, we'll often want to start over with another iteration, armed with our newfound knowledge! Perhaps the results of our model showed us something important that we had originally failed to consider the goal of the project or the scope. Perhaps we learned that the model can't be successful without more data, or different data. Perhaps our evaluation shows us that we should reconsider our approach to cleaning and structuring the data, or how we frame the project as a whole (e.g. realizing we should treat the problem as a classification rather than a regression task). In any of these cases, it is totally encouraged to revisit the earlier steps.\n",
    "\n",
    "Of course, if the results are satisfactory, then we instead move onto deployment!\n",
    "\n",
    "Deployment:\n",
    "\n",
    "During this stage, we'll focus on moving our model into production and automating as much as possible. Everything before this serves as a proof-of-concept or an investigation. If the project has proved successful, then you'll work with stakeholders to determine the best way to implement models and insights. For example, you might set up an automated ETL (Extract-Transform-Load) pipelines of raw data in order to feed into a database and reformat it so that it is ready for modeling. During the deployment step, you'll actively work to determine the best course of action for getting the results of your project into the wild, and you'll often be involved with building everything needed to put the software into production.\n",
    "\n",
    "This is one of the most rewarding steps of the entire Data Science process -- getting to see your work go live!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Knowledge Discovery in Databases, or KDD is considered the oldest Data Science process. \n",
    "\n",
    "The creation of this process is credited to Gregory Piatetsky-Shapiro, who also runs the ever-popular Data Science blog, kdnuggets (Links to an external site.). If you're interested, read the original white paper on KDD, which can be found here (Links to an external site.)!\n",
    "data\n",
    "-Selection  \n",
    "target data\n",
    "-preprocessing  \n",
    "preprocessed data\n",
    "-Transformation  \n",
    "transformed data\n",
    "-data mining  \n",
    "patterns\n",
    "-interpretation/evaluation  \n",
    "knowledge\n",
    "\n",
    "\n",
    "\n",
    "OSEMN model\n",
    "\n",
    "Obtain/scrub/explore/model/interpret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing specific lab\n",
    "\n",
    "# Run this cell without changes\n",
    "\n",
    "#heck, this entire notebook:\n",
    "https://github.com/ericthansen/dsc-sklearn-preprocessing-lab\n",
    "\n",
    "#of note - good guidance on  Missing indicator technique\n",
    "# Replace None with appropriate code\n",
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "# (1) Identify data to be transformed\n",
    "# We only want missing indicators for LotFrontage\n",
    "frontage_train = X_train[[\"LotFrontage\"]]\n",
    "\n",
    "# (2) Instantiate the transformer object\n",
    "missing_indicator = MissingIndicator()\n",
    "\n",
    "# (3) Fit the transformer object on frontage_train\n",
    "missing_indicator.fit(frontage_train)\n",
    "\n",
    "# (4) Transform frontage_train and assign the result\n",
    "# to frontage_missing_train\n",
    "frontage_missing_train = missing_indicator.transform(frontage_train)\n",
    "\n",
    "# Visually inspect frontage_missing_train\n",
    "frontage_missing_train\n",
    "\n",
    "#repeated several times.  see notebook for detail.\n",
    "\n",
    "'''neat convention:\n",
    "    The .categories_ attribute of OrdinalEncoder is only present once the .fit method has been called. (The trailing _ indicates this convention.)'''\n",
    "'''lots of useful workflow from video'''\n",
    "'''https://github.com/ericthansen/ds-multiple_linear_regression-nbz32/blob/main/multiple_linear_regression.ipynb'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactions\n",
    "#In statistics, an interaction is a particular property of three or more variables, where two or more variables interact in a non-additive manner when affecting a third variable. In other words, the two variables interact to have an effect that is more (or less) than the sum of their parts.\n",
    "#sometimes there are \"confounding factors\" - ie new paramters that account for otherwise unpredictable behavior\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv('auto-mpg.csv') \n",
    "\n",
    "\n",
    "acc = data['acceleration']\n",
    "logdisp = np.log(data['displacement'])\n",
    "loghorse = np.log(data['horsepower'])\n",
    "logweight= np.log(data['weight'])\n",
    "\n",
    "scaled_acc = (acc-min(acc))/(max(acc)-min(acc))\t\n",
    "scaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\n",
    "scaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\n",
    "scaled_weight= (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n",
    "\n",
    "data_fin = pd.DataFrame([])\n",
    "data_fin['acc']= scaled_acc\n",
    "data_fin['disp']= scaled_disp\n",
    "data_fin['horse'] = scaled_horse\n",
    "data_fin['weight'] = scaled_weight\n",
    "mpg = data['mpg']\n",
    "data_fin = pd.concat([mpg, data_fin, data['cylinders'], data['model year'], data['origin']], axis=1)\n",
    "y = data_fin[['mpg']]\n",
    "X = data_fin.drop(['mpg'], axis=1)\n",
    "\n",
    "regression = LinearRegression()\n",
    "crossvalidation = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "baseline = np.mean(cross_val_score(regression, X, y, scoring='r2', cv=crossvalidation))\n",
    "baseline\n",
    "\n",
    "'''See how we built a baseline model using some log-transformed predictors and some categorical predictors. We didn't properly convert the categorical variables to categorical yet, which we should do in the end, but we want to start with a baseline model and a baseline  𝑅2  just to get a sense of what a baseline model looks like.'''\n",
    "'''to see how horsepower and origin work together...'''\n",
    "origin_1 = data_fin[data_fin['origin'] == 1]\n",
    "origin_2 = data_fin[data_fin['origin'] == 2]\n",
    "origin_3 = data_fin[data_fin['origin'] == 3]\n",
    "origin_1.head()\n",
    "regression_1 = LinearRegression()\n",
    "regression_2 = LinearRegression()\n",
    "regression_3 = LinearRegression()\n",
    "\n",
    "horse_1 = origin_1['horse'].values.reshape(-1, 1)\n",
    "horse_2 = origin_2['horse'].values.reshape(-1, 1)\n",
    "horse_3 = origin_3['horse'].values.reshape(-1, 1)\n",
    "\n",
    "regression_1.fit(horse_1, origin_1['mpg'])\n",
    "regression_2.fit(horse_2, origin_2['mpg'])\n",
    "regression_3.fit(horse_3, origin_3['mpg'])\n",
    "\n",
    "# Make predictions using the testing set\n",
    "pred_1 = regression_1.predict(horse_1)\n",
    "pred_2 = regression_2.predict(horse_2)\n",
    "pred_3 = regression_3.predict(horse_3)\n",
    "\n",
    "# The coefficients\n",
    "print(regression_1.coef_)\n",
    "print(regression_2.coef_)\n",
    "print(regression_3.coef_)\n",
    "\n",
    "\n",
    "'''look at the plots'''\n",
    "# Plot outputs\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.scatter(horse_1, origin_1['mpg'],  color='blue', alpha=0.3, label = 'origin = 1')\n",
    "plt.scatter(horse_2, origin_2['mpg'],  color='red', alpha=0.3, label = 'origin = 2')\n",
    "plt.scatter(horse_3, origin_3['mpg'],  color='orange', alpha=0.3, label = 'origin = 3')\n",
    "\n",
    "plt.plot(horse_1, pred_1, color='blue', linewidth=2)\n",
    "plt.plot(horse_2, pred_2, color='red', linewidth=2)\n",
    "plt.plot(horse_3, pred_3, color='orange', linewidth=2)\n",
    "plt.ylabel('mpg')\n",
    "plt.xlabel('horsepower')\n",
    "plt.legend();\n",
    "'''looking at these plots, since they are parallel, no need for interaction(multiplicative) effect'''\n",
    "'''so how would we do it, if we needed it?'''\n",
    "regression = LinearRegression()\n",
    "crossvalidation = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "X_interact = X.copy()\n",
    "X_interact['horse_origin'] = X['horse'] * X['origin']\n",
    "\n",
    "interact_horse_origin = np.mean(cross_val_score(regression, X_interact, y, scoring='r2', cv=crossvalidation))\n",
    "interact_horse_origin\n",
    "\n",
    "#by including this interaction, we bump up the R^2 score from .83 to .84, about 1%, good!\n",
    "\n",
    "\n",
    "#can do similar thing with other pairs\n",
    "https://github.com/ericthansen/dsc-interaction-terms\n",
    "    \n",
    "#The lab on this:\n",
    "https://github.com/ericthansen/dsc-interaction-terms-lab\n",
    "#    with some workflow examples.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yield.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-de405503d6f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0myld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'yield.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\s+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0myld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#separating\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yield.csv'"
     ]
    }
   ],
   "source": [
    "#polynomial regression\n",
    "#an example with one predictor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "yld = pd.read_csv('yield.csv', sep='\\s+', index_col=0)\n",
    "yld.head()\n",
    "#separating\n",
    "y = yld['Yield']\n",
    "X = yld.drop(columns='Yield', axis=1)\n",
    "plt.scatter(X, y, color='green')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "#looks like no linear relationship, let's plot just to see\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X, y)\n",
    "plt.scatter(X, y, color='green')\n",
    "plt.plot(X, reg.predict(X))\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "#and get an r^2 value\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mean_squared_error(y, reg.predict(X))\n",
    "r2_score(y, reg.predict(X)) #.08\n",
    "#yep, not great.\n",
    "\n",
    "#polynomials can give a better fit\n",
    "X['Temp_sq'] = X['Temp']**2\n",
    "X.head()\n",
    "reg_q = LinearRegression().fit(X, y)\n",
    "plt.scatter(X['Temp'], y, color='green')\n",
    "plt.plot(X['Temp'], reg_q.predict(X))\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "mean_squared_error(y, reg_q.predict(X)) #.04\n",
    "r2_score(y, reg_q.predict(X)) #.69\n",
    "\n",
    "#note the model generates a smooth curve, not just a piecewise linear thing like in graph\n",
    "import numpy as np\n",
    "plt.scatter(X['Temp'], y, color='green')\n",
    "\n",
    "X_pred = pd.DataFrame(np.linspace(50, 100, 50), columns=['Temp'])\n",
    "X_pred['Temp_sq'] = X_pred**2 \n",
    "y_pred = reg_q.predict(X_pred)\n",
    "\n",
    "plt.plot(X_pred['Temp'], y_pred)\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "\n",
    "#not just limited to quadratics!\n",
    "#How about order 6\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "y = yld['Yield']\n",
    "X = yld.drop(columns='Yield', axis=1)\n",
    "\n",
    "poly = PolynomialFeatures(6)\n",
    "X_fin = poly.fit_transform(X)\n",
    "\n",
    "print('The transformed feature names are: {}'.format(poly.get_feature_names()))\n",
    "print('------------------')\n",
    "print('The first row of transformed data is: {}'.format(X_fin[0]))\n",
    "\n",
    "#now you can fit a linear regression to the various powers columns\n",
    "reg_poly = LinearRegression().fit(X_fin, y)\n",
    "X_linspace = pd.DataFrame(np.linspace(50, 100, 50), columns=['Temp'])\n",
    "\n",
    "X_linspace_fin = poly.fit_transform(X_linspace)\n",
    "y_poly_pred = reg_poly.predict(X_linspace_fin)\n",
    "plt.scatter(X['Temp'], y, color='green')\n",
    "plt.plot(X_linspace, y_poly_pred)\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "\n",
    "mean_squared_error(y, reg_poly.predict(X_fin)) #0.03\n",
    "\n",
    "r2_score(y, reg_poly.predict(X_fin))#.759\n",
    "\n",
    "\n",
    "# and this is great workflow\n",
    "https://flatiron.illumidesk.com/user/ericthansen/notebooks/dsc-polynomial-regression-lab/index.ipynb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bias/variance trade-off\n",
    "https://flatiron.illumidesk.com/user/ericthansen/notebooks/dsc-bias-variance-trade-off/index.ipynb\n",
    "'''    You will be able to:\n",
    "\n",
    "Describe the bias-variance tradeoff in machine learning\n",
    "Discuss how bias and variance are related to over and underfitting\n",
    "List the three components of error'''\n",
    "\n",
    "#one reason for train/test split is to keep honest about overfitting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "yld = pd.read_csv('yield2.csv', sep='\\s+', index_col = 0)\n",
    "\n",
    "print(yld.head())\n",
    "y = yld['Yield']\n",
    "X = yld['Temp']\n",
    "\n",
    "plt.scatter(X, y, color = 'green')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield');\n",
    "#this creates a parabolic dist.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=34567)\n",
    "reg = LinearRegression().fit(X_train.values.reshape(-1, 1), y_train)\n",
    "#simple linreg makes a bad fit for train and test\n",
    "# Plot the simple model\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, color='green')\n",
    "plt.plot(X_train.values.reshape(-1, 1), reg.predict(X_train.values.reshape(-1, 1)))\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='green')\n",
    "plt.plot(X_test.values.reshape(-1, 1), reg.predict(X_test.values.reshape(-1, 1)))\n",
    "plt.xlabel('Temperature');\n",
    "\n",
    "#but if we include polynomials, it gets better.  below, they use degree 6 (probably too much)\n",
    "# 6th degree polynomial\n",
    "poly = PolynomialFeatures(6)\n",
    "X_fin = poly.fit_transform(X_train.values.reshape(-1, 1))\n",
    "reg_poly = LinearRegression().fit(X_fin, y_train)\n",
    "'''Let's formalize this:\n",
    "\n",
    "Underfitting happens when a model cannot learn the training data, nor can it generalize to new data.\n",
    "\n",
    "The simple linear regression model fitted earlier was an underfit model.\n",
    "\n",
    "\n",
    "Overfitting happens when a model learns the training data too well. In fact, so well that it is not generalizeable to new data'''\n",
    "#the linear reg was underfit; the 6degree was overfit\n",
    "\n",
    "# 2nd degree polynomial\n",
    "poly = PolynomialFeatures(2)  \n",
    "X_fin = poly.fit_transform(X_train.values.reshape(-1, 1))\n",
    "reg_poly = LinearRegression().fit(X_fin, y_train)\n",
    "X_linspace_fin = poly.fit_transform(X_linspace)\n",
    "y_poly_pred = reg_poly.predict(X_linspace_fin)\n",
    "# Plot 2nd degree polynomial fit\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, color='green')\n",
    "plt.plot(X_linspace, y_poly_pred)\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Yield')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='green')\n",
    "plt.plot(X_linspace, y_poly_pred);\n",
    "\n",
    "X_fin_test = poly.fit_transform(X_test.values.reshape(-1, 1))\n",
    "y_pred = reg_poly.predict(X_fin_test)\n",
    "mean_squared_error(y_test, y_pred)#.06\n",
    "#training result is worse but test is improved\n",
    "\n",
    "#bias-variance tradeoff\n",
    "'''Another perspective on this problem of overfitting versus underfitting is the bias-variance tradeoff. The idea is that We can decompose the mean squared error as the sum of:\n",
    "\n",
    "bias\n",
    "variance, and\n",
    "irreducible error'''\n",
    "# https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation\n",
    "# https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\n",
    "\n",
    "\n",
    "#lab, which doesn't make a ton of sense...\n",
    "https://github.com/ericthansen/dsc-bias-variance-trade-off-lab\n",
    "    \n",
    "    \n",
    "#and the full complete process lab\n",
    "https://github.com/ericthansen/dsc-linear-regression-lab\n",
    "#    (includes fun correlation heatmap, with just lowerleft triangle and numbers!)\n",
    "sns.heatmap(df.corr() #just the basic version\n",
    "            #if that's not so helpful..\n",
    "df.corr()['metric'].map(abs).sort_values(ascending=False)\n",
    "\n",
    "    \n",
    "#One handy little trick to get only numeric columns...\n",
    "X_train.select_dtypes(['int64', 'float64'])\n",
    "#When using statsmodels, summary shows a \"cond. no.\":\n",
    "#A condition number of 10-30 indicates multicollinearity, and a condition number above 30 indicates strong multicollinearity. This print-out shows a condition number of 2.77e+03, i.e. 2770, which is well above 30.\n",
    "\n",
    "\n",
    "'''Selecting Features with sklearn.feature_selection\n",
    "Let's try a different approach. Scikit-learn has a submodule called feature_selection that includes tools to help reduce the feature set.\n",
    "\n",
    "We'll use RFECV (documentation here). \"RFE\" stands for \"recursive feature elimination\", meaning that it repeatedly scores the model, finds and removes the feature with the lowest \"importance\", then scores the model again. If the new score is better than the previous score, it continues removing features until the minimum is reached. \"CV\" stands for \"cross validation\" here, and we can use the same splitter we have been using to test our data so far.'''\n",
    "\n",
    "'''Investigating Multicollinearity (Independence Assumption)\n",
    "Another way to measure multicollinearity is with variance inflation factor (StatsModels documentation here). A \"rule of thumb\" for VIF is that 5 is too high (i.e. strong multicollinearity).\n",
    "\n",
    "Run the code below to find the VIF for each feature.'''\n",
    "# Run this cell without changes\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = [variance_inflation_factor(X_train_final.values, i) for i in range(X_train_final.shape[1])]\n",
    "pd.Series(vif, index=X_train_final.columns, name=\"Variance Inflation Factor\")\n",
    "\n",
    "\n",
    "#A video on model validation, includes \n",
    "#balancing bias and variance, Train-Test split, overfitting, underfitting, k-fold cross validation,\n",
    "https://github.com/ericthansen/ds-model_validation-nbz32\n",
    "    \n",
    "#Feature selection and engineering\n",
    "''' Topics covered in this lecture include: model selection, correlation and multicollinearity, Recursive Feature Elimination, products of features, polynomial features, Variance Inflation Factors, model condition number, exploratory data analysis'''\n",
    "https://github.com/ericthansen/ds-feature_selection_and_feature_engineering-nbz32\n",
    "'''underscored attributes of models dont exist until you've fit the model'''\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''appendix stuff on uniform distribution, poisson dist, '''\n",
    "https://github.com/ericthansen/dsc-uniform-distribution\n",
    "https://github.com/ericthansen/dsc-poisson-distribution\n",
    "import numpy as np\n",
    "from math import factorial\n",
    "def poisson_probability(lambd, x):\n",
    "return lambd**x * np.exp(1)**(-lambd)/factorial(x)\n",
    "\n",
    "https://github.com/ericthansen/dsc-exponential-distribution\n",
    "import numpy as np\n",
    "\n",
    "def exp_pdf(mu, x):\n",
    "    decay_rate = 1 / mu\n",
    "    return decay_rate * np.exp(-decay_rate * x)\n",
    "    \n",
    "def exp_cdf(mu, x):\n",
    "    decay_rate = 1 / mu\n",
    "    return 1 - np.exp(-decay_rate * x)\n",
    "\n",
    "#Monte Carlo Simulations\n",
    "https://github.com/ericthansen/dsc-monte-carlo-simulations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-obtaining-your-data-lab\n",
    "#pre-lab EDL:\n",
    "#some more sql refreshers/tricks\n",
    "# Your code here\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "####THIS is very useful to see all the tables in a db###\n",
    "# Create a connection\n",
    "con = sqlite3.connect('lego.db')\n",
    "# Create a cursor\n",
    "cur = con.cursor()\n",
    "# Select some data\n",
    "cur.execute(\"\"\"SELECT name FROM sqlite_master\n",
    "            WHERE type='table'\n",
    "            ORDER BY name;\n",
    "            \"\"\")\n",
    "df = pd.DataFrame(cur.fetchall())\n",
    "df.columns = [i[0] for i in cur.description]\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"SELECT * \n",
    "                FROM product_details\n",
    "                JOIN product_info \n",
    "                USING(prod_id)\n",
    "                JOIN product_pricing\n",
    "                USING(prod_id)\n",
    "                JOIN product_reviews\n",
    "                USING(prod_id);\n",
    "            \"\"\")\n",
    "df4 = pd.DataFrame(cur.fetchall())\n",
    "df4.columns = [i[0] for i in cur.description]\n",
    "print(df.shape)\n",
    "df4.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-965048325fad>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-965048325fad>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    https://github.com/ericthansen/dsc-scrubbing-and-cleaning-data\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#scrubbing and cleaning data review\n",
    "https://github.com/ericthansen/dsc-scrubbing-and-cleaning-data\n",
    "'''You will be able to:\n",
    "\n",
    "Cast columns to appropriate data types\n",
    "Identify and deal with null values appropriately\n",
    "Remove unnecessary columns'''\n",
    "\n",
    "\n",
    "'''A first step to uncover and investigate such issues is to use the .info() method available for all Pandas DataFrames. This will tell what type of data each column contains, as well as the number of values contained within that column (which can also help us identify columns that contain missing data)! Here's an example response:'''\n",
    "'''From here, a good next step would be to look at examples from each column encoded as strings (remember, Pandas refers to string columns as object) and confirm that this data is supposed to be encoded as strings. One method to do this is to preview a truncated version of the output from .value_counts(). For example, you could preview the 5 most frequent entries from each column with a simple loop like this:'''\n",
    "\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        print(col, df[col].value_counts()[:5])\n",
    "    except:\n",
    "        print(col, df[col].value_counts())\n",
    "        # If there aren't 5+ unique values for a column the first print statement\n",
    "        # will throw an error for an invalid idx slice\n",
    "    print('\\n') # Break up the output between columns\n",
    "    \n",
    "'''If you've identified numeric data encoded as strings, it's typically a pretty easy problem to solve. Often it's as simple as casting the string data to a numeric type:\n",
    "'''\n",
    "df['numeric_string_col'] = df['numeric_string_col'].astype('float')\n",
    "'''Sadly, it's not always that simple. For example, if there is even a single cell that contains a letter or non-numeric character such as a comma or monetary symbol ($) the above statement will fail. In such cases, a more complex cleaning function must be manually created. This could involve stripping extraneous symbols such as ',$/%', or simply casting non convertible strings as null. Recall that when NumPy sees multiple data types in an array, it defaults to casting everything as a string. If you try to cast a column from string to numeric data types and get an error, consider checking the unique values in that column -- it's likely that you may have a single letter hiding out somewhere that needs to be removed!'''\n",
    "\n",
    "\n",
    "##scrubbing and cleaning lab\n",
    "https://github.com/ericthansen/dsc-scrubbing-and-cleaning-data-lab\n",
    "    #goes through the main workflow, cleaning, etc, for numeric and categoricals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring your data\n",
    "#hist plots are cool\n",
    "#joint plots are also quite cool\n",
    "sns.jointplot(x= <column>, y= <column>, data=<dataset>, kind='reg')\n",
    "# https://seaborn.pydata.org/generated/seaborn.jointplot.html\n",
    "\n",
    "#exploring your data lab\n",
    "# https://github.com/ericthansen/dsc-exploring-your-data-lab\n",
    "\n",
    "#select just numeric columns\n",
    "result = df.select_dtypes(include='number')p\n",
    "\n",
    "#used joinplot\n",
    "sns.jointplot(x= 'piece_count', y= 'list_price', data=df, kind='reg')\n",
    "\n",
    "#did corr matrix\n",
    "df[numeric].corr()\n",
    "#used heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create a df with the target as the first column,\n",
    "# then compute the correlation matrix\n",
    "##heatmap_data = pd.concat([y_train, X_train], axis=1)\n",
    "##corr = heatmap_data.corr()\n",
    "corr = df[numeric].corr()\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, ax = plt.subplots(figsize=(5, 8))\n",
    "\n",
    "# Plot a heatmap of the correlation matrix, with both\n",
    "# numbers and colors indicating the correlations\n",
    "sns.heatmap(\n",
    "    # Specifies the data to be plotted\n",
    "    data=corr,\n",
    "    # The mask means we only show half the values,\n",
    "    # instead of showing duplicates. It's optional.\n",
    "    mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "    # Specifies that we should use the existing axes\n",
    "    ax=ax,\n",
    "    # Specifies that we want labels, not just colors\n",
    "    annot=True,\n",
    "    # Customizes colorbar appearance\n",
    "    cbar_kws={\"label\": \"Correlation\", \"orientation\": \"horizontal\", \"pad\": .2, \"extend\": \"both\"}\n",
    ")\n",
    "\n",
    "# Customize the plot appearance\n",
    "ax.set_title(\"Heatmap of Correlation Between Attributes (Including Target)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling your data\n",
    "# https://github.com/ericthansen/dsc-modeling-your-data\n",
    "\n",
    "### *** come back here ***9/6/21\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "df = pd.read_excel('mpg excercise.xls')\n",
    "df.head()\n",
    "# Define the problem\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Length', 'Wheelbase', 'Width', 'U_Turn_Space',\n",
    "          'Rear_seat', 'Luggage', 'Weight', 'Horsepower', 'Fueltank']\n",
    "# Some brief preprocessing\n",
    "df.columns = [col.replace(' ', '_') for col in df.columns]\n",
    "for col in x_cols:\n",
    "    df[col] = (df[col] - df[col].mean())/df[col].std()\n",
    "df.head()\n",
    "from statsmodels.formula.api import ols\n",
    "# Fitting the actual model\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=df).fit()\n",
    "model.summary()\n",
    "'''Assessing the Model\n",
    "When performing an initial assessment of the model you might focus on a number of different perspectives. There are metrics assessing the overall accuracy of the model including  𝑟2  and mean square error. There are also many metrics when analyzing how various features contribute to the overall model. These are essential to building a story and intuition behind the model so that educated business strategies can be implemented to optimize the target variable. After all, typically you aren't solely interested in predicting a quantity in a black box given said information. Rather, you would often like to know the underlying influencers and how those can be adjusted in order to increase or decrease the final measured quantity whether it be sales, customer base, costs, or risk. Such metrics would include p-values associated with the various features, comparing models with features removed and investigating potential multicollinearity in the model. Multicollinearity also touches upon checking model assumptions. One underlying intuition motivating the regression model is that the features constitute a set of levers which, if appropriately adjusted, account for the target variable. The theory then goes that the errors should be simply the cause of noise in our measurements, or smaller unaccounted factors. These errors are then assumed to be normally distributed.\n",
    "\n",
    "Comments on P-Values\n",
    "Based on the p-values above, you can see that there are a number of extraneous features. Recall that a common significance cutoff is 0.05. The refined model should eliminate these irrelevant features.'''\n",
    "#Initial Refinement\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Wheelbase', 'Weight', 'Fueltank']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=df).fit()\n",
    "model.summary()\n",
    "#checking for multicollinearity\n",
    "'''While you've examined the bi-variable relations previously by examining pair-wise correlation between features, you haven't fully accounted for multicollinearity which is a relation of 3 or more variables. One test for this is the variance inflation factor. Typically, variables with a vif of 5 or greater (or more definitively 10 or greater) are displaying multicollinearity with other variables in the feature set. We we'll check this here:'''\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X = df[x_cols]\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "list(zip(x_cols, vif)) #variables with vif of 5 or greater display multicollinearity\n",
    "#\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Wheelbase', 'Fueltank']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=df).fit()\n",
    "model.summary()\n",
    "#\n",
    "X = df[x_cols]\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "list(zip(x_cols, vif))\n",
    "'''results:\n",
    "[('Passengers', 1.972330344357365),\n",
    " ('Wheelbase', 5.743165022553869),\n",
    " ('Weight', 9.016035842933373),\n",
    " ('Fueltank', 5.032060527995974)]'''\n",
    "'''Comment: While the p-values indicate that all of the current features are impactful, the variance inflation factor indicates that there is moderate multicollinearity between our variables. With that, it makes sense to briefly update the features once again and recheck for multicollinearity.'''\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Wheelbase', 'Fueltank']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=df).fit()\n",
    "model.summary()\n",
    "#we removed the category with highest VIF (close to 10)\n",
    "X = df[x_cols]\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "list(zip(x_cols, vif))\n",
    "'''[('Passengers', 1.955034462110378),\n",
    " ('Wheelbase', 3.567043045106437),\n",
    " ('Fueltank', 2.378966703427496)]'''\n",
    "'''doing so, we reduce the Rsquared of the model, but reduce multicollinearity'''\n",
    "'''for now, we will return to the original model'''\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Wheelbase', 'Weight', 'Fueltank']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=df).fit()\n",
    "model.summary()\n",
    "####Checking for normality\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)\n",
    "'''looks pretty good but maybe variation at the tails'''\n",
    "\n",
    "#Check for homoscedasticity (and avoid heteroscedasticity)\n",
    "plt.scatter(model.predict(df[x_cols]), model.resid)\n",
    "plt.plot(model.predict(df[x_cols]), [0 for i in range(len(df))])\n",
    "'''some outliers have concerning variation; may want to identify and handle those outliers?'''\n",
    "\n",
    "#model refinement 3\n",
    "'''Due to the particularly large errors visible above ~37MPG, it's reasonable to remove these outliers and retrain the model on the remaining subset. While the model will be specific to this subset, it could prove to be more accurate and reflective of the general domain.'''\n",
    "#Finding a cutoff point\n",
    "for i in range(90, 99):\n",
    "    q = i / 100\n",
    "    print('{} percentile: {}'.format(q, df['MPG_Highway'].quantile(q=q)))\n",
    "\n",
    "#remove the outliers!  This is worth returning to\n",
    "subset = df[df['MPG_Highway'] < 38]\n",
    "print('Percent removed:',(len(df) - len(subset))/len(df))\n",
    "outcome = 'MPG_Highway'\n",
    "x_cols = ['Passengers', 'Wheelbase', 'Weight', 'Fueltank']\n",
    "predictors = '+'.join(x_cols)\n",
    "formula = outcome + '~' + predictors\n",
    "model = ols(formula=formula, data=subset).fit()\n",
    "model.summary()\n",
    "#rechecking normality\n",
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)\n",
    "#rechecking homosced\n",
    "plt.scatter(model.predict(subset[x_cols]), model.resid)\n",
    "plt.plot(model.predict(subset[x_cols]), [0 for i in range(len(subset))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blogging\n",
    "https://github.com/ericthansen/dsc-blogging-overview\n",
    "#project 2 submission\n",
    "https://github.com/ericthansen/dsc-project-submissions-online\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for process functionization:\n",
    "df.date =  pd.to_datetime(df['date'])\n",
    "df.loc[df.yr_renovated==0,'yr_renovated'] = df.yr_built\n",
    "target = 'price'\n",
    "X = df.drop(target, axis = 1)\n",
    "y = df[target]\n",
    "SPLIT_IS_RANDOM = True\n",
    "if SPLIT_IS_RANDOM:\n",
    "    random_state = randint(1,2**32 - 2)\n",
    "    #print(random_state)\n",
    "else:\n",
    "    random_state = 14\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = random_state)\n",
    "#do missing col fixin here#\n",
    "#X_train['yr_renovated']\n",
    "X_train[X_train['yr_renovated']==0][col]  = X_train[X_train['yr_renovated']==0]['yr_built']\n",
    "\n",
    "dropped_cols = ['id',]\n",
    "num_cols = ['date',  'sqft_living', 'sqft_lot',\n",
    "        'condition', 'grade', 'sqft_above',\n",
    "       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n",
    "       'sqft_living15', 'sqft_lot15',\n",
    "            'bedrooms', 'bathrooms',\n",
    "       ]\n",
    "#originally put bedrooms/bathrooms as categoricals...but moved back up to numeric\n",
    "cat_cols = ['floors', 'waterfront', 'view', 'waterfront_Missing', 'view_Missing','yr_renovated_Missing']\n",
    "for col in cat_cols:\n",
    "    col_train = X_train[[col]]\n",
    "    ohe = OneHotEncoder(categories='auto', sparse=False, drop='first', handle_unknown='error')\n",
    "    ohe.fit(col_train)\n",
    "    if verbose:\n",
    "        display(col, ohe.categories_)\n",
    "    # (4) Transform fireplace_qu_train using the encoder and\n",
    "    # assign the result to fireplace_qu_encoded_train\n",
    "    col_train_encoded = ohe.transform(col_train)\n",
    "    \n",
    "    if verbose:\n",
    "        pass\n",
    "        #display('featurenames:', ohe.get_feature_names([col]))\n",
    "    \n",
    "\n",
    "    # Visually inspect fireplace_qu_encoded_train\n",
    "    #fireplace_qu_encoded_train\n",
    "    \n",
    "    col_train_encoded = pd.DataFrame(\n",
    "        # Pass in NumPy array\n",
    "        col_train_encoded,\n",
    "        # Set the column names to the categories found by OHE\n",
    "        #columns=ohe.categories_[0][1:], #old and busted; instead use this cool thing:\n",
    "        columns = ohe.get_feature_names([col]),\n",
    "        # Set the index to match X_train's index\n",
    "        index=X_train.index\n",
    "    )\n",
    "    # (5b) Drop original FireplaceQu column\n",
    "    X_train.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # (5c) Concatenate the new dataframe with current X_train\n",
    "    X_train = pd.concat([X_train, col_train_encoded], axis=1)\n",
    "X_train = X_train.drop('date', axis = 1)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "cross_val_score(model, X_train, y_train, cv=10)\n",
    "\n",
    "\n",
    "#######################\n",
    "#Preprocess Test data\n",
    "#waterfront, view, yr_renovated\n",
    "\n",
    "# Add missing indicator for lot frontage\n",
    "\n",
    "\n",
    "\n",
    "wf_test = X_test[[\"waterfront\"]]\n",
    "wf_missing_test = missing_indicator1.transform(wf_test)\n",
    "X_test[\"waterfront_Missing\"] = wf_missing_test\n",
    "# Impute missing lot frontage values\n",
    "wf_imputed_test = imputer1.transform(wf_test)\n",
    "X_test[\"waterfront\"] = wf_imputed_test\n",
    "\n",
    "view_test = X_test[[\"view\"]]\n",
    "view_missing_test = missing_indicator2.transform(view_test)\n",
    "X_test[\"view_Missing\"] = view_missing_test\n",
    "# Impute missing lot frontage values\n",
    "view_imputed_test = imputer2.transform(view_test)\n",
    "X_test[\"view\"] = view_imputed_test\n",
    "\n",
    "yr_test = X_test[[\"yr_renovated\"]]\n",
    "yr_missing_test = missing_indicator3.transform(yr_test)\n",
    "X_test[\"yr_renovated_Missing\"] = yr_missing_test\n",
    "# Impute missing lot frontage values\n",
    "yr_imputed_test = imputer3.transform(yr_test)\n",
    "X_test[\"yr_renovated\"] = yr_imputed_test\n",
    "\n",
    "# Check that there are no more missing values\n",
    "X_test.isna().sum()\n",
    "\n",
    "X_test[X_test['yr_renovated']==0][col]  = X_test[X_test['yr_renovated']==0]['yr_built']\n",
    "\n",
    "for col in cat_cols:\n",
    "    col_train = X_test[[col]]\n",
    "    ohe = OneHotEncoder(categories='auto', sparse=False, drop='first', handle_unknown='error')\n",
    "    ohe.fit(col_train)\n",
    "    if verbose:\n",
    "        display(col, ohe.categories_)\n",
    "    # (4) Transform fireplace_qu_train using the encoder and\n",
    "    # assign the result to fireplace_qu_encoded_train\n",
    "    col_train_encoded = ohe.transform(col_train)\n",
    "    \n",
    "    if verbose:\n",
    "        pass\n",
    "        #display('featurenames:', ohe.get_feature_names([col]))\n",
    "    \n",
    "\n",
    "    # Visually inspect fireplace_qu_encoded_train\n",
    "    #fireplace_qu_encoded_train\n",
    "    \n",
    "    col_train_encoded = pd.DataFrame(\n",
    "        # Pass in NumPy array\n",
    "        col_train_encoded,\n",
    "        # Set the column names to the categories found by OHE\n",
    "        #columns=ohe.categories_[0][1:], #old and busted; instead use this cool thing:\n",
    "        columns = ohe.get_feature_names([col]),\n",
    "        # Set the index to match X_train's index\n",
    "        index=X_test.index\n",
    "    )\n",
    "    # (5b) Drop original FireplaceQu column\n",
    "    X_test.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # (5c) Concatenate the new dataframe with current X_train\n",
    "    X_test = pd.concat([X_test, col_train_encoded], axis=1)\n",
    "\n",
    "#clean date\n",
    "X_test = X_test.drop('date', axis = 1)\n",
    "\n",
    "\n",
    "######################## fit and score\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functionize this too:\n",
    "col = 'waterfront'\n",
    "\n",
    "# (1) Identify data to be transformed\n",
    "# We only want missing indicators for column\n",
    "col1_train = X_train[[col]]\n",
    "\n",
    "# (2) Instantiate the transformer object\n",
    "missing_indicator1 = MissingIndicator()\n",
    "\n",
    "# (3) Fit the transformer object on col\n",
    "missing_indicator1.fit(col1_train)\n",
    "\n",
    "# (4) Transform col and assign the result\n",
    "# to col_missing_train\n",
    "col1_missing_train = missing_indicator1.transform(col1_train)\n",
    "\n",
    "# Visually inspect col_missing_train\n",
    "#col1_missing_train\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "# col_missing_train should be a NumPy array\n",
    "assert type(col1_missing_train) == np.ndarray\n",
    "\n",
    "# We should have the same number of rows as the full X_train\n",
    "assert col1_missing_train.shape[0] == X_train.shape[0]\n",
    "\n",
    "# But we should only have 1 column\n",
    "assert col1_missing_train.shape[1] == 1\n",
    "\n",
    "X_train[\"waterfront_Missing\"] = col1_missing_train\n",
    "#X_train\n",
    "\n",
    "\n",
    "# (1) col1_train was created previously, so we don't\n",
    "# need to extract the relevant data again\n",
    "\n",
    "# (2) Instantiate a SimpleImputer with strategy=\"median\"\n",
    "imputer1 = SimpleImputer(strategy='median')\n",
    "\n",
    "# (3) Fit the imputer on col_train\n",
    "imputer1.fit(col1_train)\n",
    "\n",
    "# (4) Transform frontage_train using the imputer and\n",
    "# assign the result to col_imputed_train\n",
    "col1_imputed_train = imputer1.transform(col1_train)\n",
    "\n",
    "# Visually inspect col_imputed_train\n",
    "display(col1_imputed_train)\n",
    "\n",
    "# Run this cell without changes\n",
    "\n",
    "# (5) Replace value of col\n",
    "X_train[\"waterfront\"] = col1_imputed_train\n",
    "\n",
    "# Visually inspect X_train\n",
    "# display(X_train)\n",
    "\n",
    "display(X_train.waterfront.value_counts())\n",
    "display(X_train.waterfront.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snazzy project tricks and visualizations\n",
    "# Price distribution\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.distplot(df['price'], bins=50, hist_kws=dict(edgecolor=\"blue\", linewidth=1))\n",
    "\n",
    "plt.ticklabel_format(style='plain')\n",
    "\n",
    "\n",
    "#scatter plot\n",
    "#A quick look at sqft vs price\n",
    "plt.figure(figsize=(9, 5))\n",
    "sns.scatterplot(x='sqft_living', y='price', data=df).set_title('Square Feet vs. Price')\n",
    "plt.ticklabel_format(style='plain')#style='plain', 'sci', 'scientific'\n",
    "\n",
    "#2 useful plots\n",
    "#Investigating bedrooms vs price\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].set_title('Bedrooms Count')\n",
    "sns.countplot(df['bedrooms'], ax=axes[0], color='purple')\n",
    "\n",
    "axes[1].set_title('Bedrooms vs. Price')\n",
    "sns.boxplot(x='bedrooms', y='price', data=df, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#latitude/longitude heatmap!\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "# Create basic Folium heat map\n",
    "price_map = folium.Map(location=[47.5112,-122.257], \n",
    "                       tiles = \"Stamen Terrain\",\n",
    "                      zoom_start = 9)\n",
    "\n",
    "# Add data for heatmp \n",
    "data_heatmap = df[['lat','long','price']]\n",
    "data_heatmap = df.dropna(axis=0, subset=['lat','long','price'])\n",
    "data_heatmap = [[row['lat'],row['long']] for index, row in data_heatmap.iterrows()]\n",
    "HeatMap(data_heatmap, radius=10, \n",
    "        gradient = {.35: 'purple',.55: 'blue',.68:'yello',.78:'red'}).add_to(price_map)\n",
    "# Plot!\n",
    "price_map\n",
    "\n",
    "#notes from intro-chat with Jeff Herman\n",
    "\n",
    "'''non-tech presentation and insights:\n",
    "    think about who audience is - buyers/sellers/flippers/realtors, etch?  to guide insights on EDA\n",
    "    are there months when avg house price is lower, months or weekend/weekdays?  \n",
    "    \n",
    "    for bedrooms - it's numeric, but not continuous - can use  features - only one-hot it if it doesn't seem to follow a line or have a numerical underlying meaning\n",
    "    \n",
    "    residutals should be nornal, hetroscedas, no-multicollinear\n",
    "    \n",
    "    \n",
    "    consider cutting off high price housers \n",
    "    \n",
    "    blog ideas around linear regression model - assumptions of lienar regression, how you test for them \n",
    "    scedactitycy, \n",
    "    multicollin, \n",
    "    how we validated model, train/test/cross validation, mse, \n",
    "    summary table in stats models. \n",
    "    could do recursive feature elim, but better to do through EDA \n",
    "    \n",
    "    at least 3 different models\n",
    "    \n",
    "    many get an r^2 between .55 and .75'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### SECTION 3!!!! ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-introduction-to-object-orientation\n",
    "#oop - python and ruby are oop.  haskell and clojure are \"functional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Note: By convention, you should use CamelCase to name the class. Also, you can\\'t create an \"empty\" class. At the least, you need to specify the pass keyword to ensure the class definition is syntactically valid.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/ericthansen/dsc-classes-and-instances\n",
    "# https://github.com/ericthansen/dsc-classes-and-instances-lab\n",
    "# from driver import Driver\n",
    "'''Note: By convention, you should use CamelCase to name the class. Also, you can't create an \"empty\" class. At the least, you need to specify the pass keyword to ensure the class definition is syntactically valid.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-instance-methods\n",
    "# https://github.com/ericthansen/dsc-instance-methods-lab\n",
    "# https://github.com/ericthansen/dsc-instance-variables-lab\n",
    "# https://github.com/ericthansen/dsc-oop-recap-v2-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-423b95e3bc61>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-423b95e3bc61>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/ericthansen/dsc-linalg-introduction\n",
    "# https://github.com/ericthansen/dsc-lingalg-motivation - this has good additional linalg links\n",
    "# https://github.com/ericthansen/dsc-lingalg-linear-equations\n",
    "# https://github.com/ericthansen/dsc-lingalg-linear-equations-quiz\n",
    "# https://github.com/ericthansen/dsc-scalars-vectors-matrices-tensors-codealong\n",
    "    # for regular numpy matrix:\n",
    "    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "    print(X)\n",
    "    # for matlab code: \n",
    "    Y = np.mat([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "    print(Y) \n",
    "    '''Numpy matrices are always 2-dimensional, while numpy arrays (also referred to as ndarrays) are N-dimensional. Matrix objects are a subclass of ndarray, so they inherit all the attributes and methods of ndarrays. For multidimensional arrays/matrices with more than 2 dimensions, it is always best to use arrays. Arrays are the standard vector/matrix/tensor type of NumPy, and most NumPy functions return arrays and not matrices.'''\n",
    "    print (X[0, 0]) # element at first row and first column\n",
    "    print (X[-1, -1]) # element from the last row and last column \n",
    "    print (X[0, :]) # first row and all columns\n",
    "    print (X[:, 0]) # all rows and first column \n",
    "    print (X[:]) # all rows and all columns\n",
    "    A_transposed = A.T\n",
    "    A_transposed_2 = np.transpose(A)\n",
    "    print(x.shape)\n",
    "\n",
    "# https://github.com/ericthansen/dsc-linalg-mat-multiplication-codealong\n",
    "    #hadamard product : elementwise product. uses a hollow circle dot\n",
    "    import numpy as np\n",
    "    A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    print(A)\n",
    "    B = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    print(B)\n",
    "    print ('\\nHadamard product\\n\\n', A * B)\n",
    "    \n",
    "    #dot product : dot prod.  really just regular row-col multiplication\n",
    "    A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "    B = np.array([[2, 7], [1, 2], [3, 6]])\n",
    "    C = A.dot(B)\n",
    "    print(A, '\\ndot', '\\n', B, '\\n = \\n', C)     \n",
    "    \n",
    "    # matrix-vector mult\n",
    "    A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "    v = np.array([0.5, 0.5])\n",
    "    C = A.dot(v)\n",
    "    \n",
    "    # Cross product between two vectors\n",
    "    x = np.array([0, 0, 1])\n",
    "    y = np.array([0, 1, 0])\n",
    "\n",
    "    print(np.cross(x, y))\n",
    "    print(np.cross(y, x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-lineq-numpy-codealong\n",
    "#identiy matrix yay\n",
    "import numpy as np\n",
    "A = np.array([[2,1],[3,4]])\n",
    "I = np.array([[1,0],[0,1]])\n",
    "print(I.dot(A))\n",
    "print('\\n', A.dot(I))\n",
    "#inverse matrix\n",
    "A = np.array([[4, 2, 1],[4, 8, 3],[1, 1, 0]])\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(A_inv) \n",
    "#you can solve a system of linear equations with inverse - ax = b, mult both sides by a^-1\n",
    "#But there are lots of reasons to NOT calculate the inverse\n",
    "#better to \"solve directly\"; eg. use numpy builtin\n",
    "# Use Numpy's built in function solve() to solve linear equations\n",
    "x = np.linalg.solve(A, B)\n",
    "# https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/\n",
    "\n",
    "\n",
    "#https://github.com/ericthansen/dsc-lineq-numpy-lab\n",
    "'''Exercise 3\n",
    "You want to make a soup containing tomatoes, carrots, and onions.\n",
    "\n",
    "Suppose you don't know the exact mix to put in, but you know there are 7 individual pieces of vegetables, and there are twice as many tomatoes as onions, and that the 7 pieces of vegetables cost 5.25 USD in total. You also know that onions cost 0.5 USD each, tomatoes cost 0.75 USD and carrots cost 1.25 USD each.\n",
    "\n",
    "Create a system of equations to find out exactly how many of each of the vegetables are in your soup.'''\n",
    "# Create and solve the relevant system of equations\n",
    "v = np.array([[.5, .75, 1.25],[1, 1, 1],[2,-1,0]])\n",
    "total = np.array([5.25,7,0])\n",
    "np.linalg.solve(v, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-linalg-regression-codealong\n",
    "# http://math.mit.edu/~gs/linearalgebra/ila0403.pdf\n",
    "#   https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.polyfit.html\n",
    "# Code here \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([1,2,2])\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()\n",
    "#for lin regression with polyfit:\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "# Fit with polyfit function to get c(intercept) and m(slope)\n",
    "# the degree parameter = 1 to models this as a straight line\n",
    "c, m = polyfit(x, y, 1)\n",
    "\n",
    "# Plot the data points and line calculated from ployfit\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, c + (m * x), '-')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()\n",
    "print(c, m)\n",
    "##\n",
    "X = np.array([[1, 1],[1, 2],[1, 3]])\n",
    "y = np.array([1, 2, 2])\n",
    "Xt = X.T\n",
    "XtX = Xt.dot(X)\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "Xty = Xt.dot(y)\n",
    "x_hat = XtX_inv.dot(Xty) # the value for b shown above\n",
    "x_hat\n",
    "\n",
    "# Define data points\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([1, 2, 2])\n",
    "\n",
    "# Plot the data points and line parameters calculated above\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, x_hat[0] + (x_hat[1] * x), '-')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##### https://github.com/ericthansen/dsc-linalg-regression-lab\n",
    "\n",
    "beta = np.linalg.inv((X_train.T).dot(X_train)).dot(X_train.T).dot(y_train)\n",
    "y_pred = []\n",
    "for row in X_test:\n",
    "    pred = row.dot(beta)\n",
    "    y_pred.append(pred)\n",
    "y_pred[:5]\n",
    "\n",
    "# Plot predicted and actual values as line plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.scatter(y_train, y_pred)\n",
    "# Calculate RMSE\n",
    "mse = 0\n",
    "for i in range(len(y_pred)):\n",
    "    mse += (y_pred[i] - y_train[i])**2\n",
    "mse /= len(y_pred)\n",
    "rmse = mse ** (1/2)\n",
    "rmse\n",
    "\n",
    "# Calculate NRMSE\n",
    "nrmse = rmse / (max(y_train) - min(y_train))\n",
    "nrmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-computational-complexity\n",
    "    # https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\n",
    "    # https://medium.com/karuna-sehgal/a-simplified-explanation-of-the-big-o-notation-82523585e835\n",
    "# https://github.com/ericthansen/dsc-linalg-section-recap\n",
    "\n",
    "# https://github.com/ericthansen/dsc-calculus-introduction\n",
    "# https://github.com/ericthansen/dsc-derivatives-intro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-derivatives-intro-lab\n",
    "import numpy as np\n",
    "#this seems to be a way to find derivatives of polynomial functions\n",
    "array_1 = np.array([[4, 2], [4, 1], [-10, 0]])\n",
    "np.shape(array_1)\n",
    "\n",
    "def term_output(array, input_value):\n",
    "    return array[0] * (input_value)**array[1]\n",
    "term_output(np.array([3, 2]), 2) # 12\n",
    "#\n",
    "def output_at(array_of_terms, x_value):\n",
    "    s = 0\n",
    "    for t in array_of_terms:\n",
    "        s += term_output(t, x_value)\n",
    "    return s\n",
    "\n",
    "array_3 = np.array([[3, 2], [-11, 0]])\n",
    "output_at(array_3, 2)\n",
    "# 1 \n",
    "\n",
    "##\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "x_values = np.linspace(-30, 30, 100)\n",
    "y_values = list(map(lambda x: output_at(array_3, x), x_values))\n",
    "\n",
    "plt.plot(x_values, y_values, label = \"3x^2 - 11\")\n",
    "\n",
    "ax.legend(loc=\"upper center\",fontsize='large')\n",
    "plt.show()\n",
    "\n",
    "####  For a linear function\n",
    "lin_function = np.array([[4, 1], [15, 0]])\n",
    "#graphing\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "x_values = np.linspace(0, 5, 100)\n",
    "y_values = list(map(lambda x: output_at(lin_function, x), x_values))\n",
    "plt.plot(x_values, y_values, label = \"4x + 15\")\n",
    "ax.legend(loc=\"upper center\",fontsize='large')\n",
    "plt.show()\n",
    "\n",
    "### now to do derivatives (this is a little janky, with delta_x not truly a limit)\n",
    "def delta_f(array_of_terms, x_value, delta_x):\n",
    "    return output_at(array_of_terms, x_value + delta_x) - output_at(array_of_terms, x_value)\n",
    "delta_f(lin_function, 2, 1) # 4\n",
    "# plotting\n",
    "x_value = 2\n",
    "delta_x = 1\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "x_values = np.linspace(0, 5, 100)\n",
    "y_values = list(map(lambda x: output_at(lin_function, x), x_values))\n",
    "\n",
    "plt.plot(x_values, y_values, label = \"4x + 15\")\n",
    "\n",
    "# delta x\n",
    "y_val = output_at(lin_function, x_value)\n",
    "hline_lab= 'delta x = ' + str(delta_x)\n",
    "plt.hlines(y=y_val, xmin= x_value, xmax= x_value + delta_x, color=\"lightgreen\", label = hline_lab)\n",
    "\n",
    "# delta f\n",
    "y_val_max = output_at(lin_function, x_value + delta_x)\n",
    "vline_lab =  'delta f = ' + str(y_val_max-y_val)\n",
    "plt.vlines(x = x_value + delta_x , ymin= y_val, ymax=y_val_max, color=\"darkorange\", label = vline_lab)\n",
    "ax.legend(loc='upper left', fontsize='large')\n",
    "\n",
    "plt.show()\n",
    "#deriv funct\n",
    "def derivative_of(array_of_terms, x_value, delta_x):\n",
    "    del_f = delta_f(array_of_terms, x_value, delta_x)\n",
    "    return del_f/delta_x\n",
    "derivative_of(lin_function, x_value, delta_x)\n",
    "# 4.0\n",
    "\n",
    "#now the tan line\n",
    "def tangent_line(array_of_terms, x_value, line_length = 4, delta_x = .01):\n",
    "    y = output_at(array_of_terms, x_value)\n",
    "    derivative_at = derivative_of(array_of_terms, x_value, delta_x)\n",
    "    \n",
    "    x_dev = np.linspace(x_value - line_length/2, x_value + line_length/2, 50)\n",
    "    tan = y + derivative_at *(x_dev - x_value)\n",
    "    return {'x_dev':x_dev, 'tan':tan, 'lab': \" f' (x) = \" + str(derivative_at)}\n",
    "tan_line = tangent_line(lin_function, 2, line_length = 2, delta_x = .1)\n",
    "tan_line\n",
    "#and plot it\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "x_values = np.linspace(0, 5, 100)\n",
    "y_values = list(map(lambda x: output_at(lin_function, x), x_values))\n",
    "\n",
    "plt.plot(x_values, y_values, label = \"4x + 15\")\n",
    "# tangent_line\n",
    "plt.plot(tan_line['x_dev'], tan_line['tan'], color = \"yellow\", label = tan_line['lab'])\n",
    "\n",
    "# delta x\n",
    "y_val = output_at(lin_function, x_value)\n",
    "hline_lab= 'delta x = ' + str(delta_x)\n",
    "plt.hlines(y=y_val, xmin= x_value, xmax= x_value + delta_x, color=\"lightgreen\", label = hline_lab)\n",
    "\n",
    "# delta f\n",
    "y_val_max = output_at(lin_function, x_value + delta_x)\n",
    "vline_lab =  'delta f = ' + str(y_val_max-y_val)\n",
    "plt.vlines(x = x_value + delta_x , ymin= y_val, ymax=y_val_max, color=\"darkorange\", label = vline_lab)\n",
    "ax.legend(loc='upper left', fontsize='large')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "####and now original function and f' side by side\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "x_values = np.linspace(0, 5, 100)\n",
    "function_values = list(map(lambda x: output_at(lin_function, x),x_values))\n",
    "derivative_values = list(map(lambda x: derivative_of(lin_function, x, delta_x), x_values))\n",
    "\n",
    "# plot 1\n",
    "plt.subplot(121)\n",
    "plt.plot(x_values, function_values, label = \"f (x)\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=[0, 1], ncol=2, fancybox=True)\n",
    "\n",
    "# plot 2\n",
    "plt.subplot(122)\n",
    "plt.plot(x_values, derivative_values,color=\"darkorange\", label = \"f '(x)\")\n",
    "plt.legend(loc=\"upper left\");\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-derivatives-of-non-linear-functions\n",
    "#derivatives?  wow! :)\n",
    "# https://github.com/ericthansen/dsc-rules-for-derivatives\n",
    " #obvs, i know this.  but here's some code\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def jog(miles):\n",
    "    return 6*miles\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5,5.5))\n",
    "\n",
    "x = np.linspace(0, 6, 7)\n",
    "c1= np.linspace(1,2,20)\n",
    "c2= np.linspace(4,5,20)\n",
    "\n",
    "plt.plot(x, jog(x), label = \"distance given # hours\", marker=\"|\", markersize=12)\n",
    "plt.plot(c1, jog(c1), label = \"slope = 6\", color=\"green\")\n",
    "plt.plot(c2, jog(c2), label = \"slope = 6\", color=\"red\")\n",
    "\n",
    "ax.legend(loc='upper left', fontsize='large')\n",
    "plt.ylabel(\"distance in miles\")\n",
    "plt.xlabel(\"number of hours\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#\n",
    "def f(x):\n",
    "    return np.square(x)\n",
    "fig, ax = plt.subplots(figsize=(9.5,6.5))\n",
    "\n",
    "x = np.linspace(-3, 6, 100)\n",
    "c1= np.linspace(1.5,2.5,20)\n",
    "c2= np.linspace(4.5,5.5,20)\n",
    "c3= np.linspace(-2.5,-1.5,20)\n",
    "\n",
    "plt.plot(x, f(x), label = \"distance given # seconds\")\n",
    "\n",
    "x_dev = np.linspace(1.5, 3.2, 100)\n",
    "a1 = 2\n",
    "a2 = 5\n",
    "a3 = -2\n",
    "delta_a=0.001\n",
    "fprime1 = (f(a1+delta_a)-f(a1))/delta_a \n",
    "fprime2 = (f(a2+delta_a)-f(a2))/delta_a \n",
    "fprime3 = (f(a3+delta_a)-f(a3))/delta_a \n",
    "\n",
    "tan1 = f(a1)+fprime1*(c1-a1)\n",
    "tan2 = f(a2)+fprime2*(c2-a2)\n",
    "tan3 = f(a3)+fprime3*(c3-a3)\n",
    "\n",
    "# plot of the function and the tangent\n",
    "plt.plot(c1, tan1, color = \"green\", label=\"slope = 4\")\n",
    "plt.plot(c2, tan2, color = \"red\", label=\"slope = 10\")\n",
    "plt.plot(c3, tan3, color = \"orange\", label=\"slope = -4\")\n",
    "\n",
    "ax.legend(loc='upper left', fontsize='large')\n",
    "\n",
    "plt.ylabel(\"distance in feet\")\n",
    "plt.xlabel(\"number of seconds\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#also, https://github.com/ericthansen/dsc-derivatives-conclusion\n",
    "from derivatives import *\n",
    "import numpy as np\n",
    "tuple_sq_pos  = np.array([[2, 2], [-8, 1]])\n",
    "x_values = np.linspace(-6, 10, 100)\n",
    "function_values = list(map(lambda x: output_at(tuple_sq_pos, x), x_values))\n",
    "derivative_values = list(map(lambda x: derivative_at(tuple_sq_pos, x),x_values))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "# plot 1\n",
    "plt.subplot(121)\n",
    "plt.axhline(y=0, color='lightgrey', )\n",
    "plt.axvline(x=0, color='lightgrey')\n",
    "plt.plot(x_values, function_values, label = \"f (x) = 2x^2−8x \")\n",
    "\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=[0, 1], ncol=2, fancybox=True)\n",
    "\n",
    "# plot 2\n",
    "plt.subplot(122)\n",
    "plt.axhline(y=0, color='lightgrey')\n",
    "plt.axvline(x=0, color='lightgrey')\n",
    "plt.plot(x_values, derivative_values,color=\"darkorange\", label = \"f '(x) = 4x-8\")\n",
    "\n",
    "ax.grid(True, which='both')\n",
    "\n",
    "plt.legend(loc=\"upper left\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-gradient-descent-intro\n",
    "# https://github.com/ericthansen/dsc-gradient-descent-step-sizes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "def regression_formula(x):\n",
    "    return 1.319 + 52*x\n",
    "\n",
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30,1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3+ 50* x + y_randterm\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x, y, '.b')\n",
    "plt.plot(x, regression_formula(x), '-')\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);\n",
    "\n",
    "\n",
    "table_sm = np.zeros((401,2))\n",
    "for idx, val in enumerate(np.linspace(40, 60, 401)):\n",
    "    table_sm[idx,0] = val\n",
    "    table_sm[idx,1] = residual_sum_squares(x, y, val, 1.319)\n",
    "def tan_line(start, stop, delta_a):\n",
    "    x_dev = np.linspace(start, stop, 100)\n",
    "    a = (start+stop)/2 \n",
    "    f_a= table_sm[(table_sm[:,0]==a),1]\n",
    "    rounded_a_delta_a = round(a+delta_a,2)\n",
    "    f_a_delta= table_sm[(table_sm[:,0]== (rounded_a_delta_a)),1]\n",
    "    fprime = (f_a_delta-f_a)/delta_a \n",
    "    tan = f_a+fprime*(x_dev-a)\n",
    "    return fprime, x_dev, tan\n",
    "fprime_1, x_dev_1, y_dev_1 = tan_line(41, 43.5, 0.05)\n",
    "fprime_2, x_dev_2,  y_dev_2 = tan_line(45, 48, 0.05)\n",
    "fprime_3, x_dev_3,  y_dev_3 = tan_line(49, 52, 0.05)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(table[:,0], table[:,1], '-')\n",
    "plt.plot(x_dev_1, y_dev_1, color = \"red\",  label = \"slope =\" + str(fprime_1))\n",
    "plt.plot(x_dev_2, y_dev_2, color = \"green\",  label = \"slope =\" + str(fprime_2))\n",
    "plt.plot(x_dev_3, y_dev_3, color = \"orange\", label = \"slope =\" + str(fprime_3))\n",
    "\n",
    "plt.xlabel(\"m-values\", fontsize=14)\n",
    "plt.ylabel(\"RSS\", fontsize=14)\n",
    "plt.legend(loc='upper right', fontsize='large')\n",
    "\n",
    "plt.title(\"RSS with changes to slope\", fontsize=16);\n",
    "\n",
    "# https://github.com/ericthansen/dsc-gradient-descent-step-sizes-lab\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(225)\n",
    "###setup and first approx\n",
    "x = np.random.rand(30, 1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3 + 50*x + y_randterm\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, marker=\".\", c=\"b\")\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"y\", fontsize=14)\n",
    "fig.suptitle(\"Revenues\");\n",
    "def regression_formula(x):\n",
    "    return 43*x + 12\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, marker=\".\", c=\"b\")\n",
    "ax.plot(x, regression_formula(x), color=\"orange\", label=r'$y = 43x + 12$')\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"y\", fontsize=14)\n",
    "fig.suptitle(\"Revenues\", fontsize=16)\n",
    "ax.legend();\n",
    "##further approx's\n",
    "def errors(x_values, y_values, m, b):\n",
    "    y_line = (b + m*x_values)\n",
    "    return (y_values - y_line)\n",
    "\n",
    "def squared_errors(x_values, y_values, m, b):\n",
    "    return errors(x_values, y_values, m, b)**2\n",
    "\n",
    "def residual_sum_squares(x_values, y_values, m, b):\n",
    "    return sum(squared_errors(x_values, y_values, m, b))\n",
    "## cost curve\n",
    "def rss_values(x_values, y_values, m, b_values):\n",
    "    # Make a NumPy array to contain the data\n",
    "    data = np.zeros(shape=(len(b_values),2))\n",
    "    # Loop over all of the values in b_values\n",
    "    for idx, b_val in enumerate(b_values):\n",
    "        # Add the current b value and associated RSS to the\n",
    "        # NumPy array\n",
    "        data[idx] = [b_val, residual_sum_squares(x_values, y_values, m, b_val)]\n",
    "        \n",
    "    # Return the NumPy array\n",
    "    return data\n",
    "# Run this cell without changes\n",
    "example_rss = rss_values(x, y, 43, [1,2,3])\n",
    "\n",
    "# Should return a NumPy array\n",
    "assert type(example_rss) == np.ndarray\n",
    "\n",
    "# Specifically a 2D array\n",
    "assert example_rss.ndim == 2\n",
    "\n",
    "# The shape should match the number of b values passed in\n",
    "assert example_rss.shape == (3, 2)\n",
    "\n",
    "example_rss\n",
    "# Replace None with appropriate code\n",
    "b_val = np.arange(0,14.5, 0.5)\n",
    "# Replace None with appropriate code\n",
    "bval_rss = rss_values(x,y, 43, b_val)\n",
    "np.savetxt(sys.stdout, bval_rss, '%16.2f') # this line is to round your result, which will make things look nicer.\n",
    "#costcurve plot\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.plot(bval_rss[:,0], bval_rss[:,1])\n",
    "ax.set_xlabel(r'$b$ values', fontsize=14)\n",
    "ax.set_ylabel(\"RSS\", fontsize=14)\n",
    "fig.suptitle(\"RSS with Changes to Intercept\", fontsize=16);\n",
    "\n",
    "#slopes at different vals\n",
    "def slope_at(x_values, y_values, m, b):\n",
    "    delta = .001\n",
    "    base_rss = residual_sum_squares(x_values, y_values, m, b)\n",
    "    delta_rss = residual_sum_squares(x_values, y_values, m, b + delta)\n",
    "    numerator = delta_rss - base_rss\n",
    "    slope = numerator/delta\n",
    "    return slope\n",
    "#plotting it\n",
    "# Setting up to repeat the same process for 3 and 6\n",
    "# (You can change these values to see other tangent lines)\n",
    "b_vals = [3, 6]\n",
    "\n",
    "def plot_slope_at_b_vals(x, y, m, b_vals, bval_rss):\n",
    "    # Find the slope at each of these values\n",
    "    slopes = [slope_at(x, y, m, b) for b in b_vals]\n",
    "    # Find the RSS at each of these values\n",
    "    rss_values = [residual_sum_squares(x, y, m, b) for b in b_vals]\n",
    "\n",
    "    # Calculate the actual x and y locations for plotting\n",
    "    x_values = [np.linspace(b-1, b+1, 100) for b in b_vals]\n",
    "    y_values = [rss_values[i] + slopes[i]*(x_values[i] - b) for i, b in enumerate(b_vals)]\n",
    "    \n",
    "    # Plotting the same RSS curve as before\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    ax.plot(bval_rss[:,0], bval_rss[:,1])\n",
    "    ax.set_xlabel(r'$b$ values', fontsize=14)\n",
    "    ax.set_ylabel(\"RSS\", fontsize=14)\n",
    "\n",
    "    # Adding tangent lines for the selected b values\n",
    "    for i in range(len(b_vals)):\n",
    "        ax.plot(x_values[i], y_values[i], label=f\"slope={round(slopes[i], 2)}\", linewidth=3)\n",
    "\n",
    "    ax.legend(loc='upper right', fontsize='large')\n",
    "    fig.suptitle(f\"RSS with Intercepts {[round(b, 3) for b in b_vals]} Highlighted\", fontsize=16)\n",
    "    \n",
    "plot_slope_at_b_vals(x, y, 43, b_vals, bval_rss)\n",
    "##### toward gradient descent\n",
    "def updated_b(b, learning_rate, cost_curve_slope):\n",
    "    return -cost_curve_slope*learning_rate + b\n",
    "b_vals = []\n",
    "\n",
    "current_b = 3\n",
    "b_vals.append(current_b)\n",
    "\n",
    "current_cost_slope = slope_at(x, y, 43, current_b)\n",
    "new_b = updated_b(current_b, .01, current_cost_slope)\n",
    "print(f\"\"\"\n",
    "Current b: {round(current_b, 3)}\n",
    "Cost slope for current b: {round(current_cost_slope, 3)}\n",
    "Updated b: {round(new_b, 3)}\n",
    "\"\"\")\n",
    "# Current b: 3\n",
    "# Cost slope for current b: -232.731\n",
    "# Updated b: 5.327\n",
    "plot_slope_at_b_vals(x, y, 43, b_vals, bval_rss)\n",
    "\n",
    "#grad descent!\n",
    "def gradient_descent(x_values, y_values, steps, current_b, learning_rate, m):\n",
    "    listd = []\n",
    "    b = current_b\n",
    "    for step in range(steps):\n",
    "        dic = {}\n",
    "        #rss_values(x_values, y_values, m, b_values)\n",
    "        r = residual_sum_squares(x_values, y_values, m, b)\n",
    "        cost_curve_slope = slope_at(x_values, y_values, m, b)\n",
    "        dic['b'] = b\n",
    "        dic['rss'] = r\n",
    "        dic['slope'] = cost_curve_slope\n",
    "        listd.append(dic)\n",
    "        b = updated_b(b, learning_rate, cost_curve_slope)\n",
    "    #listd = np.savetxt(sys.stdout, listd, '%16.2f')\n",
    "    return listd\n",
    "descent_steps = gradient_descent(x, y, 15, 0, learning_rate = .005, m = 43)\n",
    "descent_steps\n",
    "#final result plot\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.scatter(x, y, marker=\".\", c=\"b\")\n",
    "colors = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#0000FF']\n",
    "for i, b in enumerate(b_vals+[6.83]):\n",
    "    ax.plot(x, x*43 + b, color=colors[i], label=f'$y = 43x + {round(b, 3)}$', linewidth=3)\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"y\", fontsize=14)\n",
    "fig.suptitle(\"Revenues\", fontsize=16)\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-gradient-descent-in-3d\n",
    "# https://github.com/ericthansen/dsc-the-gradient-in-gradient-descent.git\n",
    "# https://github.com/ericthansen/dsc-gradient-to-cost-function-v2-1\n",
    "#residual sum of squares as a loss function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def regression_formula(x):\n",
    "    return 1.8 + 46*x\n",
    "\n",
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30,1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3+ 50* x + y_randterm\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(x, y, '.b')\n",
    "plt.plot(x, regression_formula(x), '-', color=\"red\")\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);\n",
    "\n",
    "# plot errors\n",
    "def errors(x_values, y_values, m, b):\n",
    "    y_line = (b + m*x_values)\n",
    "    return (y_values - y_line)\n",
    "\n",
    "def squared_errors(x_values, y_values, m, b):\n",
    "    return np.round(errors(x_values, y_values, m, b)**2, 2)\n",
    "\n",
    "def residual_sum_squares(x_values, y_values, m, b):\n",
    "    return round(sum(squared_errors(x_values, y_values, m, b)), 2)\n",
    "\n",
    "table = np.zeros((20,2))\n",
    "for idx, val in enumerate(range(40, 60)):\n",
    "    table[idx,0] = val\n",
    "    table[idx,1] = residual_sum_squares(x, y, val, 1.319)\n",
    "    \n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(table[:,0], table[:,1], '-')\n",
    "plt.xlabel(\"m-values\", fontsize=14)\n",
    "plt.ylabel(\"RSS\", fontsize=14)\n",
    "plt.title(\"RSS with changes to slope\", fontsize=16);\n",
    "\n",
    "#plotting it all\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def regression_formula(x):\n",
    "    return 1.319 + 52*x\n",
    "\n",
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30,1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3+ 50* x + y_randterm\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(x, y, '.b')\n",
    "plt.plot(x, regression_formula(x), '-', color=\"red\")\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-applying-gradient-descent-lab\n",
    "# see top for theory; they're still calculating partials just for one function.\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30, 1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3 + 50* x + y_randterm\n",
    "\n",
    "data = np.array([y, x])\n",
    "data = np.transpose(data)\n",
    "\n",
    "plt.plot(x, y, '.b')\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);\n",
    "\n",
    "##first pass at setting up gradient changes\n",
    "# initial variables of our regression line\n",
    "m_current = 0; b_current = 0\n",
    "\n",
    "#amount to update our variables for our next step\n",
    "update_to_b = 0; update_to_m = 0\n",
    "\n",
    "# Define the error_at function\n",
    "def error_at(point, b, m):\n",
    "    return point[1]*m + b - point[0]\n",
    "\n",
    "# iterate through data to change update_to_b and update_to_m\n",
    "for p in data:\n",
    "    #print(p)\n",
    "    error_at_p = error_at(p, b_current, m_current)\n",
    "    update_to_b += error_at_p\n",
    "    update_to_m += error_at_p * p[1]\n",
    "\n",
    "# Create new_b and new_m by subtracting the updates from the current estimates\n",
    "new_b = b_current + 2*update_to_b\n",
    "new_m = m_current + 2*update_to_m\n",
    "\n",
    "###refining the process\n",
    "#amount to update our variables for our next step\n",
    "m_current = 0; b_current = 0\n",
    "update_to_b = 0; update_to_m = 0\n",
    "\n",
    "# define learning rate and n\n",
    "eta = 0.01; n = len(data)\n",
    "\n",
    "# create update_to_b and update_to_m\n",
    "for p in data:\n",
    "    #print(p)\n",
    "    error_at_p = error_at(p, b_current, m_current)\n",
    "    update_to_b += error_at_p\n",
    "    update_to_m += error_at_p * p[1]\n",
    "    \n",
    "# create new_b and new_m\n",
    "new_b = b_current + eta*update_to_b/n\n",
    "new_m = m_current + eta*update_to_m/n\n",
    "\n",
    "### defining step_gradient function\n",
    "def step_gradient(b_current, m_current, points):\n",
    "    eta = 0.1 #learning rate\n",
    "    #amount to update our variables for our next step\n",
    "    #m_current = 0; b_current = 0\n",
    "    update_to_b = 0; update_to_m = 0\n",
    "\n",
    "    # define learning rate and n\n",
    "    #eta = 0.01; \n",
    "    n = len(data)\n",
    "\n",
    "    # create update_to_b and update_to_m\n",
    "    for p in data:\n",
    "        #print(p)\n",
    "        error_at_p = error_at(p, b_current, m_current)\n",
    "        update_to_b += error_at_p\n",
    "        update_to_m += error_at_p * p[1]\n",
    "\n",
    "    # create new_b and new_m\n",
    "    new_b = b_current - eta*update_to_b/n\n",
    "    new_m = m_current - eta*update_to_m/n\n",
    "    return (new_b, new_m)\n",
    "b = 0; m = 0\n",
    "b, m = step_gradient(b, m, 1)\n",
    "b, m\n",
    "# b= 3.02503, m= 2.07286\n",
    "\n",
    "###let's include 2 predictors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(11)\n",
    "\n",
    "x1 = np.random.rand(100,1).reshape(100)\n",
    "x2 = np.random.rand(100,1).reshape(100)\n",
    "y_randterm = np.random.normal(0,0.2,100)\n",
    "y = 2+ 3* x1+ -4*x2 + y_randterm\n",
    "\n",
    "data = np.array([y, x1, x2])\n",
    "data = np.transpose(data)\n",
    "#data\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "ax1.set_title('x_1')\n",
    "ax1.plot(x1, y, '.b')\n",
    "ax2.set_title('x_2')\n",
    "ax2.plot(x2, y, '.b');\n",
    "##### modified fnctions\n",
    "import numpy as np\n",
    "def error_at_multi(point, b, ms):\n",
    "    mxs = 0\n",
    "    for i in range(1,len(point)):\n",
    "        mxs += point[i] * ms[i-1]\n",
    "        #print(i, mxs)\n",
    "    return -(mxs + b - point[0])\n",
    "\n",
    "def step_gradient_multi(b_current, m_current, points):\n",
    "    eta = 0.1 #learning rate\n",
    "    update_to_b = 0\n",
    "    update_to_m = list(np.zeros(len(m_current)))\n",
    "    n = len(points)\n",
    "    #print('n', n)\n",
    "    \n",
    "    #for mj in range(len(m_current)):\n",
    "    for i in range(len(points)):\n",
    "        #print(p)\n",
    "        p = points[i]\n",
    "        error_at_p = error_at_multi(p, b_current, m_current)\n",
    "        #print('error at p', error_at_p)\n",
    "        update_to_b += error_at_p\n",
    "        update_to_m += error_at_p * points[i, 1:] #np.array(p).dot(error_at_p)#m_current * error_at_p # \n",
    "\n",
    "    # create new_b and new_m\n",
    "    new_b = b_current + eta*update_to_b/n\n",
    "    new_ms = m_current + eta*update_to_m/n\n",
    "    return (new_b, new_ms)\n",
    "error_at_multi([0,0,0,0], 10, [0,0,0])\n",
    "\n",
    "#np.array([1,2,3]).dot([2,3,4])\n",
    "new_b, new_ms = step_gradient_multi(0, np.zeros(2), data)\n",
    "new_b, new_ms\n",
    "for _ in range(500):\n",
    "    new_b, new_ms = step_gradient_multi(new_b, new_ms, data)\n",
    "    \n",
    "# https://github.com/ericthansen/dsc-calculus-section-recap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-regularization-intro-v2-1\n",
    "#REGULARIZATION TECHNIQUES\n",
    "#lasso and ridge regression - penalized estimation - penalties on large coefficients; prevents overfitting\n",
    "#AIC and BIC - Hirotugu Akaike and Bayes Information Criterion\n",
    "# The formula for the AIC, invented by Hirotugu Akaike in 1973 and short for \"Akaike's Information Criterion\" is given by:\n",
    "# The BIC (Bayesian Information Criterion) is very similar to the AIC and emerged as a Bayesian response to the AIC, but can be used for the exact same purposes.\n",
    "# The Lower the values of AIC and BIC, the better your model is performing.\n",
    "\n",
    "# https://github.com/ericthansen/dsc-ridge-and-lasso-regression\n",
    "# for ridge, lambda is a hyperparam.  \n",
    "# ridge also called R2 norm regularization\n",
    "\n",
    "# for lasso, similar but coefficients not squared for penalty, just absval. \n",
    "# The name \"Lasso\" comes from \"Least Absolute Shrinkage and Selection Operator\".\n",
    "# performs estimate and selection(i.e. removing some coefficients) simulanteously\n",
    "# Lasso aka L1 norm reg\n",
    "\n",
    "\n",
    "# an example using MPG auto data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('auto-mpg.csv') \n",
    "\n",
    "y = data[['mpg']]\n",
    "X = data.drop(['mpg', 'car name', 'origin'], axis=1)\n",
    "\n",
    "# Perform test train split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "#after splitting, use minmaxscalar to transform\n",
    "scale = MinMaxScaler()\n",
    "X_train_transformed = scale.fit_transform(X_train)\n",
    "X_test_transformed = scale.transform(X_test)\n",
    "\n",
    "#now fit ridge to the transformed data.\n",
    "# alpha is the lambda hyperparam\n",
    "# Build a Ridge, Lasso and regular linear regression model  \n",
    "# Note that in scikit-learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X_train_transformed, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train_transformed, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train_transformed, y_train)\n",
    "#generate predictions\n",
    "# Generate preditions for training and test sets\n",
    "y_h_ridge_train = ridge.predict(X_train_transformed)\n",
    "y_h_ridge_test = ridge.predict(X_test_transformed)\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train_transformed), (274, 1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test_transformed), (118, 1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train_transformed)\n",
    "y_h_lin_test = lin.predict(X_test_transformed)\n",
    "#look at rss for each set\n",
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train_transformed))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test_transformed))**2))\n",
    "\n",
    "##note that unpenalized>ridge>lasso\n",
    "\n",
    "#if we look at coeffs\n",
    "print('Ridge parameter coefficients:', ridge.coef_)\n",
    "print('Lasso parameter coefficients:', lasso.coef_)\n",
    "print('Linear model parameter coefficients:', lin.coef_)\n",
    "#lasso shrunk a few to 0\n",
    "#ridge brought down the large 4th param by 25% ish\n",
    "# more resources:\n",
    "# https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-ridge-and-lasso-regression-lab\n",
    "##see also https://github.com/ericthansen/dsc-ridge-and-lasso-regression-lab/tree/solution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('Housing_Prices/train.csv')\n",
    "# Your code here\n",
    "df.info()\n",
    "# Create X and y\n",
    "target = 'SalePrice'\n",
    "y = df[target]\n",
    "X = df.drop(target, axis=1)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)\n",
    "\n",
    "# Remove \"object\"-type features from X\n",
    "cont_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Remove \"object\"-type features from X_train and X_test\n",
    "X_train_cont = X_train.select_dtypes(include=np.number)\n",
    "X_test_cont = X_test.select_dtypes(include=np.number)\n",
    "# X_train_cont.info()\n",
    "\n",
    "#build a naive linreg\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute missing values with median using SimpleImputer\n",
    "impute = SimpleImputer(strategy='median')#.fit(X_train_cont)\n",
    "X_train_imputed = impute.fit(X_train_cont).transform(X_train_cont)\n",
    "X_test_imputed = impute.fit(X_test_cont).transform(X_test_cont)\n",
    "\n",
    "# Fit the model and print R2 and MSE for training and test sets\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Print R2 and MSE for training and test sets\n",
    "display('r2 train:',linreg.score(X_train_imputed, y_train), \n",
    "        'mse train:', mean_squared_error(y_train, linreg.predict(X_train_imputed)))\n",
    "display('r2 test', linreg.score(X_test_imputed, y_test), \n",
    "        'mse test', mean_squared_error(y_test, linreg.predict(X_test_imputed)))\n",
    "##normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the train and test data\n",
    "ss = StandardScaler()\n",
    "X_train_imputed_scaled = ss.fit(X_train_imputed).transform(X_train_imputed)\n",
    "X_test_imputed_scaled = ss.fit(X_test_imputed).transform(X_test_imputed)\n",
    "\n",
    "# Fit the model\n",
    "linreg_norm = linreg.fit(X_train_imputed_scaled, y_train)\n",
    "\n",
    "\n",
    "# Print R2 and MSE for training and test sets\n",
    "display('r2 train:',linreg_norm.score(X_train_imputed_scaled, y_train), \n",
    "        'mse train:', mean_squared_error(y_train, linreg_norm.predict(X_train_imputed_scaled)))\n",
    "display('r2 test', linreg_norm.score(X_test_imputed_scaled, y_test), \n",
    "        'mse test', mean_squared_error(y_test, linreg_norm.predict(X_test_imputed_scaled)))\n",
    "##include categoricals\n",
    "# Create X_cat which contains only the categorical variables\n",
    "features_cat = X_train.select_dtypes(include='object').columns.tolist()\n",
    "X_train_cat = X_train[features_cat]\n",
    "X_test_cat = X_test[features_cat]\n",
    "\n",
    "# Fill missing values with the string 'missing'\n",
    "X_train_cat.fillna('missing')\n",
    "X_test_cat.fillna('missing')\n",
    "#X_train[features_cat].info()\n",
    "##onehot encode\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# OneHotEncode categorical variables\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')#categories='auto', sparse=True)#, drop='first', handle_unknown='error')\n",
    "\n",
    "# Transform training and test sets\n",
    "X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)\n",
    "'''ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Transform training and test sets\n",
    "X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)'''\n",
    "# Convert these columns into a DataFrame\n",
    "columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n",
    "# cat_train_df = pd.DataFrame(X_train_ohe, columns=columns)\n",
    "# cat_test_df = pd.DataFrame(X_test_ohe, columns=columns)\n",
    "\n",
    "##combine\n",
    "# Your code here\n",
    "X_train_all = pd.concat([pd.DataFrame(X_train_imputed_scaled), cat_train_df], axis=1)\n",
    "X_test_all = pd.concat([pd.DataFrame(X_test_imputed_scaled), cat_test_df], axis=1)\n",
    "##build new linreg\n",
    "# Your code here\n",
    "linreg_final = LinearRegression()\n",
    "linreg_final.fit(X_train_all, y_train)\n",
    "display('r2 train:',linreg_final.score(X_train_all, y_train), \n",
    "        'mse train:', mean_squared_error(y_train, linreg_final.predict(X_train_all)))\n",
    "display('r2 test', linreg_final.score(X_test_all, y_test), \n",
    "        'mse test', mean_squared_error(y_test, linreg_final.predict(X_test_all)))\n",
    "\n",
    "###Now with lasso and ridge\n",
    "# Your code here\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso() \n",
    "lasso.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', lasso.score(X_train_all, y_train))\n",
    "print('Test r^2:', lasso.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, lasso.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, lasso.predict(X_test_all)))\n",
    "#with higher alpha\n",
    "# Your code here\n",
    "lasso = Lasso(alpha=10) \n",
    "lasso.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', lasso.score(X_train_all, y_train))\n",
    "print('Test r^2:', lasso.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, lasso.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, lasso.predict(X_test_all)))\n",
    "##Now ridge\n",
    "# Your code here\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge() \n",
    "ridge.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', ridge.score(X_train_all, y_train))\n",
    "print('Test r^2:', ridge.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, ridge.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, ridge.predict(X_test_all)))\n",
    "#and ridge with higher alpha\n",
    "# Your code here\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha = 10) \n",
    "ridge.fit(X_train_all, y_train)\n",
    "\n",
    "print('Training r^2:', ridge.score(X_train_all, y_train))\n",
    "print('Test r^2:', ridge.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, ridge.predict(X_train_all)))\n",
    "print('Test MSE:', mean_squared_error(y_test, ridge.predict(X_test_all)))\n",
    "\n",
    "##looking at metrics - lasso removes about 25% of predictors\n",
    "# Number of Ridge params almost zero\n",
    "near0 = 10**(-10)\n",
    "print(sum(abs(ridge.coef_)<near0))\n",
    "print(len(abs(ridge.coef_)))\n",
    "# Number of Lasso params almost zero\n",
    "print(sum(abs(lasso.coef_)<near0))\n",
    "print(len(lasso.coef_))\n",
    "print(len(lasso.coef_))\n",
    "print(sum(abs(lasso.coef_) < 10**(-10))/ len(lasso.coef_))\n",
    "\n",
    "###PUT IT ALL TOGETHER\n",
    "def preprocess(X, y):\n",
    "    '''Takes in features and target and implements all preprocessing steps for categorical and continuous features returning \n",
    "    train and test DataFrames with targets'''\n",
    "    \n",
    "    # Train-test split (75-25), set seed to 10\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
    "    \n",
    "    # Remove \"object\"-type features and SalesPrice from X\n",
    "    cont_features = [col for col in X.columns if X[col].dtype in [np.float64, np.int64]]\n",
    "\n",
    "    X_train_cont = X_train.loc[:, cont_features]\n",
    "    X_test_cont = X_test.loc[:, cont_features]\n",
    "\n",
    "    # Impute missing values with median using SimpleImputer\n",
    "    impute = SimpleImputer(strategy='median')\n",
    "\n",
    "    X_train_imputed = impute.fit_transform(X_train_cont)\n",
    "    X_test_imputed = impute.transform(X_test_cont)\n",
    "\n",
    "    # Scale the train and test data\n",
    "    ss = StandardScaler()\n",
    "\n",
    "    X_train_imputed_scaled = ss.fit_transform(X_train_imputed)\n",
    "    X_test_imputed_scaled = ss.transform(X_test_imputed)\n",
    "\n",
    "    # Create X_cat which contains only the categorical variables\n",
    "    features_cat = [col for col in X.columns if X[col].dtype in [np.object]]\n",
    "    X_train_cat = X_train.loc[:, features_cat]\n",
    "    X_test_cat = X_test.loc[:, features_cat]\n",
    "\n",
    "    # Fill nans with a value indicating that that it is missing\n",
    "    X_train_cat.fillna(value='missing', inplace=True)\n",
    "    X_test_cat.fillna(value='missing', inplace=True)\n",
    "\n",
    "    # OneHotEncode Categorical variables\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "    X_test_ohe = ohe.transform(X_test_cat)\n",
    "\n",
    "    columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "    cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "    cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n",
    "    \n",
    "    # Combine categorical and continuous features into the final dataframe\n",
    "    X_train_all = pd.concat([pd.DataFrame(X_train_imputed_scaled), cat_train_df], axis=1)\n",
    "    X_test_all = pd.concat([pd.DataFrame(X_test_imputed_scaled), cat_test_df], axis=1)\n",
    "    \n",
    "    return X_train_all, X_test_all, y_train, y_test\n",
    "\n",
    "\n",
    "##Now, graph over different alphas to find optimal\n",
    "X_train_all, X_test_all, y_train, y_test = preprocess(X, y)\n",
    "\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "alphas = []\n",
    "\n",
    "for alpha in np.linspace(0, 200, num=50):\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train_all, y_train)\n",
    "    \n",
    "    train_preds = lasso.predict(X_train_all)\n",
    "    train_mse.append(mean_squared_error(y_train, train_preds))\n",
    "    \n",
    "    test_preds = lasso.predict(X_test_all)\n",
    "    test_mse.append(mean_squared_error(y_test, test_preds))\n",
    "    \n",
    "    alphas.append(alpha)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alphas, train_mse, label='Train')\n",
    "ax.plot(alphas, test_mse, label='Test')\n",
    "ax.set_xlabel('Alpha')\n",
    "ax.set_ylabel('MSE')\n",
    "\n",
    "# np.argmin() returns the index of the minimum value in a list\n",
    "optimal_alpha = alphas[np.argmin(test_mse)]\n",
    "\n",
    "# Add a vertical line where the test MSE is minimized\n",
    "ax.axvline(optimal_alpha, color='black', linestyle='--')\n",
    "ax.legend();\n",
    "\n",
    "print(f'Optimal Alpha Value: {int(optimal_alpha)}')\n",
    "#Nice graphs yo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-feature-and-model-selection-aic-and-bic.git\n",
    "\n",
    "# https://github.com/ericthansen/dsc-feature-selection-methods\n",
    "#wrapper methods, filter methods, embedded methods, remember interaction terms?  cool.  lots of \n",
    "  #other things come into play with these feature selections\n",
    "#The overview:\n",
    "#Practice on a diabetes dataset\n",
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import the data\n",
    "df = pd.read_csv('diabetes.tab.txt', sep='\\t')\n",
    "df.head()\n",
    "\n",
    "# Obtain the target and features from the DataFrame\n",
    "target = df['Y']\n",
    "features = df.drop(columns='Y')\n",
    "\n",
    "# Create dummy variable for sex\n",
    "features['female'] = pd.get_dummies(features['SEX'], drop_first=True)\n",
    "features.drop(columns=['SEX'], inplace=True)\n",
    "features.head()\n",
    "\n",
    "#For both regularization (an embedded method) and various filters, it is important to standardize the data. This next cell is fitting a StandardScaler from sklearn to the data.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=20, test_size=0.2)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale every feature except the binary column - female\n",
    "transformed_training_features = scaler.fit_transform(X_train.iloc[:,:-1])\n",
    "transformed_testing_features = scaler.transform(X_test.iloc[:,:-1])\n",
    "\n",
    "# Convert the scaled features into a DataFrame\n",
    "X_train_transformed = pd.DataFrame(scaler.transform(X_train.iloc[:,:-1]), \n",
    "                                   columns=X_train.columns[:-1], \n",
    "                                   index=X_train.index)\n",
    "X_train_transformed['female'] = X_train['female']\n",
    "\n",
    "X_test_transformed = pd.DataFrame(scaler.transform(X_test.iloc[:,:-1]), \n",
    "                                  columns=X_train.columns[:-1], \n",
    "                                  index=X_test.index)\n",
    "X_test_transformed['female'] = X_test['female']\n",
    "##see how baseline performs before doing relgularization\n",
    "def run_model(model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print('Training R^2 :', model.score(X_train, y_train))\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    print('Training Root Mean Square Error', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))\n",
    "    print('\\n----------------\\n')\n",
    "    print('Testing R^2 :', model.score(X_test, y_test))\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    print('Testing Root Mean Square Error', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test)))\n",
    "    lm = LinearRegression()\n",
    "lm.fit(X_train_transformed, y_train)\n",
    "run_model(lm, X_train_transformed, X_test_transformed, y_train, y_test)\n",
    "#not great results, so...\n",
    "##add some more features, like polynomial ones\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly_train = pd.DataFrame(poly.fit_transform(X_train_transformed), columns=poly.get_feature_names(features.columns))\n",
    "X_poly_test = pd.DataFrame(poly.transform(X_test_transformed), columns=poly.get_feature_names(features.columns))\n",
    "X_poly_train.head()\n",
    "#now we have 65 columsn and probably overfit!\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(X_poly_train, y_train)\n",
    "\n",
    "run_model(lr_poly, X_poly_train, X_poly_test, y_train, y_test)\n",
    "#training better, testing worse\n",
    "\n",
    "#let's add some filter methods - the first one is variance threshold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "threshold_ranges = np.linspace(0, 2, num=6)\n",
    "\n",
    "for thresh in threshold_ranges:\n",
    "    print(thresh)\n",
    "    selector = VarianceThreshold(thresh)\n",
    "    reduced_feature_train = selector.fit_transform(X_poly_train)\n",
    "    reduced_feature_test = selector.transform(X_poly_test)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(reduced_feature_train, y_train)\n",
    "    run_model(lr, reduced_feature_train, reduced_feature_test, y_train, y_test)\n",
    "    print('--------------------------------------------------------------------')\n",
    "#That does a lousy job.\n",
    "\n",
    "#looks like you can automate it, with different selection approaches...\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, SelectKBest\n",
    "selector = SelectKBest(score_func=f_regression)\n",
    "X_k_best_train = selector.fit_transform(X_poly_train, y_train)\n",
    "X_k_best_test= selector.transform(X_poly_test)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_k_best_train ,y_train)\n",
    "run_model(lr,X_k_best_train,X_k_best_test,y_train,y_test)\n",
    "\n",
    "selector = SelectKBest(score_func=mutual_info_regression)\n",
    "X_k_best_train = selector.fit_transform(X_poly_train, y_train)\n",
    "X_k_best_test= selector.transform(X_poly_test)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_k_best_train ,y_train)\n",
    "run_model(lr,X_k_best_train,X_k_best_test,y_train,y_test)\n",
    "\n",
    "#Now, wrapper method using Recursive feature elim\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "rfe = RFECV(LinearRegression(),cv=5)\n",
    "X_rfe_train = rfe.fit_transform(X_poly_train, y_train)\n",
    "X_rfe_test = rfe.transform(X_poly_test)\n",
    "lm = LinearRegression().fit(X_rfe_train, y_train)\n",
    "run_model(lm, X_rfe_train, X_rfe_test, y_train, y_test)\n",
    "print ('The optimal number of features is: ', rfe.n_features_)\n",
    "#slightly better\n",
    "\n",
    "#now let's try an embedded method - ie lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "lasso = LassoCV(max_iter=100000, cv=5)\n",
    "lasso.fit(X_train_transformed, y_train)\n",
    "run_model(lasso,X_train_transformed, X_test_transformed, y_train, y_test)\n",
    "print('The optimal alpha for the Lasso Regression is: ', lasso.alpha_)\n",
    "\n",
    "#now try lasso on the fully polynomial model\n",
    "lasso2 = LassoCV(max_iter=100000, cv=5)\n",
    "\n",
    "lasso2.fit(X_poly_train, y_train)\n",
    "run_model(lasso2, X_poly_train, X_poly_test, y_train, y_test)\n",
    "print('The optimal alpha for the Lasso Regression is: ', lasso2.alpha_)\n",
    "\n",
    "\n",
    "\n",
    "#mrore good links: \n",
    "# https://www.researchgate.net/profile/Amparo_Alonso-Betanzos/publication/221252792_Filter_Methods_for_Feature_Selection_-_A_Comparative_Study/links/543fd9ec0cf21227a11b8e05.pdf\n",
    "# http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-extensions-to-linear-models-lab\n",
    "# and https://github.com/ericthansen/dsc-extensions-to-linear-models-lab/blob/solution/index.ipynb\n",
    "\n",
    "#also https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html\n",
    "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py\n",
    "# https://xavierbourretsicotte.github.io/subset_selection.html\n",
    "\n",
    "#this one was...elaborate; not sure how we were meant to do it without more guidance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "df = pd.read_csv(\"ames.csv\")\n",
    "df = df[['LotArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF',\n",
    "         '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd',\n",
    "         'GarageArea', 'Fireplaces', 'SalePrice']]\n",
    "#df.head()\n",
    "#baseline\n",
    "# Your code here\n",
    "y = pd.DataFrame(df['SalePrice'])\n",
    "X = df.drop(\"SalePrice\", axis=1)\n",
    "\n",
    "X_scaled = scale(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "all_data = pd.concat([y, X_scaled], axis=1)\n",
    "all_data.head()\n",
    "regression = LinearRegression()\n",
    "\n",
    "crossvalidation = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "baseline = np.mean(cross_val_score(regression, X_scaled, y, scoring='r2', cv=crossvalidation))\n",
    "baseline\n",
    "##include interactions\n",
    "# Your code here\n",
    "combs = list(combinations(X.columns, 2))\n",
    "\n",
    "interactions = []\n",
    "data = X_scaled.copy()\n",
    "for comb in combs:\n",
    "    data['interaction'] = data[comb[0]] * data[comb[1]]\n",
    "    score = np.mean(cross_val_score(regression, data, y, scoring='r2', cv=crossvalidation))\n",
    "    if score > baseline: interactions.append((comb[0], comb[1], round(score, 3)))\n",
    "            \n",
    "print('Top 7 interactions: %s' %sorted(interactions, key=lambda inter: inter[2], reverse=True)[:7])\n",
    "\n",
    "#use top 7\n",
    "# Your code here\n",
    "top7 = sorted(interactions, key=lambda inter: inter[2], reverse=True)[:7]\n",
    "df_inter = X_scaled.copy()\n",
    "for inter in top7:\n",
    "    df_inter[inter[0]+'_'+inter[1]]=df_inter[inter[0]] * df_inter[inter[1]]\n",
    "df_inter.head()\n",
    "\n",
    "#include polynoms\n",
    "# Your code here\n",
    "polynomials = []\n",
    "for col in X.columns:\n",
    "    for degree in [2, 3, 4]:\n",
    "        data = X_scaled.copy()\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_transformed = poly.fit_transform(X[[col]])\n",
    "        data = pd.concat([data.drop(col, axis=1),pd.DataFrame(X_transformed)], axis=1)\n",
    "        score = np.mean(cross_val_score(regression, data, y, scoring='r2', cv=crossvalidation))\n",
    "        if score > baseline: polynomials.append((col, degree, round(score, 3)))\n",
    "print('Top 10 polynomials: %s' %sorted(polynomials, key=lambda poly: poly[2], reverse=True)[:10])\n",
    "# Your code here\n",
    "polynom = pd.DataFrame(polynomials)\n",
    "polynom.groupby([0], sort=False)[2].max()\n",
    "# Your code here\n",
    "for col in ['OverallQual', 'GrLivArea']:\n",
    "    poly = PolynomialFeatures(4, include_bias=False)\n",
    "    X_transformed = poly.fit_transform(X[[col]])\n",
    "    colnames= [col, col + '_' + '2',  col + '_' + '3', col + '_' + '4']\n",
    "    df_inter = pd.concat([df_inter.drop(col, axis=1), pd.DataFrame(X_transformed, columns=colnames)], axis=1)\n",
    "# Your code here\n",
    "df_inter.head()\n",
    "df_inter.info()\n",
    "\n",
    "#full model\n",
    "# Your code here\n",
    "full_model_cv_score = np.mean(cross_val_score(regression, df_inter, y, scoring='r2', cv=crossvalidation))\n",
    "full_model_cv_score\n",
    "\n",
    "#find best lasso parameter\n",
    "from sklearn.linear_model import Lasso, LassoCV, LassoLarsCV, LassoLarsIC\n",
    "# Your code here \n",
    "model_bic = LassoLarsIC(criterion='bic')\n",
    "model_bic.fit(df_inter, y)\n",
    "alpha_bic_ = model_bic.alpha_\n",
    "\n",
    "model_aic = LassoLarsIC(criterion='aic')\n",
    "model_aic.fit(df_inter, y)\n",
    "alpha_aic_ = model_aic.alpha_\n",
    "\n",
    "\n",
    "def plot_ic(model, name, color):\n",
    "    alpha_ = model.alpha_\n",
    "    alphas_ = model.alphas_\n",
    "    criterion_ = model.criterion_\n",
    "    plt.plot(-np.log10(alphas_), criterion_, '--', color=color, linewidth=2, label= name)\n",
    "    plt.axvline(-np.log10(alpha_), color=color, linewidth=2,\n",
    "                label='alpha for %s ' % name)\n",
    "    plt.xlabel('-log(alpha)')\n",
    "    plt.ylabel('criterion')\n",
    "\n",
    "plt.figure()\n",
    "plot_ic(model_aic, 'AIC', 'green')\n",
    "plot_ic(model_bic, 'BIC', 'blue')\n",
    "plt.legend()\n",
    "plt.title('Information-criterion for model selection');\n",
    "\n",
    "#analyze final result\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split X_scaled and y into training and test sets\n",
    "# Set random_state to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Code for baseline model\n",
    "linreg_f = LinearRegression()\n",
    "linreg_f.fit(X_train, y_train)\n",
    "\n",
    "# Print R-Squared and RMSE\n",
    "print('Train R^2:', linreg_f.score(X_train, y_train))\n",
    "print('Test R^2:', linreg_f.score(X_test, y_test))\n",
    "print('Train RMSE:', mean_squared_error(y_train, linreg_f.predict(X_train), squared=False))\n",
    "print('Test RMSE:', mean_squared_error(y_test, linreg_f.predict(X_test), squared=False))\n",
    "\n",
    "# Split df_inter and y into training and test sets\n",
    "# Set random_state to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_inter, y, random_state=1)\n",
    "\n",
    "# Code for lasso with alpha from AIC\n",
    "lasso = Lasso(alpha= model_aic.alpha_) \n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Print R-Squared and RMSE\n",
    "print('Training R-Squared:', lasso.score(X_train, y_train))\n",
    "print('Test R-Squared:', lasso.score(X_test, y_test))\n",
    "print('Training RMSE:', mean_squared_error(y_train, lasso.predict(X_train), squared=False))\n",
    "print('Test RMSE:', mean_squared_error(y_test, lasso.predict(X_test), squared=False))\n",
    "\n",
    "# Code for lasso with alpha from BIC\n",
    "lasso2 = Lasso(alpha= model_bic.alpha_) \n",
    "lasso2.fit(X_train, y_train)\n",
    "\n",
    "# Print R-Squared and RMSE\n",
    "print('Training R-Squared:', lasso2.score(X_train, y_train))\n",
    "print('Test R-Squared:', lasso2.score(X_test, y_test))\n",
    "print('Training RMSE:', mean_squared_error(y_train, lasso2.predict(X_train), squared=False))\n",
    "print('Test RMSE:', mean_squared_error(y_test, lasso2.predict(X_test), squared=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-generating-data\n",
    "\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import make_blobs\n",
    "from sklearn.datasets import make_blobs#from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=3, n_features=2)\n",
    "\n",
    "#now visualize\n",
    "# Plot a scatter plot, color \n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()\n",
    "\n",
    "#now, make moons - not linearly separable so good for nonlinear like tanh or sigmoid\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "# Plot a scatter plot, color \n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#now make circles\n",
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(n_samples=100, noise=0.05)\n",
    "\n",
    "# Plot a scatter plot, color \n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#now make regression\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "#and plot\n",
    "# Plot regression dataset\n",
    "plt.scatter(X,y)\n",
    "plt.show()\n",
    "\n",
    "#can make nonlinear\n",
    "# Generate new y \n",
    "y2 = y**2\n",
    "y3 = y**3\n",
    "\n",
    "# Visualize this data\n",
    "plt.scatter(X, y2)\n",
    "plt.show()\n",
    "plt.scatter(X, y3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#finally, see also https://scikit-learn.org/stable/datasets/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-generating-data-lab\n",
    "# Your code here \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import make_blobs\n",
    "from sklearn.datasets import make_blobs#from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state = 42)\n",
    "\n",
    "# Your code here \n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "df.head()\n",
    "# Your code here \n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()\n",
    "\n",
    "#diffefent clustering values\n",
    "# Your code here: \n",
    "cluster_std = 0.5\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state = 42, cluster_std = cluster_std)\n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#now do regression\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def reg_simulation(n, random_state):\n",
    "    # Generate X and y\n",
    "    X, y = make_regression(n_samples=100, n_features=1, noise=n, random_state=random_state)\n",
    "\n",
    "\n",
    "    # Use X,y to draw a scatter plot\n",
    "    plt.scatter(X[:, 0], y, color='red', s=10, label='Data')\n",
    "    \n",
    "    # Fit a linear regression model to X , y and calculate r2\n",
    "    # label and plot the regression line \n",
    "    linreg = LinearRegression().fit(X, y)\n",
    "    \n",
    "    plt.plot(X[:, 0], linreg.predict(X), color='black', label='Model')\n",
    "    plt.title('Noise: ' + str(n) + ', R-Squared: ' + str(round(linreg.score(X,y), 2)))\n",
    "    plt.tick_params(labelbottom=False, labelleft=False)\n",
    "    plt.xlabel('Variable X')\n",
    "    plt.ylabel('Variable Y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "random_state = random_state = np.random.RandomState(42)\n",
    "\n",
    "for n in [10, 25, 40, 50, 100, 200]:\n",
    "    reg_simulation(n, random_state)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-ml-fundamentals-lab\n",
    "# Like.  the whole thing.  USEFUL WORKFLOW!\n",
    "#also https://github.com/ericthansen/dsc-ml-fundamentals-lab/tree/solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-regularization-recap-v2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-logistic-regression-intro \n",
    "# https://github.com/ericthansen/dsc-intro-to-supervised-learning-v2-1 - VERY useful background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-linear-to-logistic-regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "age = np.random.uniform(18, 65, 100)\n",
    "income = np.random.normal((age/10), 0.5)\n",
    "age = age.reshape(-1,1)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "fig.suptitle('age vs income', fontsize=16)\n",
    "plt.scatter(age, income)\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.suptitle('linear regression', fontsize=16)\n",
    "plt.scatter(age, income)\n",
    "plt.plot(age, age/10, c='black')\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "income_bin = income > 4\n",
    "income_bin = income_bin.astype(int)  \n",
    "print(income_bin)\n",
    "\n",
    "\n",
    "#plots are just 0s and 1s\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.suptitle('age vs binary income', fontsize=16)\n",
    "plt.scatter(age, income_bin)\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income (> or < 4000)', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#fit a logistic regression (s-curve lookin' thing)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(age, income_bin)\n",
    "# Store the coefficients\n",
    "coef = lin_reg.coef_\n",
    "interc = lin_reg.intercept_\n",
    "# Create the line\n",
    "lin_income = (interc + age * coef)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.suptitle('linear regression', fontsize=16)\n",
    "plt.scatter(age, income_bin)\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income', fontsize=14)\n",
    "plt.plot(age, lin_income, c='black')\n",
    "plt.show()\n",
    "\n",
    "# Instantiate a Logistic regression model\n",
    "# Solver must be specified to avoid warning, see documentation for more information\n",
    "# liblinear is recommended for small datasets\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "regr = LogisticRegression(C=1e5, solver='liblinear')\n",
    "\n",
    "# Fit the model to the training set\n",
    "regr.fit(age, income_bin)\n",
    "\n",
    "# Store the coefficients\n",
    "coef = regr.coef_\n",
    "interc = regr.intercept_\n",
    "\n",
    "# Create the linear predictor\n",
    "lin_pred = (age * coef + interc)\n",
    "\n",
    "# Perform the log transformation\n",
    "mod_income = 1 / (1 + np.exp(-lin_pred))\n",
    "\n",
    "# Sort the numbers to make sure plot looks right\n",
    "age_ordered, mod_income_ordered = zip(*sorted(zip(age ,mod_income.ravel()),key=lambda x: x[0]))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.suptitle('logistic regression', fontsize=16)\n",
    "plt.scatter(age, income_bin)\n",
    "plt.xlabel('age', fontsize=14)\n",
    "plt.ylabel('monthly income', fontsize=14)\n",
    "plt.plot(age_ordered, mod_income_ordered, c='black')\n",
    "plt.show()\n",
    "\n",
    "###a real world example\n",
    "import statsmodels as sm\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "salaries = pd.read_csv('salaries_final.csv', index_col=0)\n",
    "salaries.head()\n",
    "\n",
    "# Convert race and sex using get_dummies() \n",
    "x_feats = ['Race', 'Sex', 'Age']\n",
    "X = pd.get_dummies(salaries[x_feats], drop_first=True, dtype=float)\n",
    "\n",
    "# Convert target using get_dummies\n",
    "y = pd.get_dummies(salaries['Target'], drop_first=True, dtype=float)\n",
    "y = y['>50K']\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Create intercept term required for sm.Logit, see documentation for more information\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit model\n",
    "logit_model = sm.Logit(y, X)\n",
    "\n",
    "# Get results of the fit\n",
    "result = logit_model.fit()\n",
    "\n",
    "result.summary()\n",
    "\n",
    "np.exp(result.params)\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept = False, C = 1e15, solver='liblinear')\n",
    "model_log = logreg.fit(X, y)\n",
    "model_log\n",
    "\n",
    "model_log.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://github.com/ericthansen/dsc-fitting-a-logistic-regression-model-lab\n",
    "# Import the data\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()\n",
    "\n",
    "# Total number of people who survived/didn't survive\n",
    "df['Survived'].value_counts()\n",
    "total_surv = df.Survived.sum()\n",
    "total_surv\n",
    "\n",
    "# Create dummy variables\n",
    "relevant_columns = ['Pclass', 'Age', 'SibSp', 'Fare', 'Sex', 'Embarked', 'Survived']\n",
    "dummy_dataframe = pd.get_dummies(df[relevant_columns], drop_first=True, dtype=float)\n",
    "\n",
    "dummy_dataframe.shape\n",
    "\n",
    "# Drop missing rows\n",
    "dummy_dataframe = dummy_dataframe.dropna()\n",
    "dummy_dataframe.shape\n",
    "\n",
    "# Split the data into X and y\n",
    "y = dummy_dataframe['Survived']\n",
    "X = dummy_dataframe.drop('Survived', axis=1)\n",
    "\n",
    "# Build a logistic regression model using statsmodels\n",
    "import statsmodels.api as sm\n",
    "X = sm.tools.add_constant(X)\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Summary table\n",
    "result.summary()\n",
    "###now reduce to using only the most relevant predictors\n",
    "# Your code here\n",
    "relevant_columns = ['Pclass', 'Age', 'SibSp', 'Sex', 'Survived']\n",
    "dummy_dataframe = pd.get_dummies(df[relevant_columns], drop_first=True, dtype=float)\n",
    "\n",
    "dummy_dataframe = dummy_dataframe.dropna()\n",
    "\n",
    "y = dummy_dataframe['Survived']\n",
    "X = dummy_dataframe.drop(columns=['Survived'], axis=1)\n",
    "\n",
    "X = sm.tools.add_constant(X)\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit()\n",
    "\n",
    "result.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-logistic-regression-in-scikit-learn\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()\n",
    "\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "print(df.columns)\n",
    "df.head()\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()\n",
    "\n",
    "x_feats = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare', 'Cabin', 'Embarked']\n",
    "X = pd.get_dummies(df[x_feats], drop_first=True)\n",
    "y = df['Survived']\n",
    "X.head() # Preview our data to make sure it looks reasonable\n",
    "\n",
    "# Fill missing values\n",
    "X = X.fillna(value=0) \n",
    "for col in X.columns:\n",
    "    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n",
    "    X[col] = (X[col] - min(X[col]))/ (max(X[col]) - min(X[col])) \n",
    "\n",
    "X.head()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "model_log\n",
    "\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "\n",
    "import numpy as np\n",
    "# We could subtract the two columns. If values or equal, difference will be zero. Then count number of zeros \n",
    "residuals = np.abs(y_train - y_hat_train)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))\n",
    "\n",
    "residuals = np.abs(y_test - y_hat_test)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-logistic-regression-in-scikit-learn-lab\n",
    "# Import necessary functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv('heart.csv')\n",
    "df.head()\n",
    "\n",
    "# Split the data into target and predictors\n",
    "y = df.target\n",
    "X = df.drop('target', axis = 1)\n",
    "\n",
    "# Your code here\n",
    "X = X.apply(lambda x : (x - x.min()) /(x.max() - x.min()), axis=0)\n",
    "X.head()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Instantiate the model\n",
    "C = 10**10\n",
    "logreg = LogisticRegression(fit_intercept=False, C=C, solver='liblinear')\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "##check results\n",
    "# Your code here\n",
    "import numpy as np\n",
    "residuals = np.abs(y_train - y_hat_train)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))\n",
    "##and on test set\n",
    "# Your code here\n",
    "residuals = np.abs(y_test - y_hat_test)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-confusion-matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "example_labels = [0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1]\n",
    "example_preds  = [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "cf = confusion_matrix(example_labels, example_preds)\n",
    "cf\n",
    "\n",
    "ex2_labels = [0, 1, 2, 2, 3, 1, 0, 2, 1, 2, 3, 3, 1, 0]\n",
    "ex2_preds =  [0, 1, 1, 2, 3, 3, 2, 2, 1, 2, 3, 0, 2, 0]\n",
    "\n",
    "cf2 = confusion_matrix(ex2_labels, ex2_preds)\n",
    "cf2\n",
    "\n",
    "# https://github.com/ericthansen/dsc-visualizing-confusion-matrices-lab\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Define appropriate X and y\n",
    "X = df[df.columns[:-1]]\n",
    "y = df.target\n",
    "\n",
    "# Normalize the data\n",
    "for col in df.columns:\n",
    "    df[col] = (df[col] - min(df[col]))/ (max(df[col]) - min(df[col]))\n",
    "\n",
    "# Split the data into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Fit a model\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "\n",
    "# Preview model params\n",
    "print(model_log) \n",
    "\n",
    "# Predict\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "print(\"\")\n",
    "# Data preview\n",
    "df.head()\n",
    "\n",
    "##manually create confusion matrix\n",
    "def conf_matrix(y_true, y_pred):\n",
    "\n",
    "    cmatrix = {}\n",
    "    lt = list(y_true)\n",
    "    lp = list(y_pred)\n",
    "    for i in range(len(lt)):\n",
    "        if lt[i] == 1 and lp[i] == 1:\n",
    "            cmatrix['TP'] = cmatrix.get('TP', 0) + 1\n",
    "        elif lt[i] == 1 and lp[i] == 0:\n",
    "            cmatrix['FN'] = cmatrix.get('FN', 0) + 1\n",
    "        elif lt[i] == 0 and lp[i] == 1:\n",
    "            cmatrix['FP'] = cmatrix.get('FP', 0) + 1            \n",
    "        elif lt[i] == 0 and lp[i] == 0:\n",
    "            cmatrix['TN'] = cmatrix.get('TN', 0) + 1      \n",
    "    return cmatrix\n",
    "# Test the function\n",
    "conf_matrix(y_test, y_hat_test)\n",
    "# Expected output: {'TP': 39, 'TN': 24, 'FP': 9, 'FN': 4}\n",
    "\n",
    "##check work\n",
    "# Import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_hat_test)\n",
    "print('Confusion Matrix:\\n', cnf_matrix)\n",
    "\n",
    "##create a nice visual\n",
    "# Import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Visualize your confusion matrix\n",
    "cm = confusion_matrix(y_test, y_hat_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-evaluation-metrics\n",
    "#Precision and Recall are two of the most basic evaluation metrics available to us. Precision measures how precise the predictions are, while Recall indicates what percentage of the classes we're interested in were actually captured by the model.\n",
    "# precision = true positives / all predicted positives\n",
    "# recall = true pos / actual total positives\n",
    "# accuracy  = (number of true pos + true neg)/ total obs\n",
    "# F1 score = F1 score represents the Harmonic Mean of Precision and Recall.\n",
    "#   = 2(precision*recall)/(precision+recall)\n",
    "#   penalizes either being low\n",
    "# Scikit-learn has a built-in function that will create a Classification Report. This classification report even breaks down performance by individual class predictions for your model. You can find the classification_report() function in the sklearn.metrics module, which takes labels and predictions and returns the precision, recall, F1 score and support (number of occurrences of each label in y_true) for the results of a model.\n",
    "\n",
    "\n",
    "# https://github.com/ericthansen/dsc-evaluating-logistic-regression-models-lab\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('heart.csv')\n",
    "df.head()\n",
    "\n",
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate LogisticRegression\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "\n",
    "# Fit to training data\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "model_log\n",
    "\n",
    "def precision(y, y_hat):\n",
    "    # Your code here\n",
    "    # precision = true positives / all predicted positives\n",
    "#     resid = y-y_hat\n",
    "    cmatrix = {}\n",
    "    lt = list(y)\n",
    "    lp = list(y_hat)\n",
    "    for i in range(len(lt)):\n",
    "        if lt[i] == 1 and lp[i] == 1:\n",
    "            cmatrix['TP'] = cmatrix.get('TP', 0) + 1\n",
    "        elif lt[i] == 1 and lp[i] == 0:\n",
    "            cmatrix['FN'] = cmatrix.get('FN', 0) + 1\n",
    "        elif lt[i] == 0 and lp[i] == 1:\n",
    "            cmatrix['FP'] = cmatrix.get('FP', 0) + 1            \n",
    "        elif lt[i] == 0 and lp[i] == 0:\n",
    "            cmatrix['TN'] = cmatrix.get('TN', 0) + 1    \n",
    "    return cmatrix['TP']/(cmatrix['TP']+cmatrix['FP'])\n",
    "    \n",
    "    \n",
    "def recall(y, y_hat):\n",
    "    # Your code here\n",
    "    # recall = true pos / actual total positives\n",
    "    cmatrix = {}\n",
    "    lt = list(y)\n",
    "    lp = list(y_hat)\n",
    "    for i in range(len(lt)):\n",
    "        if lt[i] == 1 and lp[i] == 1:\n",
    "            cmatrix['TP'] = cmatrix.get('TP', 0) + 1\n",
    "        elif lt[i] == 1 and lp[i] == 0:\n",
    "            cmatrix['FN'] = cmatrix.get('FN', 0) + 1\n",
    "        elif lt[i] == 0 and lp[i] == 1:\n",
    "            cmatrix['FP'] = cmatrix.get('FP', 0) + 1            \n",
    "        elif lt[i] == 0 and lp[i] == 0:\n",
    "            cmatrix['TN'] = cmatrix.get('TN', 0) + 1    \n",
    "    return cmatrix['TP']/(cmatrix['TP']+cmatrix['FN'])\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    # Your code here\n",
    "    # accuracy  = (number of true pos + true neg)/ total obs\n",
    "    cmatrix = {}\n",
    "    lt = list(y)\n",
    "    lp = list(y_hat)\n",
    "    for i in range(len(lt)):\n",
    "        if lt[i] == 1 and lp[i] == 1:\n",
    "            cmatrix['TP'] = cmatrix.get('TP', 0) + 1\n",
    "        elif lt[i] == 1 and lp[i] == 0:\n",
    "            cmatrix['FN'] = cmatrix.get('FN', 0) + 1\n",
    "        elif lt[i] == 0 and lp[i] == 1:\n",
    "            cmatrix['FP'] = cmatrix.get('FP', 0) + 1            \n",
    "        elif lt[i] == 0 and lp[i] == 0:\n",
    "            cmatrix['TN'] = cmatrix.get('TN', 0) + 1    \n",
    "    return (cmatrix['TP']+cmatrix['TN'])/len(y)\n",
    "\n",
    "def f1_score(y, y_hat):\n",
    "    # Your code here\n",
    "    # 2(precision*recall)/(precision+recall)\n",
    "    prec = precision(y, y_hat)\n",
    "    rec = recall(y, y_hat)\n",
    "    return 2*(prec*rec)/(prec+rec)\n",
    "\n",
    "# Your code here\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "display('prec train',precision(y_train, y_hat_train))\n",
    "display('prec test',precision(y_test, y_hat_test))\n",
    "\n",
    "display('recall train',recall(y_train, y_hat_train))\n",
    "display('recall test',recall(y_test, y_hat_test))\n",
    "\n",
    "\n",
    "display('acc train',accuracy(y_train, y_hat_train))\n",
    "display('acc test',accuracy(y_test, y_hat_test))\n",
    "\n",
    "\n",
    "display('F1 train', f1_score(y_train, y_hat_train))\n",
    "display('F1 test', f1_score(y_test, y_hat_test))\n",
    "\n",
    "###now with sklearn\n",
    "# Your code here\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "for [yt, yp, yset] in [[y_train, y_hat_train, 'train'],[y_test, y_hat_test, 'test']]:\n",
    "    display(yset.upper())\n",
    "    display('precision:', precision_score(yt, yp))\n",
    "    display('recall:', recall_score(yt, yp))\n",
    "    display('accuracy:', accuracy_score(yt, yp))\n",
    "    display('f1 score:', f1_score(yt, yp))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#calculate for different test sizes\n",
    "training_precision = []\n",
    "testing_precision = []\n",
    "training_recall = []\n",
    "testing_recall = []\n",
    "training_accuracy = []\n",
    "testing_accuracy = []\n",
    "training_f1 = []\n",
    "testing_f1 = []\n",
    "\n",
    "for i in range(10, 95):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= i) # replace the \"None\" here\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver='liblinear')\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    y_hat_test = model_log.predict(X_test)\n",
    "    y_hat_train = model_log.predict(X_train)\n",
    "    \n",
    "    # Your code here\n",
    "    training_precision.append(precision_score(y_train, y_hat_train))\n",
    "    testing_precision.append(precision_score(y_test, y_hat_test))\n",
    "    training_recall.append(recall_score(y_train, y_hat_train))\n",
    "    testing_recall.append(recall_score(y_test, y_hat_test))\n",
    "    training_accuracy.append(accuracy_score(y_train, y_hat_train))\n",
    "    testing_accuracy.append(accuracy_score(y_test, y_hat_test))\n",
    "    training_f1.append(f1_score(y_train, y_hat_train))\n",
    "    testing_f1.append(f1_score(y_test, y_hat_test))\n",
    "    \n",
    "# Train and test precision\n",
    "plt.scatter(list(range(10, 95)), training_precision, label='training_precision')\n",
    "plt.scatter(list(range(10, 95)), testing_precision, label='testing_precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Train and test recall\n",
    "plt.scatter(list(range(10, 95)), training_recall, label='training_recall')\n",
    "plt.scatter(list(range(10, 95)), testing_recall, label='testing_recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Train and test accuracy\n",
    "plt.scatter(list(range(10, 95)), training_accuracy, label='training_accuracy')\n",
    "plt.scatter(list(range(10, 95)), testing_accuracy, label='testing_accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Train and test F1 score\n",
    "plt.scatter(list(range(10, 95)), training_f1, label='training_f1')\n",
    "plt.scatter(list(range(10, 95)), testing_f1, label='testing_f1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-roc-curves-and-auc\n",
    "# ROC curves plot true pos rate vs false pos rate.\n",
    "# true pos rate = true pos /(true pos + false neg)\n",
    "# false pos rate = false pos / (false pos + true neg)\n",
    "# plot tpr on y axis, fpr on x.\n",
    "\n",
    "#AUC is area under curve, when plotting positives/negatives/decision boundary,\n",
    "# basically it compares the area of all false results to total\n",
    "# smaller false total, better\n",
    "# AUC of .5 is random, AUC of 1 is perfect (higher is better)\n",
    "\n",
    "\n",
    "#see this embedded below:\n",
    "https://flatiron.illumidesk.com/user/ericthansen/notebooks/dsc-roc-curves-and-auc/images/Image_146_recall.png\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC diagram <img src=\"https://flatiron.illumidesk.com/user/ericthansen/notebooks/dsc-roc-curves-and-auc/images/Image_146_recall.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continued - drawing the curve in practice\n",
    "#train a classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Define appropriate X and y\n",
    "y = df['target']\n",
    "X = df.drop(columns='target', axis=1)\n",
    "\n",
    "# Normalize the Data\n",
    "X = X.apply(lambda x : (x - x.min()) /(x.max() - x.min()),axis=0)\n",
    "\n",
    "# Split the data into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Fit a model\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "print(logreg) # Preview model params\n",
    "\n",
    "# Predict\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "# Data preview\n",
    "print(\"\")\n",
    "df.head()\n",
    "\n",
    "##Draw the ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Scikit-learn's built in roc_curve method returns the fpr, tpr, and thresholds\n",
    "# for various decision boundaries given the case member probabilites\n",
    "\n",
    "# First calculate the probability scores of each of the datapoints:\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "##calculate the AUC\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "\n",
    "##put it all together as cohesive visual\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn's beautiful styling\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-roc-curves-and-auc-lab\n",
    "## sample lab\n",
    "# Import and preview the data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('mushrooms.csv')\n",
    "display(df.head(5))\n",
    "display(df.info())\n",
    "\n",
    "#df['cap-shape'].value_counts()\n",
    "##because everything is Objects, we make dummies\n",
    "# Define y\n",
    "y = pd.get_dummies(df['class'], drop_first=True)\n",
    "y = y['p']\n",
    "\n",
    "# Define X\n",
    "X = pd.get_dummies(df.drop('class', axis=1), drop_first=True)\n",
    "#X = None\n",
    "#display(X.head())\n",
    "# Import train_test_split\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "##import logistic regression\n",
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate\n",
    "logreg = LogisticRegression(fit_intercept=False, solver='liblinear')\n",
    "\n",
    "# Fit the model to training data\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "## calculate Tpr and Fpr\n",
    "# Import roc_curve, auc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Calculate the probability scores of each point in the training set\n",
    "y_train_score = model_log.decision_function(X_train)\n",
    "\n",
    "# Calculate the fpr, tpr, and thresholds for the training set\n",
    "train_fpr, train_tpr, thresholds = roc_curve(y_train, y_train_score)\n",
    "\n",
    "# Calculate the probability scores of each point in the test set\n",
    "y_score = model_log.decision_function(X_test)\n",
    "\n",
    "## Calculate the fpr, tpr, and thresholds for the test set\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "##check out AUC\n",
    "print('AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "##draw the ROC curves\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn's beautiful styling\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "# ROC curve for training set\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(train_fpr, train_tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve for Training Set')\n",
    "plt.legend(loc='lower right')\n",
    "print('AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "plt.show()\n",
    "# ROC curve for test set\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve for Testing Set')\n",
    "plt.legend(loc='lower right')\n",
    "print('AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-class-imbalance-problems\n",
    "## synthetic minority oversampling SMOTE https://jair.org/index.php/jair/article/view/10302/24590\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = train_test\n",
    "\n",
    "# Data preview\n",
    "df.head()\n",
    "###look at level of class imbalance\n",
    "print('Raw counts: \\n')\n",
    "print(df['is_attributed'].value_counts())\n",
    "print('-----------------------------------')\n",
    "print('Normalized counts: \\n')\n",
    "print(df['is_attributed'].value_counts(normalize=True))\n",
    "##define x and y\n",
    "# Define appropriate X and y\n",
    "y = df['is_attributed']\n",
    "X = df[['ip', 'app', 'device', 'os', 'channel']]\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "##compare some different reg parameters\n",
    "# Now let's compare a few different regularization performances on the dataset:\n",
    "weights = [None, 'balanced', {1:2, 0:1}, {1:10, 0:1}, {1:100, 0:1}, {1:1000, 0:1}]\n",
    "names = ['None', 'Balanced', '2 to 1', '10 to 1', '100 to 1', '1000 to 1']\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for n, weight in enumerate(weights):\n",
    "    # Fit a model\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e20, class_weight=weight, solver='lbfgs')\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    print(model_log)\n",
    "\n",
    "    # Predict\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "    y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    \n",
    "    print('AUC for {}: {}'.format(names[n], auc(fpr, tpr)))\n",
    "    print('-------------------------------------------------------------------------------------')\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=colors[n],\n",
    "             lw=lw, label='ROC curve {}'.format(names[n]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "##Oversampling and undersampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# https://jair.org/index.php/jair/article/view/10302/24590\n",
    "#\n",
    "# Previous original class distribution\n",
    "print('Original class distribution: \\n')\n",
    "print(y.value_counts())\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train) \n",
    "# Preview synthetic sample class distribution\n",
    "print('-----------------------------------------')\n",
    "print('Synthetic sample class distribution: \\n')\n",
    "print(pd.Series(y_train_resampled).value_counts()) \n",
    "\n",
    "#now see some different ratios\n",
    "# Now let's compare a few different ratios of minority class to majority class\n",
    "ratios = [0.1, 0.25, 0.33, 0.5, 0.7, 1]\n",
    "names = ['0.1', '0.25', '0.33','0.5','0.7','even']\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for n, ratio in enumerate(ratios):\n",
    "    # Fit a model\n",
    "    smote = SMOTE(sampling_strategy=ratio)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train) \n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e20, solver ='lbfgs')\n",
    "    model_log = logreg.fit(X_train_resampled, y_train_resampled)\n",
    "    print(model_log)\n",
    "\n",
    "    # Predict\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "    y_score = logreg.decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    \n",
    "    print('AUC for {}: {}'.format(names[n], auc(fpr, tpr)))\n",
    "    print('-------------------------------------------------------------------------------------')\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=colors[n],\n",
    "             lw=lw, label='ROC curve {}'.format(names[n]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-class-imbalance-problems-lab\n",
    "# similar to above.  predicting credit card fraud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Load a compressed csv file\n",
    "df = pd.read_csv('creditcard.csv.gz', compression='gzip')\n",
    "\n",
    "# Print the first five rows of data\n",
    "df.head(5)\n",
    "\n",
    "#preview class imbalance\n",
    "# Count the number of fraudulent/infraudulent purchases\n",
    "df.Class.value_counts()\n",
    "#define target/predictors\n",
    "# Your code here\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "#find imbalance in train/test\n",
    "# Training set\n",
    "print('Raw counts: \\n')\n",
    "print(y_train.value_counts())\n",
    "print('-----------------------------------')\n",
    "print('Normalized counts: \\n')\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print('\\n')\n",
    "# Test set\n",
    "print('Raw counts: \\n')\n",
    "print(y_test.value_counts())\n",
    "print('-----------------------------------')\n",
    "print('Normalized counts: \\n')\n",
    "print(y_test.value_counts(normalize=True))\n",
    "print('\\n')\n",
    "\n",
    "#create initial model\n",
    "# Initial Model\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e20, class_weight=None, solver='liblinear')#, solver='lbfgs')\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "# Probability scores for test set\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "# False positive rate and true positive rate\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "# Seaborn's beautiful styling\n",
    "sns.set_style('darkgrid', {'axes.facecolor': '0.9'})\n",
    "\n",
    "# Print AUC\n",
    "print('AUC for {}: {}'.format(\"no balancing\", auc(fpr, tpr)))\n",
    "print('-------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# Plot the ROC curve\n",
    "colors = sns.color_palette('Set2')\n",
    "names=['No Balancing']\n",
    "n=0\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color=colors[n],\n",
    "         lw=lw, label='ROC curve {}'.format(names[n]))\n",
    "\n",
    "##plot confusion matrix\n",
    "# Plot confusion matrix of the test set \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_hat_test = model_log.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_hat_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##tune the model\n",
    "# Now let's compare a few different regularization performances on the dataset:\n",
    "C_param_range = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "names = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for n, c in enumerate(C_param_range):\n",
    "    # Fit a model\n",
    "    #print(n, c)\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=c, solver='liblinear')\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    print(model_log) # Preview model params\n",
    "\n",
    "    # Predict\n",
    "    y_hat_test = model_log.predict(X_test)\n",
    "\n",
    "    y_score = model_log.decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    \n",
    "    print('AUC for {}: {}'.format(names[n], auc(fpr, tpr)))\n",
    "    print('-------------------------------------------------------')\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=colors[n],\n",
    "             lw=lw, label='ROC curve Normalization Weight: {}'.format(names[n]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "##use SMOTE to improve on miniority class\n",
    "# Previous original class distribution\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Fit SMOTE to training data\n",
    "X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train) \n",
    "\n",
    "# Preview synthetic sample class distribution\n",
    "print('\\n')\n",
    "print(pd.Series(y_train_resampled).value_counts()) \n",
    "\n",
    "#now rebuild models on SMOTEd sets\n",
    "# Previous original class distribution\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Fit SMOTE to training data\n",
    "X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train) \n",
    "\n",
    "# Preview synthetic sample class distribution\n",
    "print('\\n')\n",
    "print(pd.Series(y_train_resampled).value_counts()) \n",
    "\n",
    "###REMEMBER!  DON\"T SMOTE then TRAIN-TEST - data leakage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-logistic-regression-section-recap\n",
    "\n",
    "#https://github.com/ericthansen/dsc-mle-logistic-reg-intro\n",
    "\n",
    "#https://github.com/ericthansen/dsc-mle-review\n",
    "\n",
    "#https://github.com/ericthansen/dsc-mle-logistic-regression\n",
    "##Super important links!\n",
    "#https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
    "#https://www.bloomberg.com/opinion/articles/2018-10-16/amazon-s-gender-biased-algorithm-is-not-alone\n",
    "# https://www.bostonglobe.com/business/2017/12/21/the-software-that-runs-our-lives-can-bigoted-and-unfair-but-can-fix/RK4xG4gYxcVNVTIubeC1JI/story.html\n",
    "# https://www.bostonglobe.com/ideas/2017/07/07/why-artificial-intelligence-far-too-human/jvG77QR5xPbpwBL2ApAFAN/story.html\n",
    "# https://www.npr.org/2016/03/14/470427605/can-computers-be-racist-the-human-like-bias-of-algorithms\n",
    "# https://onlinecourses.science.psu.edu/stat504/node/150/\n",
    "# https://web.stanford.edu/~hastie/ElemStatLearn//\n",
    "\n",
    "\n",
    "# https://github.com/ericthansen/dsc-gradient-descent-review\n",
    "\n",
    "# https://github.com/ericthansen/dsc-gradient-descent-lab\n",
    "\n",
    "# rss function\n",
    "def rss(m, X=df['budget'], y=df['domgross']):\n",
    "    model = m * X\n",
    "    residuals = model - y\n",
    "    total_rss = residuals.map(lambda x: x**2).sum()\n",
    "    return total_rss\n",
    "\n",
    "#gradient descent - first look at curve\n",
    "x = np.linspace(start=-3, stop=5, num=10**3)\n",
    "y = [rss(xi) for xi in x]\n",
    "plt.plot(x, y)\n",
    "plt.title('RSS Loss Function for Various Values of m')\n",
    "plt.show()\n",
    "\n",
    "##the instructor solution:\n",
    "# The algorithm starts at x=1.5\n",
    "cur_x = 1.5 \n",
    "\n",
    "# Initialize a step size\n",
    "alpha = 1*10**(-7)\n",
    "\n",
    "# Initialize a precision\n",
    "precision = 0.0000000001\n",
    "\n",
    "# Helpful initialization\n",
    "previous_step_size = 1 \n",
    "\n",
    "# Maximum number of iterations\n",
    "max_iters = 10000 \n",
    "\n",
    "# Iteration counter\n",
    "iters = 0 \n",
    "\n",
    "# Create a loop to iterate through the algorithm until either the max_iteration or precision conditions is met\n",
    "while (previous_step_size > precision) & (iters < max_iters):\n",
    "    print('Current value: {} RSS Produced: {}'.format(cur_x, rss(cur_x)))\n",
    "    prev_x = cur_x\n",
    "    # Calculate the gradient. This is often done by hand to reduce computational complexity.\n",
    "    # For here, generate points surrounding your current state, then calculate the rss of these points\n",
    "    # Finally, use the np.gradient() method on this survey region. \n",
    "    # This code is provided here to ease this portion of the algorithm implementation\n",
    "    x_survey_region = np.linspace(start = cur_x - previous_step_size , stop = cur_x + previous_step_size , num = 101)\n",
    "    rss_survey_region = [np.sqrt(rss(m)) for m in x_survey_region]\n",
    "    gradient = np.gradient(rss_survey_region)[50] \n",
    "    cur_x -= alpha * gradient # Move opposite the gradient\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "    iters+=1\n",
    "\n",
    "    \n",
    "# The output for the above will be: ('The local minimum occurs at', 1.1124498053361267)    \n",
    "print(\"The local minimum occurs at\", cur_x)\n",
    "\n",
    "##my solution:\n",
    "# Set a starting point\n",
    "cur_x = -3\n",
    "\n",
    "# Initialize a step size\n",
    "alpha = 10**-8\n",
    "\n",
    "# Initialize a precision\n",
    "precision = 0.0000001 \n",
    "\n",
    "# Helpful initialization\n",
    "previous_step_size = 1 \n",
    "\n",
    "# Maximum number of iterations\n",
    "max_iters = 10000 \n",
    "\n",
    "# Iteration counter\n",
    "iters = 0 \n",
    "\n",
    "# Create a loop to iterate through the algorithm until either the max_iteration or precision conditions is met\n",
    "# Your code here; create a loop as described above\n",
    "for i in range(max_iters):\n",
    "    # Calculate the gradient. This is often done by hand to reduce computational complexity.\n",
    "    # For here, generate points surrounding your current state, then calculate the rss of these points\n",
    "    # Finally, use the np.gradient() method on this survey region. \n",
    "    # This code is provided here to ease this portion of the algorithm implementation\n",
    "    x_survey_region = np.linspace(start = cur_x - previous_step_size , stop = cur_x + previous_step_size , num = 101)\n",
    "    rss_survey_region = [np.sqrt(rss(m)) for m in x_survey_region]\n",
    "    gradient = np.gradient(rss_survey_region)[50] \n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print('iter:{}, gradient: {}, alpha*grad:{}, xcur:{}'.format(i, gradient, -gradient*alpha, cur_x))\n",
    "    \n",
    "    # Update the current x, by taking an \"alpha sized\" step in the direction of the gradient\n",
    "    cur_x +=  - alpha * gradient\n",
    "    # Update the iteration number\n",
    "    iters +=1\n",
    "    if abs(alpha * gradient) < precision:\n",
    "        break\n",
    "# The output for the above will be: ('The local minimum occurs at', 1.1124498053361267)    \n",
    "print(\"The local minimum occurs at\", cur_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-coding-logistic-regression-from-scratch\n",
    "# https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "#linear regression setup\n",
    "import numpy as np\n",
    "\n",
    "def predict_y(X, w):\n",
    "    return np.dot(X,w)\n",
    "#sigmoid setup\n",
    "def sigmoid(x):\n",
    "    x = np.array(x)\n",
    "    return 1/(1 + np.e**(-1*x))\n",
    "# plot sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot sigmoid\n",
    "x = np.linspace(start=-20, stop=20, num=10**4)\n",
    "y = [sigmoid(xi) for xi in x]\n",
    "plt.scatter(x, y)\n",
    "plt.title('The Sigmoid Function')\n",
    "plt.show()\n",
    "\n",
    "# design gradient descnet with sigmoid\n",
    "# Your code here\n",
    "def grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n",
    "    \"\"\"Be sure to set default behavior for the initial_weights parameter.\"\"\"\n",
    "    if initial_weights == None:\n",
    "        initial_weights = np.ones((X.shape[1],1)).flatten()\n",
    "    weights_col= pd.DataFrame(initial_weights)\n",
    "    weights = initial_weights\n",
    "    # Create a for loop of iterations\n",
    "    for iteration in range(max_iterations):\n",
    "        # Generate predictions using the current feature weights\n",
    "        predictions = sigmoid(np.dot(X,weights))\n",
    "        # Calculate an error vector based on these initial predictions and the correct labels\n",
    "        error_vector = y - predictions\n",
    "        # Calculate the gradient \n",
    "        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n",
    "        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n",
    "        # For more details on the derivation, see the additional resources section below.\n",
    "        gradient = np.dot(X.transpose(),error_vector)\n",
    "        # Update the weight vector take a step of alpha in direction of gradient \n",
    "        weights += alpha * gradient\n",
    "        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n",
    "    # Return finalized weights\n",
    "    return weights, weights_col\n",
    "\n",
    "#Rrun the algorithm\n",
    "import pandas as pd\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Create the predictor and target variables\n",
    "y = df['target']\n",
    "X = df.drop(columns=['target'], axis=1)\n",
    "\n",
    "print(y.value_counts())\n",
    "X.head()\n",
    "\n",
    "#run grad descent\n",
    "weights, weight_col = grad_desc(X, y, 10000, 0.001)\n",
    "weight_col.columns = np.arange(len(weight_col.columns))\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for (i, j) in enumerate(weights):\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.title(list(X)[i], size='medium')\n",
    "    plt.plot(weight_col.iloc[i].T)\n",
    "    plt.axis('tight')\n",
    "    \n",
    "#same thing, but with skikitlearn\n",
    "# Your code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e16, random_state=2, solver='liblinear')\n",
    "logreg.fit(X, y)\n",
    "#compare the models\n",
    "# Your code here\n",
    "print(\"Scikit-learn's weights:\", logreg.coef_[0])\n",
    "print(\"Our manual regression weights:\", weights)\n",
    "\n",
    "# refactor the grad descent above to include the cost function - then plot it\n",
    "# Your code here\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n",
    "    \"\"\"Be sure to set default behavior for the initial_weights parameter.\"\"\"\n",
    "    if initial_weights == None:\n",
    "        initial_weights = np.ones((X.shape[1],1)).flatten()\n",
    "    weights = initial_weights\n",
    "    costs = []\n",
    "    # Create a for loop of iterations\n",
    "    for iteration in range(max_iterations):\n",
    "        # Generate predictions using the current feature weights\n",
    "        predictions = sigmoid(np.dot(X,weights))\n",
    "        # Calculate an error vector based on these initial predictions and the correct labels\n",
    "        error_vector = y - predictions\n",
    "        # Calculate the gradient (transpose of X times error is the gradient)\n",
    "        gradient = np.dot(X.transpose(),error_vector)\n",
    "        # Update the weight vector take a step of alpha in direction of gradient \n",
    "        weights += alpha * gradient\n",
    "        # Calculate the cost\n",
    "        cost = ((-y * np.log(predictions))-((1-y)* np.log(1-predictions))).mean()\n",
    "        costs.append(cost)\n",
    "    # Return finalized Weights\n",
    "    return weights, costs\n",
    "\n",
    "max_iterations = 50000\n",
    "weights, costs = grad_desc(X, y, max_iterations, 0.001)\n",
    "print('Coefficient weights:\\n', weights)\n",
    "plt.plot(range(max_iterations), costs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-logistic-regression-model-comparisons-lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-logistic-regression-lab.git\n",
    "#end to end ML workflow logistic regression on a tree cover dataset\n",
    "# Run this cell without changes\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/forest_cover.csv')  \n",
    "df\n",
    "\n",
    "# Run this cell without changes\n",
    "print(\"Raw Counts\")\n",
    "print(df[\"Cover_Type\"].value_counts())\n",
    "print()\n",
    "print(\"Percentages\")\n",
    "print(df[\"Cover_Type\"].value_counts(normalize=True))\n",
    "\n",
    "## if we just naively predicted pine, we'd be right...\n",
    "# Replace None with appropriate text\n",
    "\"\"\"\n",
    "It would be correct ~93% of the time.\n",
    "\"\"\"\n",
    "#train test split\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split df into X and y\n",
    "X = df.drop('Cover_Type', axis=1)\n",
    "y = df.Cover_Type\n",
    "\n",
    "# Perform train-test split with random_state=42 and stratify=y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "## check data shape\n",
    "# Run this cell without changes\n",
    "\n",
    "# X and y training data should have the same number of rows\n",
    "assert X_train.shape[0] == y_train.shape[0] and X_train.shape[0] == 28875\n",
    "\n",
    "# X and y testing data should have the same number of rows\n",
    "assert X_test.shape[0] == y_test.shape[0] and X_test.shape[0] == 9626\n",
    "\n",
    "# Both X should have 52 columns\n",
    "assert X_train.shape[1] == X_test.shape[1] and X_train.shape[1] == 52\n",
    "\n",
    "# Both y should have 1 column\n",
    "assert len(y_train.shape) == len(y_test.shape) and len(y_train.shape) == 1\n",
    "\n",
    "## check percentages in train/test split\n",
    "# Run this cell without changes\n",
    "print(\"Train percent cottonwood/willow:\", y_train.value_counts(normalize=True)[1])\n",
    "print(\"Test percent cottonwood/willow: \", y_test.value_counts(normalize=True)[1])\n",
    "\n",
    "## baseline model\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import relevant class and function\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a LogisticRegression with random_state=42\n",
    "baseline_model = LogisticRegression(random_state=42)\n",
    "                 #LogisticRegression(fit_intercept=False, C=1e16, random_state=42, solver='liblinear')\n",
    "\n",
    "# Use cross_val_score with scoring=\"neg_log_loss\" to evaluate the model\n",
    "# on X_train and y_train\n",
    "baseline_neg_log_loss_cv = cross_val_score(baseline_model, X_train, y_train, scoring=\"neg_log_loss\")\n",
    "\n",
    "baseline_log_loss = -(baseline_neg_log_loss_cv.mean())\n",
    "baseline_log_loss\n",
    "\n",
    "##check log loss metric\n",
    "# Run this cell without changes\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "log_loss(y_train, np.zeros(len(y_train)))\n",
    "\n",
    "\n",
    "##now we want a custom cross validation.  the built in one is good, but we can customize for different scaling and \n",
    "#SMOTE parameters by\n",
    "#unpacking cross-val into StratifiedKFold steps\n",
    "\n",
    "# Run this cell without changes\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Negative log loss doesn't exist as something we can import,\n",
    "# but we can create it\n",
    "neg_log_loss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "# Instantiate the model (same as previous example)\n",
    "baseline_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create a list to hold the score from each fold\n",
    "kfold_scores = np.ndarray(5)\n",
    "\n",
    "# Instantiate a splitter object and loop over its result\n",
    "kfold = StratifiedKFold()\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # Extract train and validation subsets using the provided indices\n",
    "    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Clone the provided model and fit it on the train subset\n",
    "    temp_model = clone(baseline_model)\n",
    "    temp_model.fit(X_t, y_t)\n",
    "    \n",
    "    # Evaluate the provided model on the validation subset\n",
    "    neg_log_loss_score = neg_log_loss(temp_model, X_val, y_val)\n",
    "    kfold_scores[fold] = neg_log_loss_score\n",
    "    \n",
    "-(kfold_scores.mean())\n",
    "\n",
    "##this produces same as original.  so now we can customize\n",
    "# Run this cell without changes\n",
    "print(baseline_neg_log_loss_cv)\n",
    "print(kfold_scores)\n",
    "\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import relevant sklearn and imblearn classes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def custom_cross_val_score(estimator, X, y):\n",
    "    # Create a list to hold the scores from each fold\n",
    "    kfold_train_scores = np.ndarray(5)\n",
    "    kfold_val_scores = np.ndarray(5)\n",
    "\n",
    "    # Instantiate a splitter object and loop over its result\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X, y)):\n",
    "        # Extract train and validation subsets using the provided indices\n",
    "        X_t, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_t, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        # Instantiate StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        # Fit and transform X_t\n",
    "        X_t_scaled = scaler.fit(X_t).transform(X_t)\n",
    "        # Transform X_val\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        # Instantiate SMOTE with random_state=42 and sampling_strategy=0.28\n",
    "        sm = SMOTE(random_state=42, sampling_strategy=0.28)\n",
    "        # Fit and transform X_t_scaled and y_t using sm\n",
    "        X_t_oversampled, y_t_oversampled = sm.fit_resample(X_t_scaled, y_t)\n",
    "        \n",
    "        # Clone the provided model and fit it on the train subset\n",
    "        temp_model = clone(estimator)\n",
    "        temp_model.fit(X_t_oversampled, y_t_oversampled)\n",
    "        \n",
    "        # Evaluate the provided model on the train and validation subsets\n",
    "        neg_log_loss_score_train = neg_log_loss(temp_model, X_t_oversampled, y_t_oversampled)\n",
    "        neg_log_loss_score_val = neg_log_loss(temp_model, X_val_scaled, y_val)\n",
    "        kfold_train_scores[fold] = neg_log_loss_score_train\n",
    "        kfold_val_scores[fold] = neg_log_loss_score_val\n",
    "        \n",
    "    return kfold_train_scores, kfold_val_scores\n",
    "        \n",
    "model_with_preprocessing = LogisticRegression(random_state=42, class_weight={1: 0.28})\n",
    "preprocessed_train_scores, preprocessed_neg_log_loss_cv = custom_cross_val_score(model_with_preprocessing, X_train, y_train)\n",
    "- (preprocessed_neg_log_loss_cv.mean())\n",
    "## wow, no more convergence warning\n",
    "\n",
    "##compare to baseline\n",
    "# Run this cell without changes\n",
    "print(-baseline_neg_log_loss_cv.mean())\n",
    "print(-preprocessed_neg_log_loss_cv.mean())\n",
    "\n",
    "#evaluate model, check if over/underfitting, refine\n",
    "# Run this cell without changes\n",
    "print(\"Train:     \", -preprocessed_train_scores)\n",
    "print(\"Validation:\", -preprocessed_neg_log_loss_cv)\n",
    "# Replace None with appropriate text\n",
    "\"\"\"\n",
    "surprisingly no, our train loss is higher than our validatoin loss; this is good news for overfitting, but likely \n",
    "means that we are underfitting.\n",
    "\"\"\"\n",
    "\n",
    "# Run this cell without changes\n",
    "model_with_preprocessing.get_params()\n",
    "\n",
    "##reducing regularization (C parameter)\n",
    "# Replace None with appropriate code\n",
    "\n",
    "model_less_regularization = LogisticRegression(\n",
    "    random_state=42,\n",
    "    class_weight={1: 0.28},\n",
    "    C=1e5\n",
    ")\n",
    "# Run this cell without changes\n",
    "\n",
    "# Check variable type\n",
    "assert type(model_less_regularization) == LogisticRegression\n",
    "\n",
    "# Check params\n",
    "assert model_less_regularization.get_params()[\"random_state\"] == 42\n",
    "assert model_less_regularization.get_params()[\"class_weight\"] == {1: 0.28}\n",
    "assert model_less_regularization.get_params()[\"C\"] != 1.0\n",
    "\n",
    "\n",
    "\n",
    "# Replace None with appropriate code\n",
    "less_regularization_train_scores, less_regularization_val_scores = custom_cross_val_score(model_less_regularization, \n",
    "                                                                                          X_train, y_train)\n",
    "\n",
    "print(\"Previous Model\")\n",
    "print(\"Train average:     \", -preprocessed_train_scores.mean())\n",
    "print(\"Validation average:\", -preprocessed_neg_log_loss_cv.mean())\n",
    "print(\"Current Model\")\n",
    "print(\"Train average:     \", -less_regularization_train_scores.mean())\n",
    "print(\"Validation average:\", -less_regularization_val_scores.mean())\n",
    "\n",
    "# Run this cell without changes\n",
    "print(\"solver:\", model_less_regularization.get_params()[\"solver\"])\n",
    "print(\"penalty:\", model_less_regularization.get_params()[\"penalty\"])\n",
    "\n",
    "'''What if we want to try a different kind of regularization penalty?\n",
    "\n",
    "From the docs:\n",
    "\n",
    "‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n",
    "‘liblinear’ and ‘saga’ also handle L1 penalty\n",
    "‘saga’ also supports ‘elasticnet’ penalty\n",
    "In other words, the only models that support L1 or elastic net penalties are liblinear and saga. liblinear is going to be quite slow with the size of our dataset, so let's use saga.\n",
    "\n",
    "In the cell below, create a model that uses solver=\"saga\" and penalty=\"elasticnet\". Then use the l1_ratio argument to specify the mixing of L1 and L2 regularization. You want a value greater than zero (zero would just mean using L2 regularization) and less than one (one would mean using just L1 regularization).\n",
    "\n",
    "Remember to also specify random_state=42, class_weight={1: 0.28}, and C equals the value you previously used.\n",
    "'''\n",
    "\n",
    "# Replace None with appropriate code\n",
    "model_alternative_solver = LogisticRegression(\n",
    "    random_state=42,\n",
    "    class_weight={1: 0.28},\n",
    "    C=1e5,\n",
    "    solver='saga', penalty=\"elasticnet\", l1_ratio=0.5\n",
    ")\n",
    "\n",
    "alternative_solver_train_scores, alternative_solver_val_scores = custom_cross_val_score(\n",
    "    model_alternative_solver,\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "print(\"Previous Model (Less Regularization)\")\n",
    "print(\"Train average:     \", -less_regularization_train_scores.mean())\n",
    "print(\"Validation average:\", -less_regularization_val_scores.mean())\n",
    "print(\"Current Model\")\n",
    "print(\"Train average:     \", -alternative_solver_train_scores.mean())\n",
    "print(\"Validation average:\", -alternative_solver_val_scores.mean())\n",
    "\n",
    "##adjusting gradient descent params\n",
    "# Replace None with appropriate code\n",
    "model_more_iterations = LogisticRegression(\n",
    "    random_state=42,\n",
    "    class_weight={1: 0.28},\n",
    "    C=1e5,\n",
    "    solver='saga', penalty=\"elasticnet\", l1_ratio=0.5,\n",
    "    max_iter=1e4\n",
    ")\n",
    "\n",
    "more_iterations_train_scores, more_iterations_val_scores = custom_cross_val_score(\n",
    "    model_more_iterations,\n",
    "    X_train,\n",
    "    y_train, \n",
    ")\n",
    "\n",
    "print(\"Previous Best Model (Less Regularization)\")\n",
    "print(\"Train average:     \", -less_regularization_train_scores.mean())\n",
    "print(\"Validation average:\", -less_regularization_val_scores.mean())\n",
    "print(\"Previous Model with This Solver\")\n",
    "print(\"Train average:     \", -alternative_solver_train_scores.mean())\n",
    "print(\"Validation average:\", -alternative_solver_val_scores.mean())\n",
    "print(\"Current Model\")\n",
    "print(\"Train average:     \", -more_iterations_train_scores.mean())\n",
    "print(\"Validation average:\", -more_iterations_val_scores.mean())\n",
    "\n",
    "\n",
    "##choose a final model\n",
    "# Run this cell without changes\n",
    "final_model = model_less_regularization\n",
    "\n",
    "##preprocess dataset\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit and transform X_train\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# Transform X_test\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate SMOTE with random_state=42 and sampling_strategy=0.28\n",
    "sm = SMOTE(random_state=42, sampling_strategy=0.28)\n",
    "# Fit and transform X_train_scaled and y_train using sm\n",
    "X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "##fit on full training data\n",
    "# Run this cell without changes\n",
    "final_model.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "##eval on test data\n",
    "# Run this cell without changes\n",
    "log_loss(y_test, final_model.predict_proba(X_test_scaled))\n",
    "\n",
    "#metrics\n",
    "# Replace None with appropriate code\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, final_model.predict(X_test_scaled))\n",
    "\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant function\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Display the precision score\n",
    "precision_score(y_test, final_model.predict(X_test_scaled))\n",
    "\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the relevant function\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Display the recall score\n",
    "recall_score(y_test, final_model.predict(X_test_scaled))\n",
    "\n",
    "##adjusting decision threshold\n",
    "# Run this cell without changes\n",
    "\n",
    "def final_model_func(model, X):\n",
    "    \"\"\"\n",
    "    Custom function to predict probability of\n",
    "    cottonwood/willow. If the model says there\n",
    "    is >1% chance, we label it as class 1\n",
    "    \"\"\"\n",
    "    probs = model.predict_proba(X)[:,1]\n",
    "    return [int(prob > 0.01) for prob in probs]\n",
    "\n",
    "# Calculate predictions with adjusted threshold and\n",
    "# display proportions of predictions\n",
    "threshold_adjusted_probs = pd.Series(final_model_func(final_model, X_test_scaled))\n",
    "threshold_adjusted_probs.value_counts(normalize=True)\n",
    "\n",
    "# Run this cell without changes\n",
    "print(\"Accuracy:\", accuracy_score(y_test, threshold_adjusted_probs))\n",
    "print(\"Recall:  \", recall_score(y_test, threshold_adjusted_probs))\n",
    "'''This means that we are able to identify 99.1% of the true positives (i.e. 99.1% of the cottonwood/willow cells are identified). However this comes at a cost; our overall accuracy is now 65.7% instead of over 90%.\n",
    "\n",
    "So we are classifying over 40% of the cells as cottonwood/willow, even though fewer than 10% of the cells are actually that category, in order to miss as few true positives as possible. Even though this seems fairly extreme, our model is still better than just choosing class 1 every time (that model would have about 7% accuracy).'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://github.com/ericthansen/dsc-mle-log-reg-recap.git\n",
    "# https://github.com/ericthansen/dsc-knn-intro\n",
    "#  https://github.com/ericthansen/dsc-distance-metrics.git\n",
    "# https://github.com/ericthansen/dsc-distance-metrics-lab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Complete this function! \n",
    "def distance(a, b, c=2, verbose=True):\n",
    "    if len(a) != len(b):\n",
    "        return None\n",
    "    x = np.array(a)\n",
    "    y = np.array(b)\n",
    "    if verbose:\n",
    "        if c==1:\n",
    "            met = 'Manhattan'\n",
    "        elif c==2:\n",
    "            met = 'Euclidean'\n",
    "        else:\n",
    "            met = \"Minkowski-\"+str(c)\n",
    "        print(\"The metric returned is {} distance.\".format(met))\n",
    "    d = np.power(np.sum(np.power(np.abs(x-y),c)), 1/c)\n",
    "    return d\n",
    "\n",
    "\n",
    "test_point_1 = (1, 2)\n",
    "test_point_2 = (4, 6)\n",
    "print(distance(test_point_1, test_point_2)) # Expected Output: 5.0\n",
    "print(distance(test_point_1, test_point_2, c=1)) # Expected Output: 7.0\n",
    "print(distance(test_point_1, test_point_2, c=3)) # Expected Output: 4.497941445275415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-k-nearest-neighbors\n",
    "# https://github.com/ericthansen/dsc-k-nearest-neighbors-lab\n",
    "## simple k-nearest-neighbors from scratch\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "\n",
    "# Define the KNN class with two empty methods - fit and predict\n",
    "class KNN():\n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self):\n",
    "        pass\n",
    "    \n",
    "def fit(self, X_train, y_train):\n",
    "    self.X_train = np.array(X_train)\n",
    "    self.y_train = np.array(y_train)\n",
    "    \n",
    "# This line updates the knn.fit method to point to the function you've just written\n",
    "KNN.fit = fit\n",
    "\n",
    "##helper functions\n",
    "def _get_distances(self, x):\n",
    "    distances = []\n",
    "    for n, v in enumerate(self.X_train):\n",
    "        distances.append((n, euclidean(v, x)))\n",
    "    return distances\n",
    "\n",
    "# This line attaches the function you just created as a method to KNN class \n",
    "KNN._get_distances = _get_distances\n",
    "\n",
    "\n",
    "def _get_k_nearest(self, dists, k):\n",
    "    dsorted = sorted(dists, key=lambda x:x[1])\n",
    "    return dsorted[:k]\n",
    "\n",
    "# This line attaches the function you just created as a method to KNN class \n",
    "KNN._get_k_nearest = _get_k_nearest\n",
    "\n",
    "def _get_label_prediction(self, k_nearest):\n",
    "    labels = [self.y_train[i] for i, _ in k_nearest]\n",
    "    counts = np.bincount(labels)\n",
    "    return np.argmax(counts)\n",
    "\n",
    "# This line attaches the function you just created as a method to KNN class\n",
    "KNN._get_label_prediction = _get_label_prediction\n",
    "\n",
    "def predict(self,X_test, k=3):\n",
    "    preds = []\n",
    "    for item in X_test:\n",
    "        distances = self._get_distances(item)\n",
    "        k_nearest = self._get_k_nearest(distances, k)\n",
    "        label = self._get_label_prediction(k_nearest)\n",
    "        preds.append(label)\n",
    "    return preds\n",
    "\n",
    "# This line updates the knn.predict method to point to the function you've just written\n",
    "KNN.predict = predict\n",
    "\n",
    "##testing the classifier\n",
    "# Import the necessary functions\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25, random_state=0)\n",
    "# Instantiate and fit KNN\n",
    "knn = KNN()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "preds = knn.predict(X_test)\n",
    "#display(preds)\n",
    "print(\"Testing Accuracy: {}\".format(accuracy_score(y_test, preds)))\n",
    "# Expected Output: Testing Accuracy: 0.9736842105263158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-finding-the-best-value-for-k\n",
    "\n",
    "# https://github.com/ericthansen/dsc-knn-with-scikit-learn\n",
    "\n",
    "# https://github.com/ericthansen/dsc-knn-with-scikit-learn-lab\n",
    "# Your code here\n",
    "# Import pandas and set the standard alias \n",
    "import pandas as pd\n",
    "\n",
    "# Import the data from 'titanic.csv' and store it in a pandas DataFrame \n",
    "raw_df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Print the head of the DataFrame to ensure everything loaded correctly \n",
    "raw_df.head()\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "df = raw_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "df.head()\n",
    "\n",
    "# Find the number of missing values in each column\n",
    "display(df.info())\n",
    "df.isna().sum()\n",
    "# Impute the missing values in 'Age'\n",
    "df['Age'] = df['Age'].fillna(df.Age.median())\n",
    "df.isna().sum()\n",
    "# Drop the rows missing values in the 'Embarked' column\n",
    "df = df.dropna()\n",
    "df.isna().sum()\n",
    "# One-hot encode the categorical columns\n",
    "one_hot_df = pd.get_dummies(df)\n",
    "one_hot_df.head()\n",
    "# Assign the 'Survived' column to labels\n",
    "labels = df['Survived']\n",
    "\n",
    "# Drop the 'Survived' column from one_hot_df\n",
    "one_hot_df = one_hot_df.drop('Survived', axis=1)\n",
    "# Import train_test_split \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_df, labels, test_size=0.25, random_state=42)\n",
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert into a DataFrame\n",
    "scaled_df_train = pd.DataFrame(scaled_data_train)\n",
    "scaled_df_train.head()\n",
    "\n",
    "##fit KNN model\n",
    "# Import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instantiate KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(scaled_data_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "test_preds = clf.predict(scaled_data_test)\n",
    "\n",
    "##eval the model\n",
    "# Your code here \n",
    "# Import the necessary functions\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "# Complete the function\n",
    "def print_metrics(labels, preds):\n",
    "    print(\"Precision Score: {}\".format(precision_score(labels, preds)))\n",
    "    print(\"Recall Score: {}\".format(recall_score(labels, preds)))\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(labels, preds)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(labels, preds)))\n",
    "    \n",
    "print_metrics(y_test, test_preds)\n",
    "'''precision: true positive predictions / all predicted positives.  Having a large number of false positives would reduce this.\n",
    "recall: true positive predictions / all actual positives.  If we miss a lot of actual positives, this metric would be reduced.\n",
    "\n",
    "accuracy: correct predictions/ all predictions.  Pretty intuitive, a good overall descriptor of how well the model predicts correctly.  \n",
    "\n",
    "F1:  2*(Recall * Precision) / (Recall + Precision).  Weighted avg of precision and recall.  Less intuitive, but does take into account accuracy of both positives and negatives.\n",
    "\n",
    "For this model, the accuracy of 78.9% is encouraging, however, F1 is lower so indicates we might be biased toward one category or the other.  This isn't all that much better than naively guessing 0 survival (62% accuracy).'''\n",
    "labels.value_counts(normalize=True)\n",
    "#note that just by naively guessing 0 all the time, we'd get accuracy of 62%\n",
    "\n",
    "##improve model perf - search for better k\n",
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "    if min_k % 2==0:\n",
    "        min_k+=1\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "    \n",
    "    print(\"Best Value for k: {}\".format(best_k))\n",
    "    print(\"F1-Score: {}\".format(best_score))\n",
    "\n",
    "find_best_k(scaled_data_train, y_train, scaled_data_test, y_test)\n",
    "# Expected Output:\n",
    "\n",
    "# Best Value for k: 17\n",
    "# F1-Score: 0.7468354430379746\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-knn-recap\n",
    "# https://github.com/ericthansen/dsc-bayesian-classification-intro-v2-1\n",
    "# https://github.com/ericthansen/dsc-classifiers-with-bayes\n",
    "# https://github.com/ericthansen/dsc-gaussian-naive-bayes\n",
    "'''uses multivariable bayes to set up relative probabilities of different outcomes given an observation\n",
    "we see the pdf formula again\n",
    "𝑃(𝑥𝑖|𝑦)=12𝜋𝜎2𝑖⎯⎯⎯⎯⎯⎯⎯⎯√𝑒−(𝑥−𝜇𝑖)22𝜎2𝑖\n",
    "'''\n",
    "##load the dataset\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = pd.DataFrame(iris.data)\n",
    "X.columns = iris.feature_names\n",
    "\n",
    "y = pd.DataFrame(iris.target)\n",
    "y.columns = ['Target']\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df.head()\n",
    "df['Target'].value_counts()\n",
    "## calc descriptives for  each feature\n",
    "aggs = df.groupby('Target').agg(['mean', 'std'])\n",
    "aggs\n",
    "\n",
    "##calculate conditional prob point estimates\n",
    "from scipy import stats\n",
    "\n",
    "def p_x_given_class(obs_row, feature, class_):\n",
    "    mu = aggs[feature]['mean'][class_]\n",
    "    std = aggs[feature]['std'][class_]\n",
    "    \n",
    "    # A single observation\n",
    "    obs = df.iloc[obs_row][feature] \n",
    "    \n",
    "    p_x_given_y = stats.norm.pdf(obs, loc=mu, scale=std)\n",
    "    return p_x_given_y\n",
    "\n",
    "# Notice how this is not a true probability; you can get values > 1\n",
    "p_x_given_class(0, 'petal length (cm)', 0) \n",
    "\n",
    "##multinomial bayes\n",
    "row = 100\n",
    "c_probs = []\n",
    "for c in range(3):\n",
    "    # Initialize probability to relative probability of class \n",
    "    p = len(df[df['Target'] == c])/len(df) \n",
    "    for feature in X.columns:\n",
    "        p *= p_x_given_class(row, feature, c) \n",
    "        # Update the probability using the point estimate for each feature\n",
    "        c_probs.append(p)\n",
    "\n",
    "c_probs\n",
    "\n",
    "##note that the denominator here doesn't matter, it's the same for every class, so just pick the result from probs that's largest\n",
    "def predict_class(row):\n",
    "    c_probs = []\n",
    "    for c in range(3):\n",
    "        # Initialize probability to relative probability of class\n",
    "        p = len(df[df['Target'] == c])/len(df) \n",
    "        for feature in X.columns:\n",
    "            p *= p_x_given_class(row, feature, c)\n",
    "        c_probs.append(p)\n",
    "    return np.argmax(c_probs)\n",
    "\n",
    "#an example\n",
    "row = 0\n",
    "df.iloc[row]\n",
    "\n",
    "predict_class(row)\n",
    "\n",
    "#calculating accuracy \n",
    "df['Predictions'] =  [predict_class(row) for row in df.index]\n",
    "df['Correct?'] = df['Target'] == df['Predictions']\n",
    "df['Correct?'].value_counts(normalize=True)\n",
    "\n",
    "#it does pretty well - 96%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-gaussian-naive-bayes-lab\n",
    "# and https://github.com/ericthansen/dsc-gaussian-naive-bayes-lab/tree/solution\n",
    "#load\n",
    "# Your code here \n",
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('heart.csv')\n",
    "df.head()\n",
    "#df.target.value_counts()\n",
    "# Your code here\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "##split\n",
    "# Your code here\n",
    "# Perform a train-test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=22)\n",
    "#get mean/std for each feature\n",
    "# Your code here \n",
    "# Calculate the mean and standard deviation for each feature within each class for the training set\n",
    "aggs = df.groupby('target').agg(['mean', 'std'])\n",
    "aggs\n",
    "#aggs['age']['std'][0]\n",
    "\n",
    "#get point estimate of PDF at each feature for a given class\n",
    "# Your code here\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import math\n",
    "# mu=aggs['age']['mean'][1]\n",
    "# std=aggs['age']['std'][1]\n",
    "# obs=df.iloc[1]['age'] \n",
    "# print(mu, std, obs)\n",
    "# display(stats.norm.pdf(obs, loc=mu, scale=std))\n",
    "# display(1/np.sqrt(2*math.pi*std**2) * math.exp(-(obs-mu)**2/(2*std**2)))\n",
    "def pdf(obs, loc, scale):\n",
    "    mu=loc\n",
    "    std=scale\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    import math\n",
    "    return 1/np.sqrt(2*math.pi*std**2) * math.exp(-(obs-mu)**2/(2*std**2))\n",
    "# pdf(obs, mu, std)\n",
    "\n",
    "\n",
    "##define a prediction function\n",
    "# Your code here\n",
    "from scipy import stats\n",
    "\n",
    "def p_x_given_class(obs_row, feature, class_):\n",
    "    mu = aggs[feature]['mean'][class_]\n",
    "    std = aggs[feature]['std'][class_]\n",
    "    \n",
    "    # Observation\n",
    "    obs = obs_row[feature] \n",
    "    \n",
    "    p_x_given_y = stats.norm.pdf(obs, loc=mu, scale=std)\n",
    "    return p_x_given_y\n",
    "p_x_given_class(X_train.iloc[0], X.columns[0], 0)\n",
    "\n",
    "#p_x_given_class(1, 'age', 1) \n",
    "\n",
    "\n",
    "def predict_class(obs_row):\n",
    "    c_probs = []\n",
    "    for c in range(2):\n",
    "        # Initialize probability to relative probability of class\n",
    "        p = len(y_train[y_train == c])/len(y_train) \n",
    "        for feature in X.columns:\n",
    "            p *= p_x_given_class(obs_row, feature, c)\n",
    "        c_probs.append(p)\n",
    "    return np.argmax(c_probs)\n",
    "##apply predictoin function to train/test sets\n",
    "y_hat_train = [predict_class(X_train.iloc[idx]) for idx in range(len(X_train))]\n",
    "y_hat_test = [predict_class(X_test.iloc[idx]) for idx in range(len(X_test))]\n",
    "##calculate accuracy\n",
    "residuals_train = y_hat_train == y_train\n",
    "acc_train = residuals_train.sum()/len(residuals_train)\n",
    "\n",
    "residuals_test = y_hat_test == y_test\n",
    "acc_test = residuals_test.sum()/len(residuals_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(acc_train, acc_test))\n",
    "\n",
    "### note that pdf is technically 0 at a point; the above is an estimate.  technically correct to take pdf over a \n",
    "    #range; ie between values of CDF.\n",
    "def p_band_x_given_class(obs_row, feature, c, range_width_std):\n",
    "    \"\"\"obs_row is the observation in question \n",
    "    feature is the feature of the observation row for which you are calculating a conditional probability \n",
    "    c is the class flag for the conditional probability \n",
    "    range_width_std is the range in standard deviations of the feature variable to calculate the integral under the PDF curve for\"\"\"\n",
    "    # Your code here \n",
    "    class_=c\n",
    "    mu = aggs[feature]['mean'][class_]\n",
    "    std = aggs[feature]['std'][class_]\n",
    "    \n",
    "    # A single observation\n",
    "    obs = obs_row[feature] \n",
    "    min_obs = obs - range_width_std*std/2\n",
    "    max_obs = obs + range_width_std*std/2\n",
    "    \n",
    "    cdf_min = stats.norm.cdf(min_obs, loc=mu, scale=std)\n",
    "    cdf_max = stats.norm.cdf(max_obs, loc=mu, scale=std)\n",
    "    \n",
    "    p_x_given_y = pdf(obs, loc=mu, scale=std)\n",
    "    \n",
    "    return p_x_given_y\n",
    "##update the prediction function\n",
    "# Your code here\n",
    "# Update the prediction function\n",
    "def predict_class(obs_row, method='bands', range_width_std=.25):\n",
    "    c_probs = []\n",
    "    for c in range(2):\n",
    "        # Initialize probability to relative probability of class\n",
    "        p = len(y_train[y_train == c])/len(y_train) \n",
    "        for feature in X.columns:\n",
    "            if method == 'bands':\n",
    "                p *= p_band_x_given_class(obs_row, feature, c, range_width_std=range_width_std)\n",
    "            else:\n",
    "                p *= p_x_given_class(obs_row, feature, c)\n",
    "        c_probs.append(p)\n",
    "    return np.argmax(c_probs)\n",
    "##experiment with different widths - this takes a few minutes\n",
    "# Your code here\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "range_stds = np.linspace(0.1, 2, num=21)\n",
    "for range_std in range_stds:    \n",
    "    y_hat_train = [predict_class(X_train.iloc[idx], range_width_std=range_std) for idx in range(len(X_train))]\n",
    "    y_hat_test = [predict_class(X_test.iloc[idx], range_width_std=range_std) for idx in range(len(X_test))]\n",
    "    \n",
    "    residuals_train = y_hat_train == y_train\n",
    "    acc_train = residuals_train.sum()/len(residuals_train)\n",
    "\n",
    "    residuals_test = y_hat_test == y_test\n",
    "    acc_test = residuals_test.sum()/len(residuals_test)\n",
    "    \n",
    "    train_accs.append(acc_train)\n",
    "    test_accs.append(acc_test)\n",
    "plt.plot(range_stds, train_accs, label='Train Accuracy')\n",
    "plt.plot(range_stds, test_accs, label='Test Accuracy')\n",
    "plt.title('Train and Test Accruaccy Versus Various Standard Deviation Bin Ranges for GNB')\n",
    "plt.ylabel('Classifier Accuracy')\n",
    "plt.xlabel('Standard Deviations Used for Integral Band Width')\n",
    "plt.legend(loc=(1.01,.85));\n",
    "\n",
    "### appendix: neat pdf graph and probability intervals\n",
    "temp = df[df['target'] == 1]['trestbps']\n",
    "aggs = temp.agg(['mean', 'std'])\n",
    "aggs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "sns.set_style('white')\n",
    "x = np.linspace(temp.min(), temp.max(), num=10**3)\n",
    "pdf = stats.norm.pdf(x, loc=aggs['mean'], scale=aggs['std'])\n",
    "xi = 145\n",
    "width = 2\n",
    "xi_lower = xi - width/2\n",
    "xi_upper = xi + width/2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(x, pdf)\n",
    "\n",
    "# Make the shaded region\n",
    "ix = np.linspace(xi_lower, xi_upper)\n",
    "iy = stats.norm.pdf(ix, loc=aggs['mean'], scale=aggs['std'])\n",
    "verts = [(xi_lower, 0), *zip(ix, iy), (xi_upper, 0)]\n",
    "poly = Polygon(verts, facecolor='0.9', edgecolor='0.5')\n",
    "ax.add_patch(poly);\n",
    "\n",
    "plt.plot((145, 145), (0, stats.norm.pdf(145, loc=aggs['mean'], scale=aggs['std'])), linestyle='dotted')\n",
    "p_area = stats.norm.cdf(xi_upper, loc=aggs['mean'], scale=aggs['std']) - stats.norm.cdf(xi_lower, loc=aggs['mean'], scale=aggs['std'])\n",
    "print('Probability of Blood Pressure Falling withing Range for the Given Class: {}'.format(p_area))\n",
    "plt.title('Conditional Probability of Resting Blood Pressure ~145 for Those With Heart Disease')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.xlabel('Resting Blood Pressure')\n",
    "## https://matplotlib.org/gallery/showcase/integral.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-document-classification-with-naive-bayes\n",
    "\n",
    " # original kaggle: https://www.kaggle.com/extralime/math-lectures\n",
    "## bag of words, still using naive bayes\n",
    "## doing document analysis\n",
    "#sample:\n",
    "doc = \"Thomas Bayes was born in the early 1700s, although his exact date of birth is unknown. As a Presbyterian in England, he took an unconventional approach to education for his day since Oxford and Cambridge were tied to the Church of England.\"\n",
    "bag = {}\n",
    "for word in doc.split():\n",
    "    # Get the previous entry, or 0 if not yet documented; add 1\n",
    "    bag[word] = bag.get(word, 0) + 1 \n",
    "bag\n",
    "\n",
    "### see the math in the file - just be aware that it uses Laplacian Smoothing (adding 1 to numerator and num of words in corpus\n",
    "    # to the denominator) to avoid multiply or divide by 0 errors.\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('raw_text.csv')\n",
    "df.head()\n",
    "df['label'].value_counts()\n",
    "##do a 2 class case\n",
    "df2 = df[df['label'].isin(['Algorithms', 'Statistics'])]\n",
    "df2['label'].value_counts()\n",
    "p_classes = dict(df2['label'].value_counts(normalize=True))\n",
    "p_classes\n",
    "df2.iloc[0]\n",
    "##split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df2['text']\n",
    "y = df2['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)\n",
    "train_df = pd.concat([X_train, y_train], axis=1) \n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "##check freq in each class\n",
    "# Will be a nested dictionary of class_i : {word1:freq, word2:freq..., wordn:freq},.... class_m : {}\n",
    "class_word_freq = {} \n",
    "classes = train_df['label'].unique()\n",
    "for class_ in classes:\n",
    "    temp_df = train_df[train_df.label == class_]\n",
    "    bag = {}\n",
    "    for row in temp_df.index:\n",
    "        doc = temp_df['text'][row]\n",
    "        for word in doc.split():\n",
    "            bag[word] = bag.get(word, 0) + 1\n",
    "    class_word_freq[class_] = bag\n",
    "##count total corpus words\n",
    "vocabulary = set()\n",
    "for text in train_df['text']:\n",
    "    for word in text.split():\n",
    "        vocabulary.add(word)\n",
    "V = len(vocabulary)\n",
    "V\n",
    "\n",
    "## create bag of words function\n",
    "def bag_it(doc):\n",
    "    bag = {}\n",
    "    for word in doc.split():\n",
    "        bag[word] = bag.get(word, 0) + 1\n",
    "    return bag\n",
    "\n",
    "##implement naive bayes\n",
    "import numpy as np\n",
    "def classify_doc(doc, class_word_freq, p_classes, V, return_posteriors=False):\n",
    "    bag = bag_it(doc)\n",
    "    classes = []\n",
    "    posteriors = []\n",
    "    for class_ in class_word_freq.keys():\n",
    "        p = p_classes[class_]\n",
    "        for word in bag.keys():\n",
    "            num = bag[word]+1\n",
    "            denom = class_word_freq[class_].get(word, 0) + V\n",
    "            p *= (num/denom)\n",
    "        classes.append(class_)\n",
    "        posteriors.append(p)\n",
    "    if return_posteriors:\n",
    "        print(posteriors)\n",
    "    return classes[np.argmax(posteriors)]\n",
    "classify_doc(train_df.iloc[0]['text'], class_word_freq, p_classes, V, return_posteriors=True)\n",
    "##note that the result here is 0, 0.  this is due to multiplying near-0 probabilities.  can avoid this by taking ln() of each\n",
    "##modify the classifier\n",
    "def classify_doc(doc, class_word_freq, p_classes, V, return_posteriors=False):\n",
    "    bag = bag_it(doc)\n",
    "    classes = []\n",
    "    posteriors = []\n",
    "    for class_ in class_word_freq.keys():\n",
    "        p = np.log(p_classes[class_])\n",
    "        for word in bag.keys():\n",
    "            num = bag[word]+1\n",
    "            denom = class_word_freq[class_].get(word, 0) + V\n",
    "            p += np.log(num/denom)##here is the changed line\n",
    "        classes.append(class_)\n",
    "        posteriors.append(p)\n",
    "    if return_posteriors:\n",
    "        print(posteriors)\n",
    "    return classes[np.argmax(posteriors)]\n",
    "\n",
    "classify_doc(train_df.iloc[0]['text'], class_word_freq, p_classes, V, return_posteriors=True)\n",
    "##now we get non-0 odds\n",
    "\n",
    "y_hat_train = X_train.map(lambda x: classify_doc(x, class_word_freq, p_classes, V))\n",
    "residuals = y_train == y_hat_train\n",
    "residuals.value_counts(normalize=True)\n",
    "##note that this does 49% accuracy.. pretty bad, but at least we learned the concept!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-document-classification-with-naive-bayes-lab\n",
    "# https://github.com/ericthansen/dsc-document-classification-with-naive-bayes-lab/tree/solution\n",
    "# Import the data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('SMSSpamCollection', sep='\\t', names=['label', 'text'])\n",
    "df.head()\n",
    "\n",
    "##account for class imbalance\n",
    "# Your code here\n",
    "minority = df[df['label'] == 'spam']\n",
    "undersampled_majority = df[df['label'] == 'ham'].sample(n=len(minority))\n",
    "df2 = pd.concat([minority, undersampled_majority])\n",
    "df2.label.value_counts()\n",
    "\n",
    "p_classes = dict(df2['label'].value_counts(normalize=True))\n",
    "p_classes\n",
    "##tt split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df2['text']\n",
    "y = df2['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)\n",
    "train_df = pd.concat([X_train, y_train], axis=1) \n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "##create word freq dict for each class\n",
    "# Will be a nested dictionary of class_i : {word1:freq, word2:freq..., wordn:freq},.... class_m : {}\n",
    "class_word_freq = {} \n",
    "classes = train_df['label'].unique()\n",
    "for class_ in classes:\n",
    "    temp_df = train_df[train_df['label'] == class_]\n",
    "    bag = {}\n",
    "    for row in temp_df.index:\n",
    "        doc = temp_df['text'][row]\n",
    "        for word in doc.split():\n",
    "            bag[word] = bag.get(word, 0) + 1\n",
    "    class_word_freq[class_] = bag\n",
    "    \n",
    "##count total unique corpus words\n",
    "vocabulary = set()\n",
    "for text in train_df['text']:\n",
    "    for word in text.split():\n",
    "        vocabulary.add(word)\n",
    "V = len(vocabulary)\n",
    "V\n",
    "\n",
    "##create bag of words function\n",
    "def bag_it(doc):\n",
    "    bag = {}\n",
    "    for word in doc.split():\n",
    "        bag[word] = bag.get(word, 0) + 1\n",
    "    return bag\n",
    "##implement naive bayes\n",
    "def classify_doc(doc, class_word_freq, p_classes, V, return_posteriors=False):\n",
    "    bag = bag_it(doc)\n",
    "    classes = []\n",
    "    posteriors = []\n",
    "    for class_ in class_word_freq.keys():\n",
    "        p = np.log(p_classes[class_])\n",
    "        for word in bag.keys():\n",
    "            num = bag[word]+1\n",
    "            denom = class_word_freq[class_].get(word, 0) + V\n",
    "            p += np.log(num/denom)\n",
    "        classes.append(class_)\n",
    "        posteriors.append(p)\n",
    "    if return_posteriors:\n",
    "        print(posteriors)\n",
    "    return classes[np.argmax(posteriors)]\n",
    "\n",
    "##test classifier\n",
    "import numpy as np\n",
    "\n",
    "y_hat_train = X_train.map(lambda x: classify_doc(x, class_word_freq, p_classes, V))\n",
    "residuals = y_train == y_hat_train\n",
    "residuals.value_counts(normalize=True)\n",
    "##not too bad ish.\n",
    "\n",
    "\n",
    "# https://github.com/ericthansen/dsc-bayesian-classification-recap-v2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-decision-trees-section-intro\n",
    "# http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "# https://github.com/ericthansen/dsc-entropy-and-information-gain\n",
    "# https://github.com/ericthansen/dsc-ID3-trees-lab\n",
    "\n",
    "##write an entropy function\n",
    "from math import log \n",
    "\n",
    "def entropy(pi):\n",
    "    \"\"\"\n",
    "    return the Entropy of a probability distribution:\n",
    "    entropy(p) = - SUM (Pi * log(Pi) )\n",
    "    \"\"\"\n",
    "    tot = sum(pi)\n",
    "    return -sum([j/tot * log(j/tot,2) if j!=0 else 0 for j in pi ])\n",
    "\n",
    "\n",
    "# Test the function \n",
    "\n",
    "print(entropy([1, 1])) # Maximum Entropy e.g. a coin toss\n",
    "print(entropy([0, 6])) # No entropy, ignore the -ve with zero , it's there due to log function\n",
    "print(entropy([2, 10])) # A random mix of classes\n",
    "\n",
    "# 1.0\n",
    "# -0.0\n",
    "# 0.6500224216483541\n",
    "#2/12 * log(2/12,2) +10/12 * log(10/12,2) # testing\n",
    "\n",
    "##write an information gain function\n",
    "def IG(D, a):\n",
    "    \"\"\"\n",
    "    return the information gain:\n",
    "    gain(D, A) = entropy(D)− SUM( |Di| / |D| * entropy(Di) )\n",
    "    \"\"\"\n",
    "    return entropy(D) - 1/sum(D)*sum([sum(di)*entropy(di) for di in a])\n",
    "\n",
    "\n",
    "# Test the function\n",
    "# Set of example of the dataset - distribution of classes\n",
    "test_dist = [6, 6] # Yes, No\n",
    "# Attribute, number of members (feature)\n",
    "test_attr = [ [4,0], [2,4], [0,2] ] # class1, class2, class3 of attr1 according to YES/NO classes in test_dist\n",
    "\n",
    "print(IG(test_dist, test_attr))\n",
    "\n",
    "# 0.5408520829727552\n",
    "\n",
    "##first iteration, decide best split for root node\n",
    "# Your code here\n",
    "play = [9, 5]# play is a class_dist, # Yes, No\n",
    "\n",
    "outlook = [\n",
    "    [3, 1],  # overcast   [yes, no]\n",
    "    [6, 1],  # sunny      \n",
    "    [0, 3]   # rain\n",
    "]\n",
    "temperature = [\n",
    "    [1, 2],  # hot\n",
    "    [4, 2],  # cool\n",
    "    [4, 1]   # mild\n",
    "]\n",
    "humidity = [\n",
    "    [4, 3],  # high\n",
    "    [5, 2]   # normal\n",
    "]\n",
    "wind = [\n",
    "    [5, 2],  # no\n",
    "    [4, 3]   # yes\n",
    "]\n",
    "\n",
    "# Information Gain:\n",
    "\n",
    "print ('Information Gain:\\n' )\n",
    "print('Outlook:', IG(play, outlook))\n",
    "print('Temperature:', IG(play, temperature))\n",
    "print('Humidity:', IG(play, humidity))\n",
    "print('Wind:,', IG(play, wind))\n",
    "\n",
    "# Outlook: 0.41265581953400066\n",
    "# Temperature: 0.09212146003297261\n",
    "# Humidity: 0.0161116063701896\n",
    "# Wind:, 0.0161116063701896\n",
    "\n",
    "##now pick best split for second node - note that we go depth first here bc greedy, but we would return to other branches later\n",
    "# Your code here \n",
    "'''\n",
    "outlook\ttemp\thumidity\twindy\tplay\n",
    "overcast\tcool\thigh\tY\tyes\n",
    "overcast\tmild\tnormal\tN\tyes\n",
    "#sunny\tcool\tnormal\tN\tyes\n",
    "overcast\thot\thigh\tY\tno\n",
    "#sunny\thot\tnormal\tY\tyes\n",
    "rain\tmild\thigh\tN\tno\n",
    "rain\tcool\tnormal\tN\tno\n",
    "#sunny\tmild\thigh\tN\tyes\n",
    "#sunny\tcool\tnormal\tY\tyes\n",
    "#sunny\tmild\tnormal\tY\tyes\n",
    "overcast\tcool\thigh\tN\tyes\n",
    "rain\tcool\thigh\tY\tno\n",
    "#sunny\thot\tnormal\tY\tno\n",
    "#sunny\tmild\thigh\tN\tyes\n",
    "'''\n",
    "play = [6, 1] \n",
    "\n",
    "temperature = [[1, 1],[3, 0], [2, 0]]  # hot, mild, cool [yes, no]  \n",
    "humidity = [[2, 0],[4, 1]]   # high, normal [yes, no]\n",
    "wind = [[3, 1],[3, 0]]      # Y, N [yes, no]\n",
    "\n",
    "\n",
    "\n",
    "# Information Gain:\n",
    "print ('Information Gain:\\n' )\n",
    "\n",
    "print('Temperature:', IG(play, temperature))\n",
    "print('Humidity:', IG(play, humidity))\n",
    "print('Wind:,', IG(play, wind))\n",
    "\n",
    "# Temperature: 0.3059584928680418\n",
    "# Humidity: 0.0760098536627829\n",
    "# Wind: 0.12808527889139443\n",
    "##and so on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-decision-trees-with-sklearn-codealong\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('tennis.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "X = df[['outlook', 'temp', 'humidity', 'windy']]\n",
    "y = df[['play']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "##categs\n",
    "# One-hot encode the training data and show the resulting DataFrame with proper column names\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "ohe.fit(X_train)\n",
    "X_train_ohe = ohe.transform(X_train).toarray()\n",
    "\n",
    "# Creating this DataFrame is not necessary its only to show the result of the ohe\n",
    "ohe_df = pd.DataFrame(X_train_ohe, columns=ohe.get_feature_names(X_train.columns))\n",
    "\n",
    "ohe_df.head()\n",
    "\n",
    "##train the decision tree \n",
    "# Create the classifier, fit it on the training data and make predictions on the test set\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "clf.fit(X_train_ohe, y_train)\n",
    "\n",
    "##plot it - very cool\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1, figsize = (3,3), dpi=300)\n",
    "tree.plot_tree(clf,\n",
    "               feature_names = ohe_df.columns, \n",
    "               class_names=np.unique(y).astype('str'),\n",
    "               filled = True)\n",
    "plt.show()\n",
    "\n",
    "##eval accuracy on test set\n",
    "X_test_ohe = ohe.transform(X_test)\n",
    "y_preds = clf.predict(X_test_ohe)\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_test, y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-decision-trees-lab.git\n",
    "# source: https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "\n",
    "##import libs\n",
    "# Import necessary libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree\n",
    "\n",
    "##and data\n",
    "# Create DataFrame\n",
    "df = pd.read_csv('data_banknote_authentication.csv', header=None,  names=['Variance', 'Skewness', 'Kurtosis', 'Entropy', 'Class'])\n",
    "df.head()\n",
    "\n",
    "##describe\n",
    "# Describe the dataset\n",
    "display(df.info())\n",
    "display(df.describe())\n",
    "# Shape of dataset\n",
    "display(df.shape)\n",
    "# Class frequency of target variable \n",
    "display(df.Class.value_counts(normalize=True))\n",
    "\n",
    "##create features, labels, train/test\n",
    "# Create features and labels\n",
    "X=df.drop('Class', axis=1)\n",
    "y=df.Class\n",
    "# Perform an 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)\n",
    "\n",
    "##train classifier, predict\n",
    "# Train a DT classifier\n",
    "clf = DecisionTreeClassifier(random_state=10)#DecisionTreeClassifier(criterion='entropy', random_state=10)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions for test data\n",
    "y_hat_test = clf.predict(X_test)\n",
    "\n",
    "##check predictive perf\n",
    "# Calculate accuracy \n",
    "#X_test_ohe = ohe.transform(X_test)\n",
    "acc = accuracy_score(y_test, y_hat_test)\n",
    "print('Accuracy is :{0}'.format(acc))\n",
    "\n",
    "# Check the AUC for predictions\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_hat_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('\\nAUC is :{0}'.format(round(roc_auc, 2)))\n",
    "\n",
    "# Create and print a confusion matrix \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "print('\\nConfusion Matrix')\n",
    "print('----------------')\n",
    "cm = confusion_matrix(y_test, y_hat_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "##do it again using Entropy instead of Gini for the Classifier\n",
    "# Instantiate and fit a DecisionTreeClassifier\n",
    "classifier_2 = DecisionTreeClassifier(criterion='entropy', random_state=10)\n",
    "classifier_2.fit(X_train,y_train)\n",
    "\n",
    "y_hat_test = classifier_2.predict(X_test)\n",
    "# Calculate accuracy \n",
    "#X_test_ohe = ohe.transform(X_test)\n",
    "acc = accuracy_score(y_test, y_hat_test)\n",
    "print('Accuracy is :{0}'.format(acc))\n",
    "\n",
    "# Check the AUC for predictions\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_hat_test)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('\\nAUC is :{0}'.format(round(roc_auc, 2)))\n",
    "\n",
    "# Create and print a confusion matrix \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "print('\\nConfusion Matrix')\n",
    "print('----------------')\n",
    "cm = confusion_matrix(y_test, y_hat_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pd.crosstab(y_test, y_hat_test, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "# Plot and show decision tree\n",
    "plt.figure(figsize=(12,12), dpi=500)\n",
    "tree.plot_tree(classifier_2, \n",
    "               feature_names=X.columns,\n",
    "               class_names=np.unique(y).astype('str'),\n",
    "               filled=True, rounded=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-tuning-decision-trees\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "    # https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview\n",
    "    # https://towardsdatascience.com/demystifying-hyper-parameter-tuning-acb83af0258f\n",
    "    # https://www.displayr.com/machine-learning-pruning-decision-trees/\n",
    "# https://github.com/ericthansen/dsc-tuning-decision-trees-lab\n",
    "##imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Import the data\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()\n",
    "# Create X and y \n",
    "y = df.Survived\n",
    "X = df.drop('Survived', axis=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=SEED)\n",
    "\n",
    "##train vanilla classifier\n",
    "# Train the classifier using training data\n",
    "dt = DecisionTreeClassifier(criterion='entropy', random_state=SEED)\n",
    "dt.fit(X_train, y_train)\n",
    "# Make predictions using test set \n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Check the AUC of predictions\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc\n",
    "\n",
    "##max tree depth\n",
    "# Identify the optimal tree depth for given data\n",
    "#from sklearn.base import clone\n",
    "max_depths = list(range(1,33))  #using a list not an array\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for md in max_depths:\n",
    "    dt = DecisionTreeClassifier(criterion='entropy', max_depth=md, random_state=SEED)\n",
    "    #print(md)\n",
    "    \n",
    "    dt.fit(X_train, y_train)\n",
    "    train_pred = dt.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    #Train score\n",
    "    train_results.append(roc_auc)\n",
    "    \n",
    "    y_pred = dt.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    # Test score\n",
    "    test_results.append(roc_auc)\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(max_depths, train_results, 'b', label='Train AUC')\n",
    "plt.plot(max_depths, test_results, 'r', label='Test AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Your observations here \n",
    "# test set doesn't improve appreciably after 3 and maxs at 6\n",
    "\n",
    "##min sample split\n",
    "# Identify the optimal min-samples-split for given data\n",
    "min_sample_splits = [i/10 for i in range(1, 11)]  #using a list not an array\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for mss in min_sample_splits:\n",
    "    dt = DecisionTreeClassifier(criterion='entropy', min_samples_split = mss, random_state=SEED)\n",
    "    #print(md)\n",
    "    \n",
    "    dt.fit(X_train, y_train)\n",
    "    train_pred = dt.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    #Train score\n",
    "    train_results.append(roc_auc)\n",
    "    \n",
    "    y_pred = dt.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    # Test score\n",
    "    test_results.append(roc_auc)\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(min_sample_splits, train_results, 'b', label='Train AUC')\n",
    "plt.plot(min_sample_splits, test_results, 'r', label='Test AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### Your observations here\n",
    "#test set doesn't improve after .7\n",
    "\n",
    "##min sample leafs\n",
    "# Calculate the optimal value for minimum sample leafs\n",
    "# Identify the optimal min-samples-split for given data\n",
    "min_samples_leafs = [i/10 for i in range(1, 6)]  #using a list not an array\n",
    "\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for msl in min_samples_leafs:\n",
    "    dt = DecisionTreeClassifier(criterion='entropy', min_samples_leaf = msl, random_state=SEED)\n",
    "    #print(md)\n",
    "    \n",
    "    dt.fit(X_train, y_train)\n",
    "    train_pred = dt.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    #Train score\n",
    "    train_results.append(roc_auc)\n",
    "    \n",
    "    y_pred = dt.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    # Test score\n",
    "    test_results.append(roc_auc)\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(min_samples_leafs, train_results, 'b', label='Train AUC')\n",
    "plt.plot(min_samples_leafs, test_results, 'r', label='Test AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Your observations here \n",
    "# optimal vals between .2 and .3 - could go .25 for middle of maxrange\n",
    "##max features\n",
    "# Find the best value for optimal maximum feature size\n",
    "## Find the best value for optimal maximum feature size\n",
    "max_features = list(range(1, X_train.shape[1]))#np.linspace(1, 12, 12) #note that here, linspace creates floats which messes things up\n",
    "#print(max_features, len(X_train.columns))\n",
    "train_results = []\n",
    "test_results = []\n",
    "for mf in max_features:\n",
    "    '''\n",
    "    dt = DecisionTreeClassifier(criterion='entropy', max_features=max_feature, random_state=SEED)\n",
    "   dt.fit(X_train, y_train)\n",
    "   train_pred = dt.predict(X_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = dt.predict(X_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   test_results.append(roc_auc)'''\n",
    "    dt = DecisionTreeClassifier(criterion='entropy', max_features=int(mf), random_state=SEED)\n",
    "    #print(mf)\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_pred = dt.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    # Add auc score \n",
    "    train_results.append(roc_auc)\n",
    "    \n",
    "    test_pred = dt.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, test_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    # Add auc score\n",
    "    test_results.append(roc_auc)\n",
    "    \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(max_features, train_results, 'b', label='Train AUC')\n",
    "plt.plot(max_features, test_results, 'r', label='Test AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Your observations here\n",
    "#flat train data not so useful.  test curve indicates 6 is optimal.\n",
    "\n",
    "##retrain with chosen features\n",
    "# Train a classifier with optimal values identified above\n",
    "dt = DecisionTreeClassifier(criterion='entropy',\n",
    "                           max_features=6,\n",
    "                           max_depth=3,\n",
    "                           min_samples_split=0.7,\n",
    "                           min_samples_leaf=0.25, \n",
    "                           random_state=SEED)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc\n",
    "\n",
    "# Your observations here\n",
    "#Small improvement in AUC - 72.9 to 73.9.  But overall is a less-overfit and more efficient classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-regression-cart-trees-codealong\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "# cart trees - categorization and regression trees\n",
    "# http://scikit-learn.org/stable/modules/tree.html\n",
    "    # https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927982/\n",
    "    # https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/\n",
    "    # http://www.statsoft.com/Textbook/Classification-and-Regression-Trees\n",
    "    # https://www.youtube.com/watch?v=DCZ3tsQIoGU\n",
    "\n",
    "## generate data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(124)\n",
    "\n",
    "# Generate 100 examples of X and y (a simple cubic function of X) \n",
    "X = np.linspace(-3, 3, 100)\n",
    "y = X ** 3 + np.random.randn(100)\n",
    "\n",
    "# Plot the data \n",
    "plt.figure(figsize=(15,6))\n",
    "plt.scatter(X, y)\n",
    "plt.title(\"Simple quadratic dataset with noise (that is, cubic not quadratic)\")\n",
    "plt.xlabel(\"Feature values\")\n",
    "plt.ylabel(\"Target values\")\n",
    "plt.show()\n",
    "\n",
    "#feature and split\n",
    "X = X.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the data dimensions\n",
    "print('Shape X_train:', X_train.shape)\n",
    "print('Shape y_train:', y_train.shape)\n",
    "print('Shape X_test:', X_test.shape)\n",
    "print('Shape y_test:', y_test.shape)\n",
    "\n",
    "### fit regression tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "##predict and evaluate\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Make predictions and evaluate \n",
    "y_pred = regressor.predict(X_test)\n",
    "print('MSE score:', mse(y_test, y_pred))\n",
    "print('R-sq score:', r2_score(y_test,y_pred))\n",
    "\n",
    "##visualize\n",
    "X_grid = np.arange(min(X), max(X), 0.01)\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.scatter(X, y, color = 'red', label='data')\n",
    "plt.plot(X_grid, regressor.predict(X_grid), color = 'green', label='Regression function')\n",
    "plt.title('Decision Tree Regression')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-regression-cart-trees-lab\n",
    "\n",
    "##skipping pasting in the usual suspects\n",
    "# Import the DecisionTreeRegressor class \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Instantiate and fit a regression tree model to training data \n",
    "regressor = DecisionTreeRegressor(random_state=42)#max_depth=3\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate these predictions\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "##the level up - checking on multiple max_depths:\n",
    "for i in range(1, 11):\n",
    "    regressor = DecisionTreeRegressor(random_state=42, max_depth=i)#\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    # Evaluate these predictions\n",
    "    print('Max_Depth {} results:'.format(i))\n",
    "    print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred))  \n",
    "    print('Mean Squared Error:', mean_squared_error(y_test, y_pred))  \n",
    "    print('Root Mean Squared Error:', mean_squared_error(y_test, y_pred, squared=False))\n",
    "    print('-----------------------\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-tuning-regression-trees-lab\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the Ames housing dataset \n",
    "data = pd.read_csv('ames.csv') \n",
    "\n",
    "# Print the dimensions of data\n",
    "print(data.shape)\n",
    "\n",
    "# Check out the info for the dataframe\n",
    "print(data.info())\n",
    "\n",
    "# Show the first 5 rows\n",
    "data.head()\n",
    "\n",
    "target = data['SalePrice']\n",
    "features = data[['LotArea', '1stFlrSF', 'GrLivArea']]\n",
    "print(target.describe())\n",
    "print(\"\")\n",
    "features.describe()\n",
    "\n",
    "\n",
    "#  some handy plots:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i, col in enumerate(features.columns):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.plot(data[col], target, 'o')\n",
    "    plt.title(col)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Prices')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Import metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Define the function\n",
    "def performance(y_true, y_predict):\n",
    "    \"\"\" \n",
    "    Calculates and returns the two performance scores between \n",
    "    true and predicted values - first R-Squared, then RMSE\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the r2 score between 'y_true' and 'y_predict'\n",
    "    r2 = r2_score(y_true, y_predict)\n",
    "\n",
    "    # Calculate the root mean squared error between 'y_true' and 'y_predict'\n",
    "    rmse = mean_squared_error(y_true, y_predict, squared=False)\n",
    "\n",
    "    # If using an older version of sklearn:\n",
    "    # rmse = np.sqrt(mean_squared_error(y_true, y_predict))\n",
    "\n",
    "    # Return the score\n",
    "    return [r2, rmse]\n",
    "\n",
    "# Test the function\n",
    "score = performance([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\n",
    "score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test subsets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "##vanilla tree\n",
    "# Import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Instantiate DecisionTreeRegressor \n",
    "# Set random_state=45\n",
    "regressor = DecisionTreeRegressor(random_state=45)\n",
    "\n",
    "# Fit the model to training data\n",
    "regressor.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "# Calculate performance using the performance() function \n",
    "score = performance(y_test, y_pred)\n",
    "score\n",
    "\n",
    "# [0.5961521990414137, 55656.48543887347] - R2, RMSE\n",
    "\n",
    "# Identify the optimal tree depth for given data\n",
    "max_depths = np.linspace(1, 30, 30, endpoint=True)\n",
    "mse_results = []\n",
    "r2_results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    regressor = DecisionTreeRegressor(max_depth=max_depth, \n",
    "                                      random_state=45)\n",
    "    regressor.fit(x_train, y_train)\n",
    "    y_pred = regressor.predict(x_test)\n",
    "    score = performance(y_test, y_pred)\n",
    "    r2_results.append(score[0])\n",
    "    mse_results.append(score[1])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(max_depths, r2_results, 'b', label='R2')\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('R-squared')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(max_depths, mse_results, 'r', label='RMSE')\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Identify the optimal minimum split size for given data\n",
    "min_samples_splits = np.arange(2, 11)\n",
    "mse_results = []\n",
    "r2_results = []\n",
    "\n",
    "for min_samples_split in min_samples_splits:\n",
    "    regressor = DecisionTreeRegressor(min_samples_split=int(min_samples_split),\n",
    "                                      random_state=45)\n",
    "    regressor.fit(x_train, y_train)\n",
    "    y_pred = regressor.predict(x_test)\n",
    "    score = performance(y_test, y_pred)\n",
    "    r2_results.append(score[0])\n",
    "    mse_results.append(score[1])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(min_samples_splits, r2_results, 'b', label='R2')\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(min_samples_splits, mse_results, 'r', label='RMSE')\n",
    "plt.show()\n",
    "\n",
    "regressor = DecisionTreeRegressor(min_samples_split=5, max_depth=7, random_state=45)\n",
    "regressor.fit(x_train, y_train)\n",
    "y_pred = regressor.predict(x_test)\n",
    "score = performance(y_test, y_pred)\n",
    "score[0], score[1], regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-nonparametric-models-lab \n",
    "## note that this lab had no specific lesson before it, it is a summary of previous techniques\n",
    "\n",
    "##furthermore, it uses sample code from above; repasting would be repeating...just see the file.\n",
    "\n",
    "# https://github.com/ericthansen/dsc-decision-trees-section-recap\n",
    "# The key takeaways from this section include:\n",
    "\n",
    "# Decision trees can be used for both categorization and regression tasks\n",
    "# They are a powerful and interpretable technique for many machine learning problems (especially when combined with ensemble methods)\n",
    "# Decision trees are a form of Directed Acyclic Graphs (DAGs) - you traverse them in a specified direction, and there are no \"loops\" in the graphs to go backward\n",
    "# Algorithms for generating decision trees are designed to maximize the information gain from each split\n",
    "\n",
    "### A popular algorithm for generating decision trees is ID3 - the Iterative Dichotomiser 3 algorithm\n",
    "\n",
    "# There are several hyperparameters for decision trees to reduce overfitting - including maximum depth, minimum samples to split a node that is currently a leaf, minimum leaf sample size, maximum leaf nodes, and maximum features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-ensemble-methods-section-intro\n",
    " # ensemble methods win most kaggle competitions \n",
    " # https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/\n",
    " # http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/\n",
    "    #####Go read this one!\n",
    "    \n",
    "# https://github.com/ericthansen/dsc-random-forests\n",
    "    # https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf\n",
    "    # https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
    "\n",
    "# https://github.com/ericthansen/dsc-tree-ensembles-random-forests-lab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "# Import the data\n",
    "salaries = pd.read_csv('salaries_final.csv', index_col=0)\n",
    "salaries.head()\n",
    "# Split the outcome and predictor variables\n",
    "target = salaries.Target\n",
    "predictor = salaries.drop('Target', axis=1)\n",
    "# Your code here\n",
    "salaries.info()\n",
    "# Create dummy variables\n",
    "data = pd.get_dummies(predictor)\n",
    "data.head()\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=.25, random_state=123)\n",
    "##baseline tree\n",
    "# Instantiate and fit a DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
    "tree_clf.fit(data_train, target_train)\n",
    "\n",
    "# Feature importance\n",
    "tree_clf.feature_importances_\n",
    "\n",
    "##neat importance plot\n",
    "def plot_feature_importances(model):\n",
    "    n_features = data_train.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), data_train.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "plot_feature_importances(tree_clf)\n",
    "\n",
    "# Test set predictions\n",
    "pred = tree_clf.predict(data_test)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "display(confusion_matrix(target_test, pred))\n",
    "print(classification_report(target_test, pred))\n",
    "\n",
    "print(\"Testing Accuracy for Decision Tree Classifier: {:.4}%\".format(accuracy_score(target_test, pred) * 100))\n",
    "\n",
    "######now use bagging classifier (bagging = bootstrap aggregate)\n",
    "# Instantiate a BaggingClassifier\n",
    "tree_clf2 = DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
    "bagged_tree = BaggingClassifier(base_estimator=tree_clf2, n_estimators=20)\n",
    "\n",
    "# Fit to the training data\n",
    "bagged_tree.fit(data_train, target_train)\n",
    "\n",
    "# Training accuracy score\n",
    "bagged_tree.score(data_train, target_train)\n",
    "\n",
    "# Test accuracy score\n",
    "bagged_tree.score(data_test, target_test)\n",
    "\n",
    "######alternately, try random forest\n",
    "# Instantiate and fit a RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "forest.fit(data_train, target_train)\n",
    "\n",
    "# Training accuracy score\n",
    "print(forest.score(data_train, target_train))\n",
    "\n",
    "# Test accuracy score\n",
    "print(forest.score(data_test, target_test))\n",
    "\n",
    "plot_feature_importances(forest)\n",
    "\n",
    "##examine first two trees, note differences in feature importance\n",
    "# Instantiate and fit a RandomForestClassifier\n",
    "forest_2 = RandomForestClassifier(n_estimators=5, max_features=10, max_depth=2)\n",
    "forest_2.fit(data_train, target_train)\n",
    "\n",
    "# First tree from forest_2\n",
    "rf_tree_1 = forest_2.estimators_[0]\n",
    "# Feature importance\n",
    "plot_feature_importances(rf_tree_1)\n",
    "\n",
    "# Second tree from forest_2\n",
    "rf_tree_2 = forest_2.estimators_[1]\n",
    "# Feature importance\n",
    "plot_feature_importances(rf_tree_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-gridsearchcv\n",
    "##grid search cross val - allows automated exhaustive combinations of different parameters\n",
    "## eg\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [1, 2, 5, 10],\n",
    "    'min_samples_split': [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "gs_tree = GridSearchCV(clf, param_grid, cv=3)\n",
    "gs_tree.fit(train_data, train_labels)\n",
    "\n",
    "gs_tree.best_params_\n",
    "\n",
    "##beware, this can take a long time!\n",
    "\n",
    "# https://github.com/ericthansen/dsc-gridsearchcv-lab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import the data\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "df.head()\n",
    "\n",
    "# Explore the dataset\n",
    "display(df.describe())\n",
    "\n",
    "# Create a box plot of each column\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.boxplot([df[col] for col in df.columns])\n",
    "plt.title(\"Box plot of all columns in dataset\")\n",
    "plt.xticks(range(len(df.columns.values)), df.columns.values)\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot of each column/quality\n",
    "\n",
    "for col in df.columns:\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.scatter(df[col], df['quality'])\n",
    "    plt.title(\"Scatter plot of {} vs quality\".format(col))\n",
    "    #plt.xticks(range(len(df.columns.values)), df.columns.values)\n",
    "    plt.show()\n",
    "    \n",
    "##preproc\n",
    "y = df.quality\n",
    "X = df.drop('quality', axis=1)\n",
    "X.head()\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "##cross val\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_cv_score = cross_val_score(dt_clf, X_train, y_train, cv=3)\n",
    "mean_dt_cv_score = np.mean(dt_cv_score)\n",
    "\n",
    "print(f\"Mean Cross Validation Score: {mean_dt_cv_score :.2%}\")\n",
    "##grid search decision trees\n",
    "dt_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 2, 3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 6]\n",
    "}\n",
    "\n",
    "##number of trees\n",
    "num_decision_trees = 3 * np.prod([len(l) for l in dt_param_grid.values()])\n",
    "print(f\"Grid Search will have to search through {num_decision_trees} different permutations.\")\n",
    "# Instantiate GridSearchCV\n",
    "dt_grid_search = GridSearchCV(dt_clf, dt_param_grid, cv=3, return_train_score=True)\n",
    "\n",
    "# Fit to the data\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "##examine best params\n",
    "# Mean training score\n",
    "dt_gs_training_score = np.mean(dt_grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "dt_gs_testing_score = np.mean(dt_grid_search.score(X_test, y_test))\n",
    "\n",
    "print(f\"Mean Training Score: {dt_gs_training_score :.2%}\")\n",
    "print(f\"Mean Test Score: {dt_gs_testing_score :.2%}\")\n",
    "print(\"Best Parameter Combination Found During Grid Search:\", dt_grid_search.best_params_)\n",
    "\n",
    "##random forest model\n",
    "rf_clf = RandomForestClassifier()\n",
    "mean_rf_cv_score = np.mean(cross_val_score(rf_clf, X_train, y_train, cv=3))\n",
    "#mean_rf_cv_score = np.mean(cross_val_score(rf_clf, scaled_data_train, y_train, cv=3, scoring = scoring_p))\n",
    "\n",
    "print(f\"Mean Cross Validation Score for Random Forest Classifier: {mean_rf_cv_score :.2%}\")\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [10, 30, 100],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 2, 6, 10],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [3, 6]\n",
    "}\n",
    "rf_grid_search = GridSearchCV(rf_clf, rf_param_grid, cv=3)\n",
    "#rf_grid_search = GridSearchCV(rf_clf, rf_param_grid, cv=3, scoring = scoring_p, verbose=verb)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Testing Accuracy: {rf_grid_search.best_score_ :.2%}\")##Note, best_score gives Mean cross-validated score of the best_estimator\n",
    "print(\"\")\n",
    "print(f\"Optimal Parameters: {rf_grid_search.best_params_}\")\n",
    "\n",
    "##check performance on test set\n",
    "dt_score = dt_grid_search.score(X_test, y_test)\n",
    "rf_score = rf_grid_search.score(X_test, y_test)\n",
    "\n",
    "print('Decision tree grid search: ', dt_score)\n",
    "print('Random forest grid search: ', rf_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-gradient-boosting-and-weak-learners\n",
    "'''In this lesson, we learned about Weak Learners, and how they are used in various Gradient Boosting algorithms. We also learned about two specific algorithms -- AdaBoost and Gradient Boosted Trees, and we compared how they are similar and how they are different!'''\n",
    "# https://github.com/ericthansen/dsc-gradient-boosting-lab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Import the data\n",
    "df = pd.read_csv('pima-indians-diabetes.csv')\n",
    "\n",
    "# Print the first five rows\n",
    "df.head()\n",
    "# Check for missing values\n",
    "df.isna().sum()\n",
    "# Number of patients with and without diabetes\n",
    "df.Outcome.value_counts()\n",
    "target = df.Outcome\n",
    "df = df.drop('Outcome', axis=1)\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=.25, random_state=42)\n",
    "##train the models \n",
    "# Instantiate an AdaBoostClassifier\n",
    "adaboost_clf = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Instantiate an GradientBoostingClassifier\n",
    "gbt_clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Fit AdaBoostClassifier\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "# Fit GradientBoostingClassifier\n",
    "gbt_clf.fit(X_train, y_train)\n",
    "\n",
    "# AdaBoost model predictions\n",
    "adaboost_train_preds = adaboost_clf.predict(X_train)\n",
    "adaboost_test_preds = adaboost_clf.predict(X_test)\n",
    "\n",
    "# GradientBoosting model predictions\n",
    "gbt_clf_train_preds = gbt_clf.predict(X_train)\n",
    "gbt_clf_test_preds = gbt_clf.predict(X_test)\n",
    "\n",
    "def display_acc_and_f1_score(true, preds, model_name):\n",
    "    acc = accuracy_score(true, preds)\n",
    "    f1 = f1_score(true, preds)\n",
    "    print(\"Model: {}\".format(model_name))\n",
    "    print(\"Accuracy: {}\".format(acc))\n",
    "    print(\"F1-Score: {}\".format(f1))\n",
    "    \n",
    "print(\"Training Metrics\")\n",
    "display_acc_and_f1_score(y_train, adaboost_train_preds, model_name='AdaBoost')\n",
    "print(\"\")\n",
    "display_acc_and_f1_score(y_train, gbt_clf_train_preds, model_name='Gradient Boosted Trees')\n",
    "print(\"\")\n",
    "print(\"Testing Metrics\")\n",
    "display_acc_and_f1_score(y_test, adaboost_test_preds, model_name='AdaBoost')\n",
    "print(\"\")\n",
    "display_acc_and_f1_score(y_test, gbt_clf_test_preds, model_name='Gradient Boosted Trees')\n",
    "\n",
    "##conf matrices\n",
    "adaboost_confusion_matrix = confusion_matrix(y_test, adaboost_test_preds)\n",
    "adaboost_confusion_matrix\n",
    "gbt_confusion_matrix = confusion_matrix(y_test, gbt_clf_test_preds)\n",
    "gbt_confusion_matrix\n",
    "\n",
    "\n",
    "adaboost_classification_report = classification_report(y_test, adaboost_test_preds)\n",
    "print(adaboost_classification_report)\n",
    "gbt_classification_report = classification_report(y_test, gbt_clf_test_preds)\n",
    "print(gbt_classification_report)\n",
    "\n",
    "print('Mean Adaboost Cross-Val Score (k=5):')\n",
    "print(np.mean(cross_val_score(adaboost_clf, df, target, cv=5)))\n",
    "# Expected Output: 0.7631270690094218\n",
    "\n",
    "print('Mean GBT Cross-Val Score (k=5):')\n",
    "print(np.mean(cross_val_score(gbt_clf, df, target, cv=5)))\n",
    "# Expected Output: 0.7591715474068416\n",
    "\n",
    "##could do better if we tuned the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-xgboost\n",
    "'''Gradient boosting is one of the most powerful concepts in machine learning right now. As you've seen, the term gradient boosting refers to a class of algorithms, rather than any single one. The version with the highest performance right now is XGBoost, which is short for eXtreme Gradient Boosting.'''\n",
    "conda install py-xgboost\n",
    "# then try\n",
    "from xgboost import XGBClassifier\n",
    "# https://xgboost.readthedocs.io/en/latest/\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "df.head()\n",
    "y = df.quality\n",
    "X = df.drop('quality', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "# Instantiate XGBClassifier\n",
    "clf = XGBClassifier()\n",
    "\n",
    "# Fit XGBClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "training_preds = clf.predict(X_train)\n",
    "test_preds = clf.predict(X_test)\n",
    "\n",
    "# Accuracy of training and test sets\n",
    "training_accuracy = accuracy_score(y_train, training_preds)\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n",
    "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))\n",
    "\n",
    "##tune parameters\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [6],\n",
    "    'min_child_weight': [1, 2],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'n_estimators': [100],\n",
    "}\n",
    "grid_clf = GridSearchCV(clf, param_grid, scoring='accuracy', cv=None, n_jobs=1)\n",
    "grid_clf.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = grid_clf.best_params_\n",
    "\n",
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "training_preds = grid_clf.predict(X_train)\n",
    "test_preds = grid_clf.predict(X_test)\n",
    "training_accuracy = accuracy_score(y_train, training_preds)\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "print('')\n",
    "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n",
    "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-ensemble-methods-section-recap\n",
    "'''Key Takeaways\n",
    "The key takeaways from this section include:\n",
    "\n",
    "Multiple independent estimates are consistently more accurate than any single estimate, so ensemble techniques are a powerful way for improving the quality of your models\n",
    "Sometimes you'll use model stacking or meta-ensembles where you use a combination of different types of models for your ensemble\n",
    "It's also common to have multiple similar models in an ensemble - e.g. a bunch of decision trees\n",
    "Bagging (Bootstrap AGGregation) is a technique that leverages Bootstrap Resampling and Aggregation\n",
    "Bootstrap resampling uses multiple smaller samples from the test dataset to create independent estimates, and aggregate these estimates to make predictions\n",
    "A random forest is an ensemble method for decision trees using Bagging and the Subspace Sampling method to create variance among the trees\n",
    "With a random forest, for each tree, we sample two-thirds of the training data and the remaining third is used to calculate the out-of-bag error\n",
    "In addition, the Subspace Sampling method is used to further increase variability by randomly selecting the subset of features to use as predictors for training any given tree\n",
    "GridsearchCV is an exhaustive search technique for finding optimal combinations of hyperparameters\n",
    "Boosting leverages an ensemble of weak learners (weak models) to create a strong combined model\n",
    "Boosting (when compared to random forests) is an iterative rather than independent process, using each iteration to strengthen the weaknesses of the previous iterations\n",
    "Two of the most common algorithms for Boosting are Adaboost (Adaptive Boosting) and Gradient Boosted Trees\n",
    "Adaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive tree\n",
    "Gradient Boosting is a more advanced boosting algorithm that makes use of Gradient Descent\n",
    "XGBoost (eXtreme Gradient Boosting) is one of the top gradient boosting algorithms currently in use\n",
    "XGBoost is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-svm-intro\n",
    "# https://github.com/ericthansen/dsc-introduction-to-support-vector-machines\n",
    "# https://github.com/ericthansen/dsc-building-an-svm-from-scratch-lab\n",
    "  # https://www.cvxpy.org/\n",
    "    ## you may need to \n",
    "pip install cvxpy\n",
    "\n",
    "\n",
    "##this lab was inscrutable, and I think that was the expectation.  \n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.title('Two blobs')\n",
    "X, labels = make_blobs(n_features=2, centers=2, cluster_std=1.25,  random_state=123)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=25);\n",
    "\n",
    "##build max margin classifier - see the intro link above to see where the formulas come in\n",
    "# Print labels\n",
    "print(labels)\n",
    "print(len(labels))\n",
    "# Assign label 0 to class_1\n",
    "class_1 = X[labels == 0]\n",
    "\n",
    "# Assign label 1 to class_2\n",
    "class_2 = X[labels == 1]\n",
    "print(len(class_2))\n",
    "print(class_2.shape)\n",
    "# Import cvxpy\n",
    "import cvxpy as cp\n",
    "\n",
    "d = 2  \n",
    "m = 50 \n",
    "n = 50  \n",
    "\n",
    "# Define the variables\n",
    "w = cp.Variable(d)\n",
    "b = cp.Variable()\n",
    "\n",
    "# Define the constraints\n",
    "x_constraints = [w.T * class_1[i] + b >= 1  for i in range(m)]\n",
    "y_constraints = [w.T * class_2[i] + b <= -1 for i in range(n)]\n",
    "\n",
    "# Sum the constraints\n",
    "constraints = x_constraints +  y_constraints \n",
    "\n",
    "# Define the objective. Hint: use cp.norm\n",
    "obj = cp.Minimize(cp.norm(w, 2))\n",
    "\n",
    "# Add objective and constraint in the problem\n",
    "prob = cp.Problem(obj, constraints)\n",
    "\n",
    "# Solve the problem\n",
    "prob.solve()\n",
    "print('Problem Status: %s'%prob.status)\n",
    "\n",
    "#define a helper plot function\n",
    "## A helper function for plotting the results, the decision plane, and the supporting planes\n",
    "\n",
    "def plotBoundaries(x, y, w, b):\n",
    "    # Takes in a set of datapoints x and y for two clusters,\n",
    "    d1_min = np.min([x[:,0], y[:,0]])\n",
    "    d1_max = np.max([x[:,0], y[:,0]])\n",
    "    # Line form: (-a[0] * x - b ) / a[1]\n",
    "    d2_at_mind1 = (-w[0]*d1_min - b ) / w[1]\n",
    "    d2_at_maxd1 = (-w[0]*d1_max - b ) / w[1]\n",
    "    sup_up_at_mind1 = (-w[0]*d1_min - b + 1 ) / w[1]\n",
    "    sup_up_at_maxd1 = (-w[0]*d1_max - b + 1 ) / w[1]\n",
    "    sup_dn_at_mind1 = (-w[0]*d1_min - b - 1 ) / w[1]\n",
    "    sup_dn_at_maxd1 = (-w[0]*d1_max - b - 1 ) / w[1]\n",
    "\n",
    "    # Plot the clusters!\n",
    "    plt.scatter(x[:,0], x[:,1], color='purple')\n",
    "    plt.scatter(y[:,0], y[:,1], color='yellow')\n",
    "    plt.plot([d1_min,d1_max], [d2_at_mind1, d2_at_maxd1], color='black')\n",
    "    plt.plot([d1_min,d1_max], [sup_up_at_mind1, sup_up_at_maxd1],'-.', color='blue')\n",
    "    plt.plot([d1_min,d1_max], [sup_dn_at_mind1, sup_dn_at_maxd1],'-.', color='blue')\n",
    "    plt.ylim([np.floor(np.min([x[:,1],y[:,1]])), np.ceil(np.max([x[:,1], y[:,1]]))])\n",
    "##need to get values from the variables\n",
    "w = w.value\n",
    "b = b.value\n",
    "print(w, b)\n",
    "# Plot \n",
    "plotBoundaries(class_1, class_2, w, b)\n",
    "\n",
    "##a more complex problem - will need Xi slack factor\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.title('Two blobs')\n",
    "X, labels = make_blobs(n_features=2, centers=2, cluster_std=3,  random_state=123)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=25);\n",
    "##if we try to do it normally, get infeasible result (can't make a hard border that properly divides)\n",
    "#so build soft margin classifier\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.title('Two blobs')\n",
    "X, labels = make_blobs(n_features=2, centers=2, cluster_std=3,  random_state=123)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=25);\n",
    "# Reassign the class labels\n",
    "class_1 = X[labels == 0]\n",
    "\n",
    "# Assign label 1 to class_2\n",
    "class_2 = X[labels == 1]\n",
    "# print(len(class_2))\n",
    "# print(class_2.shape)\n",
    "\n",
    "##designing the soft margin classifier\n",
    "d = 2  \n",
    "m = 50 \n",
    "n = 50  \n",
    "\n",
    "# Define the variables\n",
    "w = cp.Variable(d)\n",
    "b = cp.Variable()\n",
    "ksi_1 = cp.Variable(m) #this really should be Xi not ksi.  \n",
    "ksi_2 = cp.Variable(n)\n",
    "\n",
    "C=.1\n",
    "\n",
    "# Define the constraints\n",
    "x_constraints = [w.T * class_1[i] + b >= 1 - ksi_1[i]  for i in range(m)]\n",
    "y_constraints = [w.T * class_2[i] + b <= -1 + ksi_2[i] for i in range(n)]\n",
    "ksi_1_constraints = [ksi_1 >= 0  for i in range(m)]\n",
    "ksi_2_constraints = [ksi_2 >= 0  for i in range(n)]\n",
    "\n",
    "# Sum the constraints\n",
    "constraints = x_constraints +  y_constraints + ksi_1_constraints + ksi_2_constraints\n",
    "\n",
    "# Define the objective. Hint: use cp.norm. Add in a C hyperparameter and assume 1 at first\n",
    "obj = cp.Minimize(cp.norm(w,2) + C * (sum(ksi_1) + sum(ksi_2)))\n",
    "\n",
    "# Add objective and constraint in the problem\n",
    "prob = cp.Problem(obj, constraints)\n",
    "\n",
    "# Solve the problem\n",
    "prob.solve()\n",
    "print('Problem Status: %s'%prob.status)\n",
    "\n",
    "\n",
    "w = w.value\n",
    "b = b.value\n",
    "plotBoundaries(class_1, class_2, w, b)\n",
    "\n",
    "## but this is already all built into scikit learn.  so use that.  but still is a little disappointing they didn't teach it better\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-building-an-svm-using-scikit-learn-lab\n",
    "'''Note: Typically you should scale data when fitting an SVM model. This is because if some variables have a larger scale than others, they will dominate variables that are on a smaller scale. To read more about this, check out page 3 of this paper. Because these variables are all on a similar scale, we will not apply standardization. However, when performing SVM on a real-world dataset, you should ALWAYS scale the data before fitting a model.'''\n",
    "# https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\n",
    "#basically, should just open the file and read the entirety\n",
    "#some highlights:\n",
    "# Your code here\n",
    "from sklearn.svm import SVC\n",
    "sv1 = SVC(kernel='linear')\n",
    "sv1.fit(X_1, y_1)\n",
    "X_11= X_1[:, 0]\n",
    "X_12= X_1[:, 1]\n",
    "X11_min, X11_max = X_1[:, 0].min()-1, X_1[:, 0].max()+1\n",
    "X12_min, X12_max = X_1[:, 1].min()-1, X_1[:, 1].max()+1\n",
    "\n",
    "# Your code here\n",
    "x11_coord = np.linspace(X11_min, X11_max, num=10)\n",
    "x12_coord = np.linspace(X12_min, X12_max, num=10)\n",
    "# Your code here\n",
    "X12_C, X11_C = np.meshgrid(x11_coord, x12_coord)\n",
    "x11x12 = np.c_[X11_C.ravel(), X12_C.ravel()]\n",
    "\n",
    "np.shape(x11x12)\n",
    "# Your code here\n",
    "df1 = sv1.decision_function(x11x12)\n",
    "df1 = df1.reshape(X12_C.shape)\n",
    "plt.scatter(X_11, X_12, c=y_1)\n",
    "axes = plt.gca()\n",
    "axes.contour(X11_C, X12_C, df1, colors=['blue', 'black', 'blue'], \n",
    "             levels=[-1, 0, 1], linestyles=[':', '-', ':'])\n",
    "plt.show()\n",
    "## there's a lot of pre-fitting for plotting\n",
    "\n",
    "##there is much more for non-linearly-separable ones, and multi-group or moon-shaped blobs, etc...\n",
    "# https://scikit-learn.org/stable/modules/svm.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-the-kernel-trick\n",
    "## kernel options - linear, rbf (radial basis function), polynomial, sigmoid, precomputed (gram matrix)\n",
    "#recall linearsvc, nusvc\n",
    "# https://github.com/ericthansen/dsc-kernels-in-scikit-learn-lab\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.title('Four Blobs')\n",
    "X_3, y_3 = make_blobs(n_samples=100, n_features=2, centers=4, cluster_std=1.6, random_state=123)\n",
    "plt.scatter(X_3[:, 0], X_3[:, 1], c=y_3, s=25)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Two Moons with Substantial Overlap')\n",
    "X_4, y_4 = make_moons(n_samples=100, shuffle=False , noise=0.3, random_state=123)\n",
    "plt.scatter(X_4[:, 0], X_4[:, 1], c=y_4, s=25)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##explore rbf kernel\n",
    "C_range = np.array([0.1, 1, 10])\n",
    "gamma_range = np.array([0.1, 1, 100])\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "details = []\n",
    "\n",
    "# Create a loop that builds a model for each of the 9 combinations\n",
    "for c in C_range:\n",
    "    for g in gamma_range:\n",
    "        clf = svm.SVC(C=c, gamma=g)\n",
    "        clf.fit(X_4, y_4)\n",
    "        #print(clf.score(X_4, y_4))\n",
    "        details.append((c, g, clf))\n",
    "    \n",
    "# Prepare your data for plotting\n",
    "X1= X_4[:, 0]\n",
    "X2= X_4[:, 1]\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 500)\n",
    "x2_coord = np.linspace(X2_min, X2_max, 500)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "# Plot the prediction results in 9 subplots  \n",
    "plt.figure(figsize=(11, 11))\n",
    "\n",
    "for (k, (C, gamma, clf)) in enumerate(details):\n",
    "    # evaluate the predictions in a grid\n",
    "    Z = clf.predict(x1x2)  \n",
    "    Z = Z.reshape(X1_C.shape)\n",
    "\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(3, 3, k + 1)\n",
    "    plt.title(\"gam= %r, C= %r, score = %r\"  % (gamma, C, round(clf.score(X_4, y_4), 2)))\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.contourf(X1_C, X2_C, Z, alpha=1)\n",
    "    plt.scatter(X_4[:, 0], X_4[:, 1], c=y_4,  edgecolors='gray')\n",
    "    plt.axis('tight')\n",
    "\n",
    "####same thing, but use decision function instead\n",
    "# Plot the decision function results in 9 subplots\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for (k, (C, gamma, clf)) in enumerate(details):\n",
    "    # evaluate the decision functions in a grid\n",
    "    Z = clf.decision_function(x1x2)  \n",
    "    Z = Z.reshape(X1_C.shape)\n",
    "\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(3, 3, k + 1)\n",
    "    plt.title(\"gam= %r, C= %r, score = %r\"  % (gamma, C, round(clf.score(X_4, y_4), 2)))\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.contourf(X1_C, X2_C, Z, alpha=1)\n",
    "    plt.scatter(X_4[:, 0], X_4[:, 1], c=y_4,  edgecolors='gray')\n",
    "    plt.axis('tight')     \n",
    "    \n",
    "###polynomial kernel\n",
    "r_range =  np.array([0.1, 2])\n",
    "gamma_range =  np.array([0.1, 1])\n",
    "d_range = np.array([3, 4])\n",
    "param_grid = dict(gamma=gamma_range, degree=d_range, coef0=r_range)\n",
    "details = []\n",
    "\n",
    "# Create a loop that builds a model for each of the 8 combinations\n",
    "for d in d_range:\n",
    "    for gamma in gamma_range:\n",
    "         for r in r_range:\n",
    "            clf = svm.SVC(kernel='poly', coef0=r , gamma=gamma, degree=d)\n",
    "            clf.fit(X_3, y_3)\n",
    "            details.append((r, d, gamma, clf))\n",
    "# Prepare your data for plotting\n",
    "X1= X_3[:,0]\n",
    "X2= X_3[:,1]\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 500)\n",
    "x2_coord = np.linspace(X2_min, X2_max, 500)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "# Plot the prediction results in 8 subplots  \n",
    "plt.figure(figsize=(12, 14))\n",
    "\n",
    "for (k, (r, d,gamma, clf)) in enumerate(details):\n",
    "    # evaluate the predictions in a grid\n",
    "    Z = clf.predict(x1x2)  \n",
    "    Z = Z.reshape(X1_C.shape)\n",
    "\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(4, 2, k + 1)\n",
    "    plt.title(\"d= %r, gam= %r, r = %r , score = %r\"  % (d, gamma,r, round(clf.score(X_3, y_3), 2)))\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.contourf(X1_C, X2_C, Z, alpha=1)\n",
    "    plt.scatter(X_3[:, 0], X_3[:, 1], c=y_3,  edgecolors='gray')\n",
    "    plt.axis('tight')\n",
    "\n",
    "    \n",
    "##sigmoid kernel\n",
    "# Create a loop that builds a model for each of the 9 combinations\n",
    "r_range =  np.array([0.01, 1, 10])\n",
    "gamma_range =  np.array([0.001, 0.1, 1])\n",
    "\n",
    "param_grid = dict(gamma=gamma_range, coef0=r_range)\n",
    "details = []\n",
    "\n",
    "# Create a loop that builds a model for each of the 8 combinations\n",
    "for gamma in gamma_range:\n",
    "     for r in r_range:\n",
    "        clf = svm.SVC(kernel='sigmoid', coef0=r , gamma=gamma)\n",
    "        clf.fit(X_3, y_3)\n",
    "        details.append((r, gamma, clf))\n",
    "# Prepare your data for plotting\n",
    "X1= X_3[:,0]\n",
    "X2= X_3[:,1]\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 500)\n",
    "x2_coord = np.linspace(X2_min, X2_max, 500)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "# Plot the prediction results in 9 subplots  \n",
    "# Plot the prediction results in 8 subplots  \n",
    "plt.figure(figsize=(12, 14))\n",
    "\n",
    "for (k, (r, gamma, clf)) in enumerate(details):\n",
    "    # evaluate the predictions in a grid\n",
    "    Z = clf.predict(x1x2)  \n",
    "    Z = Z.reshape(X1_C.shape)\n",
    "\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(3, 3, k + 1)\n",
    "    plt.title(\"gam= %r, r = %r , score = %r\"  % (gamma,r, round(clf.score(X_3, y_3), 2)))\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.contourf(X1_C, X2_C, Z, alpha=1)\n",
    "    plt.scatter(X_3[:, 0], X_3[:, 1], c=y_3,  edgecolors='gray')\n",
    "    plt.axis('tight')\n",
    "    \n",
    "##explore polynomial again, with train-test split\n",
    "# Perform a train-test split, then create a loop that builds a model for each of the 8 combinations\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_3, y_3, test_size=0.33, random_state=123)\n",
    "\n",
    "# Create a loop that builds a model for each of the 8 combinations\n",
    "r_range =  np.array([0.1, 2])\n",
    "gamma_range =  np.array([0.1, 1]) \n",
    "d_range = np.array([3, 4])\n",
    "param_grid = dict(gamma=gamma_range, degree=d_range, coef0=r_range)\n",
    "details = []\n",
    "for d in d_range:\n",
    "    for gamma in gamma_range:\n",
    "         for r in r_range:\n",
    "            clf = svm.SVC(kernel='poly', coef0=r , gamma=gamma, degree=d)\n",
    "            clf.fit(X_train, y_train)\n",
    "            details.append((r, d, gamma, clf))\n",
    "# Prepare your data for plotting\n",
    "X1 = X_train[:, 0]\n",
    "X2 = X_train[:, 1]\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 500)\n",
    "x2_coord = np.linspace(X2_min, X2_max, 500)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "# Plot the prediction results in 8 subplots on the training set  \n",
    "plt.figure(figsize=(12, 14))\n",
    "\n",
    "for (k, (r, d,gamma, clf)) in enumerate(details):\n",
    "    # evaluate the predictions in a grid\n",
    "    Z = clf.predict(x1x2)  \n",
    "    Z = Z.reshape(X1_C.shape)\n",
    "\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(4, 2, k + 1)\n",
    "    plt.title(\"d= %r, gam= %r, r = %r , score = %r\"  % (d, gamma, r, round(clf.score(X_train, y_train), 2)))\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.contourf(X1_C, X2_C, Z, alpha = 1)\n",
    "    plt.scatter(X1, X2, c=y_train,  edgecolors='gray')\n",
    "    plt.axis('tight')\n",
    "    \n",
    "# Now plot the prediction results for the test set\n",
    "X1= X_test[:,0]\n",
    "X2= X_test[:,1]\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 500)\n",
    "x2_coord = np.linspace(X2_min, X2_max, 500)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "\n",
    "# Plot the prediction results in 8 subplots  \n",
    "plt.figure(figsize=(12, 14))\n",
    "\n",
    "for (k, (r, d,gamma, clf)) in enumerate(details):\n",
    "    # evaluate the predictions in a grid\n",
    "    Z = clf.predict(x1x2)  \n",
    "    Z = Z.reshape(X1_C.shape)\n",
    "\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(4, 2, k + 1)\n",
    "    plt.title(\"d= %r, gam= %r, r = %r , score = %r\"  % (d, gamma, r, round(clf.score(X_test, y_test), 2)))\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.contourf(X1_C, X2_C, Z, alpha=1)\n",
    "    plt.scatter(X1, X2, c=y_test,  edgecolors='gray')\n",
    "    plt.axis('tight')\n",
    "\n",
    "    \n",
    "    \n",
    "#### a higher dimensional, real-world dataset\n",
    "import statsmodels as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "salaries = pd.read_csv('salaries_final.csv', index_col=0)\n",
    "salaries.head()\n",
    "\n",
    "# Create dummy variables and \n",
    "# Split data into target and predictor variables\n",
    "target = pd.get_dummies(salaries['Target'], drop_first=True)\n",
    "xcols = salaries.columns[:-1]\n",
    "data = pd.get_dummies(salaries[xcols], drop_first=True)\n",
    "\n",
    "# Split the data into a train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25, random_state=123)\n",
    "\n",
    "# Standardize the data\n",
    "std = StandardScaler()\n",
    "X_train_transformed = std.fit_transform(X_train)\n",
    "X_test_transformed = std.transform(X_test)\n",
    "\n",
    "# Fit SVM model  \n",
    "# ⏰ This cell may take several minutes to run\n",
    "clf = svm.SVC(probability=True)\n",
    "clf.fit(X_train_transformed, y_train['>50K'])\n",
    "\n",
    "# Calculate the classification accuracy score\n",
    "clf.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-svm-recap\n",
    "# https://github.com/ericthansen/dsc-pipelines-intro\n",
    "  # https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html\n",
    "# https://github.com/ericthansen/dsc-pipelines-lab-v2-1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Import the data\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "\n",
    "# Print the first five rows\n",
    "df.head()\n",
    "\n",
    "# Print the summary stats of all columns\n",
    "df.describe()\n",
    "#the min/maxs are quite different - we should scale.\n",
    "# Split the predictor and target variables\n",
    "y = df.quality\n",
    "X = df.drop('quality', axis=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)\n",
    "\n",
    "##standardize\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert into a DataFrame\n",
    "scaled_df_train = pd.DataFrame(scaled_data_train, columns=X_train.columns)\n",
    "scaled_df_train.head()\n",
    "##train a model\n",
    "# Instantiate KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(scaled_data_train, y_train)\n",
    "\n",
    "# Print the accuracy on test set\n",
    "clf.score(scaled_data_test, y_test)\n",
    "\n",
    "# Build a pipeline with StandardScaler and KNeighborsClassifier\n",
    "scaled_pipeline_1 = Pipeline([('ss', StandardScaler()), \n",
    "                              ('knn', KNeighborsClassifier())])\n",
    "\n",
    "\n",
    "# Fit the training data to pipeline\n",
    "scaled_pipeline_1.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy on test set\n",
    "scaled_pipeline_1.score(X_test, y_test)\n",
    "\n",
    "# Build a pipeline with StandardScaler and RandomForestClassifier\n",
    "scaled_pipeline_2 = Pipeline([('ss', StandardScaler()), \n",
    "                              ('RF', RandomForestClassifier(random_state=123))])\n",
    "\n",
    "# Define the grid\n",
    "grid = [{'RF__max_depth': [4, 5, 6], \n",
    "         'RF__min_samples_split': [2, 5, 10], \n",
    "         'RF__min_samples_leaf': [1, 3, 5]}]\n",
    "\n",
    "# Define a grid search\n",
    "gridsearch = GridSearchCV(estimator=scaled_pipeline_2, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=5)\n",
    "# Fit the training data\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy on test set\n",
    "gridsearch.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-refactoring-with-pipelines\n",
    "#this is a super useful lesson, i'd recommend just checking the whole thing for thoroughly commented code.  here's a tiny bit\n",
    "import pandas as pd\n",
    "\n",
    "example_data = pd.DataFrame([\n",
    "    {\"category\": \"A\", \"number\": 7, \"target\": 1},\n",
    "    {\"category\": \"A\", \"number\": 8, \"target\": 1},\n",
    "    {\"category\": \"B\", \"number\": 9, \"target\": 0},\n",
    "    {\"category\": \"B\", \"number\": 7, \"target\": 1},\n",
    "    {\"category\": \"C\", \"number\": 4, \"target\": 0}\n",
    "])\n",
    "\n",
    "example_X = example_data.drop(\"target\", axis=1)\n",
    "example_y = example_data[\"target\"]\n",
    "\n",
    "example_X\n",
    "\n",
    "\n",
    "def preprocess_data_with_pipeline(X):\n",
    "    \n",
    "    ### Encoding categorical data ###\n",
    "    original_features_encoded = ColumnTransformer(transformers=[\n",
    "        (\"ohe\", OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\"), [\"category\"])\n",
    "    ], remainder=\"passthrough\")\n",
    "    \n",
    "    ### Feature engineering ###\n",
    "    def is_odd(data):\n",
    "        \"\"\"\n",
    "        Helper function that returns 1 if odd, 0 if even\n",
    "        \"\"\"\n",
    "        return data % 2\n",
    "\n",
    "    feature_eng = ColumnTransformer(transformers=[\n",
    "        (\"add_number_odd\", FunctionTransformer(is_odd), [\"number\"])\n",
    "    ], remainder=\"drop\")\n",
    "  \n",
    "    ### Combine encoded and engineered features ###\n",
    "    feature_union = FeatureUnion(transformer_list=[\n",
    "        (\"encoded_features\", original_features_encoded),\n",
    "        (\"engineered_features\", feature_eng)\n",
    "    ])\n",
    "    \n",
    "    ### Pipeline (including scaling) ###\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"feature_union\", feature_union),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    transformed_data = pipe.fit_transform(X)\n",
    "    \n",
    "    ### Re-apply labels (optional step for readability) ###\n",
    "    encoder = original_features_encoded.named_transformers_[\"ohe\"]\n",
    "    category_labels = encoder.categories_[0]\n",
    "    all_cols = list(category_labels) + [\"number\", \"number_odd\"]\n",
    "    return pd.DataFrame(transformed_data, columns=all_cols, index=X.index), pipe\n",
    "    \n",
    "# Reset value of example_X\n",
    "example_X = example_data.drop(\"target\", axis=1)\n",
    "# Test out our new function\n",
    "result, pipe = preprocess_data_with_pipeline(example_X)\n",
    "result\n",
    "## takes less lines of code, less data leakage, and the pipe line is pickle-able.\n",
    "\n",
    "#https://github.com/ericthansen/dsc-pipelines-recap\n",
    "#Machine Learning Pipelines create a nice workflow to combine data manipulations, preprocessing, and modeling\n",
    "#Machine Learning Pipelines can be used along with grid search to evaluate several parameter settings\n",
    "#Grid search can considerably blow up computation time when computing for several parameters along with cross-validation\n",
    "#Some models are very sensitive to hyperparameter changes, so they should be chosen with care, and even with big grids a good outcome isn't always guaranteed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-understanding-self\n",
    "\n",
    "# https://github.com/ericthansen/dsc-object-attributes-lab\n",
    "\n",
    "# https://github.com/ericthansen/dsc-object-initialization\n",
    "# https://github.com/ericthansen/dsc-object-initialization-lab\n",
    "# https://github.com/ericthansen/dsc-object-oriented-attributes-with-functions\n",
    "'''What is a Domain Model?\n",
    "A domain model is the representation of a real-world concept or structure translated into software. This is a key function of object orientation. So far, your Python classes have been used as blueprints or templates for instance objects of that class. As an example, a Driver class would create driver instance objects, and the class would define a basic structure for what that driver instance object should look like and what capabilities it should have. But a class is only one part of a domain model just as, typically, a driver is only one part of a larger structure.\n",
    "\n",
    "A domain model is meant to mirror that larger, real-world structure. It is more than just one class, it is an entire environment that often depends on other parts or classes to function properly. So, in keeping with a Driver class, you could use the example of a taxi and limousine service as our domain model. There are many more parts to a service like this than drivers alone. Imagine dispatchers, mechanics, accountants, passengers, etc., all being part of the structure of this domain model. In a simplified example, you could have instance and class methods handle things like dispatch_driver, calculate_revenue_from_rides, service_taxi, or any other function of a taxi and limousine service.\n",
    "\n",
    "As you become more fluent in object-oriented programming and your programs become more complex, you'll see that the other parts of a domain model like passengers, dispatchers, etc., will be classes of their own that interact with each other.\n",
    "\n",
    "In this lecture, you'll be using a business as our domain model. With this, you'll continue to see how attributes and methods can be combined to perform operations and store values simultaneously.'''\n",
    "\n",
    "# https://github.com/ericthansen/dsc-object-oriented-attributes-with-functions-lab\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#should autoreload from a .py file\n",
    "\n",
    "## my school.py file for reference:\n",
    "'''class School():\n",
    "    def __init__(self, name='SchoolName'):\n",
    "        self.name=name\n",
    "        self.roster = {}\n",
    "        \n",
    "    def add_student(self, name, grade):\n",
    "        #self.name = name\n",
    "        #self.grade = grade\n",
    "        x = self.roster.get(grade,[])#bag[word] = bag.get(word, 0) + 1\n",
    "        x.append(name)\n",
    "        self.roster[grade] = x\n",
    "        \n",
    "        \n",
    "    def grade(self, gr):\n",
    "        return self.roster.get(gr)\n",
    "        \n",
    "    def sort_roster(self):\n",
    "        newd = {}\n",
    "        for k in self.roster.keys():\n",
    "            #print(k,v)\n",
    "            #for l in self.roster[k]:\n",
    "            sortl = sorted(self.roster[k])#sorted(l)\n",
    "            newd[k] = sortl\n",
    "        return newd\n",
    "                \n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-object-oriented-shopping-cart-lab\n",
    "class ShoppingCart:\n",
    "    # write your code here\n",
    "    def __init__(self, emp_discount=None):\n",
    "        self.total = 0\n",
    "        self.employee_discount = emp_discount\n",
    "        self.items = []\n",
    "      \n",
    "    def add_item(self, name, price, quantity=1):\n",
    "        self.total += price*quantity\n",
    "        for i in range(quantity):\n",
    "            self.items.append((name, price))\n",
    "        return self.total\n",
    "    def mean_item_price(self):\n",
    "        runtot = 0\n",
    "        for item in self.items:\n",
    "            runtot+=item[1]\n",
    "        return runtot/len(self.items)\n",
    "\n",
    "    def median_item_price(self):\n",
    "        l = len(self.items)\n",
    "        if l == 0:\n",
    "            return 0\n",
    "        elif l%2 == 0:\n",
    "            #even nonzero\n",
    "            return (self.items[l/2][1] + self.items[l/2-1][1])/2\n",
    "        else:\n",
    "            #odd\n",
    "            return self.items[int(l/2)][1]\n",
    "            \n",
    "\n",
    "    def apply_discount(self):\n",
    "        if self.employee_discount:\n",
    "            self.disc_total = self.total * (1-self.employee_discount/100)\n",
    "            return self.disc_total\n",
    "        else:\n",
    "            return \"Sorry, there is no discount to apply to your cart :(*** \"\n",
    "\n",
    "    def void_last_item(self):      \n",
    "        if len(self.items)==0:\n",
    "            return \"There are no items in your cart!\"\n",
    "        else:\n",
    "            removed = self.items.pop()\n",
    "            self.total -= removed[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-inheritance\n",
    "## a nice little reminder of how class inheritance works, using the Beatles!\n",
    "\n",
    "# https://github.com/ericthansen/dsc-inheritance-lab\n",
    "class Animal(object):\n",
    "    def __init__(self, name, weight, size, species, food_type, nocturnal):\n",
    "        self.name = name\n",
    "        self.size = size\n",
    "        self.weight = weight\n",
    "        self.species = species\n",
    "        self.food_type = food_type\n",
    "        self.nocturnal = nocturnal\n",
    "        \n",
    "    def sleep(self):\n",
    "        if nocturnal:\n",
    "            print('The {} sleeps at night.'.format(self.species))\n",
    "        else:\n",
    "            print('The {} sleeps in the daytime.'.format(self.species))\n",
    "            \n",
    "    def eat(self, food):\n",
    "        if food == 'plants':\n",
    "            if self.food_type in ['herbivore', 'omnivore']:\n",
    "                p= f'{self.name} the {self.species} thinks {food} is yummy!'\n",
    "                \n",
    "            else:\n",
    "                p = \"I don't eat this!\"\n",
    "        else:\n",
    "            if self.food_type in ['carnivore', 'omnivore']:\n",
    "                p = f'{self.name} the {self.species} thinks {food} is yummy!'\n",
    "            else:\n",
    "                p = \"I don't eat this!\"\n",
    "        print(p)\n",
    "        return p\n",
    "            \n",
    "class Elephant(Animal):\n",
    "    def __init__(self, name, weight):\n",
    "        super().__init__(name,  weight, size='enormous', species='elephant', food_type='herbivore', nocturnal=False)\n",
    "        \n",
    "\n",
    "def add_animal_to_zoo(zoo, animal_type, name, weight):\n",
    "    if animal_type == 'Gorilla':\n",
    "        a = Gorilla(name, weight)\n",
    "    elif animal_type == 'Raccoon':\n",
    "        a = Raccoon(name, weight)\n",
    "    elif animal_type == 'Tiger':\n",
    "        a = Tiger(name, weight)\n",
    "    elif animal_type == 'Elephant':\n",
    "        a = Elephant(name, weight)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        return None\n",
    "    zoo.append(a)\n",
    "    return zoo\n",
    "# Create your animals and add them to the 'zoo' in this cell!\n",
    "zoo = []\n",
    "add_animal_to_zoo(zoo, 'Elephant', 'Stampy', '1000')\n",
    "add_animal_to_zoo(zoo, 'Elephant', 'Stompy', '1001')\n",
    "\n",
    "add_animal_to_zoo(zoo, 'Raccoon', 'Merry', '30')\n",
    "add_animal_to_zoo(zoo, 'Raccoon', 'Pip', '35')\n",
    "\n",
    "add_animal_to_zoo(zoo, 'Gorilla', 'Ivan', '350')\n",
    "\n",
    "add_animal_to_zoo(zoo, 'Tiger', 'Huey', '400')\n",
    "add_animal_to_zoo(zoo, 'Tiger', 'Dewey', '405')\n",
    "add_animal_to_zoo(zoo, 'Tiger', 'Louie', '410')\n",
    "\n",
    "\n",
    "def feed_animals(zoo, time='Day'):\n",
    "    if time=='Day':\n",
    "        for ani in zoo:\n",
    "            if not ani.nocturnal:\n",
    "                if ani.food_type == 'carnivore':\n",
    "                    print(ani.eat('meat'))\n",
    "                else:\n",
    "                    ani.eat('plants')\n",
    "    else:\n",
    "        for ani in zoo:\n",
    "            if ani.nocturnal:\n",
    "                if ani.food_type == 'carnivore':\n",
    "                    ani.eat('meat')\n",
    "                else:\n",
    "                    ani.eat('plants')\n",
    "            \n",
    "feed_animals(zoo, 'Day')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-building-an-object-oriented-simulation\n",
    "This simulation is meant to model the effects that vaccinations have on the way a communicable disease spreads through a population. The simulation you're building depends on just a few statistics from the CDC (Center for Disease Control):\n",
    "\n",
    "r0, the average number of people a contagious person infects before they are no longer contagious (because they got better or they died)\n",
    "mortality_rate, the percentage chance a person infected with a disease will die from it\n",
    "The main workflow of this simulation is as follows:\n",
    "\n",
    "Create a Person() class with the following attributes:\n",
    "\n",
    "alive\n",
    "vaccinated\n",
    "is_infected\n",
    "has_been_infected\n",
    "newly_infected\n",
    "Create a Simulation() class with the following attributes:\n",
    "\n",
    "population\n",
    "virus_name\n",
    "num_time_steps\n",
    "r0\n",
    "percent_pop_vaccinated\n",
    "Create methods for our Simulation() class that will cover each step of the simulation.\n",
    "\n",
    "In order for our simulation to work, you'll need to define some rules for it:\n",
    "\n",
    "Each infected person will \"interact\" with 100 random people from the population. If the person the infected individual interacts with is sick, vaccinated, or has had the disease before, nothing happens. However, if the person the infected individual interacts with is healthy, unvaccinated, and has not been infected yet, then that person becomes infected.\n",
    "\n",
    "At the end of each round, the following things happen:\n",
    "\n",
    "All currently infected people either get better from the disease or die, with the chance of death corresponding to the mortality rate of the disease\n",
    "All people that were newly infected during this round become the new infected for the next round\n",
    "The simulation continues for the set number of time steps. Any time someone dies or gets infected, log it in a text file called 'simulation_logs.txt'. Once the simulation is over, write some code to quickly parse the text logs into data and visualize the results, so that you can run multiple simulations and answer questions like:\n",
    "\n",
    "If vaccination rates for {disease x} dropped by 5%, how many more people become infected in an epidemic? How many more die?\n",
    "What does the spread of {disease x} through a population look like?\n",
    "If this all seems a bit daunting, don't worry! You'll be provided with much more detail as you build this step-by-step during the lab.\n",
    "\n",
    "With that, go ahead and take a look at this cool simulation lab!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-building-an-object-oriented-simulation-lab\n",
    "  #whole page is pretty neat, but this is SUPER cool\n",
    "    from tqdm.autonotebook import tqdm\n",
    "    for _ in tqdm(range(self.total_time_steps)): #tqdm puts a timer status bar for your iterable!\n",
    "        \n",
    "\n",
    "def run(self):\n",
    "    \"\"\"\n",
    "    The main function of the simulation.  This will run the simulation starting at time step 0, calculating\n",
    "    and logging the results of each time step until the final time_step is reached. \n",
    "    \"\"\"\n",
    "    \n",
    "    for _ in tqdm(range(self.total_time_steps)):\n",
    "        # Print out the current time step \n",
    "        print(\"Beginning Time Step {}\".format(self.current_time_step))\n",
    "        # Call our `_time_step()` function\n",
    "        self._time_step()\n",
    "    \n",
    "    # Simulation is over--log results to a file by calling _log_time_step_statistics(write_to_file=True)\n",
    "    self._log_time_step_statistics(write_to_file=True)\n",
    "\n",
    "# Adds the run() function to our Simulation class.\n",
    "Simulation.run = run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-rules-for-derivatives-lab\n",
    "# see the file, basically derivative rules for polynomials encoded as lists of tuples, then lots of neat plotting.\n",
    "\n",
    "# https://github.com/ericthansen/dsc-derivatives-chain-rule\\\n",
    "  # yep, the chain rule is a thing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-gradient-to-cost-function-appendix\n",
    "  #and here, they make partial derivatives 1000 times more complicated than they need to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6],\n",
       "       [5, 9],\n",
       "       [7, 9]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/ericthansen/dsc-linalg-vector-addition-codealong\n",
    "# adding\n",
    "# Code here \n",
    "import numpy as np\n",
    "\n",
    "# Adding 1D arrays\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6]) \n",
    "c = a + b\n",
    "c\n",
    "# Adding 2D matrices\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "B = np.array([[1, 4], [2, 5], [2, 3]])\n",
    "# Add matrices A and B\n",
    "C = A + B\n",
    "C\n",
    "#note dimensions have to match\n",
    "\n",
    "#broadcasting (just one way)\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "B = np.array([[2], [4], [6]])\n",
    "print(B)\n",
    "A + B\n",
    "\n",
    "# https://github.com/ericthansen/dsc-linalg-vector-matrices-numpy-lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-linalg-dot-product-properties-lab\n",
    "\n",
    "#https://github.com/ericthansen/dsc-linalg-python-vs-numpy-lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import thoughts from email\n",
    "\n",
    "#notes for the project: things to consider\n",
    "'''ridge/lasso regression penalize cost function for too many variables\n",
    "also, aic/bic does a similar thing for feature selection\n",
    "    Performing feature selection: comparing models with only a few variables and more variables, computing the AIC/BIC and select the features that generated the lowest AIC or BIC\n",
    "    Similarly, selecting or not selecting interactions/polynomial features depending on whether or not the AIC/BIC decreases when adding them in\n",
    "    Computing the AIC and BIC for several values of the regularization parameter in Ridge/Lasso models and selecting the best regularization parameter, and many more!\n",
    "\n",
    "simple logistic regression model\n",
    "max likelihood estimation (mle)\n",
    "    confusion matrices\n",
    "rOC curves and AUC\n",
    "?class imbalance problems?\n",
    "\n",
    "knn\n",
    "bayesian classification - good for bag of words, text classification, eg.\n",
    "gaussian bayes\n",
    "Decision trees!\n",
    "    entropy and info gain\n",
    "    id3 trees\n",
    "    CART trees (Classification and Regression Trees) - good for fitting to a curve (e.g. a cubic kind of thing)\n",
    "    recap nonparametric ML models-knn and decision trees - see https://flatiron.illumidesk.com/user/ericthansen/notebooks/dsc-nonparametric-models-lab/index.ipynb\n",
    "ensemble methods\n",
    "    bootstrap aggregation (bagging)\n",
    "    random forest\n",
    "    gridsearchcv\n",
    "    gradient boosting & weak learners\n",
    "    xgboost <--------------------------------\n",
    "SVMs\n",
    "    kernel trick\n",
    "Pipelines\n",
    "\n",
    "'''\n",
    "\n",
    "'''#Checklist - delete this before finalizing\n",
    "knn\n",
    "bayesian classification - good for bag of words, text classification, eg.\n",
    "gaussian bayes\n",
    "Decision trees!\n",
    "    entropy and info gain\n",
    "    id3 trees\n",
    "    CART trees (Classification and Regression Trees) - good for fitting to a curve (e.g. a cubic kind of thing)\n",
    "    recap nonparametric ML models-knn and decision trees - see https://flatiron.illumidesk.com/user/ericthansen/notebooks/dsc-nonparametric-models-lab/index.ipynb\n",
    "ensemble methods\n",
    "    bootstrap aggregation (bagging)\n",
    "    random forest\n",
    "    gridsearchcv\n",
    "    gradient boosting & weak learners\n",
    "    xgboost <--------------------------------\n",
    "SVMs\n",
    "    kernel trick\n",
    "Pipelines'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notes from pre-project meeting with claude fried\n",
    "'''\n",
    "find a project that is interesting/passionate\n",
    "60% of time doing cleaning/EDA\n",
    "%maybe not all that much doing models\n",
    "%rest of time interpreting\n",
    "pick a criter - i.e. accuracy, recall, f1, etc.\n",
    "xgboost, dec trees, etc.\n",
    "grid searchcv to tune hpyerparams\n",
    "optimize model using some criteria - by default, accuracy, \n",
    "great learning modment to start to get picture of tuning the knobs does\n",
    "-you get a feel for the impacts of these over time\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an interesting function for a dataframe to change height/width/grouping\n",
    "pd.melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   numeric1  numeric2  n1sq  n2sqrt  mod2\n",
      "0         1        25     1     5.0   3.0\n",
      "1         2        36     4     6.0   0.0\n",
      "2         5        49    25     7.0   3.0\n",
      "3        11       121   121    11.0   3.0\n",
      "4        12        81   144     9.0   0.0\n",
      "Training set accuracy: 100.0%\n",
      "Test set accuracy: 100.0%\n",
      "1.0\n",
      "Pickled and unpickled models produce same scores!\n"
     ]
    }
   ],
   "source": [
    "#work for blog on pickling the pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "\n",
    "col_d = {'numeric1':[1, 2, 5, 11, 12],      \n",
    "        'numeric2': [25, 36, 49, 121, 81], \n",
    "        'target':[1, 0, 1, 1, 0 ]} \n",
    "#note, target = ((n1**2*n2**(1/2))%2)\n",
    " \n",
    "df = pd.DataFrame(col_d)\n",
    "df_x = df.drop('target', axis=1)\n",
    "df_y = df['target']\n",
    "\n",
    "#suppose we wanted to create non-linear columns because we had reason to believe\n",
    "# such a relationship held; and perhaps even knew there was the modulus operation happening\n",
    "# but suppose we were off by a multiple, let's say 3\n",
    "\n",
    "def feat_eng(df):\n",
    "    df['n1sq'] = np.power(df['numeric1'], 2)\n",
    "    df['n2sqrt'] = np.power(df['numeric2'], 1/2)\n",
    "    df['mod2'] = 3*((np.power(df['numeric1'], 2) * np.power(df['numeric2'], 1/2)) % 2)\n",
    "    return df\n",
    "\n",
    "df_xe= feat_eng(df_x)\n",
    "print(df_xe)\n",
    "\n",
    "##\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class customFeats(TransformerMixin, BaseEstimator):\n",
    "    '''object wrapper for engineered features, suitable for pipelining'''\n",
    "    def transform(self, X):\n",
    "        X = feat_eng(X)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "##\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('customFeats', customFeats()),\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('RFclf', RandomForestClassifier( criterion='gini', n_estimators=10)) \n",
    "    ])\n",
    "\n",
    "#    \n",
    "\n",
    "col_d_test = {'numeric1':[1, 3, 5, 11, 12],      \n",
    "        'numeric2': [144, 36, 225, 121, 81], \n",
    "        'target':[0, 0, 1, 1, 0 ]} \n",
    "#note, target = ((n1**2*n2**(1/2))%2)\n",
    " \n",
    "df_test = pd.DataFrame(col_d_test)\n",
    "df_x_test = df.drop('target', axis=1)\n",
    "df_y_test = df['target']\n",
    "\n",
    "pipe.fit(df_x, df_y)\n",
    "print('Training set accuracy: {:.4}%'.format(100*pipe.score(df_x, df_y)))\n",
    "print('Test set accuracy: {:.4}%'.format(100*pipe.score(df_x_test, df_y_test)))\n",
    "\n",
    "##\n",
    "\n",
    "import pickle\n",
    "with open('clf_model_1.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe,f)\n",
    "\n",
    "with open('clf_model_1.pkl', 'rb') as f:\n",
    "    unpickled_pipe = pickle.load(f)\n",
    "    \n",
    "print(unpickled_pipe.score(df_x_test, df_y_test))\n",
    "\n",
    "if unpickled_pipe.score(df_x_test, df_y_test) == pipe.score(df_x_test, df_y_test):\n",
    "    print('Pickled and unpickled models produce same scores!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-phase-3-project-work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# phase 4 #################################\n",
    "# https://github.com/ericthansen/dsc-pca-introduction?organization=ericthansen&organization=ericthansen\n",
    "# https://github.com/ericthansen/dsc-unsupervised-learning?organization=ericthansen&organization=ericthansen\n",
    "# https://github.com/ericthansen/dsc-curse-of-dimensionality\n",
    "# https://github.com/ericthansen/dsc-curse-of-dimensionality-lab\n",
    "import numpy as np\n",
    "def euclidean_distance(p1, p2):\n",
    "    # Your code here\n",
    "    if len(p1)!=len(p2):\n",
    "        print(\"Dimensions do not match.\")\n",
    "        return None\n",
    "    tot = 0\n",
    "    for i in range(len(p1)):\n",
    "        tot+=(p1[i]-p2[i])**2\n",
    "    return np.sqrt(tot)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "#alternately, using numpy\n",
    "def euclidean_distance(p1, p2):\n",
    "    p1, p2 = np.array(p1), np.array(p2) # Ensure p1 and p2 are NumPy arrays\n",
    "    return np.sqrt(np.sum(np.square(p2 - p1)))\n",
    "\n",
    "# Your code here\n",
    "import random as rand\n",
    "# x = np.linspace(start=0, stop=100, num=10**2)\n",
    "# y = [xi**2 for xi in x]\n",
    "'''x = np.linspace(-10, 10, 101)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10,10))\n",
    "plt.title('Graphs of Various Polynomials')\n",
    "for n in range(1,9):\n",
    "    row = (n-1)//2\n",
    "    col = n%2-1\n",
    "    ax = axes[row][col]\n",
    "    y = [xi**n for xi in x]\n",
    "    ax.plot(x,y)\n",
    "    ax.set_title('x^{}'.format(n))'''\n",
    "av_distances = []\n",
    "for n in range(1, 1001):\n",
    "    av_dist = 0\n",
    "    randpoint_dists = []\n",
    "    for _ in range(100):\n",
    "        p1 = rand.choices(range(-10, 11), k=n)\n",
    "        p2 = np.zeros(n)\n",
    "        dist = euclidean_distance(p1, p2)\n",
    "        randpoint_dists.append(dist)\n",
    "        #print(pt)\n",
    "    av_dist = np.mean(randpoint_dists)\n",
    "    av_distances.append(av_dist)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,10))\n",
    "#plt.title('Average distance from origin vs increasing dimension')\n",
    "\n",
    "ax.plot(range(1,1001),av_distances)\n",
    "ax.set_title('Average distance from origin vs increasing dimension')\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "# ⏰ Your code may take some time to run \n",
    "ols = LinearRegression()\n",
    "\n",
    "sample_size = 1000\n",
    "times = []\n",
    "for n in range(1,sample_size+1):\n",
    "    x_vals = [np.random.uniform(low=-10, high=10, size=n) for i in range(sample_size)]\n",
    "    coeff = np.array(range(1, n+1))\n",
    "    #print(coeff)\n",
    "    #print(coeff*x_vals)\n",
    "    y_vals = np.sum(coeff*x_vals, axis=1) + np.random.normal(loc=0, scale=.1, size=sample_size)\n",
    "    ols = LinearRegression()\n",
    "    start = datetime.datetime.now()\n",
    "    ols.fit(x_vals, y_vals)\n",
    "    end = datetime.datetime.now()\n",
    "    elapsed = end - start\n",
    "    times.append(elapsed)\n",
    "plt.plot(range(1,sample_size+1), [t.microseconds for t in times]);\n",
    "\n",
    "# ⏰ Your code may take some time to run \n",
    "lasso = Lasso()\n",
    "\n",
    "sample_size = 1000\n",
    "times = []\n",
    "for n in range(1,sample_size+1):\n",
    "    x_vals = [np.random.uniform(low=-10, high=10, size=n) for i in range(sample_size)]\n",
    "    coeff = np.array(range(1, n+1))\n",
    "    #print(coeff)\n",
    "    #print(coeff*x_vals)\n",
    "    y_vals = np.sum(coeff*x_vals, axis=1) + np.random.normal(loc=0, scale=.1, size=sample_size)\n",
    "    lasso = LinearRegression()\n",
    "    start = datetime.datetime.now()\n",
    "    lasso.fit(x_vals, y_vals)\n",
    "    end = datetime.datetime.now()\n",
    "    elapsed = end - start\n",
    "    times.append(elapsed)\n",
    "plt.plot(range(1,sample_size+1), [t.microseconds for t in times]);\n",
    "\n",
    "###may take over an hour - \n",
    "# ⏰ This code will take some time to run\n",
    "sample_size = 10**3\n",
    "times = []\n",
    "for n in range(1, 10001):\n",
    "    xi = [np.random.uniform(low=-10, high=10, size=n) for i in range(sample_size)]\n",
    "    coeff = np.array(range(1, n + 1))\n",
    "    yi = np.sum(coeff*xi, axis=1) + np.random.normal(loc=0, scale=0.1, size=sample_size)\n",
    "    lasso = Lasso()\n",
    "    start = datetime.datetime.now()\n",
    "    lasso.fit(xi, yi)\n",
    "    end = datetime.datetime.now()\n",
    "    elapsed = end - start\n",
    "    times.append(elapsed)\n",
    "plt.plot(range(1, 10001), [t.microseconds for t in times]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://github.com/ericthansen/dsc-pca-in-scikitlearn\n",
    "import numpy as np\n",
    "#generate some data\n",
    "x1 = np.linspace(-10, 10, 100)\n",
    "# A linear relationship, plus a little noise\n",
    "x2 = np.array([xi*2 + np.random.normal(loc=0, scale=0.5) for xi in x1]) \n",
    "\n",
    "X = np.matrix(list(zip(x1, x2)))\n",
    "\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "plt.scatter(x1, x2);\n",
    "\n",
    "#pca with scikitlearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "transformed = pca.fit_transform(X)\n",
    "#and plot it\n",
    "plt.scatter(transformed[:,0], transformed[:,1]);\n",
    "#see components\n",
    "pca.components_\n",
    "pca.mean_\n",
    "#interpret results\n",
    "plt.scatter(x1, x2);\n",
    "ax1, ay1 = pca.mean_[0], pca.mean_[1]\n",
    "ax2, ay2 = pca.mean_[0] + pca.components_[0][0], pca.mean_[1] + pca.components_[0][1]\n",
    "ax3, ay3 = pca.mean_[0] + pca.components_[1][0], pca.mean_[1] + pca.components_[1][1]\n",
    "plt.plot([ax1, ax2], [ay1, ay2], color='red')\n",
    "plt.plot([ax2, ax3], [ay2, ay3], color='red');\n",
    "\n",
    "#but PCA transforms the dataset along principal axes. The first of these axes is designed to capture the maximum variance within the data. From here, additional axes are constructed which are orthogonal to the previous axes and continue to account for as much of the remaining variance as possible.\n",
    "\n",
    "#but when transformed onto highest-variance axes, \n",
    "plt.scatter(transformed[:,0], transformed[:,1])\n",
    "plt.axhline(color='red')\n",
    "plt.axvline(color='red');\n",
    "\n",
    "#\n",
    "#to replot using original scale\n",
    "plt.scatter(transformed[:,0], transformed[:,1])\n",
    "plt.axhline(color='red')\n",
    "plt.axvline(color='red')\n",
    "plt.ylim(-10,10);\n",
    "\n",
    "#typically, use PCA to reduce the dimension.  In this case, we just re-re-parametrized the data onto a more clear set of axes\n",
    "#but this makes the relationship clearer\n",
    "pca.explained_variance_ratio_\n",
    "'''Keep in mind that these quantities are cumulative: principal component 2 attempts to account for the variance not accounted for in the primary component. You can view the total variance using np.cumsum():'''\n",
    "np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "#visualize the principal component\n",
    "plt.scatter(x1,x2, c=sns.color_palette('RdBu', n_colors=100));\n",
    "\n",
    "#and on new axes\n",
    "plt.scatter(transformed[:,0], [0 for i in range(100)] , c=sns.color_palette('RdBu', n_colors=100));\n",
    "\n",
    "'''Steps for Performing PCA\n",
    "The theory behind PCA rests upon many foundational concepts of linear algebra. After all, PCA is re-encoding a dataset into an alternative basis (the axes). Here are the exact steps:\n",
    "\n",
    "Recenter each feature of the dataset by subtracting that feature's mean from the feature vector\n",
    "Calculate the covariance matrix for your centered dataset\n",
    "Calculate the eigenvectors of the covariance matrix\n",
    "You'll further investigate the concept of eigenvectors in the upcoming lesson\n",
    "Project the dataset into the new feature space: Multiply the eigenvectors by the mean-centered features'''\n",
    "#to see the intermediate steps...\n",
    "# Pulling up the original feature means which were used to center the data\n",
    "pca.mean_ \n",
    "# Pulling up the covariance matrix of the mean centered data\n",
    "pca.get_covariance() \n",
    "# Pulling up the eigenvectors of the covariance matrix\n",
    "pca.components_ \n",
    "\n",
    "##cool!  linear algebra stuff.  i think they gloss over the idea here. is this change of basis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://github.com/ericthansen/dsc-pca-in-scikitlearn-lab\n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    " \n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['Target'] = iris.get('target')\n",
    "df.head()\n",
    "##get context\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.plotting.scatter_matrix(df, figsize=(10,10));\n",
    "\n",
    "## Create features and target datasets\n",
    "features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "X = df[features]\n",
    "y = df['Target']\n",
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features\n",
    "\n",
    "X = StandardScaler().fit_transform(X, y)\n",
    "\n",
    "# Preview X\n",
    "pd.DataFrame(data=X, columns=features).head()\n",
    "\n",
    "##pca projection to 2-d\n",
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Instantiate PCA\n",
    "pca = PCA(2)\n",
    "\n",
    "# Fit PCA\n",
    "pca.fit(X)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "#principalComponents\n",
    "\n",
    "#visualize the table\n",
    "# Create a new dataset from principal components \n",
    "df = pd.DataFrame(data = principalComponents, \n",
    "                  columns = ['PC1', 'PC2'])\n",
    "\n",
    "target = pd.Series(iris['target'], name='target')\n",
    "\n",
    "result_df = pd.concat([df, target], axis=1)\n",
    "result_df.head(5)\n",
    "\n",
    "##visualize principal components\n",
    "# Principal Componets scatter plot\n",
    "\n",
    "\n",
    "# Your code here \n",
    "plt.style.use('seaborn-dark')\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('First Principal Component ', fontsize = 15)\n",
    "ax.set_ylabel('Second Principal Component ', fontsize = 15)\n",
    "ax.set_title('Principal Component Analysis (2PCs) for Iris Dataset', fontsize = 20)\n",
    "\n",
    "targets = [0, 1, 2]\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = iris['target'] == target\n",
    "    ax.scatter(result_df.loc[indicesToKeep, 'PC1'], \n",
    "               result_df.loc[indicesToKeep, 'PC2'], \n",
    "               c = color, \n",
    "               s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "\n",
    "\n",
    "##explained variance!!\n",
    "# Calculate the variance explained by pricipal components\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "##compare performance of a knn classifier with PCA (same classifier, using full data vs PCA reduced data)\n",
    "# Classification - complete Iris dataset\n",
    "\n",
    "# Your code here \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "start = time.time()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=9)\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "Yhat = model.predict(X_test)\n",
    "acc = metrics.accuracy_score(Yhat, Y_test)\n",
    "end = time.time()\n",
    "print('Accuracy:', acc)\n",
    "print ('Time Taken:', end - start)\n",
    "\n",
    "# Classification - reduced (PCA) Iris dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "X = result_df.drop('target', axis=1)\n",
    "y = result_df.target\n",
    "start = time.time()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=9)\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "Yhat = model.predict(X_test)\n",
    "acc = metrics.accuracy_score(Yhat, Y_test)\n",
    "end = time.time()\n",
    "print('Accuracy:', acc)\n",
    "print ('Time Taken:', end - start)\n",
    "\n",
    "##visualize the boundary \n",
    "# Plot decision boundary using principal components \n",
    "import numpy as np \n",
    "def decision_boundary(pred_func):\n",
    "    \n",
    "    # Set the boundary\n",
    "    x_min, x_max = X.iloc[:, 0].min() - 0.5, X.iloc[:, 0].max() + 0.5\n",
    "    y_min, y_max = X.iloc[:, 1].min() - 0.5, X.iloc[:, 1].max() + 0.5\n",
    "    h = 0.01\n",
    "    \n",
    "    # Build meshgrid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the contour\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.afmhot)\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.Spectral, marker='x')\n",
    "\n",
    "decision_boundary(lambda x: model.predict(x))\n",
    "\n",
    "plt.title('decision boundary');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-pca-and-pipelines-v2-1\n",
    "# see also https://github.com/ericthansen/dsc-pca-and-pipelines-v2-1/tree/solution\n",
    "\n",
    "#data inspection\n",
    "# Your code here\n",
    "import pandas as pd\n",
    " \n",
    "df = pd.read_csv('otto_group.csv')\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "# Your code here\n",
    "display(df.isna().sum().max())\n",
    "display(df.isna().any().any())\n",
    "\n",
    "# Your code here\n",
    "display(df.describe())\n",
    "# Your code here\n",
    "feat = df.loc[:, 'feat_1':'feat_93']#'feat_1':'feat_93']\n",
    "feat.hist(figsize=(30,20));\n",
    "# Your code here\n",
    "feat.boxplot(figsize=(10,10));\n",
    "# Your code here\n",
    "target = df['target']\n",
    "\n",
    "##feature eng and selection with pca\n",
    "# Your code here\n",
    "feat = df.loc[:, 'feat_1':'feat_93']\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(feat.corr(), center=0);\n",
    "feat.columns\n",
    "# Your code here\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "pcas = {}\n",
    "threshold = 0.01\n",
    "target = .8\n",
    "lower=10\n",
    "upper=60\n",
    "curr_comp = int((lower+upper)/2)\n",
    "while True:\n",
    "    pca_curr = PCA(n_components=curr_comp)\n",
    "    principal_comps = pca_curr.fit_transform(feat)\n",
    "    exp_var_rat = np.sum(pca_curr.explained_variance_ratio_)\n",
    "    if exp_var_rat < target - threshold:\n",
    "        lower = curr_comp\n",
    "        curr_comp = int((lower+upper)/2)\n",
    "    elif exp_var_rat > target + threshold: \n",
    "        upper = curr_comp\n",
    "        curr_comp = int((lower+upper)/2)\n",
    "    else:\n",
    "        print(np.sum(pca_curr.explained_variance_ratio_))\n",
    "        print(curr_comp)   \n",
    "        break\n",
    "\n",
    "curr_comp\n",
    "##create train/test split 40%\n",
    "# Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(feat, target, test_size=0.4, random_state=42)\n",
    "\n",
    "# Your code here\n",
    "import seaborn as sns\n",
    "sns.heatmap(pd.DataFrame(principal_comps).corr(), center=0);\n",
    "\n",
    "##baseline model\n",
    "# Your code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('pca', PCA(n_components=curr_comp, random_state=123)), \n",
    "    ('clf', LogisticRegression(random_state=123))\n",
    "    ])\n",
    "\n",
    "# Your code here\n",
    "#fitting the model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "#testing the result\n",
    "print(pipe.score(X_test, y_test))\n",
    "# Your code here\n",
    "#manually check accuracy:\n",
    "np.sum(pipe.predict(X_test) == y_test)/len(y_test)\n",
    "\n",
    "pipe_lr = pipe\n",
    "##create pipeline for svm, rf, dt\n",
    "# Your code here\n",
    "# ⏰ This cell may take several minutes to run\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#Linear SVM\n",
    "pipe_svm = Pipeline(steps=[\n",
    "    ('pca', PCA(n_components=curr_comp, random_state=123)), \n",
    "    ('clf', svm.SVC(random_state=123))\n",
    "    ])\n",
    "\n",
    "#Decision Tree\n",
    "pipe_dt = Pipeline(steps=[\n",
    "    ('pca', PCA(n_components=curr_comp, random_state=123)), \n",
    "    ('clf', tree.DecisionTreeClassifier(random_state=123))\n",
    "    ])\n",
    "\n",
    "#Random Forest\n",
    "pipe_rf = Pipeline(steps=[\n",
    "    ('pca', PCA(n_components=curr_comp, random_state=123)), \n",
    "    ('clf', RandomForestClassifier(random_state=123))\n",
    "    ])\n",
    "\n",
    "pipes=[pipe_svm, pipe_dt, pipe_rf]\n",
    "pipelabels=['SVM', 'DT', 'RF']\n",
    "for pipe, pipelabel in zip(pipes, pipelabels):\n",
    "    pipe.fit(X_train, y_train)\n",
    "    print('Accuracy score for {} model: {}'.format(pipelabel, pipe.score(X_test, y_test)))\n",
    "    \n",
    "##pipeline with gridsearch for ada, rf\n",
    "# Your code here \n",
    "# imports\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Your code here\n",
    "# ⏰ This cell may take a long time to run!\n",
    "pipe_rf = Pipeline([('pca', PCA(n_components=curr_comp)), \n",
    "                    ('clf', RandomForestClassifier(random_state = 123))])\n",
    "\n",
    "# Set grid search params\n",
    "param_grid_forest = [ \n",
    "  {'clf__n_estimators': [120],\n",
    "   'clf__criterion': ['entropy', 'gini'], \n",
    "   'clf__max_depth': [4, 5],  \n",
    "   'clf__min_samples_leaf':[0.05 ,0.1, 0.2],  \n",
    "   'clf__min_samples_split':[0.05 ,0.1, 0.2]\n",
    "  }\n",
    "]\n",
    "\n",
    "# Construct grid search\n",
    "gs_rf = GridSearchCV(estimator=pipe_rf, \n",
    "                     param_grid=param_grid_forest, \n",
    "                     scoring='accuracy', \n",
    "                     cv=3, verbose=1, return_train_score = True)\n",
    "\n",
    "# Fit using grid search\n",
    "gs_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_rf.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs_rf.best_params_)\n",
    "\n",
    "# Your code here \n",
    "display(gs_rf.cv_results_)\n",
    "\n",
    "##and gridsearch for ada\n",
    "# Your code here\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# ⏰ This cell may take several minutes to run\n",
    "pipe_ada = Pipeline([('pca', PCA(n_components=curr_comp)), \n",
    "                    ('clf', AdaBoostClassifier(random_state = 123))])\n",
    "\n",
    "# Set grid search params\n",
    "adaboost_param_grid = {\n",
    "    'clf__n_estimators': [30, 50, ],\n",
    "    'clf__learning_rate': [1.0, 0.1]\n",
    "}\n",
    "\n",
    "# Construct grid search\n",
    "gs_rf = GridSearchCV(estimator=pipe_rf, \n",
    "                     param_grid=param_grid_forest, \n",
    "                     scoring='accuracy', \n",
    "                     cv=3, verbose=1, return_train_score = True)\n",
    "\n",
    "# Fit using grid search\n",
    "gs_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_rf.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs_rf.best_params_)\n",
    "\n",
    "# Your code here \n",
    "display(gs_ada.cv_results_)\n",
    "\n",
    "## and gridsearch with svm\n",
    "# Your code here\n",
    "# ⏰ This cell may take a very long time to run!\n",
    "# Construct pipeline\n",
    "pipe_svm = Pipeline([('pca', PCA(n_components=27)), \n",
    "                     ('clf', svm.SVC(random_state=123))])\n",
    "\n",
    "# Set grid search params\n",
    "param_grid_svm = [\n",
    "  {'clf__C': [0.1, 1, 10] , 'clf__kernel': ['linear']},\n",
    "  {'clf__C': [1, 10], 'clf__gamma': [0.001, 0.01], 'clf__kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# Construct grid search\n",
    "gs_svm = GridSearchCV(estimator=pipe_svm, \n",
    "                      param_grid=param_grid_svm, \n",
    "                      scoring='accuracy', \n",
    "                      cv=3, verbose=2, return_train_score = True)\n",
    "\n",
    "# Fit using grid search\n",
    "gs_svm.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs_svm.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs_svm.best_params_)\n",
    "\n",
    "# Your code here \n",
    "display(gs_svm.cv_results_)\n",
    "\n",
    "'''Note\n",
    "Note that this solution is only one of many options. The results in the Random Forest and AdaBoost models show that there is a lot of improvement possible by tuning the hyperparameters further, so make sure to explore this yourself!'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-pca-covariance-matrix-eigendecomp.git\n",
    "'''The PCA Algorithm\n",
    "Recall that the general procedure for PCA is:\n",
    "\n",
    "Recenter each feature of the dataset by subtracting that feature's mean from the feature vector\n",
    "Calculate the covariance matrix for your centered dataset\n",
    "Calculate the eigenvectors of the covariance matrix\n",
    "Project the dataset into the new feature space: Multiply the eigenvectors by the mean-centered features'''\n",
    "\n",
    "#covarince matrix is the matrix of covariances between each variable in n-dimensions.  it's symmetric over main diag\n",
    "#calc covariance in numpy\n",
    "import numpy as np\n",
    "X = np.array([[0.1, 0.3, 0.4, 0.8, 0.9], \n",
    "              [3.2, 2.4, 2.4, 0.1, 5.5], \n",
    "              [10., 8.2, 4.3, 2.6, 0.9]])\n",
    "\n",
    "print(X)\n",
    "\n",
    "print(np.cov(X))\n",
    "#The diagonal elements,  𝐶𝑖𝑖  are the variances in the variables  𝑥𝑖  assuming  𝑁−1  degrees of freedom:\n",
    "print(np.var(X, axis=1, ddof=1))\n",
    "\n",
    "\n",
    "#eigenvector decomposition\n",
    "#A vector  𝑣  is an eigenvector of a square matrix  𝐴  if it satisfies the following equation:\n",
    "#𝐴𝑣=𝜆𝑣\n",
    "# The parent matrix can be shown to be a product of the eigenvectors and eigenvalues.\n",
    "# 𝐴=𝑄.𝑑𝑖𝑎𝑔(𝑉).𝑄−1\n",
    "\n",
    "#i'd not heard right/left row/col vectors before\n",
    "'''Eigenvectors are unit vectors, with length or magnitude equal to 1.0. They are often referred to as right vectors, which simply means a column vector (as opposed to a row vector or a left vector). Imagine a transformation matrix that, when multiplied on the left, reflected vectors in the line  𝑦=𝑥 . You can see that if there were a vector that lay on the line  𝑦=𝑥 , it’s reflection is itself. This vector (and all multiples of it), would be an eigenvector of that transformation matrix.'''\n",
    "'''A matrix that has only positive eigenvalues is referred to as a positive definite matrix, whereas if the eigenvalues are all negative, it is referred to as a negative definite matrix.'''\n",
    "#The eigendecomposition can be calculated in NumPy using the eig() function.\n",
    "# Eigendecomposition\n",
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# Define matrix\n",
    "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)\n",
    "print('-----------------')\n",
    "# Calculate eigendecomposition\n",
    "values, vectors = eig(A)\n",
    "print(values)\n",
    "print('-----------------')\n",
    "print(vectors)\n",
    "\n",
    "# Testing an eigenvector\n",
    "# Confirm first eigenvector\n",
    "B = A.dot(vectors[:, 0])\n",
    "print(B)\n",
    "print('-----------------')\n",
    "C = vectors[:, 0] * values[0]\n",
    "print(C)\n",
    "'''Reconstruct Original Matrix\n",
    "You can also reverse the process and reconstruct the original matrix given only the eigenvectors and eigenvalues.\n",
    "\n",
    "First, the list of eigenvectors must be converted into a matrix, where each vector becomes a row. \n",
    "The eigenvalues need to be arranged into a diagonal matrix. The NumPy diag() function can be used for this. \n",
    "Next, you need to calculate the inverse of the eigenvector matrix, which we can be achieved with the inv() function. \n",
    "Finally, these elements need to be multiplied together with the .dot() method.'''\n",
    "from numpy.linalg import inv\n",
    "# Create matrix from eigenvectors\n",
    "Q = vectors\n",
    "\n",
    "# Create inverse of eigenvectors matrix\n",
    "R = inv(Q)\n",
    "\n",
    "# Create diagonal matrix from eigenvalues\n",
    "L = np.diag(values)\n",
    "\n",
    "# Reconstruct the original matrix\n",
    "B = Q.dot(L).dot(R)\n",
    "print(B)\n",
    "\n",
    "#my thoughts - you can see that the third vector here is close to 0 - i think this is because it's not linearly independent\n",
    "#from the others - i.e. you can pca-out the third one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-performing-principal-component-analysis\n",
    "#pca from ground up\n",
    "\n",
    "#get some data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(42)\n",
    "\n",
    "x1 = np.random.uniform(low=0, high=10, size=100)\n",
    "x2 = [(xi*3)+np.random.normal(scale=2) for xi in x1]\n",
    "plt.scatter(x1, x2);\n",
    "\n",
    "#subtract the mean\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame([x1,x2]).transpose()\n",
    "data.columns = ['x1', 'x2']\n",
    "data.head()\n",
    "#\n",
    "data.mean()\n",
    "#\n",
    "mean_centered = data - data.mean()\n",
    "mean_centered.head()\n",
    "#\n",
    "#calculate the covariance matrix\n",
    "cov = np.cov([mean_centered['x1'], mean_centered['x2']])\n",
    "cov\n",
    "#calculate the eignevectors and eigenvalues\n",
    "eigen_value, eigen_vector = np.linalg.eig(cov)\n",
    "eigen_vector\n",
    "#\n",
    "eigen_value\n",
    "\n",
    "#choose components and forming a feature vector\n",
    "#the evector with the highest evalue is the principal component. for evects with tiny evals, they are lin dependent(or close)\n",
    "#to the prior evects\n",
    "'''Finally, you need to form a feature vector, which is just a fancy name for a matrix of vectors. \n",
    "This is constructed by taking the eigenvectors that you want to keep from the list of eigenvectors, \n",
    "and forming a matrix with these eigenvectors in the columns as shown below:'''\n",
    "# Get the index values of the sorted eigenvalues\n",
    "e_indices = np.argsort(eigen_value)[::-1] \n",
    "\n",
    "# Sort\n",
    "eigenvectors_sorted = eigen_vector[:, e_indices]\n",
    "eigenvectors_sorted\n",
    "\n",
    "\n",
    "\n",
    "## deriving new dataset\n",
    "'''This the final step in PCA, and is also the easiest. Once you have chosen the components (eigenvectors)\n",
    "that you wish to keep in our data and formed a feature vector, you simply take the transpose of the vector and \n",
    "multiply it on the left of the original dataset, transposed.'''\n",
    "#except not original dataset, the mean-centered one...\n",
    "transformed = eigenvectors_sorted.dot(mean_centered.T).T\n",
    "transformed[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-pca-numpy-lab\n",
    "#import\n",
    "import pandas as pd\n",
    "data = pd.read_csv('foodusa.csv', index_col=0)\n",
    "data.head()\n",
    "\n",
    "#normalize\n",
    "data = data - data.mean()\n",
    "data.head()\n",
    "#get cov matrix\n",
    "cov_mat = data.cov()\n",
    "cov_mat\n",
    "#calc eigvectors\n",
    "import numpy as np\n",
    "eig_values, eig_vectors = np.linalg.eig(cov_mat)\n",
    "#sort evectors\n",
    "# Get the index values of the sorted eigenvalues\n",
    "e_indices = np.argsort(eig_values)[::-1] \n",
    "\n",
    "# Sort \n",
    "eigenvectors_sorted = eig_vectors[:, e_indices]\n",
    "eigenvectors_sorted\n",
    "#reproject data\n",
    "transformed = (eigenvectors_sorted.T).dot(data).T\n",
    "display(transformed[:2])\n",
    "#orr....is it (solution set just said the latter)\n",
    "eigenvectors_sorted[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA for image recognition\n",
    "# https://github.com/ericthansen/dsc-pca-and-digital-image-processing.git\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "data = fetch_olivetti_faces()\n",
    "#preview\n",
    "fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(10,10))\n",
    "for n in range(20):\n",
    "    i = n //5\n",
    "    j = n%5\n",
    "    ax = axes[i][j]\n",
    "    ax.imshow(data.images[n], cmap=plt.cm.gray)\n",
    "plt.title('First 20 Images From the Olivetti Dataset');\n",
    "\n",
    "#train a baseline classifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)\n",
    "clf = svm.SVC(C=5, gamma=0.05)\n",
    "%timeit clf.fit(X_train, y_train)\n",
    "\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_acc, test_acc))\n",
    "##gridsearch on baseline clf - DO NOT RUN - Takes >60 minutes\n",
    "# ⏰ This cell may take over an hour to run!\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# clf = svm.SVC()\n",
    "# param_grid = {'C' : np.linspace(0.1, 10, num=11),\n",
    "#              'gamma' : np.linspace(10**-3, 5, num=11)}\n",
    "\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "# %timeit grid_search.fit(X_train, y_train)\n",
    "\n",
    "# grid_search.best_estimator_.score(X_test, y_test)\n",
    "\n",
    "#preprocessing with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "X[0].shape\n",
    "\n",
    "pca = PCA(n_components=100, whiten=True)\n",
    "X_pca_train = pca.fit_transform(X_train)\n",
    "X_pca_train.shape\n",
    "\n",
    "X_pca_train[0].shape\n",
    "\n",
    "#explore the explained variance\n",
    "plt.plot(range(1,101), pca.explained_variance_ratio_.cumsum())\n",
    "plt.title('Total Variance Explained by Varying Number of Principle Components');\n",
    "\n",
    "#train a classifier on the transformed dataset\n",
    "X_pca_test = pca.transform(X_test)\n",
    "clf = svm.SVC()\n",
    "%timeit clf.fit(X_pca_train, y_train)\n",
    "\n",
    "train_pca_acc = clf.score(X_pca_train, y_train)\n",
    "test_pca_acc = clf.score(X_pca_test, y_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_pca_acc, test_pca_acc))\n",
    "\n",
    "#gridsearch for optimal params\n",
    "# ⏰ This cell may take several minutes to run\n",
    "clf = svm.SVC()\n",
    "\n",
    "param_grid = {'C' : np.linspace(0.1, 10, num=11),\n",
    "             'gamma' : np.linspace(10**-3, 5, num=11)}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "%timeit grid_search.fit(X_pca_train, y_train)\n",
    "\n",
    "grid_search.best_params_\n",
    "\n",
    "grid_search.best_estimator_.score(X_pca_test, y_test)\n",
    "\n",
    "#though this loses some accuracy vs the 60 min plus non-PCA-reduced version, it takes much less time\n",
    "\n",
    "#visualize some of the features capturesd by PCA\n",
    "\n",
    "#vis feature means\n",
    "plt.imshow(X.mean(axis=0).reshape(data.images[0].shape), cmap=plt.cm.gray)\n",
    "\n",
    "#vis pca-compressed representations.  Note that post-pca, dimensions don't match, but there is a built in \n",
    "#inverse function we can use\n",
    "fig, axes\n",
    "plt.imshow(pca.inverse_transform(X_pca_train[0]).reshape(64,64), cmap=plt.cm.gray)\n",
    "\n",
    "# can view different re-presentations based on different # of PCA\n",
    "fig, axes = plt.subplots(ncols=4, nrows=3, figsize=(10,10))\n",
    "ax = axes[0][0]\n",
    "ax.set_title('Original Image')\n",
    "ax.imshow(X_train[0].reshape(64,64), cmap=plt.cm.gray)\n",
    "for n in range(1,12):\n",
    "    i = n //4\n",
    "    j = n%4\n",
    "    ax = axes[i][j]\n",
    "    ax.set_title('Re')\n",
    "    n_feats = n*10\n",
    "    pca = PCA(n_components=n_feats)\n",
    "    pca.fit(X_train)\n",
    "    compressed = pca.transform(X_train)\n",
    "    ax.set_title('Recovered Image from\\n{} principle components'.format(n_feats))\n",
    "    ax.imshow(pca.inverse_transform(compressed[0]).reshape(64,64), cmap=plt.cm.gray)\n",
    "plt.tight_layout()\n",
    "\n",
    "#what is unclear here is how PCA is extracting dimensionality - is it just using columns of bits or is it somehow doing a \n",
    "#different decomposition?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-pca-and-digital-image-processing-lab\n",
    "#load data\n",
    "# Load the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data = load_digits()\n",
    "print(data.data.shape, data.target.shape)\n",
    "\n",
    "#preview\n",
    "# Display the first 20 images \n",
    "fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(4,4))\n",
    "for n in range(20):\n",
    "    i = n //5\n",
    "    j = n%5\n",
    "    ax = axes[i][j]\n",
    "    ax.imshow(data.images[n], cmap=plt.cm.gray)\n",
    "plt.title('First 20 Images From the load_digits Dataset');\n",
    "\n",
    "#baseline model\n",
    "# Split the data\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "#fit a naive model\n",
    "# Fit a naive model \n",
    "clf = svm.SVC(C=5, gamma='auto')\n",
    "%timeit clf.fit(X_train, y_train)\n",
    "\n",
    "#train and test acc\n",
    "# Training and test accuracy\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print('Training Accuracy: {}\\nTesting Accuracy: {}'.format(train_acc, test_acc))\n",
    "\n",
    "#gridsearch baseline\n",
    "# Your code here\n",
    "# ⏰ Your code may take several minutes to run\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = svm.SVC()\n",
    "param_grid = {'C' : np.linspace(0.1, 10, num=5),\n",
    "             'gamma' : np.linspace(10**-3, 5, num=5)}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "%timeit grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_estimator_.score(X_test, y_test)\n",
    "\n",
    "#print best params\n",
    "# Print the best parameters \n",
    "grid_search.best_params_\n",
    "\n",
    "# Print the training and test accuracy \n",
    "clf = svm.SVC(C=2.575, gamma=0.001)\n",
    "clf.fit(X_train, y_train)\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_acc, test_acc))\n",
    "\n",
    "##compressing with PCA\n",
    "# Your code here\n",
    "from sklearn.decomposition import PCA\n",
    "X[0].shape\n",
    "\n",
    "pca = PCA(n_components=64, whiten=True)\n",
    "X_pca_train = pca.fit_transform(X_train)\n",
    "X_pca_train.shape\n",
    "\n",
    "X_pca_train[0].shape\n",
    "\n",
    "#plot explained variance\n",
    "# Your code here\n",
    "plt.plot(range(1,65), pca.explained_variance_ratio_.cumsum())\n",
    "plt.title('Total Variance Explained by Varying Number of Principle Components');\n",
    "\n",
    "#determine # of features needed to get 95% explained\n",
    "# Your code here\n",
    "for x, y in zip(range(1,65), pca.explained_variance_ratio_.cumsum()):\n",
    "    if y>.93 and y<.97:\n",
    "        print(x,y)\n",
    "#looks like 29\n",
    "#(can do another way, see solution in github)\n",
    " #i.e.\n",
    "    total_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "    n_over_95 = len(total_explained_variance[total_explained_variance >= .95])\n",
    "    n_to_reach_95 = X.shape[1] - n_over_95 + 1\n",
    "    print(\"Number features: {}\\tTotal Variance Explained: {}\".format(n_to_reach_95, total_explained_variance[n_to_reach_95-1]))\n",
    "    \n",
    "#subset the data\n",
    "# Your code here\n",
    "pca = PCA(n_components=29)#, whiten=True)\n",
    "X_pca_train = pca.fit_transform(X_train)\n",
    "X_pca_test = pca.transform(X_test)\n",
    "X_pca_train.shape\n",
    "\n",
    "X_pca_train[0].shape\n",
    "\n",
    "#refit a model on compressed data\n",
    "# Your code here\n",
    "clf = svm.SVC(C=5, gamma='auto')\n",
    "%timeit clf.fit(X_pca_train, y_train)\n",
    "\n",
    "train_acc = clf.score(X_pca_train, y_train)\n",
    "test_acc = clf.score(X_pca_test, y_test)\n",
    "print('Training Accuracy: {}\\nTesting Accuracy: {}'.format(train_acc, test_acc))\n",
    "\n",
    "#gridsearch\n",
    "# Your code here\n",
    "# ⏰ Your code may take several minutes to run\n",
    "clf = svm.SVC()\n",
    "param_grid = {'C' : np.linspace(0.1, 10, num=5),\n",
    "             'gamma' : np.linspace(10**-3, 5, num=5)}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "%timeit grid_search.fit(X_pca_train, y_train)\n",
    "\n",
    "grid_search.best_estimator_.score(X_pca_test, y_test)\n",
    "\n",
    "\n",
    "# Print the best parameters \n",
    "grid_search.best_params_\n",
    "\n",
    "# Print the training and test accuracy \n",
    "\n",
    "clf = svm.SVC(C=2.575, gamma=0.001)\n",
    "clf.fit(X_pca_train, y_train)\n",
    "\n",
    "train_acc = clf.score(X_pca_train, y_train)\n",
    "test_acc = clf.score(X_pca_test, y_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_acc, test_acc))\n",
    "\n",
    "#alternately: \n",
    "train_acc = grid_search.best_estimator_.score(X_pca_train, y_train)\n",
    "test_acc = grid_search.best_estimator_.score(X_pca_test, y_test)\n",
    "print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_acc, test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://github.com/ericthansen/dsc-pca-summary\n",
    "PCA - Recap\n",
    "GitHub RepoCreate New Issue\n",
    "Key Takeaways\n",
    "The key takeaways from this section include:\n",
    "\n",
    "PCA is an unsupervised learning technique which does not require labeled data\n",
    "It is also a dimensionality reduction technique which can be used to compress data and experiment with its effects on machine learning algorithms as a preprocessing step\n",
    "There are four steps to conducting PCA:\n",
    "Center each feature by subtracting the feature mean\n",
    "Calculate the covariance matrix for your normalized dataset\n",
    "Calculate the eigenvectors/eigenvalues for the covariance matrix\n",
    "Reorder your eigenvectors based on their accompanying eigenvalues (in descending order of the eigenvalues)\n",
    "Take the dot product of the transpose of the eigenvectors with the transpose of the normalized data\n",
    "You can also easily implement PCA using scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#clustering intro\n",
    "#https://github.com/ericthansen/dsc-clustering-intro.git\n",
    "\n",
    "Introduction\n",
    "In this section, you'll learn about a useful unsupervised learning technique: clustering. This lesson summarizes the topics you'll be covering in this section.\n",
    "\n",
    "Clustering\n",
    "Clustering techniques are very powerful when you want to group data with similar characteristics together, but have no pre-specified labels. The main goal of clustering is to create clusters that have a high similarity between the data belonging to one cluster while aiming for minimal similarity between clusters.\n",
    "\n",
    "K-Means Clustering\n",
    "We start by providing a basic intuition of the K-means clustering algorithm. When using the K-means clustering algorithm, the number of clusters that you want to obtain is specified upfront and the algorithm aims at the most \"optimal\" cluster centers, given that there are  clusters.\n",
    "\n",
    "Hierarchical Agglomerative Clustering\n",
    "A second branch of clustering algorithms is hierarchical agglomerative clustering. Using hierarchical clustering, unlike K-means clustering, you don't decide on the number of clusters beforehand. Instead, you start with  clusters, where  is the number of data points, and at each step you join two clusters. You stop joining clusters when a certain criterion is reached.\n",
    "\n",
    "Market Segmentation with Clustering\n",
    "A very common and useful application of clustering is market segmentation. You'll practice your clustering skills on a market segmentation dataset!\n",
    "\n",
    "Semi-Supervised Learning\n",
    "At the end of this section, you'll learn how semi-supervised learning techniques, which are increasingly popular in machine learning, combines both concepts of supervised and unsupervised learning.\n",
    "\n",
    "Summary\n",
    "In this section, you'll learn how to use clustering techniques which are very useful for finding patterns and grouping unlabeled data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-k-means-clustering \n",
    "#In this lesson, we'll learn about the most popular and widely-used clustering algorithm, K-means clustering.\n",
    "#implementing k-means clustering in sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_means = KMeans(n_clusters=3) \n",
    "\n",
    "k_means.fit(some_df) \n",
    "\n",
    "cluster_assignments = k_means.predict(some_df) \n",
    "\n",
    "'''Running K-means on a dataset is easy enough, but how do we know if we have the best value for  ? The best bet is to use an accepted metric for evaluating cluster fitness such as Calinski Harabasz Score (Links to an external site.), which is more often referred to by a simpler, Variance Ratio.'''\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "print(calinski_harabasz_score(some_df, cluster_assignments))\n",
    "\n",
    "'''There are other metrics that can also be used to evaluate the fitness, such as Silhouette Score (Links to an external site.). No one metric is best -- they all have slightly different strengths and weaknesses depending on the given dataset and goals. Because of this, it's generally accepted that it's best to pick one metric and stick to it.'''\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-k-means-clustering-lab.git\n",
    "#create sameple data\n",
    "# Your code here\n",
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "np.random.seed(1)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X, y = make_blobs(n_samples=400, n_features=2, centers=6, cluster_std=0.8)\n",
    "\n",
    "#visualize\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=10);\n",
    "\n",
    "#use k-means\n",
    "k_means = KMeans(n_clusters=6).fit(X,y)\n",
    "\n",
    "predicted_clusters = k_means.predict(X)\n",
    "#create scatter\n",
    "\n",
    "centers = k_means.cluster_centers_\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=predicted_clusters, s=10);\n",
    "\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=70);\n",
    "\n",
    "#tuning parameters - init parameter - how initial cluster centers are decided\n",
    " #- algorithm parameter - method for reassigning clusters iteratively.  \n",
    "    \n",
    "#dealing with unknown number of clusters\n",
    "X_2, y_2 = make_blobs(\n",
    "    n_samples=400,\n",
    "    n_features=2,\n",
    "    centers=np.random.randint(3, 8),\n",
    "    cluster_std = 0.8\n",
    ")\n",
    "\n",
    "#try a few different k assumptions, then inspect them using a plot (i.e. look for the \"elbow\")\n",
    "k_means_3 = KMeans(n_clusters=3).fit(X,y)\n",
    "k_means_4 = KMeans(n_clusters=4).fit(X,y)\n",
    "k_means_5 = KMeans(n_clusters=5).fit(X,y)\n",
    "k_means_6 = KMeans(n_clusters=6).fit(X,y)\n",
    "k_means_7 = KMeans(n_clusters=7).fit(X,y)\n",
    "\n",
    "k_list = [k_means_3, k_means_4, k_means_5, k_means_6, k_means_7 ]\n",
    "\n",
    "# Your code here\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "CH_score = []\n",
    "for m in k_list:\n",
    "    labs=m.labels_\n",
    "    CH_score.append(calinski_harabasz_score(X_2, labs))\n",
    "plt.plot([3, 4, 5, 6, 7], CH_score)\n",
    "plt.xticks([3,4,5,6,7])\n",
    "plt.title('Calinski Harabasz Scores for Different Values of K')\n",
    "plt.ylabel('Variance Ratio')\n",
    "plt.xlabel('K=')\n",
    "plt.show()\n",
    "    \n",
    "wcss_score = []\n",
    "for m in k_list:\n",
    "    #labs=m.labels_\n",
    "    wcss_score.append(m.inertia_)\n",
    "plt.plot([3, 4, 5, 6, 7], wcss_score)\n",
    "plt.xticks([3,4,5,6,7])\n",
    "plt.title('Within Cluster Sum of Squares')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xlabel('K=')\n",
    "plt.show()\n",
    "#these two plots aren't \"obviously\" best at 6, but it turns out that 6 clusters is the winner\n",
    "\n",
    "#now plot the blobs to verify\n",
    "# Your code here\n",
    "plt.scatter(X_2[:,0],X_2[:,1] ,c=y_2, s=10);\n",
    "#note that here, we could eyeball the data to validate, but in general, we won't have just 2 dimensions so we can't\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-hierarchical-agglomerative-clustering\n",
    "'''HAC clusters similar points together until some criteria is met - by default, it's number of clusters in sklearn.\n",
    "3 modes, ward/average/complete.  \n",
    "See also dendrograms and clustergrams - ways of visualizing how the clusters are combined\n",
    "smartphone photo clustering is one application of this'''\n",
    "# https://github.com/ericthansen/dsc-common-problems-with-clustering\n",
    "#there are pros/cons to k-means and HAC.\n",
    "\n",
    "#https://github.com/ericthansen/dsc-hierarchical-agglomerative-clustering-codealong\n",
    "#great codealong with good notes about differences in different clustering params\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "%matplotlib inline  \n",
    "\n",
    "# create some data\n",
    "k = 3\n",
    "m = 16\n",
    "\n",
    "X, y = make_blobs(n_samples= m, n_features=2, centers=k, cluster_std=1.3)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=10);\n",
    "\n",
    "agg_clust = AgglomerativeClustering(n_clusters=3)\n",
    "assigned_clust = agg_clust.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=assigned_clust, s=10);\n",
    "\n",
    "#sample\n",
    "from plot_agg_alg import plot_agglomerative_algorithm\n",
    "plot_agglomerative_algorithm()\n",
    "from plot_agg import plot_agglomerative # File in the repo\n",
    "plot_agglomerative()\n",
    "\n",
    "#dendrograms - implemented in scipy not sklearn\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "\n",
    "# Use the ward() function\n",
    "linkage_array = ward(X)\n",
    "\n",
    "# Now we plot the dendrogram for the linkage_array containing the distances\n",
    "# between clusters\n",
    "dendrogram(linkage_array)\n",
    "\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "ax.plot(bounds, [16, 16], '--', c='k')\n",
    "ax.plot(bounds, [9, 9], '--', c='k')\n",
    "ax.text(bounds[1], 16, ' 2 clusters', va='center', fontdict={'size': 12})\n",
    "ax.text(bounds[1], 9, ' 3 clusters', va='center', fontdict={'size': 12})\n",
    "plt.xlabel('Data index')\n",
    "plt.ylabel('Cluster distance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#back to the 6 cluster dataset\n",
    "k = 6\n",
    "m = 400\n",
    "X, y = make_blobs(n_samples= m, n_features=2, centers=k, cluster_std=0.8,  random_state=1234)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=10);\n",
    "\n",
    "#different hac approaches - NOTE: In this case, the default, ward, works best\n",
    "agg_comp = AgglomerativeClustering(linkage ='complete', n_clusters=6)\n",
    "agg_avg = AgglomerativeClustering(linkage ='average', n_clusters=6)\n",
    "agg_ward = AgglomerativeClustering(linkage ='ward', n_clusters=6)\n",
    "\n",
    "as_comp = agg_comp.fit_predict(X)\n",
    "as_avg = agg_avg.fit_predict(X)\n",
    "as_ward = agg_ward.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=as_comp, s=10);\n",
    "#\n",
    "plt.scatter(X[:, 0], X[:, 1], c=as_avg, s=10);\n",
    "#\n",
    "plt.scatter(X[:, 0], X[:, 1], c = as_ward, s = 10);\n",
    "#\n",
    "#dendro for it\n",
    "linkage_array = ward(X)\n",
    "\n",
    "# Now we plot the dendrogram for the linkage_array containing the distances\n",
    "# between clusters\n",
    "dendrogram(linkage_array)\n",
    "\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Cluster distance');\n",
    "\n",
    "#But that is messy at bottom, so truncate it\n",
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "dendrogram(linkage_array,  truncate_mode='lastp', p=12)\n",
    "plt.xlabel('cluster size')\n",
    "plt.ylabel('distance')\n",
    "plt.show()\n",
    "\n",
    "#now do k-means clustering to compare with hac\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_means = KMeans(n_clusters = 6)\n",
    "k_means.fit(X)\n",
    "y_hat = k_means.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y_hat, s = 10)\n",
    "cl_centers = k_means.cluster_centers_\n",
    "plt.scatter(cl_centers[:, 0], cl_centers[:, 1], c='black', s=40);\n",
    "\n",
    "#compare all 4\n",
    "labels_kmeans = k_means.labels_\n",
    "labels_comp = agg_comp.labels_\n",
    "labels_avg = agg_avg.labels_\n",
    "labels_ward = agg_ward.labels_\n",
    "\n",
    "\n",
    "#now consider different metrics!\n",
    "metrics.adjusted_rand_score(labels_kmeans, y)  \n",
    "#rand\n",
    "metrics.adjusted_rand_score(labels_kmeans, y)  \n",
    "metrics.adjusted_rand_score(labels_ward, y)\n",
    "metrics.adjusted_rand_score(labels_avg, y)  \n",
    "metrics.adjusted_rand_score(labels_comp, y)  \n",
    "\n",
    "#fowlkes mallows\n",
    "metrics.fowlkes_mallows_score(labels_kmeans, y)  \n",
    "metrics.fowlkes_mallows_score(labels_ward, y)\n",
    "metrics.fowlkes_mallows_score(labels_avg, y)  \n",
    "metrics.fowlkes_mallows_score(labels_comp, y)  \n",
    "\n",
    "#calinski-harabasz\n",
    "metrics.calinski_harabasz_score(X, labels_kmeans)  \n",
    "metrics.calinski_harabasz_score(X,labels_ward)\n",
    "metrics.calinski_harabasz_score(X,labels_avg)  \n",
    "metrics.calinski_harabasz_score(X,labels_comp)  \n",
    "\n",
    "#silhouette\n",
    "metrics.silhouette_score(X, labels_kmeans)\n",
    "metrics.silhouette_score(X, labels_ward) \n",
    "metrics.silhouette_score(X,labels_avg)  \n",
    "metrics.silhouette_score(X, labels_comp)  \n",
    "\n",
    "#woo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-market-segmentation-clustering \n",
    "#good article on marketing process\n",
    "'''segmentation - targeting - positioning& differentiation'''\n",
    "\n",
    "# https://github.com/ericthansen/dsc-market-segmentation-clustering-lab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "%matplotlib inline \n",
    "#read in data\n",
    "raw_df = pd.read_csv('wholesale_customers_data.csv')\n",
    "raw_df.head()\n",
    "#setup target/predictors\n",
    "channels = raw_df.Channel\n",
    "df = raw_df.drop(['Channel', 'Region'], axis=1)\n",
    "#import\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import calinski_harabasz_score, adjusted_rand_score\n",
    "#first metric\n",
    "calinski_harabasz_score(df, cluster_preds)\n",
    "#second metric\n",
    "adjusted_rand_score(channels, cluster_preds)\n",
    "\n",
    "#scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "\n",
    "#use k-means model\n",
    "scaled_k_means = KMeans(n_clusters=2).fit(scaled_df)\n",
    "\n",
    "scaled_preds = scaled_k_means.predict(scaled_df)\n",
    "adjusted_rand_score(channels, scaled_preds)\n",
    "\n",
    "\n",
    "#use PCA on it\n",
    "from sklearn.decomposition import PCA\n",
    "display(scaled_df[0].shape)\n",
    "\n",
    "pca = PCA(n_components=6)#, whiten=True)\n",
    "X_pca_train = pca.fit_transform(scaled_df)\n",
    "X_pca_train.shape\n",
    "np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(range(1,6+1), pca.explained_variance_ratio_.cumsum())\n",
    "plt.title('Total Variance Explained by Varying Number of Principle Components');\n",
    "\n",
    "##iterate through various component options\n",
    "models = []\n",
    "for n_components in range(6, 0, -1):\n",
    "    \n",
    "    pca_km = PCA(n_components = n_components)#, whiten=True)\n",
    "    X_train_kmpca = pca_km.fit_transform(scaled_df)\n",
    "    #pca_km.fit(X_train_kmpca, channels)\n",
    "    \n",
    "    \n",
    "    k_means_pca = KMeans(n_clusters=2)\n",
    "    k_means.fit(X_train_kmpca)#, channels)\n",
    "    cluster_preds = k_means.predict(X_train_kmpca)\n",
    "    \n",
    "    #scl_preds = pca_km.predict(X_train_kmpca)\n",
    "    \n",
    "    arscore = adjusted_rand_score(channels, cluster_preds)\n",
    "    \n",
    "    print('Num components:{} generates score: {}'.format(n_components, arscore))\n",
    "    \n",
    "#set up for Hierarchical Agglomerative Clustering\n",
    "pca_km = PCA(n_components = 4)\n",
    "pca_df = pca_km.fit_transform(scaled_df)\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "##\n",
    "hac = AgglomerativeClustering(n_clusters=2)\n",
    "hac.fit(pca_df)\n",
    "hac_pca_preds = hac.labels_\n",
    "#\n",
    "adjusted_rand_score(channels, hac_pca_preds)\n",
    "\n",
    "##\n",
    "hac2 = AgglomerativeClustering(n_clusters=2)\n",
    "hac2.fit(scaled_df)\n",
    "hac_scaled_preds = hac2.labels_\n",
    "#\n",
    "adjusted_rand_score(channels, hac_scaled_preds)\n",
    "##\n",
    "hac3 = AgglomerativeClustering(n_clusters=2)\n",
    "hac3.fit(df)\n",
    "hac__preds = hac3.labels_\n",
    "\n",
    "#\n",
    "adjusted_rand_score(channels, hac__preds)\n",
    "\n",
    "##note, results on the last one are WORSE than random guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-semi-supervised-learning-and-look-alike-models.git\n",
    "'''for unsupervised learning things - two techniques:\n",
    "1) look-alike models\n",
    "2) semi-supervised (aka weakly supervised) learning'''\n",
    "\n",
    "#https://github.com/ericthansen/dsc-clustering-recap.git\n",
    "'''Key Takeaways\n",
    "The key takeaways from this section include:\n",
    "\n",
    "There are two main types of clustering algorithms: non-hierarchical clustering (k-means) and hierarchical agglomerative clustering\n",
    "You can quantify the performance of a clustering algorithm using metrics such as variance ratios\n",
    "When working with the k-means clustering algorithm, it is useful to create elbow plots to find an optimal value for \n",
    "When using hierarchical agglomerative clustering, different linkage criteria can be used to determine which clusters should be merged and at what point\n",
    "Dendrograms and clustergrams are very useful visual tools in hierarchical agglomerative clustering\n",
    "Advantages of k-means clustering include easy implementation and speed, whereas the main disadvantage is that it isn't always straightforward how to pick the \"right\" value for \n",
    "Advantages of hierarchical agglomerative clustering include easy visualization and intuitiveness, whereas the main disadvantage is that the result is very distance-metric-dependent\n",
    "You can use supervised and unsupervised learning together in a few different ways. Applications of this are look-alike models in market segmentation and semi-supervised learning\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-spark-introduction.git\n",
    "#https://github.com/ericthansen/dsc-big-data-introduction\n",
    "'''In the data science domain, big data usually refers to datasets that grow so large that they become awkward to work with using traditional database management systems and analytical approaches. They are datasets whose size is beyond the ability of commonly used software tools and storage systems to capture, store, manage, as well as process the data within a tolerable elapsed time.'''\n",
    "'''Doug Laney published a paper (Links to an external site.) on three defining characteristics of big data. Three main features characterize big data: volume, variety, and velocity, or the three V’s. The volume of the data is its size, and how enormous it is. Velocity refers to the rate with which data is changing, or how often it is created. Finally, variety includes the different formats and types of data, as well as the different kinds of uses and ways of analyzing the data:'''\n",
    "'''NOTE: Some researchers have discussed the addition of a fourth V, or Veracity. Veracity focuses on the quality of the data. This characterizes big data quality as good, bad, or undefined due to data inconsistency, incompleteness, ambiguity, latency, deception, and approximations'''\n",
    "'''\n",
    "visualization and analytics: tableau, microsoft, looker, r\n",
    "computation: apache spark, hive, ms azure, digital ocean, hadoop\n",
    "storage: amazon s3, apache hbase\n",
    "distribution and data warehouse: oracle exadata, amazon emr, cassandra, cloudera\n",
    "The key activities associated with big data analytics are reflected in four main areas:\n",
    "\n",
    "Big data warehousing and distribution\n",
    "Big data storage\n",
    "Big data computational platforms\n",
    "Big data analyses, visualization, and evaluation\n",
    "\n",
    "''''\n",
    "links\n",
    "https://www.youtube.com/watch?v=0cizsKDn3TI\n",
    "https://www.educba.com/big-data-vs-data-science/\n",
    "https://pdfs.semanticscholar.org/d392/0f02dbb15da19b04d782fc0546ef113e0bf7.pdf\n",
    "https://www.ntnu.no/iie/fag/big/lessons/lesson2.pdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-parallel-and-distributed-computing-with-mapreduce\n",
    "https://en.wikipedia.org/wiki/Embarrassingly_parallel\n",
    "https://www.tutorialspoint.com/map_reduce/map_reduce_introduction.htm\n",
    "https://www.guru99.com/introduction-to-mapreduce.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-big-data-analytics-apache-spark\n",
    "# for this lesson, must read this: \n",
    "https://link.springer.com/article/10.1007/s41060-016-0027-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-sparkcontext-lab\n",
    "# https://spark.apache.org/docs/0.6.0/api/core/spark/SparkContext.html\n",
    "# Create a local spark context with pyspark\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "# Display the type of the Spark Context\n",
    "type(sc)\n",
    "\n",
    "# pyspark.context.SparkContext\n",
    "\n",
    "# Create second spark context\n",
    "sc1 = pyspark.SparkContext('local[*]')\n",
    "\n",
    "# Use Python's dir(obj) to get a list of all attributes of SparkContext\n",
    "\n",
    "# Code here \n",
    "\n",
    "dir(sc)\n",
    "\n",
    "# Use Python's help ( help(object) ) function to get information on attributes and methods for sc object. \n",
    "\n",
    "\n",
    "# Code here \n",
    "help(sc)\n",
    "\n",
    "#https://spark.apache.org/docs/0.6.0/api/core/spark/SparkContext.html\n",
    "\n",
    "# Check the number of cores being used\n",
    "print ('Default number of cores being used:', sc.defaultParallelism) \n",
    "\n",
    "# Check for the current version of Spark\n",
    "print ('Current version of Spark:', sc.version)\n",
    "\n",
    "# Default number of cores being used: 2\n",
    "# Current version of Spark: 2.3.1\n",
    "\n",
    "# Check the name of application currently running in spark environment\n",
    "sc.appName\n",
    "\n",
    "# 'pyspark-shell'\n",
    "\n",
    "#get config settings\n",
    " sc._conf.getAll()\n",
    "    \n",
    "# Shut down SparkContext\n",
    "sc.stop()\n",
    "\n",
    "#addition content:\n",
    "# https://data-flair.training/blogs/learn-apache-spark-sparkcontext/\n",
    "'''In this short lab, we saw how SparkContext is used as an entry point to Spark applications. We learned how to start a SparkContext, how to list and use some of the attributes and methods in SparkContext and how to shut it down. Students are encouraged to explore other attributes and methods offered by the sc object. Some of these, namely creating and transforming datasets as RDDs will be explored in later labs.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-resilient-distributed-datasets-rdd-lab\n",
    "#rdd lab\n",
    "'''Objectives\n",
    "You will be able to:\n",
    "\n",
    "Apply the map(func) transformation to a given function on all elements of an RDD in different partitions\n",
    "Apply a map transformation for all elements of an RDD\n",
    "Compare the difference between a transformation and an action within RDDs\n",
    "Use collect(), count(), and take() actions to trigger spark transformations\n",
    "Use filter to select data that meets certain specifications within an RDD\n",
    "Set number of partitions for parallelizing RDDs\n",
    "Create RDDs from Python collections'''\n",
    "\n",
    "'''\n",
    "Transformations\tActions\n",
    "map(func)\treduce(func)\n",
    "filter(func)\tcollect()\n",
    "groupByKey()\tcount()\n",
    "reduceByKey(func)\tfirst()\n",
    "mapValues(func)\ttake()\n",
    "sample()\tcountByKey()\n",
    "distinct()\tforeach(func)\n",
    "sortByKey()\t\n",
    "'''\n",
    "#create a collection\n",
    "data = list(range(1, 1001))\n",
    "len(data)\n",
    "\n",
    "# 1000\n",
    "#initialize an spark context\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "#initialize an rdd\n",
    "rdd = sc.parallelize(data, 10)\n",
    "print(type(rdd))\n",
    "# <class 'pyspark.rdd.RDD'>\n",
    "#find how many partitions\n",
    "rdd.getNumPartitions()\n",
    "# 10\n",
    "#basic actions\n",
    "'''count: returns the total count of items in the RDD\n",
    "first: returns the first item in the RDD\n",
    "take: returns the first n items in the RDD\n",
    "top: returns the top n items\n",
    "collect: returns everything from your RDD'''\n",
    "# count\n",
    "rdd.count()\n",
    "\n",
    "# first\n",
    "rdd.first()\n",
    "# take\n",
    "rdd.take(5)\n",
    "# top\n",
    "rdd.top(5)\n",
    "# collect\n",
    "rdd.collect()\n",
    "\n",
    "##now some map functions on some randomish data\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "nums = np.array(range(1, 1001))\n",
    "sales_figures = nums * np.random.rand(1000)\n",
    "sales_figures\n",
    "#create rdd with prices\n",
    "price_items = sc.parallelize(sales_figures, 10)\n",
    "price_items.take(5)\n",
    "#let's make a function to apply to get a new rdd\n",
    "def sales_tax(num):\n",
    "    return num*(1-0.08)\n",
    "\n",
    "revenue_minus_tax = price_items.map(sales_tax)\n",
    "#remember, spark has lazy eval, so this doesn't get run until needed\n",
    "# perform action to retrieve rdd values\n",
    "revenue_minus_tax.take(10)\n",
    "\n",
    "#our old pal lambda functions\n",
    "discounted = revenue_minus_tax.map(lambda x: x*0.9)\n",
    "discounted.take(10)\n",
    "\n",
    "#can chain methods\n",
    "price_items.map(sales_tax).map(lambda x: x*0.9).top(15)\n",
    "\n",
    "#for debugging, sometimes nice to check rdd lineage\n",
    "discounted.toDebugString()\n",
    "\n",
    "#map vs flatmap - flatmap flattens out tuples\n",
    "mapped = price_items.map(lambda x: (x, sales_tax(x)*0.9))\n",
    "print(mapped.count())\n",
    "print(mapped.take(10))\n",
    "#\n",
    "flat_mapped = price_items.flatMap(lambda x: (x,sales_tax(x)*.9))\n",
    "print(flat_mapped.count())\n",
    "print(flat_mapped.take(10))\n",
    "\n",
    "#using some filters\n",
    "# use the filter function\n",
    "selected_items = discounted.filter(lambda x: x>300)\n",
    "\n",
    "\n",
    "# calculate total remaining in inventory \n",
    "selected_items.count()\n",
    "\n",
    "#now one of the things - reduce\n",
    "selected_items.reduce(lambda x,y: x + y) #will add up total price of all items\n",
    "\n",
    "#\n",
    "#here, imagine we let 50 users buy up stuff until out of stuff\n",
    "import random\n",
    "random.seed(42)\n",
    "# generating simulated users that have bought each item\n",
    "sales_data = selected_items.map(lambda x: (random.randint(1, 50), x))\n",
    "\n",
    "sales_data.take(7)\n",
    "\n",
    "# calculate how much each user spent\n",
    "total = sales_data.reduceByKey(lambda x, y: x + y)\n",
    "total.take(10)\n",
    "\n",
    "# sort the users from highest to lowest spenders\n",
    "total.sortBy(lambda x: x[1],ascending = False).collect()\n",
    "\n",
    "#now count number of items bought by user\n",
    "items_bought = sales_data.countByKey()\n",
    "sorted(items_bought.items(),key=lambda kv:kv[1],reverse=True)\n",
    "\n",
    "#more reading\n",
    "https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf\n",
    "https://data-flair.training/blogs/create-rdds-in-apache-spark/\n",
    "https://runawayhorse001.github.io/LearningApacheSpark/rdd.html\n",
    "https://www.analyticsvidhya.com/blog/2016/10/using-pyspark-to-perform-transformations-and-actions-on-rdd/\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-word-count-with-map-reduce-lab\n",
    "'''MapReduce Framework\n",
    "Here are the steps that we will perform for our problem, under the MapReduce framework:\n",
    "\n",
    "Sequentially read a lot of data (text files in this case)\n",
    "Map:\n",
    "Extract something you care about\n",
    "Group by key: Sort and shuffle\n",
    "Reduce:\n",
    "Aggregate, summarize, filter or transform\n",
    "Write the result'''\n",
    "#https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "#word count fo ra large text\n",
    "# Start a local SparkContext\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "# Set a path variable for data \n",
    "file = 'text/hamlet.txt'\n",
    "\n",
    "#read and split\n",
    "# Read the text file into an RDD using sc.textFile()\n",
    "lines = sc.textFile(file)\n",
    "lines\n",
    "#sample lines\n",
    "# Code here \n",
    "print(lines.collect()[500])\n",
    "print(lines.collect()[501])\n",
    "print(lines.collect()[502])\n",
    "print(lines.collect()[503])\n",
    "\n",
    "# Print the text, line-by-line\n",
    "# This will output the whole of hamlet text, one line at a time. \n",
    "#print(lines.collect()[:])\n",
    "#or \n",
    "#(the second way is nicer)\n",
    "for line in lines.collect():\n",
    "    print(line)\n",
    "    \n",
    "#mapping and flatmapping\n",
    "# split the lines into words based on blanks ' ' and show ten elements from the top \n",
    "words=lines.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "# Code here \n",
    "print(words.take(10))\n",
    "\n",
    "# ['', '1604', '', '', 'THE', 'TRAGEDY', 'OF', 'HAMLET,', 'PRINCE', 'OF']\n",
    "\n",
    "##create a tuple for counts\n",
    "# Use a lambda function with map to add a 1 to each word and output a tuple\n",
    "# (word, 1) - Take ten elements\n",
    "word_tups = words.map(lambda x: (x, 1))\n",
    "\n",
    "# Code here \n",
    "for wordtuple in word_tups.take(10):\n",
    "    print(wordtuple)\n",
    "\n",
    "#change to lowercase.  ((should really strip out punctuation too!))\n",
    "# Change the words in words tuples to lowercase - take 10 elements \n",
    "lower_tuples = words.map(lambda x: (x.lower(), 1))\n",
    "\n",
    "# Code here \n",
    "for t in lower_tuples.take(10):\n",
    "    print(t)\n",
    "    \n",
    "#reduce - spark takes care of this!\n",
    "# Use reduceByKey with tuplesLCase to add all values under same keys - take 10\n",
    "reduced = lower_tuples.reduceByKey(lambda x,y: x + y)\n",
    "\n",
    "# Code here \n",
    "reduced.take(10)\n",
    "\n",
    "\n",
    "#filter rare words\n",
    "# Remove all rare words with frequency less than 5 - take 10 \n",
    "filtered = reduced.filter(lambda x:  x[1] >= 5 )\n",
    "\n",
    "# Code here \n",
    "filtered.take(10)\n",
    "\n",
    "\n",
    "#list of \"stop\" words \n",
    "# show stop word frequency in the output\n",
    "stop_words_list = ['', 'the','a','in','of','on','at','for','by','I','you','me']\n",
    "stop_words = filtered.filter(lambda x:  x[0] in stop_words_list) \n",
    "stop_words.collect()\n",
    "\n",
    "# Code here \n",
    "\n",
    "\n",
    "##list of keep words\n",
    "# Modify above filter to show top ten keep words by frequency\n",
    "keep_words = filtered.filter(lambda x: x[0] not in stop_words_list)\n",
    "\n",
    "# Code here \n",
    "#print(keep_words.collect())\n",
    "\n",
    "output = keep_words.takeOrdered(10, key=lambda x: -x[1])\n",
    "output\n",
    "\n",
    "\n",
    "##putting it all together!\n",
    "# Create a function for word count that takes in a file name and stop wordlist to perform above tasks\n",
    "\n",
    "def wordCount(filename, stopWordlist):\n",
    "    lines = sc.textFile(filename)\n",
    "    words=lines.flatMap(lambda x: x.split(' '))\n",
    "    lower_tuples = words.map(lambda x: (x.lower(), 1))\n",
    "    reduced = lower_tuples.reduceByKey(lambda x,y: x + y)\n",
    "    filtered = reduced.filter(lambda x:  x[1] >= 5 )\n",
    "    stop_words = filtered.filter(lambda x:  x[0] in stopWordlist) \n",
    "    output = stop_words.collect()\n",
    "    return output\n",
    "\n",
    "title_list = ['hamlet', 'romeoandjuliet', 'othello', 'senseandsensibility 2', 'prideandprejudice', 'emma']\n",
    "file_list = ['text/{}.txt'.format(x) for x in title_list]\n",
    "for fil in file_list:\n",
    "    try:\n",
    "        wc = wordCount(fil, stop_words_list)\n",
    "    except:\n",
    "        wc = None\n",
    "    print('Word Counts for {}: {}\\n'.format(fil, wc))\n",
    "    \n",
    "#next level - could make a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-566e8dc8f0ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#mllib is based on rdd's and is heading toward deprecation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# importing the necessary libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# sc = SparkContext('local[*]')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# https://github.com/ericthansen/dsc-machine-learning-with-spark\n",
    "\n",
    "# #note difference between mllib and ml for spark.  ml is up and coming, based on dataframes,\n",
    "    #mllib is based on rdd's and is heading toward deprecation. \n",
    "# importing the necessary libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "# sc = SparkContext('local[*]')\n",
    "# spark = SparkSession(sc)\n",
    "\n",
    "#create a spark session this way instead\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "\n",
    "#load the data\n",
    "## reading in pyspark df\n",
    "spark_df = spark.read.csv('./forestfires.csv', header='true', inferSchema='true')\n",
    "\n",
    "## observing the datatype of df\n",
    "type(spark_df)\n",
    "\n",
    "#there are lots of similar methods\n",
    "spark_df.head()\n",
    "#\n",
    "spark_df.columns\n",
    "\n",
    "#selecting multiple columns\n",
    "spark_df[['month','day','rain']]\n",
    "#selecting one columns:\n",
    "d = spark_df.select('rain') #to keep the dataframe type\n",
    "spark_df['rain'] #this becomes just a \"column\" type\n",
    "\n",
    "#look at datatypes\n",
    "spark_df.dtypes\n",
    "#aggregations\n",
    "spark_df_months = spark_df.groupBy('month').agg({'area': 'mean'})\n",
    "spark_df_months\n",
    "\n",
    "spark_df_months.collect()\n",
    "\n",
    "#boolean masking\n",
    "no_rain = spark_df.filter(spark_df['rain'] == 0.0)\n",
    "some_rain = spark_df.filter(spark_df['rain'] > 0.0)\n",
    "\n",
    "#to get mean of a col:\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "print('no rain fire area: ', no_rain.select(mean('area')).show(),'\\n')\n",
    "\n",
    "print('some rain fire area: ', some_rain.select(mean('area')).show(),'\\n')\n",
    "\n",
    "#check seasons\n",
    "summer_months = spark_df.filter(spark_df['month'].isin(['jun','jul','aug']))\n",
    "winter_months = spark_df.filter(spark_df['month'].isin(['dec','jan','feb']))\n",
    "\n",
    "print('summer months fire area', summer_months.select(mean('area')).show())\n",
    "print('winter months fire areas', winter_months.select(mean('area')).show())\n",
    "\n",
    "###machine learning - data formatting and rf model\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "\n",
    "#we don't need day\n",
    "fire_df = spark_df.drop('day')\n",
    "fire_df.head()\n",
    "\n",
    "#in order to one-hot encode the month col, first need to numeric index it\n",
    "si = StringIndexer(inputCol='month', outputCol='month_num')\n",
    "model = si.fit(fire_df)\n",
    "new_df = model.transform(fire_df)\n",
    "\n",
    "## this is an estimator (an untrained transformer)\n",
    "type(si)\n",
    "\n",
    "## this is a transformer (a trained transformer)\n",
    "type(model)\n",
    "\n",
    "#\n",
    "model.labels\n",
    "#\n",
    "new_df.head(4)\n",
    "\n",
    "new_df.select('month_num').distinct().collect()\n",
    "\n",
    "## fitting and transforming the OneHotEncoder\n",
    "ohe = feature.OneHotEncoder(inputCols=['month_num'], outputCols=['month_vec'], dropLast=True)\n",
    "one_hot_encoded = ohe.fit(new_df).transform(new_df)\n",
    "one_hot_encoded.head()\n",
    "\n",
    "#this is a little odd but is how the features are handled\n",
    "features = ['X',\n",
    " 'Y',\n",
    " 'FFMC',\n",
    " 'DMC',\n",
    " 'DC',\n",
    " 'ISI',\n",
    " 'temp',\n",
    " 'RH',\n",
    " 'wind',\n",
    " 'rain',\n",
    " 'month_vec']\n",
    "\n",
    "target = 'area'\n",
    "\n",
    "vector = VectorAssembler(inputCols=features, outputCol='features')\n",
    "vectorized_df = vector.transform(one_hot_encoded)\n",
    "\n",
    "vectorized_df.head()\n",
    "\n",
    "#Original RF model\n",
    "## instantiating and fitting the model\n",
    "rf_model = RandomForestRegressor(featuresCol='features', \n",
    "                                 labelCol='area', predictionCol='prediction').fit(vectorized_df)\n",
    "\n",
    "rf_model.featureImportances\n",
    "\n",
    "## generating predictions\n",
    "predictions = rf_model.transform(vectorized_df).select('area', 'prediction')\n",
    "predictions.head(10)\n",
    "\n",
    "#see how well it did\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='area')\n",
    "\n",
    "## evaluating r^2\n",
    "evaluator.evaluate(predictions,{evaluator.metricName: 'r2'})\n",
    "## evaluating mean absolute error\n",
    "evaluator.evaluate(predictions,{evaluator.metricName: 'mae'})\n",
    "\n",
    "##put it all in a pipeline\n",
    "# importing relevant libraries\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "## instantiating all necessary estimator objects\n",
    "\n",
    "string_indexer = StringIndexer(inputCol='month', outputCol='month_num', handleInvalid='keep')\n",
    "one_hot_encoder = OneHotEncoder(inputCols=['month_num'], outputCols=['month_vec'], dropLast=True)\n",
    "vector_assember = VectorAssembler(inputCols=features, outputCol='features')\n",
    "random_forest = RandomForestRegressor(featuresCol='features', labelCol='area')\n",
    "stages = [string_indexer, one_hot_encoder, vector_assember, random_forest]\n",
    "\n",
    "# instantiating the pipeline with all the estimator objects\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "#note that we didn't cross validate or use grid search.  we do that now.\n",
    "# creating parameter grid\n",
    "\n",
    "params = ParamGridBuilder()\\\n",
    "          .addGrid(random_forest.maxDepth, [5, 10, 15])\\\n",
    "          .addGrid(random_forest.numTrees, [20 ,50, 100])\\\n",
    "          .build()\n",
    "\n",
    "#look at params we just built\n",
    "print('total combinations of parameters: ', len(params))\n",
    "\n",
    "params[0]\n",
    "\n",
    "## instantiating the evaluator by which we will measure our model's performance\n",
    "reg_evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='area', metricName = 'mae')\n",
    "\n",
    "## instantiating crossvalidator estimator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=reg_evaluator, parallelism=4)\n",
    "\n",
    "## fitting crossvalidator\n",
    "cross_validated_model = cv.fit(fire_df)\n",
    "\n",
    "cross_validated_model.avgMetrics\n",
    "\n",
    "#look at best performer\n",
    "predictions = cross_validated_model.transform(spark_df)\n",
    "predictions.select('prediction', 'area').show(300)\n",
    "\n",
    "type(cross_validated_model.bestModel)\n",
    "#\n",
    "cross_validated_model.bestModel.stages\n",
    "# we can get out the rf model\n",
    "optimal_rf_model = cross_validated_model.bestModel.stages[3]\n",
    "#\n",
    "optimal_rf_model.featureImportances\n",
    "optimal_rf_model.getNumTrees\n",
    "\n",
    "#wow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-machine-learning-with-spark-lab\n",
    "\n",
    "#create sparksession and import file\n",
    "# import necessary libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "# initialize Spark Session\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "\n",
    "# read in csv to a spark dataframe\n",
    "spark_df = spark.read.csv('./credit_card_default.csv', header='true', inferSchema='true')\n",
    "\n",
    "#check datatypes\n",
    "spark_df.dtypes\n",
    "\n",
    "#check missing\n",
    "for col in spark_df.columns:\n",
    "    # your code here\n",
    "    x = spark_df.filter(spark_df[col].isNull()).count()\n",
    "    #print(x)\n",
    "    print('column', col, spark_df.filter(spark_df[col].isNull()).count())\n",
    "    \n",
    "#check categories in categoricals\n",
    "for column, data_type in spark_df.dtypes:\n",
    "   # your code here\n",
    "    if data_type == 'string':\n",
    "        print('Feature {} has: {}'.format(column, spark_df.select(column).distinct().collect()))\n",
    "        \n",
    "#visualize the extraneous ones\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def extract_pairs(idx,group):\n",
    "    return [x[idx] for x in group]\n",
    "\n",
    "ed_pairs = spark_df.groupBy('EDUCATION').count().collect()\n",
    "ed_x = extract_pairs(0, ed_pairs)\n",
    "ed_y = extract_pairs(1, ed_pairs)\n",
    "\n",
    "## plotting the categories for education\n",
    "\n",
    "sns.barplot(x=ed_x,y=ed_y)\n",
    "plt.show()\n",
    "\n",
    "## plotting the categories for marriage\n",
    "mar_pairs =  spark_df.groupby('MARRIAGE').count().collect()\n",
    "sns.barplot(x=bar_plot_values(0, mar_pairs), y=bar_plot_values(1, mar_pairs))\n",
    "plt.show()\n",
    "\n",
    "#rebucket the extras\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "## changing the values in the education column\n",
    "spark_df_2 = spark_df.withColumn('EDUCATION',\n",
    "                    when(spark_df.EDUCATION == '0', 'Other')\n",
    "                    .when(spark_df.EDUCATION == '5', 'Other')\n",
    "                    .when(spark_df.EDUCATION == '6', 'Other')\n",
    "                    .otherwise(spark_df['EDUCATION']))\n",
    "\n",
    "## chaning the values in the marriage column\n",
    "spark_df_done = spark_df_2.withColumn('MARRIAGE',\n",
    "                                   when(spark_df.MARRIAGE == '0', 'Other')\\\n",
    "                                   .otherwise(spark_df['MARRIAGE']))\n",
    "\n",
    "spark_df_done.head()\n",
    "##\n",
    "#recheck categoricals\n",
    "for column, data_type in spark_df_done.dtypes:\n",
    "    # your code here\n",
    "    if data_type == 'string':\n",
    "        print(\"Feature {} has: {}\".format(column, spark_df_done.select(column).distinct().collect()))\n",
    "##EDA\n",
    "\n",
    "number_of_defaults = spark_df_done.groupBy('default').count().collect()\n",
    "default = [x[0] for x in number_of_defaults]\n",
    "num_defaults = [x[1] for x in number_of_defaults]\n",
    "ax = sns.barplot(default,num_defaults)\n",
    "ax.set_ylabel('Number of Defaults')\n",
    "ax.set_xticklabels(['No Default (0)','Default (1)'])\n",
    "\n",
    "# perform a groupby for default and sex\n",
    "number_of_defaults = spark_df_done.groupBy(['default', 'sex']).count().collect()\n",
    "number_of_defaults\n",
    "\n",
    "# make barplot for female and male default v no default rate\n",
    "female =  [number_of_defaults[0],number_of_defaults[3]]\n",
    "male = [number_of_defaults[1],number_of_defaults[2]]\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(1,2)\n",
    "f.set_figwidth(10)\n",
    "sns.barplot(x= bar_plot_values(0,female),y=bar_plot_values(2,female),ax=axes[0])\n",
    "sns.barplot(x= bar_plot_values(0,male),y=bar_plot_values(2,male),ax=axes[1])\n",
    "axes[0].set_title('Female Default Rate')\n",
    "axes[1].set_title('Male Default Rate')\n",
    "axes[0].set_ylabel('Number of Defaults')\n",
    "axes[0].set_xticklabels(['No Default (0)','Default (1)'])\n",
    "axes[1].set_xticklabels(['No Default (0)','Default (1)'])\n",
    "\n",
    "##onto the ML\n",
    "##fit data to a pipeline\n",
    "# importing the necessary modules\n",
    "from pyspark.ml.feature import StringIndexer,  VectorAssembler, StringIndexerModel#, OneHotEncoderEstimator,\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "stages = []\n",
    "indexers = []\n",
    "\n",
    "# creating the string indexers\n",
    "for col in ['EDUCATION','SEX','MARRIAGE']:\n",
    "    indexers.append(StringIndexer(inputCol =col,outputCol=col+'_',handleInvalid='keep'))\n",
    "    \n",
    "input_columns = [indexer.getOutputCol() for indexer in indexers]\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(inputCols=input_columns,\n",
    "                                         outputCols=[col + 'ohe' for col in input_columns],dropLast=True)\n",
    "\n",
    "\n",
    "# features to be included in the model \n",
    "features = ['LIMIT_BAL','AGE','PAY_0','PAY_2','PAY_3',\n",
    "            'PAY_4','PAY_5','PAY_6', 'BILL_AMT1','BILL_AMT2',\n",
    "            'BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']\n",
    "\n",
    "# adding the categorical features\n",
    "features.extend(one_hot_encoder.getOutputCols())\n",
    "\n",
    "# putting all of the features into a single vector\n",
    "vector_assember = VectorAssembler(inputCols= features , outputCol='features')\n",
    "\n",
    "stages.extend(indexers)\n",
    "stages.extend([one_hot_encoder,vector_assember])\n",
    "print(stages)\n",
    "\n",
    "##check out how that transforms things\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "data_transformer = pipeline.fit(spark_df_done)\n",
    "transformed_data = data_transformer.transform(spark_df_done)\n",
    "p = transformed_data.select('features')\n",
    "p.head()\n",
    "# 17 numerical features and 6 categorical ones (the argument dropLast = True makes us have Sex, 3 Edu variables and 2 marriage).\n",
    "\n",
    "###fitting ML models\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier, DecisionTreeClassifier, LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import numpy as np\n",
    "\n",
    "# your code here\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='default')\n",
    "p = Pipeline(stages=stages + [lr])\n",
    "evaluation = BinaryClassificationEvaluator(labelCol = 'default',metricName='areaUnderROC')\n",
    "\n",
    "lr_params = ParamGridBuilder().addGrid(lr.regParam,[0.0,0.2,0.5,1.0])\\\n",
    ".addGrid(lr.standardization,[True,False])\\\n",
    ".build()\n",
    "\n",
    "cv = CrossValidator(estimator=p, estimatorParamMaps=lr_params,evaluator=evaluation,parallelism=4)\n",
    "model = cv.fit(spark_df_done)\n",
    "\n",
    "##trying again with other models.  See function below for replication\n",
    "#see also the AUC values (area under ROC curve)\n",
    "# create function to cross validate models with different parameters\n",
    "def test_model(model, params):\n",
    "    p = Pipeline(stages=stages+[model])\n",
    "    evaluation = BinaryClassificationEvaluator(labelCol = 'default',metricName='areaUnderROC')\n",
    "    cv = CrossValidator(estimator=p, estimatorParamMaps=params,evaluator=evaluation,parallelism=4)\n",
    "    model = cv.fit(spark_df_done)\n",
    "    \n",
    "    index_best_mod = np.argmax(model.avgMetrics)\n",
    "    print(model.avgMetrics[index_best_model],'AUC')\n",
    "    print('best parameters : ',params[index_best_model])\n",
    "    return model\n",
    "\n",
    "'''solution:\n",
    "def create_model(ml_model,\n",
    "                 preprocessing_stages,\n",
    "                 param_grid,\n",
    "                 parallel = 4,\n",
    "                 evaluation_metric = 'areaUnderROC',\n",
    "                 parafeaturesCol = 'features',\n",
    "                 label='default'):\n",
    "    \n",
    "    stage_with_ml = preprocessing_stages + [ml_model]\n",
    "    pipe = Pipeline(stages=stage_with_ml)\n",
    "    \n",
    "    evaluation = BinaryClassificationEvaluator(labelCol = label,metricName=evaluation_metric)\n",
    "    model = CrossValidator(estimator = pipe,\n",
    "                        estimatorParamMaps=param_grid,\n",
    "                        evaluator = evaluation,\n",
    "                       parallelism = parallel).fit(spark_df_done)\n",
    "\n",
    "    index_best_model = np.argmax(model.avgMetrics)\n",
    "    print('best performing model: ', model.avgMetrics[index_best_model],'AUC')\n",
    "    print('best parameters: ',param_grid[index_best_model])\n",
    "    return model\n",
    "    '''\n",
    "\n",
    "#RF classifier\n",
    "# code to train Random Forest Classifier\n",
    "# ⏰ This cell may take a long time to run\n",
    "rf = RandomForestClassifier(featuresCol='features',labelCol='default')\n",
    "rf_params = ParamGridBuilder()\\\n",
    ".addGrid(rf.maxDepth, [5,10])\\\n",
    " .addGrid(rf.numTrees, [20,50,100,200])\\\n",
    " .build()\n",
    "\n",
    "rf_model = test_model(rf,rf_params)\n",
    "\n",
    "##gradient boosted classifier\n",
    "# code to train Gradient Boosting Classifier\n",
    "# ⏰ This cell may take a long time to run\n",
    "gb = GBTClassifier(featuresCol='features',labelCol='default')\n",
    "param_gb = ParamGridBuilder().addGrid(gb.maxDepth,[1,5]).addGrid(gb.maxIter,[20,60]).build()\n",
    "\n",
    "gb_model = test_model(gb, param_gb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#https://github.com/ericthansen/dsc-spark-section-recap\n",
    "Key Takeaways\n",
    "The key takeaways from this section include:\n",
    "\n",
    "Big Data usually refers to datasets that grow so large that they become awkward to work with using traditional database management systems and analytical approaches\n",
    "Big data refers to data that is terabytes (TB) to petabytes (PB) in size\n",
    "MapReduce can be used to split big datasets up in smaller sets to be distributed over several machines to deal with Big Data Analytics\n",
    "Before starting to work, you need to install Docker and Kinematic on your environment\n",
    "Make sure to test your installation so you're sure everything is working\n",
    "When you start working with PySpark, you have to create a SparkContext()\n",
    "The creation or RDDs is essential when working with PySpark\n",
    "Examples of actions and transformations include collect(), count(), filter(), first(), take(), and reduce()\n",
    "Machine Learning on the scale of big data can be done with Spark using the ml library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-recommender-section-intro\n",
    "# https://github.com/ericthansen/dsc-recommendation-system-introduction\n",
    "We want to learn a function that predicts the relevance score for a given (typically unseen) item based on user user profile and context\n",
    "\n",
    "Within personalized recommendation systems there are many different possible algorithms. We're going to go over the important ones now.\n",
    "\n",
    "Each of these techniques make use of different similarity metrics to determine how \"similar\" items are to one another. The most common similarity metrics are Euclidean distance (Links to an external site.), cosine similarity (Links to an external site.), Pearson correlation (Links to an external site.) and the Jaccard index (useful with binary data) (Links to an external site.). Each one of these distance metrics has its advantages and disadvantages depending on the type of ratings you are using and the characteristics of your data.\n",
    "https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
    "https://en.wikipedia.org/wiki/Jaccard_index\n",
    "   \n",
    "Content-Based Recommenders\n",
    "Main Idea: If you like an item, you will also like \"similar\" items.\n",
    "    \n",
    "Collaborative Filtering Systems\n",
    "Main Idea: If user A likes items 5, 6, 7, and 8 and user B likes items 5, 6, and 7, then it is highly likely that user B will also like item 8.\n",
    "    \n",
    "http://infolab.stanford.edu/~ullman/mmds/ch9.pdf\n",
    "    \n",
    "#lots of good reading content here and descriptions\n",
    "#also, an old pal - Singular Value Decomposition! here, it's considered a dimensionality reduction technique...\n",
    "# SVD has a great property that it has the minimal reconstruction Sum of Square Error (SSE); therefore, it is also commonly used in dimensionality reduction\n",
    "\n",
    "#SVD in python!\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Create a sparse matrix \n",
    "A = csc_matrix([[1, 0, 0], [5, 0, 2], [0, 1, 0], [0, 0, 3], [4, 0, 9]], dtype=float)\n",
    "\n",
    "# Apply SVD\n",
    "u, s, vt = svds(A, k=2) # k is the number of stretching factors\n",
    "\n",
    "print ('A:\\n', A.toarray())\n",
    "print ('=')\n",
    "print ('\\nU:\\n', u)\n",
    "print ('\\nΣ:\\n', s)\n",
    "print ('\\nV.T:\\n', vt)\n",
    "'''n this example, consider  𝐴  as the utility matrix with users and products links.\n",
    "\n",
    "After the decomposition  𝑈  will be the user features matrix,  Σ  will be the diagonal matrix of singular values (essentially weights), and  𝑉.𝑇  will be the movie features matrix.\n",
    "\n",
    "𝑈  and  𝑉.𝑇  are orthogonal, and represent different things.  𝑈  represents how much users like each feature and  𝑉.𝑇  represents how relevant each feature is to each movie.'''\n",
    "#recreate A matrix\n",
    "import numpy as np\n",
    "print('Approximation of Ratings Matrix')\n",
    "u.dot(np.diag(s).dot(vt))\n",
    "\n",
    "print('Rounded Approximation of Ratings Matrix')\n",
    "np.round(u.dot(np.diag(s).dot(vt)))\n",
    "\n",
    "'''As you can see, the matrix has now been almost recreated to the exact specifications of the original matrix. Out of the 12 user-item ratings, we have incorrectly rated one of them (Row 3, Column 2). SVD is not a perfect solution, but when we have enough users and items, we are able to gain valuable insights about the underlying relationships found in our data.'''\n",
    "\n",
    "#additional reading\n",
    "https://pdfs.semanticscholar.org/943a/e455fafc3d36ae4ce68f1a60ae4f85623e2a.pdf\n",
    "https://www.upwork.com/hiring/data/how-collaborative-filtering-works/\n",
    "https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-Singular-Value-Decomposition/\n",
    "http://math.mit.edu/classes/18.095/2016IAP/lec2/SVD_Notes.pdf\n",
    "http://cs229.stanford.edu/proj2006/KleemanDenuitHenderson-MatrixFactorizationForCollaborativePrediction.pdf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-implementing-recommender-systems.git\n",
    "#import dataset - may require download\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "jokes = Dataset.load_builtin(name='jester')\n",
    "\n",
    "type(jokes)\n",
    "#\n",
    "# Split into train and test set\n",
    "trainset, testset = train_test_split(jokes, test_size=0.2)\n",
    "#note there is no x/y here\n",
    "print('Type trainset :',type(trainset),'\\n')\n",
    "print('Type testset :',type(testset))\n",
    "\n",
    "#wow ,they have different datatypes\n",
    "print(len(testset))\n",
    "print(testset[0])\n",
    "\n",
    "#load things\n",
    "from surprise.prediction_algorithms import knns\n",
    "from surprise.similarities import cosine, msd, pearson\n",
    "from surprise import accuracy\n",
    "\n",
    "#between items or users, usually pick whichever is smaller\n",
    "print('Number of users: ', trainset.n_users, '\\n')\n",
    "print('Number of items: ', trainset.n_items, '\\n')\n",
    "\n",
    "#here are there are more users than items, so use items # it will be more efficient to calculate item-item similarity rather than user-user similarity.\n",
    "\n",
    "##memory based methods\n",
    "sim_cos = {'name':'cosine', 'user_based':False}\n",
    "\n",
    "#time to train model.  if we did user, this would take a long time!\n",
    "basic = knns.KNNBasic(sim_options=sim_cos)\n",
    "basic.fit(trainset)\n",
    "# look at similarity metrics\n",
    "basic.sim\n",
    "#now test the model\n",
    "predictions = basic.test(testset)\n",
    "print(accuracy.rmse(predictions))\n",
    "#rmse of 4.5, not bad but not great on a scale of 20\n",
    "#try a different similarity metric\n",
    "sim_pearson = {'name':'pearson', 'user_based':False}\n",
    "basic_pearson = knns.KNNBasic(sim_options=sim_pearson)\n",
    "basic_pearson.fit(trainset)\n",
    "predictions = basic_pearson.test(testset)\n",
    "print(accuracy.rmse(predictions))\n",
    "#this does a little bit better than cosine similarity...4.2\n",
    "#but now we can change basic knn to knn with means\n",
    "sim_pearson = {'name':'pearson', 'user_based':False}\n",
    "knn_means = knns.KNNWithMeans(sim_options=sim_pearson)\n",
    "knn_means.fit(trainset)\n",
    "predictions = knn_means.test(testset)\n",
    "print(accuracy.rmse(predictions))\n",
    "#gives 4.136\n",
    "\n",
    "#and can go one better https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline\n",
    "#uses a bias term that mnimnizes the cost function\n",
    "sim_pearson = {'name':'pearson', 'user_based':False}\n",
    "knn_baseline = knns.KNNBaseline(sim_options=sim_pearson)\n",
    "knn_baseline.fit(trainset)\n",
    "predictions = knn_baseline.test(testset)\n",
    "print(accuracy.rmse(predictions))\n",
    "#gives 4.131\n",
    "\n",
    "##model based methods\n",
    "'''It's worth pointing out that when SVD is calculated for recommendation systems, it is preferred to be done with a modified version called \"Funk's SVD\" that only takes into account the rated values, ignoring whatever items have not been rated by users. The algorithm is named after Simon Funk, who was part of the team who placed 3rd in the Netflix challenge with this innovative way of performing matrix decomposition. Read more about Funk's SVD implementation at his original blog post. There is no simple way to include for this fact with SciPy's implementation of svd(), but luckily the surprise library has Funk's version of SVD implemented to make our lives easier!'''\n",
    "#https://sifter.org/~simon/journal/20061211.html\n",
    "\n",
    "#funk SVD is implemented in Surprise\n",
    "#this takes a while to gridsearch it, but if you want to, uncomment this:\n",
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {'n_factors':[20, 100],'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "#               'reg_all': [0.4, 0.6]}\n",
    "# gs_model = GridSearchCV(SVD,param_grid=param_grid,n_jobs = -1,joblib_verbose=5)\n",
    "# gs_model.fit(jokes)\n",
    "##the optimal features are given here:\n",
    "svd = SVD(n_factors=100, n_epochs=10, lr_all=0.005, reg_all=0.4)\n",
    "svd.fit(trainset)\n",
    "predictions = svd.test(testset)\n",
    "print(accuracy.rmse(predictions))\n",
    "#this gives RMSE 4.277\n",
    "#but this model performed worse than the others!\n",
    "'''In general, the advantages of matrix factorization starts to show itself when the size of the dataset becomes massive. At that point, the storage challenges increase for the memory-based models, and there is enough data for latent factors to become extremely apparent.'''\n",
    "##making predictions\n",
    "user_34_prediction = svd.predict('34', '25')\n",
    "user_34_prediction\n",
    "#gives a prediction tuple\n",
    "#get the stimated rating\n",
    "user_34_prediction[3]\n",
    "'''You might be wondering, \"OK I'm making predictions about certain items rated by certain users, but how can I actually give certain N recommendations to an individual user?\" Although surprise is a great library, it does not have this recommendation functionality built into it, but in the next lab, you will get some experience not only fitting recommendation system models, but also programmatically retrieving recommended items for each user.'''\n",
    "# Sources\n",
    "# Jester dataset originally obtained from:\n",
    "http://www.ieor.berkeley.edu/~goldberg/pubs/eigentaste.pdf\n",
    "# Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.\n",
    "\n",
    "# Additional Resources\n",
    "https://surprise.readthedocs.io/en/stable/index.html\n",
    "https://blog.cambridgespark.com/tutorial-practical-introduction-to-recommender-systems-dbe22848392b\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-implementing-recommender-systems-lab\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.info()\n",
    "# Drop unnecessary columns\n",
    "new_df = df.drop('timestamp', axis=1)\n",
    "\n",
    "from surprise import Reader, Dataset\n",
    "# read in values as Surprise dataset \n",
    "reader=Reader()\n",
    "data = Dataset.load_from_df(new_df, reader)\n",
    "\n",
    "dataset = data.build_full_trainset()\n",
    "print('Number of users: ', dataset.n_users, '\\n')\n",
    "print('Number of items: ', dataset.n_items)\n",
    "\n",
    "#determine best model\n",
    "\n",
    "# importing relevant libraries\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.prediction_algorithms import KNNWithMeans, KNNBasic, KNNBaseline\n",
    "from surprise.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "from surprise import accuracy\n",
    "#model 1\n",
    "param_grid = {'n_factors':[20, 50, 100],'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "              'reg_all': [0.02, 0.05, 0.1]}\n",
    "g_s_svd = GridSearchCV(SVD,param_grid=param_grid,n_jobs = -1,joblib_verbose=5)\n",
    "g_s_svd.fit(daata)\n",
    "\n",
    "# print out optimal parameters for SVD after GridSearch\n",
    "print(g_s_svd.best_score)\n",
    "print(g_s_svd.best_params)\n",
    "#look, I did better than .869\n",
    "\n",
    "#model 2\n",
    "# cross validating with KNNBasic\n",
    "knn_basic = KNNBasic(sim_options={'name':'pearson', 'user_based':True})\n",
    "cv_knn_basic = cross_validate(knn_basic, data, n_jobs=-1)\n",
    "\n",
    "# print out the average RMSE score for the test set\n",
    "for i in cv_knn_basic.items():\n",
    "    print(i)\n",
    "print('-----------------------')\n",
    "print(np.mean(cv_knn_basic['test_rmse']))\n",
    "\n",
    "#model 3\n",
    "# cross validating with KNNBaseline\n",
    "knn_baseline = KNNBaseline(sim_options={'name':'pearson', 'user_based':True})\n",
    "cv_knn_baseline = cross_validate(knn_baseline,data)\n",
    "\n",
    "# print out the average score for the test set\n",
    "for i in cv_knn_baseline.items():\n",
    "    print(i)\n",
    "\n",
    "np.mean(cv_knn_baseline['test_rmse'])\n",
    "\n",
    "##best model looks to be the gridsearched one\n",
    "\n",
    "#making recommendations\n",
    "df_movies = pd.read_csv('./ml-latest-small/movies.csv')\n",
    "df_movies.head()\n",
    "\n",
    "#simple preds\n",
    "svd = SVD(n_factors= 50, reg_all=0.05)\n",
    "svd.fit(dataset)\n",
    "svd.predict(2, 4)\n",
    "\n",
    "##obtaining user ratings - write function to ask user for some ratings on random movies\n",
    "def movie_rater(movie_df,num, genre=None):\n",
    "    userID = 1000\n",
    "    rating_list = []\n",
    "    while num > 0:\n",
    "        if genre:\n",
    "            movie = movie_df[movie_df['genres'].str.contains(genre)].sample(1)\n",
    "        else:\n",
    "            movie = movie_df.sample(1)\n",
    "        print(movie)\n",
    "        rating = input('How do you rate this movie on a scale of 1-5, press n if you have not seen :\\n')\n",
    "        if rating == 'n':\n",
    "            continue\n",
    "        else:\n",
    "            rating_one_movie = {'userId':userID,'movieId':movie['movieId'].values[0],'rating':rating}\n",
    "            rating_list.append(rating_one_movie) \n",
    "            num -= 1\n",
    "    return rating_list  \n",
    "\n",
    "# try out the new function here!\n",
    "user_rating = movie_rater(df_movies, 4, 'Comedy')\n",
    "\n",
    "#making preds using new ratings\n",
    "## add the new ratings to the original ratings DataFrame\n",
    "new_ratings_df = new_df.append(user_rating,ignore_index=True)\n",
    "new_data = Dataset.load_from_df(new_ratings_df,reader)\n",
    "\n",
    "# train a model using the new combined DataFrame\n",
    "'''{'rmse': {'n_factors': 20, 'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.02}, 'mae': {'n_factors': 20, 'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.02}}'''\n",
    "svd_ = SVD(n_factors= 20, n_epochs= 10, lr_all= 0.005, reg_all=0.02)\n",
    "svd_.fit(new_data.build_full_trainset())\n",
    "\n",
    "# make predictions for the user\n",
    "# you'll probably want to create a list of tuples in the format (movie_id, predicted_score)\n",
    "list_of_movies = []\n",
    "for m_id in new_df['movieId'].unique():\n",
    "    list_of_movies.append( (m_id,svd_.predict(1000,m_id)[3]))\n",
    "    \n",
    "# order the predictions from highest to lowest rated\n",
    "\n",
    "ranked_movies = sorted(list_of_movies, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "# return the top n recommendations using the \n",
    "def recommended_movies(user_ratings,movie_title_df,n):\n",
    "        for idx, rec in enumerate(user_ratings):\n",
    "            title = movie_title_df.loc[movie_title_df['movieId'] == int(rec[0])]['title']\n",
    "            print('Recommendation # ', idx+1, ': ', title, '\\n')\n",
    "            n-= 1\n",
    "            if n == 0:\n",
    "                break\n",
    "recommended_movies(ranked_movies,df_movies,5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-matrix-factorization-als\n",
    "https://datasciencemadesimpler.wordpress.com/tag/alternating-least-squares/\n",
    "https://sparkhub.databricks.com/video/a-more-scalable-way-of-making-recommendations-with-mllib/\n",
    "http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf\n",
    "https://www.elenacuoco.com/2016/12/22/alternating-least-squares-als-spark-ml/\n",
    "#introducing a learning function\n",
    "#if full rating matrix is R, can decompose it as P and Q\n",
    "import numpy as np\n",
    "\n",
    "# users X factors\n",
    "P = np.array([[-0.63274434,  1.33686735, -1.55128517], \n",
    "              [-2.23813661,  0.5123861 ,  0.14087293], \n",
    "              [-1.0289794 ,  1.62052691,  0.21027516], \n",
    "              [-0.06422255,  1.62892864,  0.33350709]])\n",
    "\n",
    "# factors X items\n",
    "Q = np.array([[-2.09507374,  0.52351075,  0.01826269], \n",
    "              [-0.45078775, -0.07334991,  0.18731052], \n",
    "              [-0.34161766,  2.46215058, -0.18942263], \n",
    "              [-1.0925736 ,  1.04664756,  0.69963111], \n",
    "              [-0.78152923,  0.89189076, -1.47144019]])\n",
    "\n",
    "# the original \n",
    "R = np.array([[2, np.nan, np.nan, 1, 4], \n",
    "              [5, 1, 2, np.nan, 2], \n",
    "              [3, np.nan, np.nan, 3, np.nan], \n",
    "              [1, np.nan, 4, 2, 1]])\n",
    "\n",
    "print(P[2])\n",
    "print(Q.T[:,4])\n",
    "P[2].dot(Q.T[:,4])\n",
    "\n",
    "P.dot(Q.T)\n",
    "\n",
    "#we establish a loss funciton to minimize in order to solve for P, Q\n",
    "𝐿=∑𝑢,𝑖∈𝜅(𝑟𝑢,𝑖−𝑞𝑇𝑖𝑝𝑢)2+λ(||𝑞𝑖||2+|𝑝𝑢||2)\n",
    "##alternating least squares is one way of doing this.  continue alternating (about 10 times works well) till \n",
    "##desired convergence calculating p, q\n",
    "First, let's assume the item vectors are fixed, we first solve for the user vectors:\n",
    "\n",
    "𝑝𝑢=(∑𝑟𝑢,𝑖∈𝑟𝑢∗𝑞𝑖𝑞𝑇𝑖+𝜆𝐼𝑘)−1∑𝑟𝑢,𝑖∈𝑟𝑢∗𝑟𝑢𝑖𝑞𝑖\n",
    " \n",
    "Then we hold the user vectors constant and solve for the item vectors:\n",
    "\n",
    "𝑞𝑖=(∑𝑟𝑢,𝑖∈𝑟𝑖∗𝑝𝑢𝑝𝑇𝑢+𝜆𝐼𝑘)−1∑𝑟𝑢,𝑖∈𝑟𝑢∗𝑟𝑢𝑖𝑝𝑢\n",
    "\n",
    "#modification to include bias for item, user average\n",
    "## Modification to Include Bias\n",
    "\n",
    "Putting these biases into an equation becomes:\n",
    "\n",
    "𝑟̂ 𝑢𝑖=𝜇+𝑏𝑖+𝑏𝑢+𝑞𝑇𝑖𝑝𝑢\n",
    " \n",
    "and the overall loss function becomes:\n",
    "\n",
    "𝐿=∑𝑢,𝑖∈𝜅(𝑟𝑢,𝑖−𝜇−𝑏𝑢−𝑏𝑖−𝑝𝑇𝑢𝑞𝑖)2+𝜆(||𝑞𝑖||2+|𝑝𝑢||2+𝑏2𝑢+𝑏2𝑖)\n",
    "#due to alternating nature, this can be done well in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/als-recommender-system-pyspark-lab\n",
    "#building a recommendation system in pyspark\n",
    "# import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# instantiate SparkSession object\n",
    "# spark = SparkSession.builder.master('local').getOrCreate()\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('ALSExample').config('spark.driver.host', 'localhost')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# read in the dataset into pyspark DataFrame\n",
    "movie_ratings = spark.read.csv('./data/ratings.csv', header='true', inferSchema='true')\n",
    "#check datatypes\n",
    "movie_ratings.dtypes\n",
    "\n",
    "#we won't need timestamp\n",
    "movie_ratings = movie_ratings.drop('timestamp')\n",
    "\n",
    "##fitting the alternating least squares model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.recommendation import ALS \n",
    "\n",
    "# split into training and testing sets\n",
    "(training, test) = movie_ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5,rank=4, regParam=0.01, userCol='userId', itemCol='movieId', ratingCol='rating',\n",
    "          coldStartStrategy='drop')\n",
    "\n",
    "# fit the ALS model to the training set\n",
    "model = als.fit(training)\n",
    "\n",
    "# importing appropriate library\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating',\n",
    "                                predictionCol='prediction')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print('Root-mean-square error = ' + str(rmse))\n",
    "\n",
    "\n",
    "##cross validation(and gridsearch) to find optimal model\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# initialize the ALS model\n",
    "als_model = ALS(userCol='userId', itemCol='movieId', \n",
    "                ratingCol='rating', coldStartStrategy='drop')\n",
    "\n",
    "# create the parameter grid              \n",
    "params = ParamGridBuilder()\\\n",
    "          .addGrid(als_model.regParam, [0.01, 0.001, 0.1])\\\n",
    "          .addGrid(als_model.rank, [4, 10, 50]).build()\n",
    "\n",
    "\n",
    "# instantiating crossvalidator estimator\n",
    "cv = CrossValidator(estimator=als_model, estimatorParamMaps=params,evaluator=evaluator,parallelism=4)\n",
    "best_model = cv.fit(movie_ratings)    \n",
    "\n",
    "# We see the best model has a rank of 50, so we will use that in our future models with this dataset\n",
    "best_model.bestModel.rank\n",
    "\n",
    "##incorporating the names of movies\n",
    "movie_titles = spark.read.csv('./data/movies.csv',header='true',inferSchema='true')\n",
    "\n",
    "movie_titles.head(5)\n",
    "\n",
    "##function to match up ids with titles\n",
    "def name_retriever(movie_id, movie_title_df):\n",
    "    return movie_title_df.where(movie_title_df.movieId == movie_id).take(1)[0]['title']\n",
    "\n",
    "print(name_retriever(1023, movie_titles))\n",
    "\n",
    "##getting recs\n",
    "users = movie_ratings.select(als.getUserCol()).distinct().limit(1)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "recs = userSubsetRecs.take(1)\n",
    "\n",
    "# use indexing to obtain the movie id of top predicted rated item\n",
    "first_recommendation = recs[0]['recommendations'][0][0]\n",
    "\n",
    "# use the name retriever function to get the values\n",
    "name_retriever(first_recommendation,movie_titles)\n",
    "\n",
    "##create RDD with top 5 for every user, then get a user and check predictions\n",
    "recommendations = model.recommendForAllUsers(5)\n",
    "recommendations.where(recommendations.userId == 3).collect()\n",
    "\n",
    "##getting preds for a new user\n",
    "def new_user_recs(user_id, new_ratings, rating_df, movie_title_df, num_recs):\n",
    "    # turn the new_recommendations list into a spark DataFrame\n",
    "    new_user_ratings = spark.createDataFrame(new_ratings,rating_df.columns)\n",
    "    \n",
    "    # combine the new ratings df with the rating_df\n",
    "    movie_ratings_combined = rating_df.union(new_user_ratings)\n",
    "    \n",
    "    # split the dataframe into a train and test set\n",
    "#     (training, test) = movie_ratings_combined.randomSplit([0.8, 0.2],seed=0)\n",
    "      #(training, test) = movie_ratings.randomSplit([0.8, 0.2])\n",
    "    \n",
    "    # create an ALS model and fit it\n",
    "    als = ALS(maxIter=5,rank=50, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "    model = als.fit(movie_ratings_combined)\n",
    "    \n",
    "    # make recommendations for all users using the recommendForAllUsers method\n",
    "    recommendations = model.recommendForAllUsers(num_recs)\n",
    "    \n",
    "    # get recommendations specifically for the new user that has been added to the DataFrame\n",
    "    recs_for_user = recommendations.where(recommendations.userId == user_id).take(1)\n",
    "    \n",
    "    for ranking, (movie_id, rating) in enumerate(recs_for_user[0]['recommendations']):\n",
    "        movie_string = name_retriever(movie_id,movie_title_df)\n",
    "        print('Recommendation {}: {}  | predicted score :{}'.format(ranking+1,movie_string,rating))\n",
    "        \n",
    "        \n",
    "# try out your function with the movies listed above\n",
    "user_id = 100000\n",
    "user_ratings_1 = [(user_id,3253,5),\n",
    "                  (user_id,2459,5),\n",
    "                  (user_id,2513,4),\n",
    "                  (user_id,6502,5),\n",
    "                  (user_id,1091,5),\n",
    "                  (user_id,441,4)]\n",
    "new_user_recs(user_id,\n",
    "             new_ratings=user_ratings_1,\n",
    "             rating_df=movie_ratings,\n",
    "             movie_title_df=movie_titles,\n",
    "             num_recs = 10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://github.com/ericthansen/dsc-recommendation-section-recap\n",
    "Key Takeaways\n",
    "The key takeaways from this section include:\n",
    "\n",
    "Recommendation approaches can consist of simply recommending popular items (without personalization), or using algorithms which takes into account past customer behavior  \n",
    "When using algorithms, the two main types are content-based algorithms (recommending new content based on similar content), or collaborative filtering based (recommending new content based on similar types of users)  \n",
    "Collaborative Filtering (CF) is currently the most widely used approach to build recommendation systems  \n",
    "The key idea behind CF is that similar users have similar interests and that a user generally likes items that are similar to other items they like  \n",
    "CF is filling an \"empty cell\" in the utility matrix based on the similarity between users or item. Matrix factorization or decomposition can help us solve this problem by determining what the overall \"topics\" are when a matrix is factored  \n",
    "Matrix decomposition can be reformulated as an optimization problem with loss functions and constraints  \n",
    "Matrix decomposition can be done using either Singular Value Decomposition (SVD) or Alternating Least Squares (ALS)  \n",
    "Spark's ALS implementation can be used to build a scalable and efficient recommendation system  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-time-series-section-intro\n",
    "#loading time series data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "temp_data = pd.read_csv('min_temp.csv')\n",
    "temp_data.head(15)\n",
    "\n",
    "#looking at info\n",
    "temp_data.info()\n",
    "\n",
    "##convert data object to better format\n",
    "##To ensure dates are understood correctly, you can use Pandas' Timestamp or base Python’s Datetime types; and they are interchangeable in most cases.\n",
    "# Convert Date to a datetime column\n",
    "temp_data['Date'] = pd.to_datetime(temp_data['Date'], format='%d/%m/%y')\n",
    "\n",
    "temp_data.info()\n",
    "\n",
    "# Make Date the index \n",
    "temp_data.set_index('Date', inplace=True)\n",
    "\n",
    "temp_data.info()\n",
    "\n",
    "temp_data.head(15)\n",
    "\n",
    "#Resampling - i.e rolling finer to coarser\n",
    "temp_monthly = temp_data.resample('MS')\n",
    "month_mean = temp_monthly.mean()\n",
    "month_mean.head(15)\n",
    "\n",
    "##upsampling - putting in extra samples\n",
    "temp_bidaily = temp_data.resample('12H').asfreq()\n",
    "temp_bidaily.head()\n",
    "\n",
    "##but notice, every other reading is NaN, so we can fill in \n",
    "temp_bidaily_fill = temp_bidaily.ffill() #that's forward fill, cand also use backfill\n",
    "temp_bidaily_fill.head()\n",
    "\n",
    "##selecting and slicing time series data\n",
    "temp_1985_onwards = temp_data['1985':]\n",
    "print(temp_1985_onwards.head())\n",
    "print(temp_1985_onwards.tail())\n",
    "\n",
    "##missing data\n",
    "temp_data.isnull().sum()\n",
    "\n",
    "##visualizing time series data!\n",
    "nyse = pd.read_csv('NYSE_monthly.csv')\n",
    "nyse['Month'] = pd.to_datetime(nyse['Month'])\n",
    "nyse.set_index('Month', inplace=True)\n",
    "\n",
    "nyse.head()\n",
    "\n",
    "##tim series line plot\n",
    "# Draw a line plot using nyse and .plot() method \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nyse.plot(figsize = (16,6));\n",
    "\n",
    "##time series dot plot\n",
    "# Draw a dot plot using temp and .plot() method \n",
    "nyse.plot(figsize = (20,6), style = '.b');\n",
    "\n",
    "##grouping and visualizing time series data\n",
    "# Use pandas grouper to group values using annual frequency\n",
    "year_groups = nyse.groupby(pd.Grouper(freq ='A'))\n",
    "\n",
    "# Create a new DataFrame and store yearly values in columns \n",
    "nyse_annual = pd.DataFrame()\n",
    "\n",
    "for yr, group in year_groups:\n",
    "    nyse_annual[yr.year] = group.values.ravel()\n",
    "    \n",
    "# Plot the yearly groups as subplots\n",
    "nyse_annual.plot(figsize = (13,8), subplots=True, legend=True);\n",
    "##that gives 6 subplots, but to superimpose them together\n",
    "# Plot overlapping yearly groups \n",
    "nyse_annual.plot(figsize = (15,5), subplots=False, legend=True);\n",
    "\n",
    "##time series histograms and density plots\n",
    "nyse.hist(figsize = (10,6));\n",
    "\n",
    "##that doesn't look exactly normal, change buckets\n",
    "nyse.hist(figsize = (10,6), bins = 7);\n",
    "\n",
    "# Plot a density plot for nyse dataset\n",
    "nyse.plot(kind='kde', figsize = (15,10));\n",
    "\n",
    "##time series box/whisker plots by year\n",
    "# Generate a box and whiskers plot for nyse_annual\n",
    "nyse_annual.boxplot(figsize = (12,7));\n",
    "##good to find outliers\n",
    "\n",
    "##time series heatmaps\n",
    "year_matrix = nyse_annual.T\n",
    "plt.matshow(year_matrix, interpolation=None, aspect='auto', cmap=plt.cm.Spectral_r);\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/timeseries.html\n",
    "\n",
    "#manipulation tricks:\n",
    "https://towardsdatascience.com/basic-time-series-manipulation-with-pandas-4432afee64ea\n",
    "\n",
    "#and then two labs on it, but these were actually just using the things above\n",
    "#https://github.com/ericthansen/dsc-visualizing-time-series-data-lab\n",
    "#https://github.com/ericthansen/dsc-managing-time-series-data-lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-types-of-trends.git\n",
    "##most time series models rely on the time series data to be STATIONARY - i.e. properties like mean and variance don't change over time\n",
    "##some things to check:\n",
    "#no trend:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('NYSE_monthly.csv')\n",
    "data['Month'] = pd.to_datetime(data['Month'])\n",
    "data.set_index('Month', inplace=True)\n",
    "data.plot(figsize=(12,6), linewidth=2, fontsize=14)\n",
    "plt.xlabel('Month', fontsize=20)\n",
    "plt.ylabel('Monthly NYSE returns', fontsize=20)\n",
    "plt.ylim((-0.15, 0.15));\n",
    "\n",
    "data.head()\n",
    "\n",
    "##linear trend - upward or down\n",
    "data = pd.read_csv('uk_imports.csv')\n",
    "data['Quarter'] = pd.to_datetime(data['Quarter'])\n",
    "data.set_index('Quarter', inplace=True)\n",
    "\n",
    "data.plot(figsize=(12,4), linewidth=2, fontsize=14)\n",
    "plt.ylabel('UK imports (Million Pounds)', fontsize=20)\n",
    "plt.xlabel('Year', fontsize=20);\n",
    "\n",
    "##downward:\n",
    "data = pd.read_csv('winning_400m.csv')\n",
    "data['year'] = pd.to_datetime(data['year'].astype(str))\n",
    "\n",
    "data.set_index('year', inplace=True)\n",
    "\n",
    "data.plot(figsize=(12,6), linewidth=2, fontsize=14)\n",
    "plt.xlabel('year', fontsize=20)\n",
    "plt.ylabel(\"winning times (in seconds)\", fontsize=20);\n",
    "\n",
    "##exponential trend\n",
    "# generated data \n",
    "years = pd.date_range('2012-01', periods=72, freq='M')\n",
    "index = pd.DatetimeIndex(years)\n",
    "\n",
    "np.random.seed(3456)\n",
    "sales = np.random.randint(-4, high=4, size=72)\n",
    "bigger = np.array([0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,3,3,3,3,\n",
    "                   3,3,3,3,3,3,3,3,7,7,7,7,7,7,7,7,7,7,7,\n",
    "                   11,11,11,11,11,11,11,11,11,11,18,18,18,\n",
    "                   18,18,18,18,18,18,26,26,26,26,26,36,36,36,36,36])\n",
    "data = pd.Series(sales+bigger+6, index=index)\n",
    "ts = data\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(data)\n",
    "plt.xlabel('month', fontsize=20)\n",
    "plt.ylabel('monthly sales', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "##periodic trend\n",
    "data = pd.read_csv('min_temp.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%y')\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "data.head()\n",
    "\n",
    "data_slice= data['1984':'1986']\n",
    "data_slice.plot(figsize=(14,6), linewidth=2, fontsize=20)\n",
    "plt.xlabel('Date', fontsize=20)\n",
    "plt.ylabel('Temperature (Degrees Celsius)', fontsize=20);\n",
    "\n",
    "##trend with increasing variance\n",
    "# Generated data \n",
    "years = pd.date_range('2012-01', periods=72, freq='M')\n",
    "index = pd.DatetimeIndex(years)\n",
    "\n",
    "np.random.seed(3456)\n",
    "sales= np.random.randint(-4, high=4, size=72)\n",
    "\n",
    "add = np.where(sales>0, 1, -1)\n",
    "bigger = np.array([0,0,0,0,0,0,0,0,0,1,1,1,1,2,2,2,2,2,2,2,2,2,3,3,\n",
    "                   3,3,3,3,4,4,5,5,6,6,6,6,6,7,7,7,7,7,7,8,8,8,8,8,8,8,\n",
    "                   9,9,9,9,9,9,9,9,10,10,10,10,10,10,12,12,12,12,12,12,12,12])\n",
    "data = pd.Series((sales+add*bigger)+40, index=index)\n",
    "\n",
    "fig = plt.figure(figsize=(14,6))\n",
    "plt.plot(data)\n",
    "plt.ylabel('Time series with increasing variance', fontsize=14)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "##periodic and upward trend\n",
    "air = pd.read_csv('airpassengers.csv')\n",
    "air['Month'] = pd.to_datetime(air['Month'])\n",
    "air.set_index('Month', inplace=True)\n",
    "air.head()\n",
    "\n",
    "air.plot(figsize=(14,6), linewidth=2, fontsize=14)\n",
    "plt.xlabel('year', fontsize=20)\n",
    "plt.ylabel('Miles', fontsize=20);\n",
    "\n",
    "##testing for trends\n",
    "# generated data \n",
    "years = pd.date_range('2012-01', periods=72, freq='M')\n",
    "index = pd.DatetimeIndex(years)\n",
    "\n",
    "np.random.seed(3456)\n",
    "sales= np.random.randint(-4, high=4, size=72)\n",
    "bigger = np.array([0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,3,3,3,3,\n",
    "                   3,3,3,3,3,3,3,3,7,7,7,7,7,7,7,7,7,7,7,\n",
    "                   11,11,11,11,11,11,11,11,11,11,18,18,18,\n",
    "                   18,18,18,18,18,18,26,26,26,26,26,36,36,36,36,36])\n",
    "data = pd.Series(sales+bigger+6, index=index)\n",
    "ts = data\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(data)\n",
    "plt.xlabel('month', fontsize=16)\n",
    "plt.ylabel('monthly sales', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "##rolling statistics\n",
    "roll_mean = ts.rolling(window=8, center=False).mean()\n",
    "roll_std = ts.rolling(window=8, center=False).std()\n",
    "\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "plt.plot(ts, color='blue', label='Original')\n",
    "plt.plot(roll_mean, color='red', label='Rolling Mean')\n",
    "plt.plot(roll_std, color='black', label = 'Rolling Std')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.show(block=False)\n",
    "\n",
    "##dickey-fuller test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "dftest = adfuller(ts)\n",
    "\n",
    "# Extract and display test results in a user friendly manner\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "for key,value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "print(dftest)\n",
    "print ('Results of Dickey-Fuller test: \\n')\n",
    "\n",
    "print(dfoutput)\n",
    "\n",
    "#more info:\n",
    "# https://machinelearningmastery.com/time-series-data-stationary-python/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-testing-for-trends-lab\n",
    "#just applying the above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-removing-trends\n",
    "#eliminating the trend.\n",
    "\n",
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Generated monthly sales\n",
    "years = pd.date_range('2012-01', periods=72, freq='M')\n",
    "index = pd.DatetimeIndex(years)\n",
    "\n",
    "np.random.seed(3456)\n",
    "sales= np.random.randint(-4, high=4, size=72)\n",
    "bigger = np.array([0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,3,3,3,3,\n",
    "                   3,3,3,3,3,3,3,3,7,7,7,7,7,7,7,7,7,7,7,\n",
    "                   11,11,11,11,11,11,11,11,11,11,18,18,18,\n",
    "                   18,18,18,18,18,18,26,26,26,26,26,36,36,36,36,36])\n",
    "final_series = sales+bigger+6\n",
    "data = pd.Series(final_series, index=index)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(data, color='blue')\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('Monthly sales', fontsize=14)\n",
    "plt.show()\n",
    "#log transform\n",
    "data = pd.Series(np.log(final_series), index=index)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(data, color='blue')\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('log(Monthly sales)', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "##take square root instead, this can sometimes reduce heteroscedasticity/other undesirable things\n",
    "data = pd.Series(np.sqrt(final_series), index=index)\n",
    "fig = plt.figure(figsize=(11,7))\n",
    "plt.plot(data, color='blue')\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('sqrt(Monthly sales)', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "#sqrt doesn't do much better than log\n",
    "##subtracting the rolling mean\n",
    "roll_mean = data.rolling(window=4).mean()\n",
    "fig = plt.figure(figsize=(11,7))\n",
    "plt.plot(data, color='blue',label='Original')\n",
    "plt.plot(roll_mean, color='red', label='Rolling Mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Subtract the moving average from the original data\n",
    "data_minus_roll_mean = data - roll_mean\n",
    "data_minus_roll_mean.head(15)\n",
    "\n",
    "# Drop the missing values from time series calculated above\n",
    "data_minus_roll_mean.dropna(inplace=True)\n",
    "\n",
    "##plot\n",
    "fig = plt.figure(figsize=(11,7))\n",
    "plt.plot(data_minus_roll_mean, color='blue',label='Sales - rolling mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Sales while the rolling mean is subtracted')\n",
    "plt.show(block=False)\n",
    "##result is much more stationary\n",
    "\n",
    "##the Weighted Rolling Mean! -there's some cool theory here, but takes more recent values to be more heavily weighted than others\n",
    "#one advantage here is that you don't need to know the window exactly, just the decay rate (or something related to it)\n",
    "# Use Pandas ewm() to calculate Exponential Weighted Moving Average\n",
    "exp_roll_mean = data.ewm(halflife=2).mean()\n",
    "\n",
    "# Plot the original data with exp weighted average\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "orig = plt.plot(data, color='blue',label='Original')\n",
    "mean = plt.plot(exp_roll_mean, color='red', label='Exponentially Weighted Rolling Mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Exponentially Weighted Rolling Mean & Standard Deviation')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Subtract the moving average from the original data\n",
    "data_minus_exp_roll_mean = data - exp_roll_mean\n",
    "data_minus_exp_roll_mean.head(15)\n",
    "\n",
    "fig = plt.figure(figsize=(11,7))\n",
    "plt.plot(data_minus_exp_roll_mean, color='blue',label='Sales - rolling mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Sales while the rolling mean is subtracted')\n",
    "plt.show(block=False)\n",
    "##but this doesn't appear to work all that much better than just regular mean\n",
    "\n",
    "##differencing - subtracting prior value from next value\n",
    "data_diff = data.diff(periods=1)\n",
    "data_diff.head(10)\n",
    "\n",
    "fig = plt.figure(figsize=(11,7))\n",
    "plt.plot(data_diff, color='blue',label='Sales - rolling mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Differenced sales series')\n",
    "plt.show(block=False)\n",
    "\n",
    "##differencing is a good tool to remove seasonality\n",
    "data = pd.read_csv('min_temp.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%y')\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "data.plot(figsize=(18,6), color='blue', linewidth=1, fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Temperature (Degrees Celsius)', fontsize=14);\n",
    "\n",
    "data_diff = data.diff(periods=365)\n",
    "data_diff.dropna(inplace=True)\n",
    "data_diff.plot(figsize=(18,6), color='blue', linewidth=1, fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Differenced Temperature (Degrees Celsius)', fontsize=14);\n",
    "\n",
    "##see also https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-removing-trends-lab\n",
    "##detrending air passenger data\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import 'passengers.csv' dataset\n",
    "data = pd.read_csv('passengers.csv')\n",
    "\n",
    "# Change the data type of the 'Month' column\n",
    "data['Month'] = pd.to_datetime(data['Month'])\n",
    "\n",
    "# Set the 'Month' column as the index\n",
    "ts = data.set_index('Month')\n",
    "\n",
    "# Print the first five rows\n",
    "ts.head()\n",
    "\n",
    "# Plot the time series\n",
    "ts.plot()\n",
    "\n",
    "##create a stationarity check\n",
    "# Create a function to check for the stationarity of a given time series using rolling stats and DF test\n",
    "# Collect and package the code from previous labs\n",
    "def stationarity_check(ts, window=8):\n",
    "    roll_mean = ts.rolling(window=8, center=False).mean()\n",
    "    roll_std = ts.rolling(window=8, center=False).std()\n",
    "\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    plt.plot(ts, color='blue', label='Original')\n",
    "    plt.plot(roll_mean, color='red', label='Rolling Mean')\n",
    "    plt.plot(roll_std, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    ###\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    dftest = adfuller(ts)\n",
    "    # Extract and display test results in a user friendly manner\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    #print(dftest)\n",
    "    print ('Results of Dickey-Fuller test: \\n')\n",
    "\n",
    "    print(dfoutput)\n",
    "    \n",
    "    # Code here\n",
    "stationarity_check(ts)\n",
    "\n",
    "##perform log and sqrt trans\n",
    "# Plot a log transform\n",
    "lts = ts.copy()\n",
    "lts['#Passengers'] = np.log(lts['#Passengers'])\n",
    "lts.plot()\n",
    "\n",
    "# Plot a square root transform\n",
    "sqts = ts.copy()\n",
    "sqts['#Passengers'] = np.sqrt(lts['#Passengers'])\n",
    "sqts.plot()\n",
    "\n",
    "##subtract rolling mean\n",
    "# your code here\n",
    "roll_mean = lts.rolling(window=7).mean() #np.log(ts).rolling(window=7).mean()\n",
    "fig = plt.figure(figsize=(11,7)) \n",
    "plt.plot(lts, color='blue',label='Original')\n",
    "plt.plot(roll_mean, color='red', label='Rolling Mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Subtract the moving average from the log transformed data\n",
    "data_minus_roll_mean = np.log(ts) - roll_mean\n",
    "\n",
    "# Print the first 10 rows\n",
    "data_minus_roll_mean.head(10)\n",
    "\n",
    "# Drop the missing values\n",
    "data_minus_roll_mean.dropna(inplace=True)\n",
    "\n",
    "# Plot the result\n",
    "fig = plt.figure(figsize=(11,7)) \n",
    "#plt.plot(lts, color='blue',label='Original')\n",
    "plt.plot(data_minus_roll_mean, color='red', label='Data minus Rolling Mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Data minus Rolling Mean')\n",
    "plt.show(block=False)\n",
    "\n",
    "##use the function above\n",
    "# Your code here\n",
    "stationarity_check(data_minus_roll_mean)\n",
    "\n",
    "##based on visuals and dickey-fuller, what's the conclusion?\n",
    "# Your conclusion here\n",
    "#It's still not totally stationary (p value of .15 is larger than .05, but much better than before.\n",
    "\n",
    "##subtract weighted rolling mean\n",
    "# Calculate Weighted Moving Average of log transformed data\n",
    "exp_roll_mean = lts.ewm(halflife=4).mean()\n",
    "\n",
    "# Plot the original data with exp weighted average\n",
    "fig = plt.figure(figsize=(11,7)) \n",
    "plt.plot(lts, color='blue',label='Original')\n",
    "plt.plot(exp_roll_mean, color='red', label='Exp Weighted Rolling Mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Data vs EWM')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Subtract the exponential weighted rolling mean from the original data \n",
    "data_minus_exp_roll_mean = lts - exp_roll_mean\n",
    "\n",
    "# Plot the time series\n",
    "fig = plt.figure(figsize=(11,7)) \n",
    "#plt.plot(lts, color='blue',label='Original')\n",
    "plt.plot(data_minus_exp_roll_mean, color='red', label='Data minus Exp Weighted Rolling Mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Data minus EWM')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Do a stationarity check\n",
    "stationarity_check(data_minus_exp_roll_mean)\n",
    "\n",
    "##conclusion now?\n",
    "# Your conclusion here\n",
    "#This p-value of 0.015 is below .05 - compelling suggestion that it is stationary!\n",
    "\n",
    "#@now let's do differencing on that transformed data\n",
    "# Difference your data\n",
    "data_diff = data_minus_exp_roll_mean.diff(periods=12)\n",
    "\n",
    "# Drop the missing values\n",
    "data_diff.dropna(inplace=True)\n",
    "\n",
    "# Check out the first few rows\n",
    "data_diff.head(15)\n",
    "\n",
    "# Plot your differenced time series\n",
    "fig = plt.figure(figsize=(11,7)) \n",
    "plt.plot(data_diff, color='red', label='Differenced Data')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Differenced Data')\n",
    "plt.show(block=False)\n",
    "\n",
    "# Perform the stationarity check\n",
    "stationarity_check(data_diff)\n",
    "\n",
    "##final conclusion\n",
    "# Your conclusion here\n",
    "#The p-value is now rather low.  The rolling avg and std do fluctuate, but seemingly randomly.  This seems ready to go!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-time-series-decomposition\n",
    "#http://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the check_stationarity function from previous lab\n",
    "def stationarity_check(TS):\n",
    "    \n",
    "    # Import adfuller\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    roll_mean = TS.rolling(window=8, center=False).mean()\n",
    "    roll_std = TS.rolling(window=8, center=False).std()\n",
    "    \n",
    "    # Perform the Dickey Fuller test\n",
    "    dftest = adfuller(TS) \n",
    "    \n",
    "    # Plot rolling statistics:\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    orig = plt.plot(TS, color='blue',label='Original')\n",
    "    mean = plt.plot(roll_mean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(roll_std, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # Print Dickey-Fuller test results\n",
    "    print('Results of Dickey-Fuller Test: \\n')\n",
    "\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', \n",
    "                                             '#Lags Used', 'Number of Observations Used'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Import 'passengers.csv' dataset\n",
    "data = pd.read_csv('passengers.csv')\n",
    "\n",
    "# Change the data type of the 'Month' column\n",
    "data['Month'] = pd.to_datetime(data['Month'])\n",
    "\n",
    "# Set the 'Month' column as the index\n",
    "ts = data.set_index('Month')\n",
    "\n",
    "# Plot the time series\n",
    "ts.plot(figsize=(10,4), color='blue');\n",
    "\n",
    "##additive or multiplicative seasonality can be complicated.  but built-in stuff can do well\n",
    "# Import and apply seasonal_decompose()\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(np.log(ts))\n",
    "\n",
    "# Gather the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "# Plot gathered statistics\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(411)\n",
    "plt.plot(np.log(ts), label='Original', color='blue')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend', color='blue')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality', color='blue')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals', color='blue')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "\n",
    "##check results\n",
    "# Drop missing values from residuals \n",
    "ts_log_decompose = residual\n",
    "ts_log_decompose.dropna(inplace=True)\n",
    "\n",
    "# Check stationarity\n",
    "stationarity_check(ts_log_decompose)\n",
    "\n",
    "#pretty good!\n",
    "#more reading:\n",
    "##https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/\n",
    "#https://github.com/ericthansen/dsc-time-series-section-recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-time-series-models-introduction\n",
    "# https://machinelearningmastery.com/white-noise-time-series-python/\n",
    "# https://machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/\n",
    "\n",
    "# Basic time series models\n",
    "#white noise model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nyse = pd.read_csv('NYSE_monthly.csv')\n",
    "nyse['Month'] = pd.to_datetime(nyse['Month'])\n",
    "nyse.set_index('Month', inplace=True)\n",
    "\n",
    "nyse.plot(figsize = (14,4))\n",
    "plt.show();\n",
    "'''The white noise model has three properties:\n",
    "\n",
    "Fixed and constant mean\n",
    "Fixed and constant variance\n",
    "No correlation over time (we'll talk about correlation in time series later, essentially, what this means is that the pattern seems truly \"random\")'''\n",
    "#random walk - y_t = y_(t-1) + e_t where e_t is white noise\n",
    "#note random walk does have heavy correlation over time (since bsed on prioer term)\n",
    "xr = pd.read_csv('exch_rates.csv')\n",
    "\n",
    "xr['Frequency'] = pd.to_datetime(xr['Frequency'])\n",
    "xr.set_index('Frequency', inplace=True)\n",
    "\n",
    "xr.tail()\n",
    "#\n",
    "xr['Euro'].plot(figsize = (14,5));\n",
    "#\n",
    "xr['Australian Dollar'].plot(figsize = (14,5));\n",
    "#\n",
    "##can also do random walk with drift =  y_t = y_(t-1) + e_t + C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-basic-time-series-models-lab\n",
    "#basic ts model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Do not change this seed\n",
    "np.random.seed(12) \n",
    "#a white noise model\n",
    "# Create dates\n",
    "dates = pd.date_range('2018-08-01', '2018-10-31', freq='B')#pd.date_range('08-01-2018', '10-31-2018')\n",
    "print(len(dates))\n",
    "# \n",
    "# len()\n",
    "dates\n",
    "\n",
    "# Generate values for white noise\n",
    "commute = np.random.normal(25, 4, len(dates))\n",
    "# Create a time series\n",
    "commute_series = pd.Series(commute, index=dates)\n",
    "# Visualize the time series \n",
    "#commute_series.plot()\n",
    "ax = commute_series.plot(figsize=(14,6))\n",
    "ax.set_ylabel('Commute time (min)', fontsize=20)\n",
    "ax.set_xlabel('Date', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# Shortest commute\n",
    "commute_series.min()\n",
    "\n",
    "# Longest commute\n",
    "commute_series.max()\n",
    "\n",
    "# Distribution of commute times\n",
    "display(commute_series.describe())\n",
    "\n",
    "commute_series.hist(grid=False);\n",
    "\n",
    "# Mean of commute_series\n",
    "commute_series.mean()\n",
    "\n",
    "# Standard deviation of commute_series\n",
    "commute_series.std()\n",
    "\n",
    "# Mean and standard deviation for August and October\n",
    "aug_series = commute_series['08-2018']\n",
    "oct_series = commute_series['10-2018']\n",
    "print('August: ', np.mean(aug_series), np.std(aug_series), '\\n')\n",
    "print('October: ', np.mean(oct_series), np.std(oct_series))\n",
    "\n",
    "##random walk model\n",
    "# Keep the random seed\n",
    "np.random.seed(11)\n",
    "\n",
    "# Create a series with the specified dates\n",
    "dates = pd.date_range('2010-01-01', '2010-11-30', freq='B')\n",
    "\n",
    "# White noise error term\n",
    "error = np.random.normal(0, 10, len(dates))\n",
    "\n",
    "# Define random walk\n",
    "def random_walk(start, error):        \n",
    "    vals=[]\n",
    "    for i in range(len(error)):\n",
    "        start += error[i]\n",
    "        vals.append(start)\n",
    "    return vals\n",
    "        \n",
    "'''the solution broadcasts it, which is probably nicer:\n",
    "def random_walk(start, error):        \n",
    "    Y_0 = start\n",
    "    cum_error = np.cumsum(error)\n",
    "    Y = cum_error + Y_0 \n",
    "    return Y\n",
    "\n",
    "'''\n",
    "\n",
    "shares_value = random_walk(1000, error)\n",
    "\n",
    "shares_series = pd.Series(shares_value, index=dates)\n",
    "\n",
    "#visualize\n",
    "# Your code here\n",
    "ax = shares_series.plot(figsize=(14,6))\n",
    "ax.set_ylabel('Stock value', fontsize=16)\n",
    "ax.set_xlabel('Date', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "##random walk with drift\n",
    "# Keep the random seed\n",
    "np.random.seed(11)\n",
    "\n",
    "# Create a series with the specified dates\n",
    "dates = pd.date_range('2010-01-01', '2010-11-30', freq='B')\n",
    "\n",
    "# White noise error term\n",
    "error = np.random.normal(0, 10, len(dates))\n",
    "\n",
    "# Define random walk\n",
    "def random_walk(start, error):        \n",
    "    Y_0 = start\n",
    "    cum_error = np.cumsum(error + 8)\n",
    "    Y = cum_error + Y_0 \n",
    "    return Y\n",
    "\n",
    "shares_value_drift = random_walk(1000, error)\n",
    "\n",
    "shares_series_drift =  pd.Series(shares_value_drift, index=dates)\n",
    "\n",
    "\n",
    "##plot\n",
    "ax = shares_series_drift.plot(figsize=(14,6))\n",
    "ax.set_ylabel('Stock value', fontsize=16)\n",
    "ax.set_xlabel('Date', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "##differencing in random walk model\n",
    "# Your code here\n",
    "sdiff = shares_series.diff(periods=1)\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "plt.plot(sdiff)\n",
    "plt.title('Differenced shares series')\n",
    "plt.show(block=False)\n",
    "\n",
    "##and for drift - the mean is just at 8(the drift)\n",
    "# Your code here \n",
    "sddiff = shares_series_drift.diff(periods=1)\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "plt.plot(sddiff)\n",
    "plt.title('Differenced shares series with drift')\n",
    "plt.show(block=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-corr-autocorr-in-time-series\n",
    "#https://www.datacamp.com/community/tutorials/time-series-analysis-tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "gtrends = pd.read_csv('google_trends.csv', skiprows=1)\n",
    "gtrends.head()\n",
    "\n",
    "gtrends.columns = ['Month', 'Diet', 'Gym', 'Finance']\n",
    "gtrends['Month'] = pd.to_datetime(gtrends['Month'])\n",
    "gtrends.set_index('Month', inplace=True)\n",
    "\n",
    "gtrends.plot(figsize=(18,6))\n",
    "plt.xlabel('Year', fontsize=14);\n",
    "\n",
    "gtrends.corr()\n",
    "\n",
    "#initial correlation matrix doesn't show much though.  bc they are time series and aren't yet staritonary!\n",
    "#detrend them\n",
    "gtrends_diff = gtrends.diff(periods=1)\n",
    "gtrends_diff.plot(figsize=(18,6))\n",
    "plt.xlabel('Year', fontsize=14);\n",
    "\n",
    "gtrends_diff.corr()\n",
    "#ah that's better\n",
    "#so there is correlation between the 3 things with seasonality...but what about one thing with itself(over time)?\n",
    "diet = gtrends[['Diet']]\n",
    "diet_shift_1 = diet.shift(periods=1)\n",
    "diet_shift_1.head()\n",
    "#implement a lag\n",
    "lag_1 = pd.concat([diet_shift_1, diet], axis=1)\n",
    "\n",
    "lag_1.corr()\n",
    "#see that the corr between original and lagged version is 0.62\n",
    "lag_1.plot(figsize=(18,6));\n",
    "##look at lag 2\n",
    "diet_shift_2 = diet.shift(periods=2)\n",
    "\n",
    "lag_2 = pd.concat([diet_shift_2, diet], axis=1)\n",
    "\n",
    "lag_2.corr()\n",
    "#.52, so a little less than above\n",
    "diet_shift_12 = diet.shift(periods=12)\n",
    "\n",
    "lag_12 = pd.concat([diet_shift_12, diet], axis=1)\n",
    "\n",
    "lag_12.corr()\n",
    "\n",
    "#.74 which is higher - makes sense for 12 mo lag\n",
    "lag_12.plot(figsize=(18,6));\n",
    "\n",
    "##autocorrelation checks this for all lags\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.plotting.autocorrelation_plot(diet);\n",
    "##here, there are spikes in autocorr for lag 12 and 24.\n",
    "#now try to do it for the differenced series\n",
    "diet_diff = gtrends_diff[['Diet']].dropna()\n",
    "plt.figure(figsize=(12,6))\n",
    "pd.plotting.autocorrelation_plot(diet_diff);\n",
    "## here, the ac graph seems more stable, centered on the mean, but also it decays over time! weird\n",
    "'''The Partial Autocorrelation Function\n",
    "Similar to the autocorrelation function, the Partial Autocorrelation Function (or PACF) gives the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags (unlike the autocorrelation function, which does not control for other lags). PACF can be thought of as a summary of the relationship between a time series element with observations at a lag, with the relationships of intervening observations removed.\n",
    "\n",
    "Let's plot the partial autocorrelation function of our \"Diet\" series. Although Pandas doesn't have a partial autocorrelation function, but luckily, statsmodels has one in its tsaplots module!'''\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "\n",
    "plot_pacf(diet, lags=50);\n",
    "'''NOTE: There is also a function plot_acf() in statsmodels, which serves as an alternative to Pandas' autocorrelation_plot().'''\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "\n",
    "plot_acf(diet, lags=50);\n",
    "\n",
    "# https://stackoverflow.com/questions/36038927/whats-the-difference-between-pandas-acf-and-statsmodel-acf\n",
    "# https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-corr-autocorr-in-time-series-lab\n",
    "##lots of acf and pacf plotting\n",
    "#exchange rate \n",
    "# Import all packages and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "# Import data\n",
    "xr = pd.read_csv('exch_rates.csv')\n",
    "\n",
    "# Change the data type of the 'Frequency' column \n",
    "xr['Frequency'] = pd.to_datetime(xr['Frequency'])\n",
    "\n",
    "# Set the 'Frequency' column as the index\n",
    "xr.set_index('Frequency', inplace=True)\n",
    "\n",
    "# Plot here\n",
    "xr.plot()\n",
    "plt.xlabel('Year', fontsize=14);\n",
    "plt.ylabel('Exchange Rate', fontsize=14);\n",
    "\n",
    "# Correlation\n",
    "xr.corr()\n",
    "##conclusion:\n",
    "#There is substantial correlation between all the currencies, but most strong between DK and Euro. AusDollar correlation with others\n",
    "#is .88 - strong but not absolute.\n",
    "\n",
    "# 1-lag differenced series \n",
    "xr_diff = xr.diff(periods=1)\n",
    "\n",
    "# Plot\n",
    "xr_diff.plot(figsize=(13,8), subplots=True, legend=True);\n",
    "\n",
    "# Correlation of differenced TS\n",
    "xr_diff.corr()\n",
    "##explain:\n",
    "#there is less correlation between ADollar now and others.  Since seasonality has been removed, this removes some of\n",
    "#the correlative effect of seasonality.\n",
    "\n",
    "##1-lagged\n",
    "# Isolate the EUR/USD exchange rate\n",
    "eur = xr[['Euro']]\n",
    "\n",
    "# \"Shift\" the time series by one period\n",
    "eur_shift_1 = eur.shift(periods=1)\n",
    "\n",
    "# Combine the original and shifted time series\n",
    "lag_1 = pd.concat([eur_shift_1, eur], axis=1)\n",
    "\n",
    "# Plot \n",
    "lag_1.plot(figsize=(13,8), alpha=0.7);\n",
    "\n",
    "# Correlation\n",
    "lag_1.corr()\n",
    "\n",
    "##lag-50\n",
    "# \"Shift\" the time series by 50 periods\n",
    "eur_shift_50 = eur.shift(periods=50)\n",
    "\n",
    "# Combine the original and shifted time series\n",
    "lag_50 = pd.concat([eur_shift_50, eur], axis=1)\n",
    "\n",
    "# Plot\n",
    "lag_50.plot(figsize=(13,8), alpha=0.7);\n",
    "\n",
    "# Correlation\n",
    "lag_50.corr()\n",
    "\n",
    "##conclusion:\n",
    "#1-day lag is pretty correlated since one day later; the rate will not have changed much.  \n",
    "#50 days later is less correlated since there's less temporal connection, and the period isn't 50 days.\n",
    "#However, it's still pretty high (.96), so this suggests AC will be high.\n",
    "\n",
    "# Plot ACF\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.plotting.autocorrelation_plot(eur.dropna());\n",
    "\n",
    "##heavily autocorrelated at first, then decays\n",
    "# Plot PACF\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "plot_pacf(eur.dropna(), lags=50);\n",
    "\n",
    "##really only 1 day of AC\n",
    "\n",
    "##air passenger data\n",
    "# Import and process the air passenger data\n",
    "air = pd.read_csv('passengers.csv')\n",
    "air['Month'] = pd.to_datetime(air['Month'])\n",
    "air.set_index('Month', inplace=True)\n",
    "air.head()\n",
    "\n",
    "# Plot ACF (regular)\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.plotting.autocorrelation_plot(air.dropna());\n",
    "\n",
    "# Plot PACF (regular)\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "plot_pacf(air.dropna(), lags=50, method='ywm');\n",
    "\n",
    "# Generate a differenced series\n",
    "air_diff = air.diff(periods=1)\n",
    "\n",
    "# Plot ACF (differenced)\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.plotting.autocorrelation_plot(air_diff.dropna());\n",
    "\n",
    "# Plot PACF (differenced)\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "plot_pacf(air_diff.dropna(), lags=50, method='ywm');\n",
    "\n",
    "##conclusion:\n",
    "#there is some 12-month period seasonality here.\n",
    "\n",
    "##NYSE data\n",
    "# Import and process the NYSE data\n",
    "nyse = pd.read_csv('NYSE_monthly.csv') \n",
    "nyse['Month'] = pd.to_datetime(nyse['Month'])\n",
    "nyse.set_index('Month', inplace=True)\n",
    "nyse.head()\n",
    "\n",
    "# Plot ACF\n",
    "plt.figure(figsize=(12,5))\n",
    "pd.plotting.autocorrelation_plot(nyse.dropna());\n",
    "\n",
    "# Plot PACF\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "plot_pacf(nyse.dropna(), lags=35, method='ywm');\n",
    "\n",
    "##conclusion:\n",
    "#AC is very low for all lags here - indicative of random walk behavior of stock prices.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-31f50acf9a61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m##look at acf and pacf of model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsaplots\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_pacf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsaplots\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_acf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##https://github.com/ericthansen/dsc-arma-models.git\n",
    "#https://onlinecourses.science.psu.edu/stat510/node/41/\n",
    "\n",
    "#https://github.com/ericthansen/dsc-arma-models-statsmodels\n",
    "##todo: fit an ar model and an ma model using statsmodels\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(11225)\n",
    "\n",
    "# Create a series with the specified dates\n",
    "dates = pd.date_range('2017-01-01', '2018-03-31')\n",
    "\n",
    "#generate AR model\n",
    "error = np.random.normal(0, 2, len(dates))\n",
    "Y_0 = 8\n",
    "mu = 5\n",
    "phi = 0.7\n",
    "\n",
    "TS = [None] * len(dates)\n",
    "y = Y_0\n",
    "for i, row in enumerate(dates):\n",
    "    TS[i] = mu + y * phi + error[i]\n",
    "    y = TS[i] - mu\n",
    "    \n",
    "#plot\n",
    "series = pd.Series(TS, index=dates)\n",
    "\n",
    "series.plot(figsize=(14,6), linewidth=2, fontsize=14);\n",
    "##look at acf and pacf of model\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,3))\n",
    "plot_acf(series, ax=ax, lags=40);\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,3))\n",
    "plot_pacf(series, ax=ax, lags=40);\n",
    "\n",
    "##check the model with arma in statsmodels\n",
    "# Import ARMA\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Instantiate an AR(1) model to the simulated data\n",
    "mod_arma = ARMA(series, order=(1,0))\n",
    "\n",
    "##fit\n",
    "# Fit the model to data\n",
    "res_arma = mod_arma.fit()\n",
    "\n",
    "# Print out summary information on the fit\n",
    "print(res_arma.summary())\n",
    "\n",
    "# Print out the estimate for the constant and for theta\n",
    "print(res_arma.params)\n",
    "\n",
    "##generate a first order MA model\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Create a series with the specified dates\n",
    "dates = pd.date_range('2015-04-01', '2015-08-31')\n",
    "\n",
    "error = np.random.normal(0, 4, len(dates))\n",
    "mu = 7\n",
    "theta = 0.9\n",
    "\n",
    "TS = [None] * len(dates)\n",
    "error_prev = error[0]\n",
    "for i, row in enumerate(dates):\n",
    "    TS[i] = mu + theta * error_prev + error[i]\n",
    "    error_prev = error[i]\n",
    "    \n",
    "##plot it\n",
    "series = pd.Series(TS, index=dates)\n",
    "\n",
    "series.plot(figsize=(14,6), linewidth=2, fontsize=14);\n",
    "\n",
    "#look at acf/pacf\n",
    "fig, ax = plt.subplots(figsize=(16,3))\n",
    "plot_acf(series, ax=ax, lags=40);\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,3))\n",
    "plot_pacf(series, ax=ax, lags=40);\n",
    "\n",
    "##check it with arma\n",
    "\n",
    "# Instantiate and fit an MA(1) model to the simulated data\n",
    "mod_arma = ARMA(series, order=(0,1))\n",
    "res_arma = mod_arma.fit()\n",
    "\n",
    "# Print out summary information on the fit\n",
    "print(res_arma.summary())\n",
    "\n",
    "# Print out the estimate for the constant and for theta\n",
    "print(res_arma.params)\n",
    "\n",
    "#woo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-arma-models-statsmodels-lab\n",
    "#dataset import\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('winning_400m.csv')\n",
    "data['year'] = pd.to_datetime(data['year'].astype(str))\n",
    "data.set_index('year', inplace=True)\n",
    "\n",
    "# Preview the dataset\n",
    "data\n",
    "\n",
    "##plot it\n",
    "# Plot the time series\n",
    "data.plot()\n",
    "\n",
    "data.plot(figsize=(12,8), linewidth=2, fontsize=12)\n",
    "plt.xlabel('Year', fontsize=20)\n",
    "plt.ylabel('Winning times (in seconds)', fontsize=14);\n",
    "\n",
    "##it's not stationary.  look at diffs\n",
    "# Difference the time series\n",
    "data_diff = data.diff().dropna()\n",
    "data_diff\n",
    "\n",
    "#look at acf and pacf of differenced TS\n",
    "# Plot the ACF\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plot_acf(data_diff,ax=ax, lags=9);\n",
    "\n",
    "# Plot the PACF\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plot_pacf(data_diff,ax=ax, lags=9);\n",
    "\n",
    "##try some different combinations for ARMA models\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n",
    "                        FutureWarning)\n",
    "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n",
    "                        FutureWarning)\n",
    "\n",
    "# Fit an ARMA(1,0) model\n",
    "mod_arma = ARMA(data_diff, order=(1,0))\n",
    "res_arma = mod_arma.fit()\n",
    "\n",
    "# Print out summary information on the fit\n",
    "print(res_arma.summary())\n",
    "\n",
    "###\n",
    "# Fit an ARMA(2,1) model\n",
    "mod_arma = ARMA(data_diff, order=(2,1))\n",
    "res_arma = mod_arma.fit()\n",
    "\n",
    "# Print out summary information on the fit\n",
    "print(res_arma.summary())\n",
    "\n",
    "###\n",
    "\n",
    "# Fit an ARMA(2,2) model\n",
    "mod_arma = ARMA(data_diff, order=(2,2))\n",
    "res_arma = mod_arma.fit()\n",
    "\n",
    "# Print out summary information on the fit\n",
    "print(res_arma.summary())\n",
    "\n",
    "##final model and discussion\n",
    "# Your comments here\n",
    "'''They all fit fairly well.  Between the 1,0 and 2,2 models, the 22 has lower(better) AIC while the 10 has better BIC.\n",
    "So, either could be chosen with good justification.  One could pick the simpler one as a tie breaker.\n",
    "For fun, I tried some 9,x models but they did not work.'''\n",
    "\n",
    "#https://github.com/ericthansen/dsc-time-series-models-section-recap\n",
    "'''The key takeaways from this section include:\n",
    "\n",
    "A white noise model has a fixed and constant mean and variance, and no correlation over time\n",
    "A random walk model has no specified mean or variance, but has a strong dependence over time\n",
    "The Pandas .corr() method can be used to return the correlation between various time series in the DataFrame\n",
    "Autocorrelation allows us to identify how strongly each time series observation is related to previous observations\n",
    "The Autocorrelation Function (ACF) is a function that represents autocorrelation of a time series as a function of the time lag\n",
    "The Partial Autocorrelation Function (or PACF) gives the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags\n",
    "ARMA (Autoregressive and Moving Average) modeling is a tool for forecasting time series values by regressing the variable on its own lagged (past) values\n",
    "ARMA models assume that you've already detrended your data and that there is no seasonality'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://github.com/ericthansen/dsc-nlp-section-intro\n",
    "#good stuff - Naive Bayesian Classification, bag of words, cleaning/tokenization, stemming/lemmatization, stop words,\n",
    "#Count vectorization and TF-IDF vectorization\n",
    "'''TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a combination of two individual metrics, which are the TF and IDF, respectively. TF-IDF is used when we have multiple documents. It is based on the idea that rare words contain more information about the content of a document than words that are used many times throughout all the documents. For instance, if we treated every article in a newspaper as a separate document, looking at the amount of times the word \"he\" or \"she\" is used probably doesn't tell us much about what that given article is about -- however, the amount of times \"touchdown\" is used can provide good signal that the article is probably about sports.\n",
    "\n",
    "Term Frequency is calculated with the following formula:\n",
    "https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Ctext%7BTerm%20Frequency%7D(t)%20=%20%5Cfrac%7B%5Ctext%7Bnumber%20of%20times%20t%20appears%20in%20a%20document%7D%7D%7B%5Ctext%7Btotal%20number%20of%20terms%20in%20the%20document%7D%7D\n",
    "Inverse Document Frequency is calculated with the following formula:\n",
    "https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Ctext%7BIDF%7D(t)%20=%20log_e(%5Cfrac%7B%5Ctext%7BTotal%20Number%20of%20Documents%7D%7D%7B%5Ctext%7BNumber%20of%20Documents%20with%20t%20in%20it%7D%7D)\n",
    "The TF-IDF value for a given word in a given document is just found by multiplying the two!'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-word-vectorization-lab \n",
    "#there are tools for this, but here it is by hand - I think one of the solution functions is wonky.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "np.random.seed(0)\n",
    "\n",
    "#the corpus\n",
    "filenames = ['song'+str(num)+'.txt' for num in range(1,21)]\n",
    "#filenames\n",
    "\n",
    "songs_df = pd.DataFrame()\n",
    "\n",
    "# Import and print song11.txt for sample\n",
    "with open('data/song11.txt') as f:\n",
    "    test_song = f.readlines()\n",
    "    print(test_song)\n",
    "    \n",
    "##tokenizing the data\n",
    "def clean_song(song):\n",
    "    cleaned_song = []\n",
    "    for line in song:\n",
    "        if not '[' in line and  not ']' in line:\n",
    "            for symbol in \",.?!''\\n\":\n",
    "                line = line.replace(symbol, '').lower()\n",
    "            cleaned_song.append(line)\n",
    "\n",
    "    return cleaned_song\n",
    "\n",
    "song_without_brackets = clean_song(test_song)\n",
    "song_without_brackets\n",
    "\n",
    "##may need to import and download this thing:\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def tokenize(song):\n",
    "    \n",
    "#     concatsong = ''\n",
    "#     for line in song:\n",
    "#         for word in line:\n",
    "#             concatsong += word\n",
    "#         concatsong += ' '\n",
    "#     #that worked, but this is better\n",
    "    concatsong = ' '.join(song)\n",
    "    return word_tokenize(concatsong)\n",
    "\n",
    "tokenized_test_song = tokenize(song_without_brackets)\n",
    "tokenized_test_song[:10]\n",
    "\n",
    "##count vectorization - just word frequencies\n",
    "def count_vectorize(song, vocab=None):\n",
    "    if vocab:\n",
    "        word_set = vocab\n",
    "    else:\n",
    "        word_set = list(set(song))\n",
    "    \n",
    "    song_dict = {i:0 for i in word_set}\n",
    "    \n",
    "    for word in song:\n",
    "        song_dict[word] += 1\n",
    "    \n",
    "    return song_dict\n",
    "test_vectorized = count_vectorize(tokenized_test_song)\n",
    "print(test_vectorized)\n",
    "\n",
    "##working toward TF-idf vect\n",
    "def term_frequency(BoW_dict):\n",
    "    tot_terms = sum(BoW_dict.values())\n",
    "    freqs = {i:0 for i in BoW_dict.keys()}\n",
    "    for k, v in freqs.items():\n",
    "        freqs[k] = BoW_dict[k]/tot_terms\n",
    "    return freqs\n",
    "\n",
    "test = term_frequency(test_vectorized)\n",
    "#print(test)\n",
    "print(list(test)[10:20])\n",
    "\n",
    "def inverse_document_frequency(list_of_dicts):\n",
    "    tot_docs = len(list_of_dicts)\n",
    "    unique_words = set()\n",
    "    for dic in list_of_dicts:\n",
    "        for k in dic.keys():\n",
    "            unique_words.add(k)\n",
    "    full_dic = {i:0 for i in unique_words}\n",
    "    for word in full_dic.keys():\n",
    "        num_docs_with_word_in_it = 0\n",
    "        for dic in list_of_dicts:\n",
    "            if word in dic.keys():\n",
    "                num_docs_with_word_in_it += 1\n",
    "        \n",
    "        full_dic[word] = np.log((tot_docs/ num_docs_with_word_in_it))\n",
    "    \n",
    "    return full_dic\n",
    "                \n",
    "    \n",
    "##computing TF-IDF\n",
    "            \n",
    "#solution for reference:  Seems...weird\n",
    "'''def tf_idf(list_of_dicts):\n",
    "    # Create empty dictionary containing full vocabulary of entire corpus\n",
    "    doc_tf_idf = {}\n",
    "    idf = inverse_document_frequency(list_of_dicts)\n",
    "    full_vocab_list = {i:0 for i in list(idf.keys())}\n",
    "    \n",
    "    # Create tf-idf list of dictionaries, containing a dictionary that will be updated for each document\n",
    "    tf_idf_list_of_dicts = []\n",
    "    \n",
    "    # Now, compute tf and then use this to compute and set tf-idf values for each document\n",
    "    for doc in list_of_dicts:\n",
    "        doc_tf = term_frequency(doc)\n",
    "        for word in doc_tf:\n",
    "            doc_tf_idf[word] = doc_tf[word] * idf[word]\n",
    "        tf_idf_list_of_dicts.append(doc_tf_idf)\n",
    "    \n",
    "    return tf_idf_list_of_dicts'''\n",
    "#solution for reference\n",
    "\n",
    "def tf_idf(list_of_dicts):\n",
    "    import copy\n",
    "    idf = inverse_document_frequency(list_of_dicts)\n",
    "    final_scores = []\n",
    "    full_vocab_list = {i:0 for i in list(idf.keys())}\n",
    "    for d in list_of_dicts:\n",
    "        d_scores = copy.copy(full_vocab_list)\n",
    "        tf = term_frequency(d)\n",
    "        for t, tf_score in d.items():\n",
    "            t_score = idf[t] * tf[t]\n",
    "            d_scores[t] = t_score\n",
    "        final_scores.append(d_scores)\n",
    "    return final_scores\n",
    "            \n",
    "\n",
    "    \n",
    "##vectorizing all docs\n",
    "def main(filenames):\n",
    "    cv_all_docs = []\n",
    "    for fn in filenames:\n",
    "        #read in song\n",
    "        with open('data/'+fn) as f:\n",
    "            song = f.readlines()\n",
    "        \n",
    "        #tokenize song\n",
    "        tok_song = tokenize(clean_song(song))\n",
    "        \n",
    "        #convert to bow\n",
    "        cv_all_docs.append(count_vectorize(tok_song, vocab=None))\n",
    "    \n",
    "    tf_idf_all_docs = tf_idf(cv_all_docs)\n",
    "    return tf_idf_all_docs\n",
    "\n",
    "tf_idf_all_docs = main(filenames)\n",
    "print(list(tf_idf_all_docs[0])[:10])\n",
    "\n",
    "##visualizing\n",
    "num_dims = len(tf_idf_all_docs[0])\n",
    "print(\"Number of Dimensions: {}\".format(num_dims))\n",
    "\n",
    "tf_idf_vals_list = []\n",
    "\n",
    "for i in tf_idf_all_docs:\n",
    "    tf_idf_vals_list.append(list(i.values()))\n",
    "    \n",
    "tf_idf_vals_list[0][:10]\n",
    "\n",
    "##\n",
    "t_sne_object_3d = TSNE(n_components=3)\n",
    "transformed_data_3d = t_sne_object_3d.fit_transform(tf_idf_vals_list)\n",
    "transformed_data_3d\n",
    "\n",
    "##\n",
    "t_sne_object_2d = TSNE(n_components=2)\n",
    "transformed_data_2d = t_sne_object_2d.fit_transform(tf_idf_vals_list)\n",
    "transformed_data_2d\n",
    "\n",
    "##\n",
    "##visualize everything\n",
    "kendrick_3d = transformed_data_3d[:10]\n",
    "k3_x = [i[0] for i in kendrick_3d]\n",
    "k3_y = [i[1] for i in kendrick_3d]\n",
    "k3_z = [i[2] for i in kendrick_3d]\n",
    "\n",
    "garth_3d = transformed_data_3d[10:]\n",
    "g3_x = [i[0] for i in garth_3d]\n",
    "g3_y = [i[1] for i in garth_3d]\n",
    "g3_z = [i[2] for i in garth_3d]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(k3_x, k3_y, k3_z, c='b', s=60, label='Kendrick')\n",
    "ax.scatter(g3_x, g3_y, g3_z, c='red', s=60, label='Garth')\n",
    "ax.view_init(30, 10)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "kendrick_2d = transformed_data_2d[:10]\n",
    "k2_x = [i[0] for i in kendrick_2d]\n",
    "k2_y = [i[1] for i in kendrick_2d]\n",
    "\n",
    "garth_2d = transformed_data_2d[10:]\n",
    "g2_x = [i[0] for i in garth_2d]\n",
    "g2_y = [i[1] for i in garth_2d]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(222)\n",
    "ax.scatter(k2_x, k2_y, c='b', label='Kendrick')\n",
    "ax.scatter(g2_x, g2_y, c='red', label='Garth')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "##really pretty curious how this was supposed to work and why there's discrepancies in our solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-introduction-to-nltk\n",
    "'''The most common Python library used for NLP tasks is Natural Language Tool Kit, or NLTK for short. This library was developed by researchers at the University of Pennsylvania, and quickly became the most powerful and complete library of NLP tools available.'''\n",
    "\n",
    "# https://github.com/ericthansen/dsc-introduction-to-regular-expressions\n",
    "#cheat sheet:\n",
    "# https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-regular-expressions/master/images/regex_cheat_sheet.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-regular-expressions-codealong\n",
    " # regex tester sites: \n",
    "    https://www.regexpal.com/       #this one is pretty good\n",
    "    https://regexr.com/             #this one is very nice\n",
    "# the data\n",
    "import re\n",
    "\n",
    "with open('menu.txt', 'r') as f:\n",
    "    file = f.read()\n",
    "\n",
    "print(file)\n",
    "\n",
    "pattern = '\\d'\n",
    "p = re.compile(pattern)\n",
    "digits = p.findall(file)\n",
    "digits\n",
    "#\n",
    "#escaping metacharacters\n",
    "pattern = '\\$\\d'\n",
    "p = re.compile(pattern)\n",
    "digits = p.findall(file)\n",
    "digits\n",
    "\n",
    "# #get digit followed by 3...still doesn't get just what we want\n",
    "pattern = '\\$\\d.{3}'\n",
    "p = re.compile(pattern)\n",
    "digits = p.findall(file)\n",
    "digits\n",
    "\n",
    "#groups ranges and quantifiers\n",
    "pattern = '[a-zA-Z0-9]'\n",
    "p = re.compile(pattern)\n",
    "digits = p.findall(file)\n",
    "digits\n",
    "\n",
    "# #the final thing - finding all \"prices\"\n",
    "#this is a dollarsign, then 1 or more digits, then 0 or 1 .s, then 0 or more digits.\n",
    "pattern = '(\\$\\d+\\.?\\d*)'\n",
    "p = re.compile(pattern)\n",
    "digits = p.findall(file)\n",
    "digits\n",
    "\n",
    "##putting it all together\n",
    "pattern = '(\\(\\d{3}\\) (\\d{3}-\\d{4}))'\n",
    "p = re.compile(pattern)\n",
    "digits = p.findall(file)\n",
    "digits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-feature-engineering-for-text-data\n",
    "'''In this lesson, we'll focus on the following topics:\n",
    "\n",
    "Stopword Removal\n",
    "Frequency Distributions\n",
    "Stemming and Lemmatization\n",
    "Bigrams, N-grams, and Mutual Information Score'''\n",
    "\n",
    "##to get all stopwords from English\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "stopwords_list += list(string.punctuation)\n",
    "\n",
    "##then we can remove them from our data\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(some_text_data)\n",
    "\n",
    "stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "\n",
    "##frequncy dist - basically a dictionary with a few more methods\n",
    "from  nltk import FreqDist\n",
    "freqdist = FreqDist(tokens)\n",
    "\n",
    "most_common = freqdist.most_common(200)\n",
    "\n",
    "#stemming and lemmatization\n",
    "'''best stemmer currently: Porter stemmer'''\n",
    "# http://www.nltk.org/howto/stem.html\n",
    "    \n",
    "'''Stemming follows a predetermined set of rules to reduce a word to its stem.\n",
    "Lemmatization differs from stemming in that it reduces each word down to a linguistically valid lemma, or root word\n",
    "Lemmatization is generally more complex, but also more accurate.'''\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize('feet') # foot\n",
    "lemmatizer.lemmatize('running') # run\n",
    "#bigrams\n",
    "'''Another alternative to tokenization is to instead create Bigrams out of the text. A bigram is just a pair of adjacent words, treated as a single unit.'''\n",
    "'''Consider the sentence \"the dog played outside\". If we created bigrams out of this sentence, we would get ('the', 'dog'), ('dog', 'played'), ('played', 'outside')'''\n",
    "#can do n other than 2, too\n",
    "\n",
    "'''Another way we can make use of bigrams is to calculate their Pointwise Mutual Information Score. This is a statistical measure from information theory that generally measures the mutual dependence between two words. '''\n",
    "'''For instance, the bigram ('San', 'Francisco') would likely have a high mutual information score,'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-corpus-statistics-lab\n",
    "\n",
    "#get corpus\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "#needed to download it locally\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "file_ids = gutenberg.fileids()\n",
    "file_ids\n",
    "\n",
    "macbeth_text = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "macbeth_text[:1000]\n",
    "\n",
    "#preprocessing\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "macbeth_tokens_raw = nltk.regexp_tokenize(macbeth_text, pattern)\n",
    "#\n",
    "macbeth_tokens = [token.lower() for token in macbeth_tokens_raw]\n",
    "##freq dists\n",
    "macbeth_freqdist = FreqDist(macbeth_tokens)\n",
    "macbeth_freqdist.most_common(50)\n",
    "\n",
    "##removing stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += [str(x) for x in range(10)]\n",
    "\n",
    "macbeth_words_stopped = [w for w in macbeth_tokens if w not in stopwords_list]\n",
    "#\n",
    "macbeth_stopped_freqdist = FreqDist(macbeth_words_stopped)\n",
    "macbeth_stopped_freqdist.most_common(50)\n",
    "#\n",
    "#questions about this corpus\n",
    "#vocab size\n",
    "len(macbeth_stopped_freqdist)\n",
    "#$normalized word freq\n",
    "total_word_count = sum(macbeth_stopped_freqdist.values())\n",
    "macbeth_top_50 = macbeth_stopped_freqdist.most_common(50)\n",
    "print('Word\\t\\t\\tNormalized Frequency')\n",
    "for word in macbeth_top_50:\n",
    "    normalized_frequency = macbeth_stopped_freqdist[word[0]]/total_word_count\n",
    "    print('{} \\t\\t\\t {:.4}'.format(word[0], normalized_frequency))\n",
    "    \n",
    "#creating bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "macbeth_finder = BigramCollocationFinder.from_words(macbeth_words_stopped)\n",
    "macbeth_scored = macbeth_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "# Display the first 50 elements of macbeth_scored\n",
    "macbeth_scored[:50]\n",
    "\n",
    "##mutual information scores\n",
    "macbeth_pmi_finder = BigramCollocationFinder.from_words(macbeth_words_stopped)\n",
    "macbeth_pmi_finder.apply_freq_filter(5)\n",
    "macbeth_pmi_scored = macbeth_pmi_finder.score_ngrams(bigram_measures.pmi)\n",
    "macbeth_pmi_scored[:50]\n",
    "\n",
    "##cool!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-context-free-grammars-and-POS-tagging\n",
    "# very cool lesson on CFG; not much to copy/paste though -just reread if necessary\n",
    "# https://github.com/ericthansen/dsc-context-free-grammars-codealong\n",
    "#generating a cfg\n",
    "import nltk\n",
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "#\n",
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "#create parse trees\n",
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)\n",
    "\n",
    "#parsing a sentence\n",
    "# Step 1\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | \n",
    "VP -> V NP | VP PP\n",
    "Det -> 'the'\n",
    "Adj -> '100m'\n",
    "N -> 'usain_bolt' | 'record' | \n",
    "V -> 'broke'\n",
    "P -> \n",
    "\"\"\")\n",
    "\n",
    "# Step 2\n",
    "from nltk import word_tokenize\n",
    "sent = 'usain_bolt broke the 100m record'\n",
    "# Step 3\n",
    "tokenized_sent = word_tokenize(sent)\n",
    "\n",
    "# Step 4\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "# Step 5\n",
    "for tree in parser.parse(tokenized_sent):\n",
    "    print(tree)\n",
    "#note this didn't print anything bc we don't have enough rules in our grammar yet    \n",
    "\n",
    "#by analyzing the grammar for the Usain Bolt sentence, we can come up with this:\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N PP | N | Det NP | Adj NP\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'the'\n",
    "Adj -> '100m'\n",
    "N -> 'usain_bolt' | 'record' | \n",
    "V -> 'broke'\n",
    "P -> \n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "#\n",
    "for tree in parser.parse(tokenized_sent):\n",
    "    print(tree)\n",
    "#this works but generates lots of extra trees\n",
    "\n",
    "#generating tags with NLTK\n",
    "nltk.pos_tag(tokenized_sent)\n",
    "#note that NLTK uses the Penn Tree Bank for parts of speech:\n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://github.com/ericthansen/dsc-text-classification\n",
    "## just a bunch of things to consider, not a lot of theory or code\n",
    "'''Do we remove stop words or not?\n",
    "Do we stem or lemmatize our text data, or leave the words as is?\n",
    "Is basic tokenization enough, or do we need to support special edge cases through the use of regex?\n",
    "Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?\n",
    "Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?\n",
    "What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?\n",
    "These are all questions that we'll need to think about pretty much anytime we begin working with text data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-text-classification-lab\n",
    "\n",
    "#Getting started\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "##pick some categories\n",
    "categories = ['alt.atheism', 'comp.windows.x', 'rec.sport.hockey', 'sci.crypt', 'talk.politics.guns']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "##store data and targets\n",
    "data = newsgroups_train.data\n",
    "target = newsgroups_train.target\n",
    "label_names = newsgroups_train.target_names\n",
    "label_names\n",
    "\n",
    "#check shape\n",
    "# Your code here\n",
    "newsgroups_train.filenames.shape\n",
    "\n",
    "# stop words\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "extras = [\"''\", '\"\"', '...', '``']\n",
    "stopwords_list += extras\n",
    "#stopwords_list\n",
    "\n",
    "#make a process function\n",
    "def process_article(article):\n",
    "    tokens = nltk.word_tokenize(article)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords_list]\n",
    "    return stopwords_removed   \n",
    "\n",
    "##clever use of map function builtin\n",
    "processed_data = list(map(process_article, data))\n",
    "\n",
    "#\n",
    "processed_data[0]\n",
    "##getting total vocab\n",
    "total_vocab = set()\n",
    "for article in processed_data:\n",
    "    total_vocab.update(article)\n",
    "len(total_vocab)\n",
    "\n",
    "##exploring with freq dists\n",
    "\n",
    "articles_concat = []\n",
    "for article in processed_data:\n",
    "    articles_concat += article\n",
    "    \n",
    "articles_freqdist = FreqDist(articles_concat)\n",
    "articles_freqdist.most_common(200)\n",
    "\n",
    "##vectorizing with tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tf_idf_data_train = vectorizer.fit_transform(data)\n",
    "\n",
    "tf_idf_data_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "##modelling\n",
    "# Your code here\n",
    "tf_idf_data_train.shape\n",
    "\n",
    "##checking avg # of zeroes in a column - this shows sparsity; and that articles don't really use that\n",
    "#high percentage of the vocab vector\n",
    "non_zero_cols = tf_idf_data_train.nnz / float(tf_idf_data_train.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tf_idf_data_train.shape[1]))\n",
    "print('Percentage of columns containing 0: {}'.format(percent_sparse))\n",
    "\n",
    "##create naive bayes and random forests\n",
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "nb_classifier.fit(tf_idf_data_train, target)\n",
    "nb_train_preds = nb_classifier.predict(tf_idf_data_train)\n",
    "nb_test_preds = nb_classifier.predict(tf_idf_data_test)\n",
    "\n",
    "rf_classifier.fit(tf_idf_data_train, target)\n",
    "rf_train_preds = rf_classifier.predict(tf_idf_data_train)\n",
    "rf_test_preds = rf_classifier.predict(tf_idf_data_test)\n",
    "#\n",
    "nb_train_score = accuracy_score(target, nb_train_preds)\n",
    "nb_test_score = accuracy_score(newsgroups_test.target, nb_test_preds)\n",
    "rf_train_score = accuracy_score(target, rf_train_preds)\n",
    "rf_test_score = accuracy_score(newsgroups_test.target, rf_test_preds)\n",
    "\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(nb_train_score, nb_test_score))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Random Forest')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(rf_train_score, rf_test_score))\n",
    "\n",
    "##interpret\n",
    "# Your answer here\n",
    "'''model fit is pretty good.  Random guessing would only give .20.  There is probably some over-fitting, especially for the \n",
    "RF model.'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ericthansen/dsc-nlp-section-recap\n",
    "'''The key takeaways from this section include:\n",
    "\n",
    "NLP has become increasingly popular over the past few years, and NLP researchers have achieved very insightful insights\n",
    "The Natural Language Tool Kit (NLTK) is one of the most popular Python libraries for NLP\n",
    "Regular Expressions are an important part of NLP, which can be used for pattern matching and filtering\n",
    "Regular Expressions can become confusing, so make sure to use our provided cheat sheet the first few times you work with regex\n",
    "It is strongly recommended you take some time to use regex tester websites to ensure you understand how changing your regex pattern affects your results when working towards a correct answer!\n",
    "Feature Engineering is essential when working with text data, and to understand the dynamics of your text\n",
    "Common feature engineering techniques are removing stop words, stemming, lemmatization, and n-grams\n",
    "When diving deeper into grammar and linguistics, context-free grammars and part-of-speech tagging is important\n",
    "In this context, parse trees can help computers when dealing with ambiguous words\n",
    "How you clean and preprocess your data will have a major effect on the conclusions you'll be able to draw in your NLP classification problems\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-neural-networks-section-intro\n",
    "# https://github.com/ericthansen/dsc-introduction-to-neural-networks\n",
    "# https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f\n",
    "# https://playground.tensorflow.org/\n",
    "\n",
    "\n",
    "# https://github.com/ericthansen/dsc-introduction-to-neural-networks-lab\n",
    "#import pkgs\n",
    "!pip install pillow\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#data\n",
    "# Directory path\n",
    "train_data_dir = 'data/train'\n",
    "test_data_dir = 'data/validation'\n",
    "\n",
    "# Get all the data in the directory data/validation (132 images), and reshape them\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(64, 64), batch_size=132)\n",
    "\n",
    "# Get all the data in the directory data/train (790 images), and reshape them\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(64, 64), batch_size=790)\n",
    "\n",
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)\n",
    "\n",
    "\n",
    "#inspect and prepare\n",
    "# Preview an image\n",
    "array_to_img(train_images[7])\n",
    "# Preview another image\n",
    "array_to_img(train_images[29])\n",
    "\n",
    "#shape of the data\n",
    "# Preview the shape of both the images and labels for both the train and test sets (4 objects total)\n",
    "print(np.shape(train_images))\n",
    "print(np.shape(train_labels))\n",
    "print(np.shape(test_images))\n",
    "print(np.shape(test_labels))\n",
    "\n",
    "# Reshape the train images \n",
    "train_img_unrow = train_images.reshape(790, -1).T\n",
    "\n",
    "# Preview the shape of train_img_unrow\n",
    "np.shape(train_img_unrow)\n",
    "\n",
    "# Define appropriate m \n",
    "m = 132\n",
    "test_img_unrow = test_images.reshape(m, -1).T\n",
    "\n",
    "# Preview the shape of test_img_unrow\n",
    "np.shape(test_img_unrow)\n",
    "\n",
    "##train labels and test labels\n",
    "# Run this cell; no need to edit\n",
    "train_labels \n",
    "'''Having this information, we still don't know which pair corresponds with santa versus not_santa. Luckily, this was \n",
    "stored using keras.preprocessing_image, and you can get more info using the command train_generator.class_indices.'''\n",
    "\n",
    "# Run this cell; no need to edit\n",
    "train_generator.class_indices \n",
    "\n",
    "# Your code here\n",
    "train_labels_final = train_labels.T[[1]]\n",
    "\n",
    "# Run this cell; no need to edit\n",
    "np.shape(train_labels_final) \n",
    "\n",
    "# Your code here\n",
    "test_labels_final = test_labels.T[[1]]\n",
    "\n",
    "# Run this cell; no need to edit\n",
    "np.shape(test_labels_final) \n",
    "\n",
    "##check\n",
    "# Preview train image at index 240\n",
    "array_to_img(train_images[240])\n",
    "\n",
    "# Preview train label at index 240\n",
    "train_labels_final[:,240]\n",
    "\n",
    "##standardize the data\n",
    "# Your code here \n",
    "train_img_final = train_img_unrow/255\n",
    "test_img_final = test_img_unrow/255\n",
    "\n",
    "type(test_img_unrow)\n",
    "\n",
    "##build a logistic regression based NN\n",
    "# init b\n",
    "# Your code here\n",
    "b=0\n",
    "\n",
    "#init w\n",
    "# Define your function\n",
    "def init_w(n):\n",
    "    w = np.zeros((n, 1))\n",
    "    return w\n",
    "# Call your function using appropriate parameters\n",
    "w = init_w(64*64*3)\n",
    "##forward prop\n",
    "# Define the propagation function\n",
    "def propagation(w, b, x, y):\n",
    "    l = x.shape[1]\n",
    "    y_hat = 1/(1 + np.exp(- (np.dot(w.T, x) + b)))                                  \n",
    "    cost = -(1/l) * np.sum(y * np.log(y_hat) + (1-y)* np.log(1 - y_hat))    \n",
    "    djdw = (1/l) * np.dot(x,(y_hat - y).T)\n",
    "    djdb = (1/l) * np.sum(y_hat - y)\n",
    "    return djdw, djdb, cost\n",
    "\n",
    "# Use the propogation function\n",
    "dw, db, cost = propagation(w, b, train_img_final, train_labels_final)\n",
    "\n",
    "print(dw)\n",
    "\n",
    "print(db)\n",
    "\n",
    "print(cost)\n",
    "\n",
    "##optimization\n",
    "# Complete the function below using your propagation function to define dw, db and cost \n",
    "# Then use the formula above to update w and b in the optimization function \n",
    "def optimization(w, b, x, y, num_iterations, learning_rate, print_cost = False):\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        dw, db, cost = propagation(w, b, x, y)\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "        # Record the costs and print them every 50 iterations\n",
    "        if i % 50 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 50 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    return w, b, costs\n",
    "\n",
    "# Run this block of code as is\n",
    "w, b, costs = optimization(w, b, train_img_final, train_labels_final, \n",
    "                           num_iterations= 151, learning_rate = 0.0001, print_cost = True)\n",
    "#EH: do note that changing the learning rate to .001 gives better results\n",
    "\n",
    "##label prediction - santa or not\n",
    "def prediction(w, b, x):\n",
    "    l = x.shape[1]\n",
    "    y_prediction = np.zeros((1, l))\n",
    "    w = w.reshape(x.shape[0], 1)\n",
    "    y_hat = 1/(1 + np.exp(- (np.dot(w.T, x) + b))) \n",
    "    p = y_hat\n",
    "    \n",
    "    for i in range(y_hat.shape[1]):\n",
    "        # Transform the probability into a binary classification using 0.5 as the cutoff\n",
    "        if (y_hat[0,i] > 0.5): \n",
    "            y_prediction[0, i] = 1\n",
    "        else:\n",
    "            y_prediction[0, i] = 0\n",
    "    return y_prediction\n",
    "\n",
    "##try it on a small example\n",
    "# Run this block of code as is\n",
    "w = np.array([[0.035], [0.123], [0.217]])\n",
    "b = 0.2\n",
    "x = np.array([[0.2, 0.4, -1.2, -2], \n",
    "              [1, -2., 0.1, -1], \n",
    "              [0.2, 0.4, -1.2, -2]])\n",
    "\n",
    "prediction(w, b, x)\n",
    "\n",
    "##the overall model\n",
    "# Review this code carefully\n",
    "def model(x_train, y_train, x_test, y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "\n",
    "    b = 0\n",
    "    w = init_w(np.shape(x_train)[0]) \n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    w, b, costs = optimization(w, b, x_train, y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    y_pred_test = prediction(w, b, x_test)\n",
    "    y_pred_train = prediction(w, b, x_train)\n",
    "\n",
    "    # Print train/test errors\n",
    "    print('train accuracy: {} %'.format(100 - np.mean(np.abs(y_pred_train - y_train)) * 100))\n",
    "    print('test accuracy: {} %'.format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))\n",
    "\n",
    "    output = {'costs': costs, \n",
    "              'y_pred_test': y_pred_test,  \n",
    "              'y_pred_train' : y_pred_train,  \n",
    "              'w' : w, \n",
    "              'b' : b, \n",
    "              'learning_rate' : learning_rate, \n",
    "              'num_iterations': num_iterations}\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Run the model!\n",
    "# ⏰ Expect your code to take several minutes to run\n",
    "output = model(train_img_final, train_labels_final, test_img_final, test_labels_final,\n",
    "               num_iterations=2000, learning_rate=0.005, print_cost=True)\n",
    "\n",
    "'''\n",
    "Cost after iteration 1950: 0.161424\n",
    "train accuracy: 96.9620253164557 %\n",
    "test accuracy: 75.0 %\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-introduction-to-keras\n",
    "# quick ref:\n",
    "#unrowing a matrix:\n",
    "img_unrow = img.reshape(790, -1).T  \n",
    "#increasing the rank given a vector with np.shape() (790,)\n",
    "np.reshape(vector, (1,790)) \n",
    "\n",
    "###tensor slicing\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "digit = train_images[10] # Select an arbitrary case for our example\n",
    "\n",
    "# Checking the shape of our tensor (in this case, the image)\n",
    "print('Raw Tensor shape:', train_images.shape)\n",
    "\n",
    "# Now performing some slices of our image:\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100].shape)\n",
    "\n",
    "# Equivalently\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100, :, :].shape)\n",
    "\n",
    "# Or verbosely:\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100, :28, :28].shape)\n",
    "\n",
    "# Display an example image for context\n",
    "plt.imshow(digit, cmap=plt.cm.binary) \n",
    "plt.show()\n",
    "\n",
    "## taking, for example, only the lower right quadrant of every image in the set\n",
    "lower_right_quadrant = train_images[:,14:,14:]\n",
    "print('Sliced tensor shape (includes all images but only the lower right hand corner of each:',\n",
    "      lower_right_quadrant.shape)\n",
    "plt.imshow(lower_right_quadrant[10], cmap=plt.cm.binary) # Display the 10th image from our sliced tensor\n",
    "plt.show()\n",
    "\n",
    "##tensor operations\n",
    "#element wise    \n",
    "#broadcasting\n",
    "#tensor dot\n",
    "import numpy as np\n",
    "np.array([1, 2, 3, 4]) + np.array([5, 6, 7, 8])\n",
    "##broadcasting works with tensors of different size\n",
    "A = np.array(range(12)).reshape(4,3)\n",
    "print('A:\\n', A, '\\n')\n",
    "\n",
    "B = np.array([1, 2, 3]) #.reshape(1, -1)\n",
    "print('B:', B, '\\n')\n",
    "\n",
    "A += B # Update with broadcasting\n",
    "print('Updated A:\\n', A)\n",
    "\n",
    "##\n",
    "# Recall that B is the vector [1, 2, 3]\n",
    "# Taking the dot product of B and itself is equivalent to\n",
    "# 1*1 + 2*2 + 3*3 = 1 + 4 + 9 = 14\n",
    "print(np.dot(B,B))\n",
    "##or slightly more complicated\n",
    "A = np.array(range(12)).reshape(4, 3)\n",
    "print('A:\\n', A, '\\n')\n",
    "\n",
    "B = np.array([1,2,3]) #.reshape(1, -1)\n",
    "print('B:', B, '\\n')\n",
    "\n",
    "np.dot(A, B)\n",
    "#i.e. standard matrix mult by vector\n",
    "\n",
    "##\n",
    "##build a Neural Net with Keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "##decide on net architecture\n",
    "model = models.Sequential()\n",
    "\n",
    "##add layers\n",
    "model.add(layers.Dense(units, activation, input_shape))\n",
    "\n",
    "##compile the model\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "##train the model\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n",
    "## woo, remmeber batches!  fun.\n",
    "'''Sample: one element of a dataset.\n",
    "\n",
    "Example: one image is a sample in a convolutional network\n",
    "Example: one audio file is a sample for a speech recognition model\n",
    "Batch: a set of  𝑁  samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model.\n",
    "\n",
    "A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluation/prediction).\n",
    "\n",
    "Epoch: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.\n",
    "\n",
    "When using validation_data or validation_split with the .fit() method of Keras models, evaluation will be run at the end of every epoch.\n",
    "\n",
    "Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving).'''\n",
    "#plotting\n",
    "history.history\n",
    "\n",
    "history.history['loss']\n",
    "history.history['accuracy']\n",
    "\n",
    "##make predictions\n",
    "y_hat = model.predict(x)\n",
    "\n",
    "##eval the model\n",
    "model.evaluate(X_train, X_train_labels)\n",
    "\n",
    "#\n",
    "model.evaluate(X_test, X_test_labels)\n",
    "\n",
    "#\n",
    "https://keras.io/getting-started/\n",
    "https://keras.io/getting-started/sequential-model-guide/#compilation\n",
    "https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop\n",
    "https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent\n",
    "A full book on Keras by the author of Keras himself:\n",
    "https://www.manning.com/books/deep-learning-with-python\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-introduction-to-keras-lab\n",
    "# setup packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "##load data\n",
    "# Import data\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "\n",
    "# Inspect data\n",
    "print(df.info())\n",
    "df.head()\n",
    "\n",
    "# Your code here\n",
    "df['Product'].value_counts(normalize=True)\n",
    "\n",
    "# As a quick preliminary, briefly review the docstring for keras.preprocessing.text.Tokenizer\n",
    "Tokenizer?\n",
    "\n",
    "# ⏰ This cell may take about thirty seconds to run\n",
    "\n",
    "# Raw text complaints\n",
    "complaints = df['Consumer complaint narrative'] \n",
    "\n",
    "# Initialize a tokenizer \n",
    "tokenizer = Tokenizer(num_words=2000) \n",
    "\n",
    "# Fit it to the complaints\n",
    "tokenizer.fit_on_texts(complaints) \n",
    "\n",
    "# Generate sequences\n",
    "sequences = tokenizer.texts_to_sequences(complaints) \n",
    "print('sequences type:', type(sequences))\n",
    "\n",
    "# Similar to sequences, but returns a numpy array\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary') \n",
    "print('one_hot_results type:', type(one_hot_results))\n",
    "\n",
    "# Useful if we wish to decode (more explanation below)\n",
    "word_index = tokenizer.word_index \n",
    "\n",
    "# Tokens are the number of unique words across the corpus\n",
    "print('Found %s unique tokens.' % len(word_index)) \n",
    "\n",
    "# Our coded data\n",
    "print('Dimensions of our coded results:', np.shape(one_hot_results)) \n",
    "\n",
    "\n",
    "##reverse entries of dictionary\n",
    "# Your code here\n",
    "reverse_index = {}\n",
    "for k, v in word_index.items():\n",
    "    reverse_index[v] = k\n",
    "#word_index.items()\n",
    "##this also works:\n",
    "##reverse_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "##decoding our word vectors\n",
    "comment_idx_to_preview = 19\n",
    "print('Original complaint text:')\n",
    "print(complaints[comment_idx_to_preview])\n",
    "print('\\n\\n')\n",
    "\n",
    "# The reverse_index cell block above must be complete in order for this cell block to successively execute \n",
    "decoded_review = ' '.join([reverse_index.get(i) for i in sequences[comment_idx_to_preview]])\n",
    "print('Decoded review from Tokenizer:')\n",
    "print(decoded_review)\n",
    "\n",
    "\n",
    "###convert products to numerical cats\n",
    "product = df['Product']\n",
    "\n",
    "# Initialize\n",
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(product)\n",
    "print('Original class labels:')\n",
    "print(list(le.classes_))\n",
    "print('\\n')\n",
    "product_cat = le.transform(product)  \n",
    "\n",
    "# If you wish to retrieve the original descriptive labels post production\n",
    "# list(le.inverse_transform([0, 1, 3, 3, 0, 6, 4])) \n",
    "\n",
    "print('New product labels:')\n",
    "print(product_cat)\n",
    "print('\\n')\n",
    "\n",
    "# Each row will be all zeros except for the category for that observation \n",
    "print('One hot labels; 7 binary columns, one for each of the categories.') \n",
    "product_onehot = to_categorical(product_cat)\n",
    "print(product_onehot)\n",
    "print('\\n')\n",
    "\n",
    "print('One hot labels shape:')\n",
    "print(np.shape(product_onehot))\n",
    "\n",
    "###train test split\n",
    "random.seed(123)\n",
    "test_index = random.sample(range(1,10000), 1500)\n",
    "\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "print('Test label shape:', np.shape(label_test))\n",
    "print('Train label shape:', np.shape(label_train))\n",
    "print('Test shape:', np.shape(test))\n",
    "print('Train shape:', np.shape(train))\n",
    "\n",
    "###building the network\n",
    "# Initialize a sequential model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Two layers with relu activation\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "\n",
    "# One layer with softmax activation \n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "##compiling the model\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='SGD'\n",
    "             ,metrics=['acc'])\n",
    "\n",
    "##training the model\n",
    "# Train the model \n",
    "history = model.fit(train,\n",
    "                    label_train,\n",
    "                    epochs=120,\n",
    "                    batch_size=256)\n",
    "\n",
    "##check out the history characteristic\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "##plot results!!\n",
    "# Plot the loss vs the number of epoch\n",
    "loss_values = history_dict['loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training accuracy vs the number of epochs\n",
    "loss_values = history_dict['acc']\n",
    "plt.plot(epochs, loss_values, 'g', label='Training accuracy')\n",
    "\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "###make predictions\n",
    "# Output (probability) predictions for the test set \n",
    "y_hat_test = model.predict(test)\n",
    "\n",
    "##evaluate performance\n",
    "# Print the loss and accuracy for the training set \n",
    "results_train = model.evaluate(train, label_train)\n",
    "results_train\n",
    "\n",
    "# Print the loss and accuracy for the test set \n",
    "results_test = model.evaluate(test, label_test)\n",
    "results_test\n",
    "\n",
    "#groovy!\n",
    "\n",
    "\n",
    "# https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "# https://catalog.data.gov/dataset/consumer-complaint-database\n",
    "\n",
    "#https://github.com/ericthansen/dsc-neural-networks-section-recap\n",
    "'''The key takeaways from this section include:\n",
    "\n",
    "Neural networks are powerful models that can be customized and tweaked using various amounts of nodes, layers, ...\n",
    "The most basic neural networks are single-layer densely connected neural networks, which have very similar properties as logistic regression models\n",
    "Compared to more traditional statistics and ML techniques, neural networks perform particularly well when using unstructured data\n",
    "Apart from densely connected networks, other types of neural networks include convolutional neural networks, recurrent neural networks, and generative adversarial neural networks\n",
    "When working with image data, it's important to understand how image data is stored when working with them in Python\n",
    "Logistic regression can be seen as a single-layer neural network with a sigmoid activation function\n",
    "Neural networks use loss and cost functions to minimize the \"loss\", which is a function that summarizes the difference between the actual outcome (eg. pictures contain santa or not) and the model prediction (whether the model correctly identifies pictures with santas)\n",
    "Backward and forward propagation are used to estimate the so-called \"model weights\"\n",
    "Adding more layers to neural networks can substantially increase model performance\n",
    "Several activations can be used in model nodes, you can explore with different types and evaluate how it affects performance'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-deep-learning-section-intro\n",
    "\n",
    "# https://github.com/ericthansen/dsc-deeper-neural-networks\n",
    "\n",
    "#activation functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    f = 1 / (1 + np.exp(-x))\n",
    "    if (derivative == True):\n",
    "        return f * (1 - f)\n",
    "    return f\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    f = np.tanh(x)\n",
    "    if (derivative == True):\n",
    "        return (1 - (f ** 2))\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = 0\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = 0\n",
    "    return f\n",
    "\n",
    "def leaky_relu(x, leakage = 0.05, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = leakage\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = x[i]* leakage\n",
    "    return f\n",
    "\n",
    "def arctan(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return 1/(1+np.square(x))\n",
    "    return np.arctan(x)\n",
    "\n",
    "z = np.arange(-10, 10, 0.2)\n",
    "\n",
    "##sigmoid function\n",
    "y = sigmoid(z)\n",
    "dy = sigmoid(z, derivative=True)\n",
    "plt.title('sigmoid')\n",
    "plt.axhline(color='gray', linewidth=1,)\n",
    "plt.axvline(color='gray', linewidth=1,)\n",
    "plt.plot(z, y, 'r', label='original (y)')\n",
    "plt.plot(z, dy, 'b', label='derivative (dy)')\n",
    "plt.legend();\n",
    "\n",
    "##tanh function\n",
    "y = tanh(z)\n",
    "dy = tanh(z, derivative=True)\n",
    "plt.title('tanh')\n",
    "plt.axhline(color='gray', linewidth=1,)\n",
    "plt.axvline(color='gray', linewidth=1,)\n",
    "plt.plot(z, y, 'r', label='original (y)')\n",
    "plt.plot(z, dy, 'b', label='derivative (dy)')\n",
    "plt.legend();\n",
    "\n",
    "##arctan function\n",
    "y = arctan(z)\n",
    "dy = arctan(z, derivative = True)\n",
    "plt.title('arctan')\n",
    "plt.axhline(color='gray', linewidth=1,)\n",
    "plt.axvline(color='gray', linewidth=1,)\n",
    "plt.plot(z, y, 'r', label='original (y)')\n",
    "plt.plot(z, dy, 'b', label='derivative (dy)')\n",
    "plt.legend();\n",
    "\n",
    "#rectified linear unit (relu)\n",
    "plt.title('ReLU')\n",
    "y = relu(z)\n",
    "dy = relu(z, derivative=True)\n",
    "plt.axhline(color='gray', linewidth=1,)\n",
    "plt.axvline(color='gray', linewidth=1,)\n",
    "plt.plot(z, y, 'b', label='original (y)')\n",
    "plt.plot(z, dy, 'r', label='derivative (dy)')\n",
    "plt.legend();\n",
    "\n",
    "##leaky relu\n",
    "# the default leakage here is 0.05!\n",
    "y = leaky_relu(z)\n",
    "dy = leaky_relu(z, derivative=True)\n",
    "plt.axhline(color='gray', linewidth=1,)\n",
    "plt.axvline(color='gray', linewidth=1,)\n",
    "plt.title('leaky ReLU')\n",
    "plt.xlim(-10,10)\n",
    "plt.plot(z, y, 'r', label='original (y)')\n",
    "plt.plot(z, dy, 'b', label='derivative (dy)')\n",
    "plt.legend();\n",
    "\n",
    "# https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-deeper-neural-networks-lab\n",
    "\n",
    "#getting started\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "##load \n",
    "bc_dataset = load_breast_cancer()\n",
    "data = bc_dataset.data\n",
    "target = bc_dataset.target\n",
    "col_names = bc_dataset.feature_names\n",
    "\n",
    "##inspect\n",
    "\n",
    "df = pd.DataFrame(data, columns=col_names)\n",
    "df.head()\n",
    "\n",
    "#prep for deep learning\n",
    "df.info()\n",
    "#good, it's numeric.  check if it needs to be normalized\n",
    "df.head()\n",
    "\n",
    "##yes it does\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "##binarizing the labels\n",
    "binarizer = LabelBinarizer()\n",
    "labels = binarizer.fit_transform(target)\n",
    "\n",
    "##building the MLP (multi layer perceptron)\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(10, activation='tanh', input_shape=(30,)))\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "##compiling\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "\n",
    "##fitting the model\n",
    "results_1 = model_1.fit(scaled_data, labels, epochs=25, batch_size=1, validation_split=0.2)\n",
    "\n",
    "##create a function for visualizing\n",
    "\n",
    "def visualize_training_results(results):\n",
    "    history = results.history\n",
    "    plt.figure()\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.legend(['val_loss', 'loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.plot(history['acc'])\n",
    "    plt.legend(['val_acc', 'acc'])\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "visualize_training_results(results_1)\n",
    "\n",
    "##detecting overfitting\n",
    "#iterating on the model - more layers\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(10, activation='tanh', input_shape=(30,)))\n",
    "model_2.add(Dense(5, activation='tanh'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "results_2 = model_2.fit(scaled_data, labels, epochs=25, batch_size=1, validation_split=0.2)\n",
    "\n",
    "visualize_training_results(results_2)\n",
    "#results not that much better - need more training data\n",
    "\n",
    "##visualizing why we normalize\n",
    "model_3 = Sequential()\n",
    "model_3.add(Dense(5, activation='tanh', input_shape=(30,)))\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "model_3.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "results_3 = model_3.fit(data, labels, epochs=25, batch_size=1, validation_split=0.2)\n",
    "#result - both train and validation sets are much lower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-image-classification-with-mlps\n",
    "# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/rz9xJ/why-deep-representations\n",
    "    \n",
    "# https://github.com/ericthansen/dsc-image-classification-with-mlps-lab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import mnist\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' #This prevents kernel shut down due to xgboost conflict\n",
    "\n",
    "#data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "##look at sample img\n",
    "sample_image = X_train[0]\n",
    "sample_label = y_train[0]\n",
    "display(plt.imshow(sample_image))\n",
    "print('Label: {}'.format(sample_label))\n",
    "\n",
    "##preprocessing - need to ensure dimensions fit\n",
    "sample_image\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train = X_train.reshape(60000, 28*28).astype('float32')\n",
    "X_test = X_test.reshape(10000, 28*28).astype('float32')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "##normalizing img data\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "\n",
    "####preprocess labels\n",
    "y_train[:10]\n",
    "\n",
    "##onehot encode the labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "##examine label on first img\n",
    "display(y_train[0])\n",
    "\n",
    "###building model\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(64, activation='tanh', input_shape=(784,)))\n",
    "model_1.add(Dense(10, activation='softmax'))\n",
    "\n",
    "##compile, note parameters\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "\n",
    "model_1.summary()\n",
    "\n",
    "##fit, get results\n",
    "results_1 = model_1.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "##visualize!  reuse vis function from before\n",
    "def visualize_training_results(results):\n",
    "    history = results.history\n",
    "    plt.figure()\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.legend(['val_loss', 'loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.plot(history['acc'])\n",
    "    plt.legend(['val_acc', 'acc'])\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "visualize_training_results(results_1)\n",
    "\n",
    "##make bigger model\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(64, activation='tanh', input_shape=(784,)))\n",
    "model_2.add(Dense(32, activation='tanh'))\n",
    "model_2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_2.summary()\n",
    "\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "\n",
    "results_2 = model_2.fit(X_train, y_train, batch_size=64, epochs=5, validation_data=(X_test, y_test))\n",
    "\n",
    "visualize_training_results(results_2)\n",
    "\n",
    "\n",
    "##tune the  activation function - change tanh to relu\n",
    "model_3 = Sequential()\n",
    "model_3.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "model_3.add(Dense(32, activation='relu'))\n",
    "model_3.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_3.summary()\n",
    "\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "\n",
    "results_3 = model_3.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "visualize_training_results(results_3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-deep-learning-section-recap\n",
    "'''Key Takeaways\n",
    "The key takeaways from this section include:\n",
    "\n",
    "Deep neural network representations can lighten the burden and automate certain tasks of heavy data preprocessing\n",
    "Deep representations need exponentially fewer hidden units than shallow networks, to obtain the same performance\n",
    "Parameter initialization, forward propagation, cost function evaluation, and backward propagation are again the cornerstones of deep networks\n",
    "Tensors are the building blocks of neural networks and a good understanding of them and how to use them in Python is crucial\n",
    "Scalars can be seen as 0-D tensors. Vectors can be seen as 1-D tensors, and matrices as 2-D tensors\n",
    "The usage of tensors reaches beyond matrices: tensors can have N dimensions\n",
    "Tensors can be created and manipulated using Numpy\n",
    "Keras makes building neural networks in Python easy, and you learned how to do that in this section\n",
    "You can use Keras to do some NLP as well, e.g. for tokenization'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-tuning-neural-networks-intro\n",
    "# regularization, l1/l2, \n",
    "#normalization helps vs getting stuck in local min\n",
    "#other optimization vs local mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: [5.4649857  0.36596619]\n",
      "5.477225575051661\n"
     ]
    }
   ],
   "source": [
    "#a quick aside about frobenius norm\n",
    "#frob norm(A) = sqrt(sum of squares of all entries of matrix)\n",
    "            # = sqrt (trace(A* A)) ; A* is conjugate transpose, trace is sum of diagonal entries\n",
    "            # = sqrt (sum of squares of singular values of A)\n",
    "#let A = [[1,2],[3,4]]\n",
    "#1) sqrt(1 + 4 + 9 + 16) = sqrt(30)\n",
    "#2) A*A = [[10,14],[14,20]]; sqrt(10+20)\n",
    "from numpy.linalg import svd\n",
    "u, s, v = svd([[1,2],[3,4]])\n",
    "tot = 0\n",
    "print(\"s:\",s)\n",
    "for val in s:\n",
    "    tot += val**2\n",
    "print(tot**0.5)\n",
    "print(30**0.5)\n",
    "#yay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-tuning-neural-networks-with-regularization\n",
    "#loss function with l2 reg:\n",
    "# https://render.githubusercontent.com/render/math?math=J%20(w%5E%7B%5B1%5D%7D,b%5E%7B%5B1%5D%7D,...,w%5E%7B%5BL%5D%7D,b%5E%7B%5BL%5D%7D)%20=%20%5Cdfrac%7B1%7D%7Bm%7D%20%20%5Cdisplaystyle%5Csum%5Em_%7Bi=1%7D%5Cmathcal%7BL%7D(%5Chat%20y%5E%7B(i)%7D,%20y%5E%7B(i)%7D)%2b%20%5Cdfrac%7B%5Clambda%7D%7B2m%7D%20%5Cdisplaystyle%5Csum%5EL_%7Bl=1%7D%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C%5E2\n",
    "# https://render.githubusercontent.com/render/math?math=%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C%5E2%20=%20%20%5Cdisplaystyle%5Csum%5E%7Bn%5E%7B%5Bl-1%5D%7D%7D_%7Bi=1%7D%20%20%5Cdisplaystyle%5Csum%5E%7Bn%5E%7B%5Bl%5D%7D%7D_%7Bj=1%7D%20(w_%7Bij%7D%5E%7B%5Bl%5D%7D)%5E2\n",
    "# This matrix norm is called the \"Frobenius norm\", also referred to as ||w_ij^[l]||^2\n",
    "# L2-regularization is called weight decay, because regularization will make your load smaller\n",
    "# hence your weights will become smaller by a factor (1-alpha*lambda/m).\n",
    "\n",
    "#drop out regularization\n",
    "#http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "#in keras, specifiy dropout layers between layers, e.g.\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(5, activation='relu', input_shape=(500,)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-tuning-neural-networks-with-regularization-lab-v2-1\n",
    "##load the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()\n",
    "'''Preprocessing Overview\n",
    "Before you begin to practice some of your new tools such as regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "Train - test split\n",
    "One-hot encoding your complaint text\n",
    "Transforming your category labels'''\n",
    "\n",
    "##preproc: generate random sample\n",
    "# Downsample the data\n",
    "df_sample = df.sample(10000, random_state=123)\n",
    "\n",
    "# Split the data into X and y\n",
    "y = df_sample['Product']\n",
    "X = df_sample['Consumer complaint narrative']\n",
    "##prproc: traintest split\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1500, random_state=42)\n",
    "\n",
    "##validation set\n",
    "# Split the data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=1000, random_state=42)\n",
    "\n",
    "##preproc -onehot encode the complaints\n",
    "# Use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "# Only keep the 2000 most common words \n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train_final)\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_matrix(X_train_final, mode='binary')\n",
    "X_val_tokens = tokenizer.texts_to_matrix(X_val, mode='binary')\n",
    "X_test_tokens = tokenizer.texts_to_matrix(X_test, mode='binary')\n",
    "\n",
    "##preproc: encode the products\n",
    "# Transform the product labels to numerical values\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final)\n",
    "\n",
    "y_train_lb = to_categorical(lb.transform(y_train_final))[:, :, 1]\n",
    "y_val_lb = to_categorical(lb.transform(y_val))[:, :, 1]\n",
    "y_test_lb = to_categorical(lb.transform(y_test))[:, :, 1]\n",
    "\n",
    "##baseline model\n",
    "# Build a baseline neural network model using Keras\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "baseline_model = models.Sequential()\n",
    "baseline_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "baseline_model.add(layers.Dense(25, activation='relu'))\n",
    "baseline_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "##compile\n",
    "# Compile the model\n",
    "baseline_model.compile(optimizer='SGD', \n",
    "                       loss='categorical_crossentropy', \n",
    "                       metrics=['acc'])\n",
    "\n",
    "#train the model\n",
    "# Train the model\n",
    "baseline_model_val = baseline_model.fit(X_train_tokens, \n",
    "                                        y_train_lb, \n",
    "                                        epochs=150, \n",
    "                                        batch_size=256, \n",
    "                                        validation_data=(X_val_tokens, y_val_lb))\n",
    "\n",
    "\n",
    "##model performance\n",
    "# Access the history attribute and store the dictionary\n",
    "baseline_model_val_dict = baseline_model_val.history\n",
    "\n",
    "# Print the keys\n",
    "baseline_model_val_dict.keys()\n",
    "\n",
    "##evaluate it on training data\n",
    "results_train = baseline_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print('----------')\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "##evaluate on test data\n",
    "results_test = baseline_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print('----------')\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')\n",
    "\n",
    "##plot\n",
    "# Loss vs number of epochs with train and validation sets\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "loss_values = baseline_model_val_dict['loss']\n",
    "val_loss_values = baseline_model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "ax.plot(epochs, loss_values, label='Training loss')\n",
    "ax.plot(epochs, val_loss_values, label='Validation loss')\n",
    "\n",
    "ax.set_title('Training & validation loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();\n",
    "\n",
    "##plot training & val accuracy\n",
    "# Accuracy vs number of epochs with train and validation sets\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "acc_values = baseline_model_val_dict['acc'] \n",
    "val_acc_values = baseline_model_val_dict['val_acc']\n",
    "\n",
    "ax.plot(epochs, acc_values, label='Training acc')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc')\n",
    "ax.set_title('Training & validation accuracy')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend();\n",
    "\n",
    "##early stopping - helps against overfitting\n",
    "random.seed(123)\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model_2.add(layers.Dense(25, activation='relu'))\n",
    "model_2.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model_2.compile(optimizer='SGD', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['acc'])\n",
    "\n",
    "# Import EarlyStopping and ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = [EarlyStopping(monitor='val_loss', patience=10), \n",
    "                  ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "#@#train model 2\n",
    "model_2_val = model_2.fit(X_train_tokens, \n",
    "                          y_train_lb, \n",
    "                          epochs=150, \n",
    "                          callbacks=early_stopping, \n",
    "                          batch_size=256, \n",
    "                          validation_data=(X_val_tokens, y_val_lb))\n",
    "##load the best model\n",
    "# Load the best (saved) model\n",
    "\n",
    "from keras.models import load_model\n",
    "saved_model = load_model('best_model.h5')\n",
    "\n",
    "##use best model to find train/test acc\n",
    "results_train = saved_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = saved_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')\n",
    "\n",
    "##l2 regularization\n",
    "# Import regularizers\n",
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "L2_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "L2_model.add(layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,)))\n",
    "\n",
    "# Add another hidden layer\n",
    "L2_model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "L2_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L2_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['acc'])\n",
    "\n",
    "# Train the model \n",
    "L2_model_val = L2_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))\n",
    "\n",
    "##inspect\n",
    "# L2 model details\n",
    "L2_model_dict = L2_model_val.history\n",
    "L2_acc_values = L2_model_dict['acc'] \n",
    "L2_val_acc_values = L2_model_dict['val_acc']\n",
    "\n",
    "# Baseline model\n",
    "baseline_model_acc = baseline_model_val_dict['acc'] \n",
    "baseline_model_val_acc = baseline_model_val_dict['val_acc']\n",
    "\n",
    "# Plot the accuracy for these models\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, L2_acc_values, label='Training acc (L2)')\n",
    "ax.plot(epochs, L2_val_acc_values, label='Validation acc (L2)')\n",
    "ax.plot(epochs, baseline_model_acc, label='Training acc (Baseline)')\n",
    "ax.plot(epochs, baseline_model_val_acc, label='Validation acc (Baseline)')\n",
    "ax.set_title('Training & validation accuracy L2 vs regular')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend();\n",
    "\n",
    "##note - l2 reg didn't help much here\n",
    "\n",
    "##l1 reg:\n",
    "random.seed(123)\n",
    "L1_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "L1_model.add(layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,)))\n",
    "\n",
    "# Add a hidden layer\n",
    "L1_model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "L1_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L1_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['acc'])\n",
    "\n",
    "# Train the model \n",
    "L1_model_val = L1_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))\n",
    "\n",
    "##plot l1 reg\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "L1_model_dict = L1_model_val.history\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, acc_values, label='Training acc L1')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc L1')\n",
    "ax.set_title('Training & validation accuracy with L1 regularization')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend();\n",
    "\n",
    "##try dropout reg\n",
    "# ⏰ This cell may take about a minute to run\n",
    "random.seed(123)\n",
    "dropout_model = models.Sequential()\n",
    "\n",
    "# Implement dropout to the input layer\n",
    "# NOTE: This is where you define the number of units in the input layer\n",
    "dropout_model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "\n",
    "# Add the first hidden layer\n",
    "dropout_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Implement dropout to the first hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the second hidden layer\n",
    "dropout_model.add(layers.Dense(25, activation='relu'))\n",
    "\n",
    "# Implement dropout to the second hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the output layer\n",
    "dropout_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "dropout_model.compile(optimizer='SGD', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['acc'])\n",
    "\n",
    "# Train the model\n",
    "dropout_model_val = dropout_model.fit(X_train_tokens, \n",
    "                                      y_train_lb, \n",
    "                                      epochs=150, \n",
    "                                      batch_size=256, \n",
    "                                      validation_data=(X_val_tokens, y_val_lb))\n",
    "\n",
    "# check results\n",
    "results_train = dropout_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = dropout_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')   \n",
    "\n",
    "####bigger data\n",
    "df_bigger_sample = df.sample(40000, random_state=123)\n",
    "\n",
    "X = df['Consumer complaint narrative']\n",
    "y = df['Product']\n",
    "\n",
    "# Train-test split\n",
    "X_train_bigger, X_test_bigger, y_train_bigger, y_test_bigger = train_test_split(X, \n",
    "                                                                                y, \n",
    "                                                                                test_size=6000, \n",
    "                                                                                random_state=42)\n",
    "\n",
    "# Validation set\n",
    "X_train_final_bigger, X_val_bigger, y_train_final_bigger, y_val_bigger = train_test_split(X_train_bigger, \n",
    "                                                                                          y_train_bigger, \n",
    "                                                                                          test_size=4000, \n",
    "                                                                                          random_state=42)\n",
    "\n",
    "\n",
    "# One-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train_final_bigger)\n",
    "\n",
    "X_train_tokens_bigger = tokenizer.texts_to_matrix(X_train_final_bigger, mode='binary')\n",
    "X_val_tokens_bigger = tokenizer.texts_to_matrix(X_val_bigger, mode='binary')\n",
    "X_test_tokens_bigger = tokenizer.texts_to_matrix(X_test_bigger, mode='binary')\n",
    "\n",
    "# One-hot encoding of products\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final_bigger)\n",
    "\n",
    "y_train_lb_bigger = to_categorical(lb.transform(y_train_final_bigger))[:, :, 1]\n",
    "y_val_lb_bigger = to_categorical(lb.transform(y_val_bigger))[:, :, 1]\n",
    "y_test_lb_bigger = to_categorical(lb.transform(y_test_bigger))[:, :, 1]\n",
    "\n",
    "# run model\n",
    "# ⏰ This cell may take several minutes to run\n",
    "random.seed(123)\n",
    "bigger_data_model = models.Sequential()\n",
    "bigger_data_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "bigger_data_model.add(layers.Dense(25, activation='relu'))\n",
    "bigger_data_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "bigger_data_model.compile(optimizer='SGD', \n",
    "                          loss='categorical_crossentropy', \n",
    "                          metrics=['acc'])\n",
    "\n",
    "bigger_data_model_val = bigger_data_model.fit(X_train_tokens_bigger,  \n",
    "                                              y_train_lb_bigger,  \n",
    "                                              epochs=150,  \n",
    "                                              batch_size=256,  \n",
    "                                              validation_data=(X_val_tokens_bigger, y_val_lb_bigger))\n",
    "\n",
    "##results\n",
    "results_train = bigger_data_model.evaluate(X_train_tokens_bigger, y_train_lb_bigger)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = bigger_data_model.evaluate(X_val_tokens_bigger, y_val_lb_bigger)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')\n",
    "\n",
    "##bigger data helped out a lot!\n",
    "\n",
    "# Additional Resources¶\n",
    "# https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "# https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "# https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-tuning-neural-networks-with-normalization\n",
    "#see original for formulas\n",
    "\n",
    "\n",
    "#initializing weights in general, \n",
    "Var(w_i) = 1/n or 2/n ; the number of input features in a layer\n",
    "#initializing weights for Relu activation - \n",
    "w^{[l]} = np.random.randn(shape)*np.sqrt(2/n_(l-1)) \n",
    "\n",
    "##gradient descent can oscillate somewhat on its way to true optimum; can get better by using\n",
    "#gradient descent with momentum, i.e. take exponentially weighted moving averages of dW, db\n",
    "#generally beta = 0.9 is a good value here\n",
    "\n",
    "#RMSProp\n",
    "#slows down learnign in one direction, speeds it in other.  uses exp. wtd avgs of the squares of derivatives\n",
    "'''In the direction where we want to learn fast, the corresponding  will be small, so dividing by a small number. On the other hand, in the direction where we will want to learn slow, the corresponding  will be relatively large, and updates will be smaller.\n",
    "\n",
    "Often, add small  in the denominator to make sure that you don't end up dividing by 0.'''\n",
    "\n",
    "#Adam optimization\n",
    "'''\"Adaptive Moment Estimation\", basically using the first and second moment estimations. Works very well in many situations! It takes momentum and RMSprop to put it together!'''\n",
    "'''Hyperparameters:\n",
    "alpha, beta1 = 0.9, beta2 = 0.999, epsilon = 10^-8; generally only alpha gets tuned\n",
    "'''\n",
    "\n",
    "#learning rate decay\n",
    "#can dcay according to a formula over epochs, or manual method\n",
    "\n",
    "#hyperparam tuning - alpha is most important.  then beta(momentum), #of Hidden Units,  batch size, \n",
    "#then num layers, learning rate decay\n",
    "#almost never tuned: beta1, beta2, epsilon (adam)\n",
    "\n",
    "#Don't use a grid, because hard to say in advance which hyperparameters will be important\n",
    "\n",
    "# https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs\n",
    "# https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-tuning-neural-networks-with-normalization-lab-v2-1\n",
    " \n",
    "# see data_preprocessing.ipynb in this folder for the pre-proc steps\n",
    "\n",
    "'''In this lab you will:\n",
    "\n",
    "Fit a neural network to normalized data\n",
    "Implement and observe the impact of various initialization techniques\n",
    "Implement and observe the impact of various optimization techniques'''\n",
    "##load the data\n",
    "# Necessary libraries and classes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##data already preproccessed (see filename above)\n",
    "# Load all numeric features\n",
    "X_train_numeric = pd.read_csv('data/X_train_numeric.csv')\n",
    "X_val_numeric = pd.read_csv('data/X_val_numeric.csv')\n",
    "X_test_numeric = pd.read_csv('data/X_test_numeric.csv')\n",
    "\n",
    "# Load all categorical features\n",
    "X_train_cat = pd.read_csv('data/X_train_cat.csv')\n",
    "X_val_cat = pd.read_csv('data/X_val_cat.csv')\n",
    "X_test_cat = pd.read_csv('data/X_test_cat.csv')\n",
    "\n",
    "# Load all targets\n",
    "y_train = pd.read_csv('data/y_train.csv')\n",
    "y_val = pd.read_csv('data/y_val.csv')\n",
    "y_test = pd.read_csv('data/y_test.csv')\n",
    "\n",
    "# Combine all features\n",
    "X_train = pd.concat([X_train_numeric, X_train_cat], axis=1)\n",
    "X_val = pd.concat([X_val_numeric, X_val_cat], axis=1)\n",
    "X_test = pd.concat([X_test_numeric, X_test_cat], axis=1)\n",
    "\n",
    "# Number of features\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Preview the data\n",
    "X_train.head()\n",
    "\n",
    "## baseline model\n",
    "np.random.seed(123)\n",
    "baseline_model = Sequential()\n",
    "\n",
    "# Hidden layer with 100 units\n",
    "baseline_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "\n",
    "# Hidden layer with 50 units\n",
    "baseline_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "baseline_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "baseline_model.compile(optimizer='SGD', \n",
    "                       loss='mse', \n",
    "                       metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "baseline_model.fit(X_train, \n",
    "                   y_train, \n",
    "                   batch_size=32, \n",
    "                   epochs=150, \n",
    "                   validation_data=(X_val, y_val))\n",
    "\n",
    "##that didn't converge - normalize\n",
    "# Numeric column names\n",
    "numeric_columns = X_train_numeric.columns \n",
    "\n",
    "# Instantiate StandardScaler\n",
    "ss_X = StandardScaler()\n",
    "\n",
    "# Fit and transform train data\n",
    "X_train_scaled = pd.DataFrame(ss_X.fit_transform(X_train_numeric), columns=numeric_columns)\n",
    "\n",
    "# Transform validate and test data\n",
    "X_val_scaled = pd.DataFrame(ss_X.transform(X_val_numeric), columns=numeric_columns)\n",
    "X_test_scaled = pd.DataFrame(ss_X.transform(X_test_numeric), columns=numeric_columns)\n",
    "\n",
    "# Combine the scaled numerical features and categorical features\n",
    "X_train = pd.concat([X_train_scaled, X_train_cat], axis=1)\n",
    "X_val = pd.concat([X_val_scaled, X_val_cat], axis=1)\n",
    "X_test = pd.concat([X_test_scaled, X_test_cat], axis=1)\n",
    "\n",
    "##make new model\n",
    "# Model with all normalized inputs\n",
    "np.random.seed(123)\n",
    "normalized_input_model = Sequential()\n",
    "normalized_input_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "normalized_input_model.add(layers.Dense(50, activation='relu'))\n",
    "normalized_input_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "normalized_input_model.compile(optimizer='SGD', \n",
    "                               loss='mse', \n",
    "                               metrics=['mse'])\n",
    "\n",
    "##train \n",
    "# Train the model \n",
    "normalized_input_model.fit(X_train,  \n",
    "                           y_train, \n",
    "                           batch_size=32, \n",
    "                           epochs=150, \n",
    "                           validation_data=(X_val, y_val))\n",
    "\n",
    "##normalizing output\n",
    "# Instantiate StandardScaler\n",
    "ss_y = StandardScaler()\n",
    "\n",
    "# Fit and transform train labels\n",
    "y_train_scaled = ss_y.fit_transform(y_train)\n",
    "\n",
    "# Transform validate and test labels\n",
    "y_val_scaled = ss_y.transform(y_val)\n",
    "y_test_scaled = ss_y.transform(y_test)\n",
    "\n",
    "# Model with all normalized inputs and outputs\n",
    "np.random.seed(123)\n",
    "normalized_model = Sequential()\n",
    "normalized_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "normalized_model.add(layers.Dense(50, activation='relu'))\n",
    "normalized_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "normalized_model.compile(optimizer='SGD', \n",
    "                         loss='mse', \n",
    "                         metrics=['mse']) \n",
    "\n",
    "# Train the model\n",
    "normalized_model.fit(X_train, \n",
    "                     y_train_scaled, \n",
    "                     batch_size=32, \n",
    "                     epochs=150, \n",
    "                     validation_data=(X_val, y_val_scaled))\n",
    "\n",
    "# Evaluate the model on training data\n",
    "normalized_model.evaluate(X_train, y_train_scaled)\n",
    "\n",
    "# Evaluate the model on validate data\n",
    "normalized_model.evaluate(X_val, y_val_scaled)\n",
    "\n",
    "##note that here, since output is normalized, the metrics aren't interpretable - so reverse scale and check rmse\n",
    "\n",
    "\n",
    "# Generate predictions on validate data\n",
    "y_val_pred_scaled = normalized_model.predict(X_val)\n",
    "\n",
    "# Transform the predictions back to original scale\n",
    "y_val_pred = ss_y.inverse_transform(y_val_pred_scaled)\n",
    "\n",
    "# RMSE of validate data\n",
    "np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "##now we can work on optimizing by using initialization and opt techniques\n",
    "##he initialization (not familiar with this one)\n",
    "np.random.seed(123)\n",
    "he_model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "he_model.add(layers.Dense(100, kernel_initializer='he_normal', activation='relu', input_shape=(n_features,)))\n",
    "\n",
    "# Add another hidden layer\n",
    "he_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "he_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "he_model.compile(optimizer='SGD', \n",
    "                 loss='mse', \n",
    "                 metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "he_model.fit(X_train, \n",
    "             y_train_scaled, \n",
    "             batch_size=32, \n",
    "             epochs=150, \n",
    "             validation_data=(X_val, y_val_scaled))\n",
    "\n",
    "# Evaluate the model on training data\n",
    "he_model.evaluate(X_train, y_train_scaled)\n",
    "\n",
    "# Evaluate the model on validate data\n",
    "he_model.evaluate(X_val, y_val_scaled)\n",
    "\n",
    "###lecun init (Yann LeCun!  I know this from coursera.  neat!)\n",
    "np.random.seed(123)\n",
    "lecun_model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "lecun_model.add(layers.Dense(100, kernel_initializer='lecun_normal', activation='relu', input_shape=(n_features,)))\n",
    "\n",
    "# Add another hidden layer\n",
    "lecun_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "lecun_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "lecun_model.compile(optimizer='SGD', \n",
    "                    loss='mse', \n",
    "                    metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "lecun_model.fit(X_train, \n",
    "                y_train_scaled, \n",
    "                batch_size=32, \n",
    "                epochs=150, \n",
    "                validation_data=(X_val, y_val_scaled))\n",
    "\n",
    "# Evaluate the model on training data\n",
    "lecun_model.evaluate(X_train, y_train_scaled)\n",
    "\n",
    "# Evaluate the model on validate data\n",
    "lecun_model.evaluate(X_val, y_val_scaled)\n",
    "\n",
    "####RMS Prop\n",
    "np.random.seed(123)\n",
    "rmsprop_model = Sequential()\n",
    "rmsprop_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "rmsprop_model.add(layers.Dense(50, activation='relu'))\n",
    "rmsprop_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "rmsprop_model.compile(optimizer='rmsprop', \n",
    "                      loss='mse', \n",
    "                      metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "rmsprop_model.fit(X_train, \n",
    "                  y_train_scaled, \n",
    "                  batch_size=32, \n",
    "                  epochs=150, \n",
    "                  validation_data=(X_val, y_val_scaled))\n",
    "\n",
    "# Evaluate the model on training data\n",
    "rmsprop_model.evaluate(X_train, y_train_scaled)\n",
    "\n",
    "# Evaluate the model on validate data\n",
    "rmsprop_model.evaluate(X_val, y_val_scaled)\n",
    "\n",
    "###ADAM optimizer\n",
    "np.random.seed(123)\n",
    "adam_model = Sequential()\n",
    "adam_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "adam_model.add(layers.Dense(50, activation='relu'))\n",
    "adam_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "adam_model.compile(optimizer='Adam', \n",
    "                   loss='mse', \n",
    "                   metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "adam_model.fit(X_train, \n",
    "               y_train_scaled, \n",
    "               batch_size=32, \n",
    "               epochs=150, \n",
    "               validation_data=(X_val, y_val_scaled))\n",
    "\n",
    "# Evaluate the model on training data\n",
    "adam_model.evaluate(X_train, y_train_scaled)\n",
    "\n",
    "# Evaluate the model on validate data - instruction above should read val, not train\n",
    "adam_model.evaluate(X_val, y_val_scaled)\n",
    "\n",
    "###select a final model based on metrics.  From what i can tell, none of these are un-scaled RMSE, but we are \n",
    "#basing our \"best model\" on not un-scaled RMSE - which will then unscale later.  Not sure if it'd be better\n",
    "#to unscale them all first to compare performance, or if this suffices \n",
    "#RMSProp did the best on validation data.\n",
    "# Evaluate the best model on test data\n",
    "rmsprop_model.evaluate(X_test, y_test_scaled)\n",
    "\n",
    "# Generate predictions on test data\n",
    "y_test_pred_scaled = rmsprop_model.predict(X_test)\n",
    "\n",
    "# Transform the predictions back to original scale\n",
    "y_test_pred = ss_y.inverse_transform(y_test_pred_scaled)\n",
    "\n",
    "# MSE of test data\n",
    "np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-tuning-neural-networks-from-start-to-finish-lab-v2-1\n",
    "##probably a great idea to read this one again in full-process work\n",
    "  \n",
    "    \n",
    "# Necessary libraries and classes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "##load the data\n",
    "# Necessary libraries and classes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Import the data\n",
    "data = pd.read_csv('loan_final.csv', header=0)\n",
    "\n",
    "# Drop rows with no target value\n",
    "data.dropna(subset=['total_pymnt'], inplace=True)\n",
    "\n",
    "# Print the first five rows\n",
    "data.head()\n",
    "\n",
    "# Print the dimensions of data \n",
    "data.shape\n",
    "\n",
    "##generate hold-out set \n",
    "# Features to build the model\n",
    "features = ['loan_amnt', 'funded_amnt_inv', 'installment', 'annual_inc', \n",
    "            'home_ownership', 'verification_status', 'emp_length']\n",
    "\n",
    "X = data[features]\n",
    "y = data[['total_pymnt']]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "##preproc numericals\n",
    "# Select continuous features\n",
    "cont_features = ['loan_amnt', 'funded_amnt_inv', 'installment', 'annual_inc']\n",
    "\n",
    "X_train_cont = X_train.loc[:, cont_features]\n",
    "X_test_cont = X_test.loc[:, cont_features]\n",
    "\n",
    "# Instantiate SimpleImputer - fill the missing values with the mean\n",
    "si = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_imputed = si.fit_transform(X_train_cont)\n",
    "\n",
    "# Transform test data\n",
    "X_test_imputed = si.transform(X_test_cont)\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "ss_X = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = pd.DataFrame(ss_X.fit_transform(X_train_imputed), columns=cont_features)\n",
    "\n",
    "# Transform test data\n",
    "X_test_scaled = pd.DataFrame(ss_X.transform(X_test_imputed), columns=cont_features)\n",
    "\n",
    "##ppreproc categoricals\n",
    "# Select only the categorical features\n",
    "cat_features = ['home_ownership', 'verification_status', 'emp_length']\n",
    "X_train_cat = X_train.loc[:, cat_features]\n",
    "X_test_cat = X_test.loc[:, cat_features]\n",
    "\n",
    "# Fill missing values with the string 'missing'\n",
    "X_train_cat.fillna(value='missing', inplace=True)\n",
    "X_test_cat.fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# OneHotEncode categorical variables\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Transform training and test sets\n",
    "X_train_ohe = ohe.fit_transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)\n",
    "\n",
    "# Get all categorical feature names\n",
    "cat_columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_categorical = pd.DataFrame(X_train_ohe.todense(), columns=cat_columns)\n",
    "\n",
    "# Transform test data\n",
    "X_test_categorical = pd.DataFrame(X_test_ohe.todense(), columns=cat_columns)\n",
    "\n",
    "##combine\n",
    "# Combine continuous and categorical feature DataFrames\n",
    "X_train_all = pd.concat([X_train_scaled, X_train_categorical], axis=1)\n",
    "X_test_all = pd.concat([X_test_scaled, X_test_categorical], axis=1)\n",
    "\n",
    "# Number of input features\n",
    "n_features = X_train_all.shape[1]\n",
    "\n",
    "##scalars\n",
    "# Instantiate StandardScaler\n",
    "ss_y = StandardScaler()\n",
    "\n",
    "# Fit and transform Y (train)\n",
    "y_train_scaled = ss_y.fit_transform(y_train)\n",
    "\n",
    "# Transform test Y (test)\n",
    "y_test_scaled = ss_y.transform(y_test)\n",
    "\n",
    "##defining k-fold cross validation methodology.   \n",
    "##keras handles it this way, I guess\n",
    "# Define a function that returns a compiled Keras model \n",
    "def create_baseline_model():\n",
    "    \n",
    "    # Initialize model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(layers.Dense(10, activation='relu', input_shape=(n_features,)))\n",
    "\n",
    "    # Second hidden layer\n",
    "    model.add(layers.Dense(5, activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='SGD', \n",
    "                  loss='mse',  \n",
    "                  metrics=['mse']) \n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model\n",
    "\n",
    "\n",
    "# Wrap the above function for use in cross-validation\n",
    "keras_wrapper_1 = KerasRegressor(create_baseline_model,  \n",
    "                                 epochs=150, \n",
    "                                 batch_size=256, \n",
    "                                 verbose=0)\n",
    "\n",
    "# ⏰ This cell may take several mintes to run\n",
    "# Generate cross-validated predictions\n",
    "np.random.seed(123)\n",
    "cv_baseline_preds = cross_val_predict(keras_wrapper_1, X_train_all, y_train_scaled, cv=5)\n",
    "\n",
    "# RMSE on train data (scaled)\n",
    "np.sqrt(mean_squared_error(y_train_scaled, cv_baseline_preds))\n",
    "\n",
    "# Convert the predictions back to original scale\n",
    "baseline_preds = ss_y.inverse_transform(cv_baseline_preds)\n",
    "\n",
    "# RMSE on train data (original scale)\n",
    "np.sqrt(mean_squared_error(y_train, baseline_preds))\n",
    "#RMSE 4016\n",
    "\n",
    "##intentionally overfit a model to see how ithat goes\n",
    "# Define a function that returns a compiled Keras model \n",
    "# Define a function that returns a compiled Keras model \n",
    "def create_bigger_model():\n",
    "    \n",
    "    # Initialize model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(layers.Dense(10, activation='relu', input_shape=(n_features,)))\n",
    "\n",
    "    # Second hidden layer\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    \n",
    "    # Third hidden layer\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    \n",
    "    # Fourth hidden layer\n",
    "    model.add(layers.Dense(8, activation='relu'))\n",
    "    \n",
    "    # Fifth hidden layer\n",
    "    model.add(layers.Dense(8, activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='SGD', \n",
    "                  loss='mse',  \n",
    "                  metrics=['mse']) \n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model\n",
    "\n",
    "# Wrap the above function for use in cross-validation\n",
    "keras_wrapper_2 = KerasRegressor(create_bigger_model,  \n",
    "                                 epochs=150, \n",
    "                                 batch_size=256, \n",
    "                                 verbose=0)\n",
    "\n",
    "# ⏰ This cell may take several mintes to run\n",
    "# Generate cross-validated predictions\n",
    "np.random.seed(123)\n",
    "cv_bigger_model_preds = cross_val_predict(keras_wrapper_2, X_train_all, y_train_scaled, cv=5)\n",
    "\n",
    "# RMSE on train data (scaled)\n",
    "np.sqrt(mean_squared_error(y_train_scaled, cv_bigger_model_preds))# RMSE on train data (scaled)\n",
    "np.sqrt(mean_squared_error(y_train_scaled, cv_bigger_model_preds))\n",
    "\n",
    "##regularizing the model to achieve balance - here this includes dropout layers and \n",
    "## l2 reg - still using SGD optimizer\n",
    "## will try this, then also update using adam optimizer\n",
    "# Define a function that returns a compiled Keras model \n",
    "def create_regularized_model():\n",
    "    \n",
    "    # Initialize model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer with dropout\n",
    "    model.add(layers.Dropout(0.3, input_shape=(n_features,)))\n",
    "    \n",
    "    # First hidden layer\n",
    "    model.add(layers.Dense(10, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Second hidden layer\n",
    "    model.add(layers.Dense(8, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # Third hidden layer\n",
    "    model.add(layers.Dense(8, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='SGD', \n",
    "                  loss='mse',  \n",
    "                  metrics=['mse']) \n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model\n",
    "\n",
    "# Wrap the above function for use in cross-validation\n",
    "keras_wrapper_3 = KerasRegressor(create_regularized_model,  \n",
    "                                 epochs=150, \n",
    "                                 batch_size=256, \n",
    "                                 verbose=0)\n",
    "\n",
    "# ⏰ This cell may take several mintes to run\n",
    "# Generate cross-validated predictions\n",
    "np.random.seed(123)\n",
    "cv_dropout_preds = cross_val_predict(keras_wrapper_3, X_train_all, y_train_scaled, cv=5)\n",
    "\n",
    "# RMSE on train data (scaled)\n",
    "np.sqrt(mean_squared_error(y_train_scaled, cv_dropout_preds))\n",
    "#.6097\n",
    "\n",
    "##round 2, with adam op\n",
    "def create_regularized_model():\n",
    "    \n",
    "    # Initialize model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer with dropout\n",
    "    model.add(layers.Dropout(0.3, input_shape=(n_features,)))\n",
    "    \n",
    "    # First hidden layer\n",
    "    model.add(layers.Dense(10, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Second hidden layer\n",
    "    model.add(layers.Dense(8, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # Third hidden layer\n",
    "    model.add(layers.Dense(8, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='Adam', \n",
    "                   loss='mse', \n",
    "                   metrics=['mse'])\n",
    "    \n",
    "    # Return the compiled model\n",
    "    return model\n",
    "\n",
    "# Wrap the above function for use in cross-validation\n",
    "keras_wrapper_4 = KerasRegressor(create_regularized_model,  \n",
    "                                 epochs=200, \n",
    "                                 batch_size=256, \n",
    "                                 verbose=0)\n",
    "\n",
    "# ⏰ This cell may take several mintes to run\n",
    "# Generate cross-validated predictions\n",
    "np.random.seed(123)\n",
    "cv_dropout_preds = cross_val_predict(keras_wrapper_4, X_train_all, y_train_scaled, cv=5)\n",
    "\n",
    "# RMSE on train data (scaled)\n",
    "np.sqrt(mean_squared_error(y_train_scaled, cv_dropout_preds))\n",
    "#rmse .601\n",
    "\n",
    "\n",
    "##final evaluation\n",
    "# ⏰ This cell may take several mintes to run\n",
    "# Initialize model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Input layer with dropout\n",
    "model.add(layers.Dropout(0.3, input_shape=(n_features,)))\n",
    "\n",
    "# First hidden layer\n",
    "model.add(layers.Dense(10, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(layers.Dense(8, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Third hidden layer\n",
    "model.add(layers.Dense(8, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam', \n",
    "               loss='mse', \n",
    "               metrics=['mse'])\n",
    "\n",
    "model.fit(X_train_all, \n",
    "          y_train_scaled, \n",
    "          epochs=150, \n",
    "          batch_size=256, \n",
    "          verbose=0)\n",
    "\n",
    "final_preds_scaled = model.predict(X_test_all)\n",
    "\n",
    "# Convert the predictions back to original scale \n",
    "final_preds = ss_y.inverse_transform(final_preds_scaled)\n",
    "\n",
    "# RMSE on test data (original scale)\n",
    "np.sqrt(mean_squared_error(y_test, final_preds))\n",
    "\n",
    "#RMSE 5513 (unscaled)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-tuning-neural-networks-recap\n",
    "'''Key Takeaways\n",
    "The key takeaways from this section include:\n",
    "\n",
    "Validation and test sets are used when iteratively building deep neural networks\n",
    "Like traditional machine learning models, we need to watch out for the bias variance trade-off when building deep learning models\n",
    "Several regularization techniques can help us limit overfitting: L1 Regularization, L2 Regularization, Dropout Regularization, etc ...\n",
    "Training of deep neural networks can be sped up by using normalized inputs\n",
    "Normalized inputs can also help mitigate a common issue of vanishing or exploding gradients\n",
    "Examples of alternatives for gradient descent are: RMSprop, Adam, Gradient Descent with Momentum, etc.\n",
    "Hyperparameter tuning is of crucial importance when working with deep learning models, as setting the parameters right can lead to great improvements in model performance\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-productionizing-machine-learning-models-section-intro\n",
    "''' a good summary of data science (vs machine learning engineers)career responsibilities; along with some developments in machine learning.\n",
    "ability to put things in to production is big! '''\n",
    "\n",
    "# https://github.com/ericthansen/dsc-the-aws-ecosystem\n",
    "https://aws.amazon.com/getting-started/\n",
    "\n",
    "    \n",
    "# https://github.com/ericthansen/dsc-introduction-to-aws-sagemaker\n",
    "'''Amazon SageMaker is a platform created by Amazon to centralize all the various services related to Data Science \n",
    "and Machine Learning. If you're a data scientist working on AWS, chances are that you'll be spending most (if not all) \n",
    "of your time in SageMaker getting things done. You can get to SageMaker by just searching for \"SageMaker\" inside the \n",
    "spotlight search bar in the AWS Console.'''\n",
    "\n",
    "#can search for sagemaker, create notebooks.  fun!\n",
    "# https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/\n",
    "# see also docker training, \n",
    "# recommender training\n",
    "# lots of other built-in models! (but they cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-productionizing-models-with-sagemaker\n",
    "##will want to refer to this!\n",
    "# code comes from :\n",
    "    https://github.com/aws-samples/amazon-sagemaker-keras-text-classification\n",
    "#and\n",
    "    https://github.com/aws-samples\n",
    "'''When productionizing a machine learning model using AWS, you'll typically use the following workflow:\n",
    "\n",
    "Explore and preprocess data\n",
    "Build SageMaker container (Docker)\n",
    "Test training and inference code on your local machine\n",
    "Train and deploy model with SageMaker'''\n",
    "\n",
    "#step 0: do steps 1-=3 from this:\n",
    "https://github.com/ericthansen/amazon-sagemaker-keras-text-classification\n",
    "    \n",
    "###hmm...probably too much to copy/paste here  see actual repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####!!! need to complete amazon stuff here !!!###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-sarima-models-lab\n",
    "\n",
    "## https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\n",
    "# https://www.quantstart.com/articles/Autoregressive-Integrated-Moving-Average-ARIMA-p-d-q-Models-for-Time-Series-Analysis/\n",
    "# https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html\n",
    "\n",
    "##Fancy sarima models\n",
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Load the dataset\n",
    "dataset = sm.datasets.co2.load().data\n",
    "\n",
    "# Convert into DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Update to datetime type\n",
    "df['date'] = pd.to_datetime(df['index'])\n",
    "\n",
    "# Set as index\n",
    "df.set_index(df['date'], inplace=True)\n",
    "\n",
    "df.drop(['date', 'index'], axis=1, inplace=True)\n",
    "## https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\n",
    "df = df.asfreq('W-SAT')\n",
    "df\n",
    "# The 'MS' string groups the data in buckets by start of the month\n",
    "CO2 = df['co2'].resample('MS').mean()\n",
    "\n",
    "# The term bfill means that we use the value before filling in missing values\n",
    "CO2 = CO2.fillna(CO2.bfill())\n",
    "\n",
    "# Plot the time series\n",
    "CO2.plot(figsize=(15, 6))\n",
    "plt.show()\n",
    "print(CO2.head())\n",
    "\n",
    "##see documentation above for pdq\n",
    "# Define the p, d and q parameters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "pdqs = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "##use AIC regularization metric\n",
    "# Run a grid with pdq and seasonal pdq parameters calculated above and get the best AIC value\n",
    "ans = []\n",
    "for comb in pdq:\n",
    "    for combs in pdqs:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(CO2,\n",
    "                                            order=comb,\n",
    "                                            seasonal_order=combs,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            output = mod.fit()\n",
    "            ans.append([comb, combs, output.aic])\n",
    "            print('ARIMA {} x {}12 : AIC Calculated ={}'.format(comb, combs, output.aic))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "# Find the parameters with minimal AIC value\n",
    "ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'aic'])\n",
    "ans_df.loc[ans_df['aic'].idxmin()]\n",
    "\n",
    "##fitting an ARIMA time series model\n",
    "# Plug the optimal parameter values into a new SARIMAX model\n",
    "ARIMA_MODEL = sm.tsa.statespace.SARIMAX(CO2, \n",
    "                                        order=(1, 1, 1), \n",
    "                                        seasonal_order=(1, 1, 1, 12), \n",
    "                                        enforce_stationarity=False, \n",
    "                                        enforce_invertibility=False)\n",
    "\n",
    "# Fit the model and print results\n",
    "output = ARIMA_MODEL.fit()\n",
    "\n",
    "print(output.summary().tables[1])\n",
    "\n",
    "# Call plot_diagnostics() on the results calculated above \n",
    "output.plot_diagnostics(figsize=(15, 18))\n",
    "plt.show()\n",
    "\n",
    "##validating the model\n",
    "# Get predictions starting from 01-01-1998 and calculate confidence intervals\n",
    "pred = output.get_prediction(start=pd.to_datetime('1998-01-01'), dynamic=False)\n",
    "pred_conf = pred.conf_int()\n",
    "\n",
    "##\n",
    "# Plot real vs predicted values along with confidence interval\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "# Plot observed values\n",
    "ax = CO2['1990':].plot(label='observed')\n",
    "\n",
    "# Plot predicted values\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=0.9)\n",
    "\n",
    "# Plot the range for confidence intervals\n",
    "ax.fill_between(pred_conf.index,\n",
    "                pred_conf.iloc[:, 0],\n",
    "                pred_conf.iloc[:, 1], color='g', alpha=0.5)\n",
    "\n",
    "# Set axes labels\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('CO2 Levels')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##\n",
    "# Get the real and predicted values\n",
    "CO2_forecasted = pred.predicted_mean\n",
    "CO2_truth = CO2['1998-01-01':]\n",
    "\n",
    "# Compute the mean square error\n",
    "mse = ((CO2_forecasted - CO2_truth) ** 2).mean()\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n",
    "\n",
    "##Dynamic forecasting\n",
    "# Get dynamic predictions with confidence intervals as above \n",
    "pred_dynamic = output.get_prediction(start=pd.to_datetime('1998-01-01'), dynamic=True, full_results=True)\n",
    "pred_dynamic_conf = pred_dynamic.conf_int()\n",
    "\n",
    "# Plot the dynamic forecast with confidence intervals as above\n",
    "ax = CO2['1990':].plot(label='observed', figsize=(20, 15))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_conf.index,\n",
    "                pred_dynamic_conf.iloc[:, 0],\n",
    "                pred_dynamic_conf.iloc[:, 1], color='g', alpha=.3)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime('1998-01-01'), CO2_forecasted.index[-1], alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('CO2 Levels')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##quantify performance with MSE\n",
    "# Extract the predicted and true values of our time series\n",
    "CO2_forecasted = pred_dynamic.predicted_mean\n",
    "CO2_truth = CO2['1998-01-01':]\n",
    "\n",
    "# Compute the mean square error\n",
    "mse = ((CO2_forecasted - CO2_truth) ** 2).mean()\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n",
    "\n",
    "##producing and visualizing forecasts\n",
    "# Get forecast 500 steps ahead in future\n",
    "prediction = output.get_forecast(steps=500)\n",
    "\n",
    "# Get confidence intervals of forecasts\n",
    "pred_conf = prediction.conf_int()\n",
    "\n",
    "# Plot future predictions with confidence intervals\n",
    "ax = CO2.plot(label='observed', figsize=(20, 15))\n",
    "prediction.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "ax.fill_between(pred_conf.index,\n",
    "                pred_conf.iloc[:, 0],\n",
    "                pred_conf.iloc[:, 1], color='k', alpha=0.25)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('CO2 Levels')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6f6a2d94043d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "# https://github.com/ericthansen/dsc-facebook-prophet-lab\n",
    "### as it stands, pystan install doesn't work.  So not doing this lab.  But pasting \"solution\" here for reference\n",
    "# see also https://github.com/ericthansen/dsc-facebook-prophet-lab/tree/solution\n",
    "\n",
    "# If installing from terminal\n",
    "# pip install pystan ##probably need # pip install pystan==2.19.1.1  - this code won't work as is.  I may return to it\n",
    "##using conda, try this thread: https://stackoverflow.com/questions/56701359/running-setup-py-install-for-fbprophet-error\n",
    "# pip install fbprophet\n",
    "\n",
    "# If installing from a jupyter notebook\n",
    "#!pip install pystan==2.19.1.1\n",
    "#!pip install fbprophet\n",
    "\n",
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "# Import passengers.csv and set it as a time series\n",
    "ts = pd.read_csv('passengers.csv')\n",
    "ts['Month'] = pd.DatetimeIndex(ts['Month'])\n",
    "\n",
    "##fbprophet library requires that the input cols be named ds (the time column) and y(the metric column)\n",
    "# Rename the columns [Month, AirPassengers] to [ds, y]\n",
    "ts = ts.rename(columns={'Month': 'ds',\n",
    "                        '#Passengers': 'y'})\n",
    "\n",
    "ts.head(5)\n",
    "\n",
    "# Plot the timeseries \n",
    "pd.plotting.register_matplotlib_converters()\n",
    "ax = ts.set_index('ds').plot(figsize=(15, 8))\n",
    "ax.set_ylabel('No. of Airline Passengers/Month')\n",
    "ax.set_xlabel('Date')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "###time series forecasting with prophet\n",
    "# Set the uncertainty interval to 95% (the Prophet default is 80%)\n",
    "Model = Prophet(interval_width=0.95)\n",
    "\n",
    "##\n",
    "# Fit the timeseries to Model\n",
    "Model.fit(ts)\n",
    "\n",
    "##we need a ds col for future dates; proph has a helper method for this\n",
    "# Use make_future_dataframe() with a monthly frequency and periods = 36 for 3 years\n",
    "future_dates = Model.make_future_dataframe(periods=36, freq='MS')\n",
    "future_dates.tail()\n",
    "\n",
    "##these future date can now be used for prediction\n",
    "# Predict the values for future dates and take the head of forecast\n",
    "forecast = Model.predict(future_dates)\n",
    "forecast.head()\n",
    "\n",
    "'''We can see that Prophet returns a large table with many interesting columns, but we subset our output to the columns most relevant to forecasting, which are:\n",
    "\n",
    "ds: the datestamp of the forecasted value\n",
    "yhat: the forecasted value of our metric (in Statistics, yhat is a notation traditionally used to represent the predicted values of a value y)\n",
    "yhat_lower: the lower bound of our forecasts\n",
    "yhat_upper: the upper bound of our forecasts'''\n",
    "\n",
    "# Subset above mentioned columns and view the tail \n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "\n",
    "'''A variation in values from the output presented above is to be expected as Prophet relies on Markov chain Monte Carlo (MCMC) methods to generate its forecasts. MCMC is a stochastic process, so values will be slightly different each time.'''\n",
    "\n",
    "# Use Prophet's plot method to plot the predictions\n",
    "Model.plot(forecast, uncertainty=True)\n",
    "plt.show()\n",
    "\n",
    "##proph also nicely separates the components - in this case, overall trend vs seasonality\n",
    "\n",
    "# Plot model components \n",
    "Model.plot_components(forecast)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-network-introduction\n",
    "\n",
    "# https://github.com/ericthansen/dsc-intro-graph-theory\n",
    "\n",
    "# https://github.com/ericthansen/dsc-networkX-intro\n",
    "\n",
    "##note, of course, I've made graph objects and dijkstra's myself... :)\n",
    "#but this package includes nice visualizations!\n",
    "import networkx as nx\n",
    "#create graph\n",
    "G = nx.Graph()\n",
    "##add nodes\n",
    "G.add_node('Bob')\n",
    "\n",
    "people = ['Sally', 'Kate', 'Jen', 'Jake', 'Doug']\n",
    "for person in people:\n",
    "    G.add_node(person)\n",
    "    \n",
    "#add edges\n",
    "G.add_edge('Bob', 'Sally')\n",
    "\n",
    "relations = {'Bob': ['Jen', 'Kate'],\n",
    "            'Jen': ['Bob', 'Sally', 'Jake', 'Doug', 'Kate'],\n",
    "            'Doug': ['Bob']\n",
    "            }\n",
    "for p1 in relations.keys():\n",
    "    p2s = relations[p1]\n",
    "    for p2 in p2s:\n",
    "        G.add_edge(p1, p2)\n",
    "        \n",
    "##visualize!\n",
    "%matplotlib inline\n",
    "nx.draw(G, with_labels=True, node_color='#1cf0c7', node_size=1500, alpha=0.7, font_weight='bold')\n",
    "\n",
    "https://networkx.github.io/documentation/stable/index.html\n",
    "https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.drawing.nx_pylab.draw_networkx_nodes.html\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-networkX-intro-lab\n",
    "##import data\n",
    "\n",
    "# Your code here\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Yelp_reviews.csv')\n",
    "df.head()\n",
    "\n",
    "##create graph\n",
    "# Your code here\n",
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "\n",
    "##add nodes\n",
    "# Your code here\n",
    "names = {}\n",
    "node_colors = []\n",
    "for n, person in enumerate(df.user_id.unique()):\n",
    "    name = 'User{}'.format(n)\n",
    "    names[person] = name\n",
    "    G.add_node(name)\n",
    "    node_colors.append('green')\n",
    "for n, biz in enumerate(df.business_id.unique()):\n",
    "    name = 'Business{}'.format(n)\n",
    "    names[biz] = name\n",
    "    G.add_node(name)\n",
    "    node_colors.append('blue')  \n",
    "    \n",
    "##add edges\n",
    "# Your code here\n",
    "for row in df.index:\n",
    "    user = df['user_id'][row]\n",
    "    u_name = names[user]\n",
    "    business = df['business_id'][row]\n",
    "    b_name = names[business]\n",
    "    G.add_edge(u_name, b_name)\n",
    "    \n",
    "##visualize!\n",
    "# Your code here\n",
    "%matplotlib inline\n",
    "nx.draw(G, with_labels=True, alpha=0.7, font_size=6, node_size=500, node_color=node_colors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-graph-theory-shortest-path\n",
    "\n",
    "##simple and shortest paths\n",
    "\n",
    "#imports\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "##create a [small world] network\n",
    "G = nx.navigable_small_world_graph(3, seed=3)\n",
    "G = nx.relabel_nodes(G, dict(zip(G.nodes, ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'])))\n",
    "nx.draw(G, pos=nx.random_layout(G, seed=9), with_labels=True, node_color='#1cf0c7',\n",
    "        node_size=500, font_weight='bold', width=2, alpha=0.8)\n",
    "\n",
    "##retrieving shortest paths\n",
    "nx.has_path(G, 'F', 'G')\n",
    "#\n",
    "nx.shortest_path(G, 'F', 'G')\n",
    "#\n",
    "nx.shortest_path_length(G, 'F', 'G')\n",
    "\n",
    "##can also get with dijkstra call\n",
    "nx.dijkstra_path(G, 'F', 'G')\n",
    "nx.dijkstra_path_length(G, 'F', 'G')\n",
    "\n",
    "###visualizing a path\n",
    "G.edges\n",
    "\n",
    "#retrieving a specific edge\n",
    "G.edges[('F', 'E')]\n",
    "\n",
    "##note, this gives not much back, but doesn't error.\n",
    "##on the other hand, this one does give eerror bc it doesn't have an edge\n",
    "G.edges[('F', 'A')]\n",
    "\n",
    "## Retrieving outbound connections for a given node\n",
    "G['F']\n",
    "\n",
    "G['C']\n",
    "\n",
    "##coloring edges\n",
    "colors = []\n",
    "for edge in G.edges:\n",
    "    # To learn more about what's happening, uncomment this line (warning: verbose printout!)\n",
    "     # print(type(edge), edge) \n",
    "    if edge[0] == 'F':\n",
    "        colors.append('#ffd43d')\n",
    "    else:\n",
    "        colors.append('black')\n",
    "nx.draw(G, pos=nx.random_layout(G, seed=9), with_labels=True, node_color='#1cf0c7',\n",
    "        node_size=500, font_weight='bold', width=2, alpha=.8, edge_color=colors)\n",
    "\n",
    "##see just the colored ones\n",
    "nx.draw_networkx_edges(G, nx.random_layout(G, seed=9), [e for e in G.edges() if e[0]=='F'], edge_color='#ffd43d');\n",
    "\n",
    "##overlay on entire graph\n",
    "nx.draw(G, pos=nx.random_layout(G, seed=9), with_labels=True, node_color='#1cf0c7',\n",
    "        node_size=500, font_weight='bold', width=2, alpha=0.8, edge_color='black')\n",
    "nx.draw_networkx_edges(G, edgelist=[e for e in G.edges() if e[0]=='F'], pos=nx.random_layout(G, seed=9),\n",
    "                       width=3, edge_color='#ffd43d');\n",
    "\n",
    "##another set\n",
    "nx.draw(G, pos=nx.random_layout(G, seed=9), with_labels=True, node_color='#1cf0c7',\n",
    "        node_size=500, font_weight='bold', width=2, alpha=0.8)\n",
    "nx.draw_networkx_edges(G, nx.random_layout(G, seed=9), width=3,\n",
    "                       edgelist=[('F', 'I'), ('I','G')], edge_color='#ffd43d');\n",
    "\n",
    "\n",
    "##our old pal dijkstra's algorithm\n",
    "'''\n",
    "Under the Hood: Dijkstra's Algorithm¶\n",
    "Dijkstra's algorithm is essentially a depth based search. It commences at the starting node, spanning out to neighboring nodes and in turn visiting their neighbors in search of the destination. More formally, here's a general pseudo-code outline for the algorithm:\n",
    "\n",
    "Mark all nodes as unvisited\n",
    "Set the distance of the starting node as 0, and  ∞  for all other nodes\n",
    "Set the starting node as the current node\n",
    "Visit each of the neighbors of the current node\n",
    "For each neighbor, calculate the distance to that node traveling through the current node\n",
    "If this distance is less then the current distance recorded for that node, update the record accordingly\n",
    "Mark the current node as 'visited'\n",
    "Of the unvisited nodes, set the one with the smallest distance to the current node\n",
    "Repeat steps 4 through 6 until one of the following:\n",
    "The algorithm terminates when the destination node is the current node\n",
    "Alternatively, if the smallest distance of the unvisited nodes is  ∞ , then no path exists to the destination node\n",
    "Note: Dijkstra's algorithm (and NetworkX's implementations demonstrated above) returns a single path. In many cases, there may be multiple paths which are tied for the shortest distance between two nodes. In such cases, it is arbitrary which path is returned.\n",
    "'''\n",
    "# https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A {'B': {}, 'C': {}}\n",
      "B {'A': {}, 'D': {}, 'E': {}, 'C': {}}\n",
      "C {'A': {}, 'E': {}, 'G': {}, 'F': {}}\n",
      "D {'B': {}, 'F': {}, 'D': {}}\n",
      "E {'B': {}, 'C': {}, 'F': {}, 'H': {}}\n",
      "F {'D': {}, 'E': {}, 'I': {}}\n",
      "D {'B': {}, 'F': {}, 'D': {}}\n",
      "E {'B': {}, 'C': {}, 'F': {}, 'H': {}}\n",
      "I {'F': {}, 'H': {}, 'G': {}}\n",
      "B {'A': {}, 'D': {}, 'E': {}, 'C': {}}\n",
      "H {'E': {}, 'G': {}, 'I': {}}\n",
      "C {'A': {}, 'E': {}, 'G': {}, 'F': {}}\n",
      "G {'C': {}, 'H': {}, 'B': {}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['F', 'D', 'B', 'A']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['A', 'C', 'F']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAACXNUlEQVR4nOydd1hUR9vG7y0ssCy9iIgNLKiAhgUVKyKCDRR7V1TsUazRaEyMLRpNjAW7YMEKih0Ve0XsgGLBDiJIb8uWM98fvJxPBJSy9PldF1cMZ87MM8vu3mdmnsIhhBBQKBQKhVJD4Fa0ARQKhUKhlCdU+CgUCoVSo6DCR6FQKJQaBRU+CoVCodQoqPBRKBQKpUZBhY9CoVAoNQoqfBQKhUKpUVDho1AoFEqNggofhUKhUGoUVPgoFAqFUqOgwkehUCiUGgUVPgqFQqHUKKjwUSgUCqVGQYWPQqFQKDUKKnwUCoVCqVFQ4aNQKBRKjYIKH4VCoVBqFFT4KBQKhVKjoMJHoVAolBoFv6INoFQOFITgM5FCSggEHA5qcQTgcTgVbRaFQqEoHSp8NZhUIsd5aRJOyRPwmpEAADgAyP+um3HV0JuvDxeBLjQ59K1CoVCqBxxCCPlxM0p1QkYY+Enj4CuNhZwQ8DkcqIEL7lcrPIYQSMCw18cIjDFcYAQVDt0dp1AoVRsqfDWMOEaK2VlReK2QQIPDA78I25lyQpBBFDDjqWGtujmMuIJysJRCoVDKBip8NYg4RgrPzBdIZGQQcXjgFOMMjxCCdKKAHlcF24VNqPhRKJQqC923qiHICIPZWVFIZGTQ5PKLJXoAwOFwoMnlI5GRYU5WFOT0eYlCoVRRqPDVEPykcXitkEDE4ZWqHxGHh9cKCfykn5VkGYVCoZQvdKuzBpBK5HBLD4cA3ELP9FLDniPa7zhSw56DkUghMNCFbttWaDBtJLgqKnnaygmBFAxOiiyptyeFQqly0BVfDeC8NIn1ziyI+Iu3EP7zEiTeegDVWgYwdOkINRMjxJ4IBiOR5mvP53AgJwTnZUllbTqFQqEoHfq4XgM4JU8oVPQUkmy8/mcnCMPA0LkjGv86GRxuzvNQVnQsuGoFO7HwORyckiWgv8CwzOymUCiUsoAKXzVHQQheMxIIC1ncp4U9hzwtAwBQd5Q7K3oAoF7HuNB+1cBFFCOBghCa4YVCoVQp6FZnNeczydmq5BYiTrKkFPbfqsZFX73l9pfbP4VCoVQVqPBVc6SE4HvrMRVdbfbf2bHxxeqb+7/+KRQKpSpBha+aI+Bw8D1p0rRsAr6mBgDgw55jIAzDXpPExoORywu9l/lf/xQKhVKVoOEM1RwFIeiS/hjCb3Jxfk38+et4uWIzCMNAo1F9iJqZQ/olCSmhYbA7vpUVxq9hCEEmGFwWtaRnfBQKpUpBnVuqOTwOB2ZcNbxlJBCi4OB1Q+eOEBjpI3r/CaRFvETW22gIDPVQy9WxUK9OCRiYc9Wo6FEolCoHXfHVAPyz47Eu+yO0uKV/zmEUCiQkJACaGpivbU7DGSgUSpWDCl8NII3I4fqDzC1FJfbTJ8QnJYGrJoDO5NXo59wdVlZWEIlEEIlEaNasGXR1dZVkOYVCoSgfKnw1BN/sWGzP/gStYlZl+JrUlFS8ffsG0BIh3fsgUrYcAgBoaGigVq1aUFVVRZMmTRAYGKhEyykUCkW5UK/OGsJwgRHMeGpIJ4oS95GVlQmOlgjSl2+Rst2f/X1GRgZev36Nt2/fokGDBkqwlkKhUMoOKnw1BBUOF2vVzaHHVUEaI0dxF/qEEKgZ6kMPPCRMXgoo8gtoVlYWAgIC4O/vX0APFAqFUjmgwleDMOIKsF3YBA14akhh5Pj4KQYpyck/vE9OCNKIAuZ8dQRZdIMxX63AdjweD4mJiRg9ejQ6dOiAiIgIJc+AQqFQSg8VvhqGEVeAXcKmaBMZh1RJFhKkEmQSBZhvVoAMIcgkCqQycqRLszEgSx27hBaoraKGjRs3QlNTM1/fmpqasLa2BpfLxcOHD2Fvb49Ro0YhuQjiSqFQKOUFFb4aiAqHi4RN+6E3dQ0GJXDQkKuGTDBIJwpkEkXOf8GgIVcNXoI6eCHuhz+btcPZU6cAAN27d0erVq3A5/9/eISKigqys7MRFhYGBwcHtGjRAgqFAgEBAWjatClWrlwJ5qusMBQKhVJRUK/OGsiLFy8wbNgwaGpqIigoCKqqqlAQgs9ECikhEHA4qMURgMfhIDs7GyYmJkhJSUHt2rUxceJEzJkzB+vXr8fWrVuRkJCA7OxsADniJ5fLweFw0KBBA3h4eGDjxo2Ij8/JAWpqaop///0XPXv2rMjpUyiUGg5d8dVAcsMNevToAVVVVQA5GV5MuKpowFODCVeVzcjC4XBgYGAAQgg+ffoEX19fjBo1Cj169ECPHj1w9OhRBAYGQiQSQaFQQFVVFSoqKnj79i1WrlyJefPmYdq0aVBTU8PHjx8xcOBAdO3aFVFRURU1fQqFUsOhwlfDkEgkOHPmDADA3d39h+05HA5EIhEb+xcdHY2nT5/i3r172LhxIxwdHeHi4oKLFy9CT08PUmlOmSJDQ0NIJBLMnTsXr169wv379+Hs7AwOh4M7d+7AxsYGEyZMQHp6etlNlkKhUAqACl8N4+LFi0hPT0eLFi3QuHHjH7bPFTxVVVXweDyoqKhgyJAhGDx4cJ521tbWuH37NurVqweZTIaEhAT89NNP4HA4CAoKgpOTE3777TecP38eTZo0gVwuh5+fHxo3bowNGzbQ8z8KhVJuUOGrYRw7dgwA0Ldv3yK15/F4MDIyQvPmzdGyZUs0bNgQtWvXhlAozNfW1NQUoaGhaNmyJRiGwaNHj+Di4gINDQ18+vQJjo6OuHnzJh4+fIh///0Xurq6SE1Nxfz582FtbY3Lly8rc6oUCoVSINS5pQbx5s0bDBw4EOrq6jh37lyB4lUQSUlJIITg9evXmDRpEjQ1NXHixIkCQxoAQC6XY/DgwQgKCgIA9OrVC69fv8azZ88AAB06dMCRI0cgEAgwb9487N27F1KpFBwOB506dcLWrVtRt25d5UyaQqFQvoGu+GoQx48fBwC4uLgUWfQAQFdXF3p6erC1tYVYLEZaWhoOHDhQaHs+n4+AgAB4enoCAE6fPg1tbW2MHTsWXC4XN27cgKWlJe7evYv169fj0aNH6NSpEwDg6tWrsLa2hpeXF+stSqFQKMqECl8NQSqV4tT/4vCK4tRSGBMnTgQA+Pn5ITU19btt161bh6VLl4LL5eLOnTu4fv06tm/fDm1tbSQkJKB3795YuHAh6tati7Nnz+L48eNo0KABpFIptm/fjkaNGmHnzp0ltpVCoVAKggpfDeHq1atITk5Go0aN0Lx58xL3Y2Njg9atWyMjIwN+fn4/bD9r1iz4+PhAVVUVL1++xK+//orjx49DLBZDoVBg3bp16NSpE+Lj49G1a1eEhYVh+fLl0NLSQmJiIqZPnw6xWIzbt2+X2GYKhUL5Gip8NYRcpxZ3d/cSlyXKZdKkSQCAAwcOICUl5YftBwwYgNOnT0NTUxOfP3+Gq6srVq5ciTlz5oDP5+Phw4do2bIlTp8+DS6XCy8vL7x8+RLDhw8Hn89HZGQkunXrhn79+uHz58+lsp1CoVCo8NUAYmJicPfuXQgEAvTo0aPU/VlbW8Pe3h6ZmZnYu3dvke6xt7fHzZs3YWxsjLS0NPTu3RtWVlY4ffo0DAwMkJKSgsGDB2PatGlgGAYikQjbtm3DvXv30KZNGxBCcO7cOTRv3hwLFiyAXC4v9TwoFErNhApfDSA3U4uTkxO0tLSU0mfuWd+hQ4eQlJRUpHvMzc0RGhqKJk2aQCqVwsPDAyEhIYiIiEDnzp1BCIGPjw/s7Ozw/v17AEDjxo1x6dIlHDhwAHXq1IFEIsH69evRqFGj7zrYUCgUSmFQ4avmKBQKnDx5EkDRY/eKgqWlJTp06ICsrKwir/oAQE9PDyEhIWjfvj0YhsHixYuxcOFCnDlzBn/++ScEAgEiIyNha2ubR9jc3NwQGRmJRYsWQUNDA/Hx8Rg/fjzs7e3x6NEjpc2LQqFUf6jwVXNu3ryJ+Ph41KtXDz/99JNS+/561ZeYmFjk+wQCAYKCgjBo0CAAwI4dO+Du7o4ZM2bg0qVLMDExQUZGBjw9PTF69Gh2W5PL5WLBggV48eIF3N3dwePx8OTJE3Ts2BHDhg0rlg0UCqXmQoWvmpO7zdm3b99SO7V8S7NmzdCpUydkZ2fD19e3WPdyuVz4+Phg7ty54HA4OH/+PDp16oTGjRsjLCwMvXr1AiEE/v7+aNmyJSIjI9l7dXR0sG/fPty6dQutWrUCwzA4fvw4LCws8Oeff9L0ZxQK5fsQSrXl8+fPxM7OjrRp04YkJCSUyRiRkZFELBYTe3t7Eh8fX6I+du7cSbS0tIhQKCQWFhbkw4cPhBBCduzYQfT09IhQKCR6enpky5YtBd5/8OBB0rBhQyIUColQKCSNGjUix48fL/GcKBRK9Yau+KoxJ0+eBMMw6Ny5M/T09MpkjKZNm8LR0RFSqbTYq75cxo4di8OHD0MoFOL9+/fsud24ceNw584dNGzYEBKJBLNmzUK/fv2QmZmZ5/7BgwfjxYsX8PLygpqaGmJiYjB06FA4ODiwadIoFAolFyp81ZTc7T9AuU4tBTFhwgQAwNGjRxEXF1eiPlxcXHD58mXo6ekhMTERTk5OOHfuHBo3boxHjx5h8ODB4HA4OHfuHKysrHD//v089/P5fCxfvhzPnj1Djx49wOVyERoairZt22Ls2LG0/BGFQmGhwldNCQ0NRUxMDGrXro02bdqU6ViNGjWCk5MTpFIpfHx8StyPpaUlQkJCUK9ePWRlZWHQoEHYsWMH+Hw+du3aBR8fH2hoaCA2NhZdu3bF6tWr8/VhZGQEf39/XLp0Cc2aNYNcLsehQ4fQqFEjrF27lp7/USgUKnzVldxMLX369AGXW/Z/5gkTJoDD4eDYsWOIjY0tcT8mJiYIDQ1Fq1atIJfL4eXlhcWLFwMABg4ciAcPHqBZs2aQyWRYsmQJnJ2dC8wZamdnxxbL1dfXR1paGhYvXgxLS0ucP3++xPZRKJSqDxW+akhSUhKuXLkCLpcLNze3chnTzMwMzs7OkMvl2LVrV6n6EolEuHr1Knr06AFCCNauXYvRo0eDYRiYmpri7t27mDBhArhcLm7evAlLS0tcuXKlwL48PDzw6tUrTJw4EQKBAO/evUO/fv3g4uKCN2/elMpOCoVSNaHCVw05deoU5HI52rVrByMjo3Ib19PTE1wuF8ePH0dMTEyp+uLz+fD392fPD/39/eHs7AypVAoul4t///0XR44cYSs9uLm5YeHChQVuZQoEAvzzzz8ICwuDo6MjAODGjRv46aefMHXqVGRlZZXKVgqFUrWgwlfNIISwsXulKT9UEho0aIDu3btDoVAorZzQv//+i+XLl4PL5eL27dto3bo1vnz5AgDo3r07njx5kqfSQ8eOHQt1sDE1NcXJkydx6tQpmJubQyaTwdfXF40aNcKWLVuUYi+FQqn8UOGrZjx69Ajv3r2DgYEBOnToUO7jjx8/HlwuFydPnsTHjx+V0qeXl1ee0katW7fGq1evAAAGBga4du0a5s6dCz6fj0ePHqFly5ZsmraCcHBwwKNHj/D3339DW1sbycnJmD17Nlq2bIkbN24oxWYKhVJ5ocJXzch1anFzcwOPxyv38evVq4devXqBYRilFpHNLW2kpaWFz58/o0OHDrh58yZ7/Y8//sCZM2dgaGiI1NRUDB06FFOnTi3Ui5PL5WLKlCl49eoVRo8eDRUVFbx69Qrdu3dHnz59EB0drTTbKRRK5YIKXzUiNTUVwcHBAFBuTi0FkbvqO336NFtlQRnY29vjxo0beUobHT58mL3evn17hIeHs5UefH19YWtri3fv3hXap1AohLe3N+7fv4/27duDEILg4GBYWVlh7ty5kMlkSrOfQqFUDqjwVSOCgoIglUrRunVrmJqaVpgdderUgZubGxiGwY4dO5Tad25po6ZNm0IqlWLcuHFYu3Yte10kEuWp9PD8+XPY2dn9sFq8ubk5zp8/j6NHj6JevXrIzs6Gt7c3GjVqhN27dyt1DhQKpYKp2IxpFGXBMAwZOnQoEYvF5Ny5cxVtDomOjiatW7cmdnZ25M2bN0rvPzs7m3Tr1o3Nz/nzzz/na/Pw4UPSuHFjIhQKiYaGBhk5ciSRyWQ/7FuhUJDVq1cTIyMjtn87OzsSGhqq9HlQKJTyh674qgnPnj3DixcvoK2tDQcHh4o2ByYmJujTpw8YhsG2bduU3n9uaaPBgwcDAHbu3Im+ffvmqczeqlUrPHnyBL179wYhBAEBAbC2tv5h/k4ul4u5c+fi5cuXGDRoEPh8PiIiItClSxcMGjSI9SqlUChVEyp81YRcp5ZevXpBIBBUsDU5jB07FioqKrhw4QJev36t9P65XC527dqFefPmgcPh4MKFC+jYsWOevJxqamo4dOgQ1q9fDzU1Nbx79w7t27fH1q1bf9i/lpYWfHx8cOvWLYjFYjAMg9OnT8PCwgKLFy/OI7IUCqXqQIWvGpCZmYlz584BKP/Yve9hbGwMd3d3EELKZNWXy++//44NGzaAz+fjyZMnsLW1zRdKMW7cOISEhKBhw4bIzs7GrFmz4O7unq/SQ0G0aNEC165dw759+1C7dm1kZWVh7dq1aNq0Kfz9/ctqWhQKpYygwlcNuHDhAjIzM9GyZUs0bNiwos3Jw5gxYyAQCBAcHIyXL1+W2TgeHh7w9/eHhoYGPnz4gLZt2+LRo0d52jRq1AiPHj3CkCFD2OK3VlZWuHfvXpHGcHd3R2RkJObNmwehUIjY2FiMHj0aHTp0QERERBnMikKhlAVU+KoBuduclWm1l4uRkRH69esHAGW66gOAbt264dKlS9DT00NSUhKcnJxw9uzZPG34fD527twJX19fiEQixMbGwsnJCatWrSrSGHw+H7///juePXsGV1dXcLlcPHz4EPb29hg1ahRSUlLKYmoUCkWJUOGr4rx69Qrh4eHQ0NBA165dK9qcAsld9V2+fBnPnz8v07G+LW00ZMgQbN++PV+7AQMG4P79+2jevDlkMhn+/PNPdOvWrcjCZWBggIMHD+Lq1auwsrKCQqFAQEAAmjRpgpUrV9LyRxRKJYYKXxUnd7XXo0cPqKurV7A1BWNgYICBAwcCQJGcSkrLt6WNZs6ciUWLFuVrZ2pqipCQELbSw61bt75b6aEgbGxscOfOHWzbtg2GhoZIT0/HsmXL0KxZM5w+fVqJs6JQKEqjouMpKCVHIpEQBwcHIhaLSWRkZEWb810SEhJI+/btiVgsJhEREeUypkwmIwMGDGBj8UaOHEkUCkWBbYOCgoiJiQkRCoVEU1OTzJ8/v9C23xtv3rx5RE9Pjx3T0dGRvHr1ShnToVAoSoKu+Kowly5dQlpaGpo1a4amTZtWtDnfRU9PD4MGDQJQPqs+IOc87siRI5g4cSI4HA4CAgLQrVs3SCSSfG1dXFzw+PFj2NraQqFQYP369d+t9FDYeKtWrUJERAScnZ3B4XBw584d2NjYYMKECXnCLCgUSsVBha8Kk7vN2bdv34o1pIiMHDkS6urquHnzJsLCwspt3H/++QfLly8Hj8fDnTt30KZNmwKD0A0MDHD16tV8lR5OnDhRrPGMjY1x7NgxnD9/Hk2aNIFcLoefnx8aN26M9evX0/M/CqWCocJXRXn//j0ePHgANTU1dO/evaLNKRK6uroYMmQIgLL38PyWGTNmsKWNXr16BTs7u0LDK/744w+cPXuWrfQwbNiw71Z6KIx27drh4cOH+Pfff6Grq4vU1FQsWLAA1tbWuHz5sjKmRaFQSgAVvipKbrFZZ2dnaGhoVKwxxWDEiBEQCoW4ffs2Hj9+XK5j9+/fny1tFBcXh44dOxZaf69du3YIDw+Hg4MDW+lBLBZ/t9JDYUyYMAGvXr3CuHHjIBAI8ObNG7i6uqJnz5748OFDaadFoVCKCRW+KohMJsOpU6cAVJ1tzly0tbUxbNgwAOV31vc135Y2cnV1xaFDhwpsKxKJcPr0aSxbtgwCgQAvXryAra3tDys9FISamhrWr1+PR48eoVOnTgCAq1evwtraGl5eXsjOzi7VvCgUStGhwlcFuX79OhITE2FmZgYrK6uKNqfYDBs2DCKRCHfv3sWDBw/Kffzc0kYWFhaQSqUYP358ntJG3zJz5kxcuXIFderUQWZmJiZOnIiRI0eWKFdn/fr1cfbsWRw/fhwNGjSAVCrF9u3b0ahRI6UW7qVQKIVDha8K8rVTC4fDqWBrio+WllaFrvqAHC/T27dvo0OHDmAYBosXL8a0adMKbd+yZUuEhYXB1dUVhBAcPXoUVlZWePr0aYnG79q1K8LCwrB8+XJoaWkhMTER06dPh1gsxp07d0o6LQqFUgSo8FUxPn36hDt37kBFRQW9evWqaHNKzLBhw6CpqYn79+8XOVemshEIBDh79iyGDh0KAPDx8UGfPn0KXcmpqqri4MGD2LBhA9TU1PD+/Xt06NABmzdvLtH4XC4XXl5eePnyJYYPHw4+n4/IyEg4OTmhX79++Pz5c4nnRqFQCocKXxXjxIkTIITA0dER2traFW1OiRGJRBg5ciQAYMuWLSCEVIgdXC4XO3bsYEsbBQcHo0OHDt+NuRs7dizu3r0LMzMzZGdnY86cOUWu9FAQIpEI27Ztw71799CmTRsQQnDu3Dk0b94cCxYsoOWPKBQlQ4WvCsEwDI4fPw6gciakLi5DhgyBlpYWHj16hLt371aoLbmljVRUVBAWFgaxWJyvtNHXmJub4+HDhxg6dGiJKj0UROPGjXHp0iUcOHAAderUgUQiwfr169GoUSPs37+/xP1SKJS8UOGrQty6dQtxcXEwNTWFjY1NRZtTaoRCIUaNGgWgYld9uXh4eODIkSPQ0NDAx48f0bZt2+863/D5fOzYsSNfpYeVK1eWyg43NzdERkZi0aJF0NDQQHx8PDw9PWFvb1/uISAUSnWECl8VIjd2r2/fvuByq8efbtCgQdDR0UFYWBhu375d0eagW7duuHz5MvT19ZGUlARnZ+d8pY2+JbfSQ4sWLSCTybBs2TJ069YNycnJJbaDy+ViwYIFePHiBdzd3cHj8fDkyRN06NABw4YNQ2JiYon7plBqOtXj27MG8OXLF1y7dg08Hg+urq4VbY7SEAqFGD16NIDKseoDciqu37lzB/Xr12dLG/0o04ypqSnu3LmDSZMmsZUerKysilXpoSB0dHSwb98+3Lp1C61atWK3uy0sLPDnn3/S9GcUSgmgwldFOHnyJBiGQadOnaCvr1/R5iiVgQMHQk9PD0+fPi00k0p5Y2Jigrt378LGxgZyuRyzZs3CwoULv3sPl8vF2rVr4e/vDx0dHSQmJsLNzQ3z588vtUBZWlri5s2b2LVrF2rVqoWMjAysWrUKTZs2LXYuUQqlpkOFrwrAMEyebc7qhpqaGsaMGQOg8qz6gBxvy8uXL6Nnz54ghGDdunUYMWLED0XMxcUFT548gZ2dHRQKBTZs2IAOHTooJTxh8ODBePHiBby8vKCmpoaYmBgMHToUnTt3RmRkZKn7p1BqAlT4qgD3799HdHQ0atWqBXt7+4o2p0zo378/9PX18fz5c1y9erWizWH5trTRsWPH4OTkVGBpo6/R19fHlStXMG/ePPD5fDx+/BitWrVSyuqMz+dj+fLlePbsGXr06AEul8uGQowdO5aWP6JQfgAVvipAbqaWPn36VBunlm9RVVXF2LFjAeRUbqhsZ1dflzYKCQkptLTRt/z+++8ICgrKU+lhypQpUCgUpbbJyMgI/v7+uHTpEpo1awa5XI5Dhw6hUaNGWLt2baV7DSmUykL1/BatRiQnJ+Py5cvgcDhwc3OraHPKFHd3dxgZGeHFixeldgopC2bMmIHdu3fnKW304sWLH95nb2+P8PBwdOnSBYQQ7N69G7a2tiWq9FAQdnZ2uHfvHjZu3Ah9fX2kpaVh8eLFsLS0xPnz55UyBoVSnaDCV8k5c+YMZDIZ7O3tYWxsXNHmlCkCgYBd9W3ZsqVSrljc3d1x9uxZtrRRp06dcP369R/eJxKJcOrUKSxfvrzUlR4Kw8PDA69evcLEiRMhEAjw7t079OvXDy4uLnjz5o3SxqFQqjpU+CoxhJAqV2W9tLi5uaFWrVp4/fo1goODK9qcAmnTpg1u3brFljZyc3MrtLTRt3h5eeHq1at5Kj2MGDFCaWnJBAIB/vnnH4SFhcHR0REAcOPGDfz000+YOnUqsrKylDIOhVKVocJXiQkLC8ObN2+gp6fH1nCr7ggEAowbNw4AsH379jyrPgUhiGGy8VYhQQyTDUUFen82bNgQ9+/fz1Pa6O+//y7SvdbW1ggPD2crPRw7dgxWVlaIiIhQmn2mpqY4efIkTp06BXNzc8hkMvj6+qJRo0bYsmWL0sahUKoiVPgqMbmrPVdXV/D5/Aq2pvxwdXWFiYkJ3rx5g5NXLsI/Ox5jMiLRJf0xhmQ8w+jMSAzJeIYu6Y8xJiMS/tnxSCPln8hZR0cHt2/fRseOHcEwDP744w9MnTq1SPcKBAIcPHgQGzduhLq6Ot6/f4+OHTvC29tbqTY6ODjg0aNH+Pvvv6GtrY3k5GTMnj0bLVu2rDQxkxRKecMhlSVoipKH9PR0uLi4IDs7G0ePHkW9evUq2qRy5cyF85hz+yzUx/eHukgDfA4HauCC+1X9QYYQSMBATgj4HA7GCIwxXGAEFU75Ps8xDIMJEybgwIEDAAAnJycEBAQU+WElKioK7u7uiIqKYu8/cOAAhEKhUu3MzMzEnDlzsH//fshkMnA4HDg6OsLb2xumpqZKHYtCqczQFV8lJSgoCNnZ2RCLxTVO9OIYKQ60qwvtGSOgL9KCFpcPIYeXR/QAgMvhQMjhQYvLhwBcbM/+hLGZzxHHSMvV3tzSRr/88kuRSxt9jbm5OR48eIDhw4ez91tZWSE0NFSpdgqFQnh7e+P+/fto3749CCG4ePEirK2tMXfuXMhkMqWOR6FUVqjwVVJyM7VUh/JDxSGOkcIz8wXeKiTQ4aqAzy1ahXk+hwMtDg9vFRJ4Zr4od/EDgMWLF2PTpk1FLm30NXw+H9u2bcPu3buVWumhIMzNzXH+/Hl2JyE7Oxve3t5o1KgRdu/erfTxKJTKBt3qrIRERkZixIgR0NLSQlBQEAQCQUWbVC7ICIOxmc/xViGBJrfkZ5ppjBwNeWrYJbQAn1M04VQmFy9exNChQ5GRkQFdXV2cOHGiWGWkoqOj4e7uzjq72Nvbs/k/lQ3DMFi7di3WrFnDrlBbtGgBb29v2NraKn08CqUyQFd8lZBcp5aePXvWGNEDAD9pHF4rJBBxeKXqR8Th4bVCAj9p6XNjloSuXbvmK2105syZIt9fp06dPJUebt++DSsrK1y+fFnptnK5XMydOxcvX77EoEGDwOfzERERgS5dumDQoEFFyk5DoVQ16IqvkpGVlQUXFxdkZmbi0KFDMDc3r2iTyoVUIodbejgE4OZbpd0bOA3Zn3O+gDlcLvjamtBq0RgNpo2EmkmtAvuTEwIpGJwUWUKTUzEesbGxsejatSvevn0LPp+P1atXY+LEicXq4/z58/Dw8EBycjJ4PB4mT56MlStXllnquoiICEyePBn3798HAKirq2PKlClYvHhxjfIsplRv6IqvkhEcHIzMzExYWVnVGNEDgPPSJNY7szD02tnAuG838DWESLhxD69WFV4jj8/hQE4IzsuSysLcImFsbIzQ0FC2tNHs2bN/WNroW5ydnREWFobWrVtDoVBg48aN6NChA2JjY8vE5hYtWuDatWvYt28fateujaysLKxduxZNmzaFv79/mYxJoZQ3VPgqGbnbnDXNqeWUPOGH53FGvbrAzMsDDaYMBwBkvY/+bns+h4NTsgSl2VgShEIhrl69il69ehWrtNHX6Onp4fLly2VS6aEw3N3dERkZiXnz5kEoFCI2NhajR49Ghw4dlBpoT6FUBFT4KhGvX7/GkydPIBQK4eTkVNHmlBsKQvCakUDtB2/HuNOX8XqdD95u2Q8A0O/U+rvt1cBFFCOp0AwvQM452uHDhzFp0qRilTb6lq8rPaSlpWHYsGGYPHmy0tKdfQufz8fvv/+OZ8+ewdXVFVwuFw8fPoS9vT1GjRqF5OTkMhmXQilrqPBVInJDGLp376704OXKzGeSE3rwbZzetyTeeoBPR88h630MuCoq0Ghq9t32uf3l9l/RrF27FitXrmRLG7Vu3brYziPfVnrYs2cPbG1tyzQJtYGBAQ4ePIirV6/C0tISCoUCAQEBaNKkCVauXFkpk4lTKN+DCl8lQSqV4tSpUwBqTkLqXKSEoChBBxbLZ6Pd1QOw3rIUhGEQtXobJJ/ivnsP93/9VxZ+/vln7NmzB2pqaoiKioKtrS2eP39erD5yKz2sXLkSAoEAL1++ROvWrbFv374ysjoHGxsbhISEYNu2bTA0NERGRgaWLVuGZs2a4fTp02U6NoWiTKjwVRKuXLmC1NRUNGnSBM2aNatoc8oVAYeDokoTh8OBqKkZeOpqIAwDSXQhIQuEIFuSDTnDQMDhgGEYvHz5Ev7+/li8eDF27dqlNPuLS9++fXHmzBloa2sjPj4enTp1wrVr14rdz/Tp03H9+nWYmpqylR6GDx9e5hlYhg8fjlevXmHatGlQU1PDx48fMWjQIHTt2pVNu0ahVGZoOEMlYfLkyQgNDcUvv/yCgQMHVrQ55YqCEHRJfwzhN7k4c8kNZ9BrZwPV2kbIePEGqWHPwVNVhc2h9RDoaee7JyU5GdGfYkHUBchymghGLodAIIBAIIBCoUDDhg1LJDbK5M2bN+jWrRs+ffoEgUAAb29vDB06tNj9SKVSjB49mnV2qVevHvz9/dGiRQtlm5yP2NhYTJ48GRcuXAAhBHw+H4MHD8Y///wDkUhU5uNTKCWBrvgqAR8/fkRoaChUVVXRvXv3ijan3OFxODDjqkGC758VJd56gE8BQch88xFaVk1h8dfcAkUPyIk/U9UWgbyOxpe4OCQkJODTp0949+4dYmJi8PbtW6xcuRLPnj0riykViYYNG+LevXtsaaMJEyZg9erVxe5HIBDgwIED+So9bNy4sQyszouxsTGOHTuG8+fPo0mTJpDL5fDz80Pjxo2xfv16ev5HqZTQFV8lYNOmTfDx8UGvXr2wZMmSijan3Hn9+jWWhV1HeLumMFDXUFq/qYwcU2CIa/OW4dChQ0hNTQXDMOBwOODz+VBRUQEAaGtrw8LCAh06dECfPn3w008/lVmAeEHI5XK4urqyK9AxY8Zg06ZNJeqrvCo9FMa2bdvw559/IikpJ36yYcOG2LBhA7p06VIu41MoRYEKXwUjl8vRq1cvJCQkYMeOHWjVqlVFm1RmvH79GmfPnkV2djbkcjnev3+P8+fPQ6FQQLtObUh3L4GJgWHp8msSICUlGRwVFci5QHSn0RBI5RAKhXj//j2ysrKgoqICBwcHfPz4EW/fvs0XDqChoYHGjRujXbt26NWrFzp16lTmQsgwDCZNmgQ/Pz8AOWnPjh49WqJsKXK5HFOmTMH+/ftBCEGtWrVw8OBBtG79/fAPZSGRSDBv3jzs3bsXUqkUHA4HnTp1wtatW1G3bt1ysYFC+R5U+CqYK1euYM6cOWjQoAGOHDkCTgUkVS4vFi5cCH9/f6SmpiIpKQlSaU6YgYaGBhYtWgTtSYOwl5MMLQ6vxK+DQqHA8+cvAJE6DM+GIHzRWshkMhBCwOPxQAiBSCTChw8fIBKJIJVKce7cOQQFBSEkJARRUVGsXbmoqqrC3NwcrVu3Rs+ePeHs7MyuFpXNsmXL8Ndff4EQghYtWiA4OBhaWlol6isgIABTp05FWloa+Hw+fvnlF/z6669Ktrhw3r17h0mTJuH69esghEAgEGD06NFYtWoVVFVVy80OCuVbqPBVMDNmzMDNmzfh5eWFESNGVLQ5ZcqhQ4cwYcIEpKamsr8TiUTw8/ODm5ub0qozxKQmIfnxM7x3mwYoFOD8z6uTw+GAx+Oha9eu2L59e4GrD7lcjhs3buDUqVO4ffs2Xrx4gczMzDxt+Hw+GjRoALFYjO7du6Nnz55KdeTYs2cPpk+fDplMhjp16iA4OLjENRljYmLg7u6O8PBwAEDbtm3h7+8PXV1dpdn7Iy5evIjp06fj7du3AHIy0fzxxx8YN25cudlAoXwNFb4K5PPnz2xGjLNnz5brl1F5kpiYiLlz5yIgIADp6elQKBTgcrkwNTXF8uXL8wh+bj2+REYGUTFXfoQQpBMF9LgqGB+WhIFduiE7OztPGy6XCysrKwgEAvTo0QPjxo37rqgwDIP79+/jxIkTuHnzJp49e5ZHuHP7NDU1hY2NDZycnNCnTx/o6ekV2e6C+Lq0kY6ODk6ePFms0kbfzuGXX37Bli1bwDAM9PT0sHv3bjg6OpbKxuLasH79eqxatYp9/SwsLLBx40bY29uXmx0UCkCFr0LZvn07tm7dim7dupVJwdGKJjMzEwsXLsTevXuRlZUFADAyMmJXX7169cKGDRvynZ/FMVLMzorCa4UEGhxekc785IQgkyhgxlPDGnVzGHEF2LhxIxYsWJCnEnqbNm3Qs2dPnD59mhXg7t27Y9y4cahfv36R5hUREYETJ07g6tWriIiIQGJiYp7rHA4HxsbGaNmyJRwdHdG3b1/UqVOnSH1/O07Pnj3x5csXqKmpYffu3ejdu3ex+8klODgYY8aMQVJSErhcLiZPnoy//vqrXB150tPTMWvWLBw6dAhyuRwcDgfOzs7YvHkzatUquNIGhaJsqPBVEAzDwM3NDbGxsfD29i43x4PyQC6XY9myZdiyZQvS0tIA5Li9L1q0CKNHj8aiRYvw/PlzbN26FQYGBgX2ISMM9kvj4CONZas2qH0T58cQAgkY9rqHwBjDBbVYoczMzETfvn1x69YtdrtSXV0dQ4YMweLFi7F3716cPHmSFUBnZ2eMGzcODRs2LNZ83717h2PHjuHKlSsICwvD58+f8e3HytDQEC1atICDgwP69etX5MobuZXY37x5U+LSRl+TmJiIAQMGICQkBABgbW2NY8eOwdjYuMR9loSXL19i4sSJrB1qamqYMGECli5dSssfUcocKnwVxK1btzB9+nSYmJggMDCwXJ+6y4rc7ay1a9eyqyBdXV3MnDkTM2fOLNEc04gc52VJOCVLQBSTk9SZC7ARf+ZcNfRW0Yezim6BdfeOHTuG3377DaqqqtDQ0GDrzNWpUwd79+5F3bp14evrixMnTrArkG7dumH8+PEwM/t+LtDCiIuLQ2BgIC5duoSHDx8iJiYmXzybjo4OmjVrho4dO6JPnz7f9ebNzMxE9+7dcf/+fXA4HEyfPh0rVqwokW25LF26FGvWrIFcLoempia2bt2KPn36lKrPknDixAnMmTMH0dE5lTYMDQ2xYsUKDBs2rNxtodQgCKVCmDt3LhGLxWTnzp0VbYpS8PX1Jebm5kQoFBKhUEiMjIzIb7/9RmQymdLGkDMMiVZIyBt5FolWSIicYX54j0KhINu3bydXrlwhhBCydetWYmBgQIRCIdHU1CRz5swhCoWCfPr0iaxcuZK0bduWiMViYmtrS+bPn09evXpVartTU1PJvn37yKhRo4ilpSXR0tJiX6evX6+OHTuSefPmkRs3bhCFQpFvHoMHD2bbDxs2LF+b4nLr1i3SoEEDIhQKiYaGBpk4caJS/15FRaFQkBUrVhBDQ0N2fm3btiUPHz4sd1soNQMqfBVAQkICad26NbGzsyNxcXEVbU6pOH78OGnRogX7haWnp0emT59O0tLSKtq0Qvnw4QNp3749a7O1tTWJiIgghBASGxtLVq1axQqgWCwm8+bNIy9fvlTa+FlZWSQwMJBMnDiR2NjYEB0dnXxCqKenR1q3bk2mT59OgoKCWEGaM2cO0dDQIEKhkHTp0oVkZWWVypa0tDTSu3fvPK/F69evlTHNYpOUlESGDx9ONDU1iVAoJCKRiAwdOpQkJCRUiD2U6gsVvgpg9+7dRCwWk5kzZ1a0KSXmypUrxM7Ojv3C1NbWJmPGjCHx8fEVbVqRWblyJSs6Ojo6ZMWKFey1z58/k9WrVxN7e3tWAOfOnUueP3+udDtkMhk5f/488fLyIm3btiV6enr5hFBbW5u0atWKeHp6krFjxxKRSESEQiGxsrJSymu+fv169rXQ19cnu3fvVsLMSkZYWBhp164dO3dDQ0OyZMkSIpfLK8wmSvWCCl85wzAMcXd3J2KxmFy9erWizSk29+/fJ507d2ZXHZqamqR///7kw4cPFW1aiXj69CmxtrZmv2Tt7e3zzCUuLo6sWbMmjwDOnj2bREZGlplNCoWC3Lp1iyxYsIB06tSJ1KpVK58QqqurEy6XS3g8HjEwMCD37t0r9bhhYWGkSZMmebZTs7OzlTCjknHw4EHSsGFD1p5GjRqRwMDACrOHUn2gwlfO3Lt3j4jFYtK9e/cq9QT76tUr0rNnT1bwNDQ0iIuLC3n69GlFm1ZqFAoFmTt3Lnv2pq+vT7Zs2ZKnTXx8PFm7di1p164dK4AzZ84st/k/fvyYLFmyhHTr1o3UqVOHCIVCoqamRjgcDgFAAJA6deqQAQMGkC1btpBPnz6VaJzs7GwyZMgQVmyaNm1KwsPDlTyboiOTycivv/6aZxXcqVOnavG+o1QcVPjKmUWLFhGxWEy8vb0r2pQiER0dTQYPHsyeuwiFQtKhQwdy9+7dijZN6YSEhJDGjRuz83R2diZfvnzJ0+bLly/k33//zSOAM2bMYM8Iy4tXr16RNWvWEEdHR8Ln81nxEwgE7INJgwYNiJubG1m7dm2xz+18fX2Jvr4+uw28fv36MppJ0fj8+TPp378/u8WrpaVFxowZU6nPkimVFyp85UhKSgqxt7cntra2JDo6uqLN+S5JSUlk/PjxeRwvbGxsyIULFyratDJFJpOR8ePHsytbY2NjcuTIkXztEhISyH///Ufat2/PCuDPP/9MwsLCyt3mpKQkYmVlRVRUVAifzyf6+vqs/V//mJqakm7dupGlS5cWaRX36tWrPNvAbm5uFS40d+/eJWKxmLWpVq1aZM2aNaX2cKXULKjwlSMHDhwgYrGYTJkypaJNKZSMjAwya9Ys9mlfKBQSCwuLAr/8qzPnzp0j9erVY1dPAwcOJBkZGfnaJSYmkvXr15MOHTqwAjht2jTy+PHjcrVXJpORHj16sH+zsWPHEl9fXzJ8+HDSvHnzPCv2r0Wjc+fO5NdffyV37twpUDxkMhmZMGECK6QNGzYkd+7cKde5FcSuXbtI3bp187xHg4KCKtosShWBCl85wTAMGTRoEBGLxZVy1SSTycgff/xBjI2N2S+Thg0bkh07dtTYp+m0tDTSv39/9vWoX78+CQ4OLrBtUlIS2bhxI+nYsSMrgFOmTCGPHj0qN3sVCgXx9PRk7e3duzcbBpGRkUGOHDlCxo8fT1q2bEm0tbXzCaG+vj6xt7cnM2fOJMHBwXli+gICAlgnGy0tLbJs2bJym1dhZGdnk5kzZ7K7EhoaGqRbt24VFo5BqTpQ4SsnwsLCiFgsJk5OTkQqlVa0OSwKhYKsX78+z9OziYkJ+fvvv2us4H3LwYMH2QcCkUhEPD09Cw30Tk5OJps2bcojgJMnTyYPHjwoN3uXLVvGrtDs7OxISkpKvjYymYwEBQWRadOmETs7uwJDKHR0dIiNjQ2ZNGkSOX78OHnz5g1p06YNe71Lly6VIsbuw4cPpHfv3uyctbW1yZQpUwpcoVMohFDhKzf+/PNPIhaLybp16yraFJY9e/aQRo0a5cke8uuvv1aoC3tlJT4+nnTr1o19rZo0aUJCQ0MLbZ+SkkI2b95MOnXqxArgxIkTyf3798vF3j179rCrusaNG5N37959t71CoSDXr18nc+fOJR06dMiTRSX3R0tLi7Ro0YJYWloSNTU1oq6uTurUqVPoKri8uXz5MrGysmLtrV27dpVxIqOUL1T4yoGMjAz2DOjt27cVbQ45ceIEsbS0zJMlZNq0aRXuuFAV8Pb2Zs8/tbS0yC+//PLdlXFKSgrZunUr6dy5MyuAnp6eJDQ0lDBFSLlWGi5dusQKmImJSbFEl2EY8uDBA/L777+Trl27ktq1a+cRQYFAQAAQDodD+Hw+cXFxIZ8/fy7D2RQNhUJBNm3alMdea2trcu3atYo2jVKJoMJXDgQEBBCxWEzGjx9foXZcu3YtX7aVUaNGValsK5WBd+/e5dnya9Wq1Q8zuqSmppJt27YRBwcHVgDHjx9PQkJCylQAw8PDWScdPT09cuLEiRL39eLFC7Jq1SrSo0cPUq9ePTaIPjeUgsvlknr16pG+ffuS9evXV2hSg4yMDDJ58mR21auhoUFcXV2rbKIFinKhwlcOjBw5kojFYnL69OkKGf/hw4fEwcGBPQMRiUSkX79+P9z+ohSOQqEgy5Yty5PybNWqVT+8Ly0tjezYsYN06dKFFcCxY8eSO3fulJkAxsbGsvlUtbS0lLb99+HDB7J+/XrSrFmzPIH0ubGEQqGQ1K1bl3Tv3p2sWLGiTLPdFMarV6+Ik5MTa4+uri6ZM2cO3c6v4VDhK2OeP39OxGIxcXBwIBKJpFzHfvXqFenVqxcb9KuhoUGcnZ3LPdi6OhMeHp7nXKlDhw5FWlWkp6eTnTt35hFADw8PcuvWrTIRwIyMDNKxY0f2fTB//nyl9n/nzh1St25dIhAICI/HI9ra2gXGEtauXZt06dKF/Pbbb+T+/fvl5kB19uxZYmFhkUeQfX19y2VsSuWDCl8Zs2rVKiIWi4u0GlAWnz59IoMHD85T/qZ9+/aVIv6qOqJQKMjMmTPZWDlDQ0OyY8eOIt2bkZFBfHx8iKOjIyuAo0ePJjdv3lS6AJZFaaOvSUtLI25ubmz/lpaWZN26dWTMmDHEysqqwHJMhoaGpH379mTOnDnk8uXLZSqECoWCrF69mhgZGbHj29raftdJiVI9ocJXhmRlZbFODWWR1f9bkpOTiaenZ55sKz/99BM5f/58mY9Nyalv97WXbI8ePUhiYmKR7s3IyCC7d+8mTk5OrACOGjWKXL9+XekCOHfuXHY15uDgUOrSRt/ybaWH3JVVdnY2OX78OJkyZQoRi8UFlmPS1dUltra2ZMqUKeT06dNlUh8wJSWFjBkzhhVikUhEBg4cSM+6axBU+MqQ06dPE7FYTEaOHFmm42RlZZE5c+awBVZzM1kcPny4TMel5Cc7O5uMGTOGFZbatWuTgICAIt+fmZlJ9uzZQ7p168YK4MiRI8nVq1eVKoAbNmxgV6iWlpZK98gMDw8nTZs2Zd+PQ4YMyXeuJpPJyOXLl8msWbOIvb19nvfv1w5Y1tbWZNy4ceTw4cMkPT1dqTbmbv/mivSiRYvyiG1MTEyxX/eSFEymlC9U+MqQ8ePHE7FYTI4ePVom/ctkMrJ06dI82VYaNGhAtm7dSoPPK5gzZ84QU1NT9kxtyJAhxQqozsrKIvv27csjgMOHDydXrlxRmgAeP36cDVyvX7++0iseZGdnk2HDhrHvzaZNm343l6lCoSB3794lCxcuJA4ODnne17k/IpGIWFhYkGHDhpFdu3YpJYA+ICCAmJubs2OYmZmRI0eOkEOHDhEbGxuyadOmH/aRwsjIEUkcGZ3+jHRMfUg6pj4knf73346pD8no9GfkiCSOpDLlX+Gekh8OIYSAonTevn2LAQMGQF1dHefOnYNQKFRa3wzDYNOmTVizZg2+fPkCANDR0cGMGTMwa9Ys8Pl8pY1FKTnp6ekYNWoUzp07BwAwMjKCj48PHBwcityHRCLBsWPHsHv3bvZv3aRJE3h6eqJz587gcrmlsjE0NBR9+vRBSkoKRCIRDh06VCz7isKePXswa9YsZGVlQSAQ4M8//8TPP/9cpHsjIiJw4sQJXL16FREREUhMTMxzncPhoHbt2rC2tkbXrl3Rt29fmJiYFNtGuVyO5cuXY+PGjcjMzAQhBCoqKuByuTAzM8OePXvQokWLfPfJCAM/aRx8pbGQEwI+hwM1cMHlcNg2DCGQgGGvjxEYY7jACCqc0v3tKCWHCl8ZsW7dOuzbtw99+vTBb7/9prR+/fz8sGTJEkRHRwMANDQ04Onpid9//x0CgUBp41CUh5+fH2bPno20tDRwuVyMGDECGzZsKNYDSnZ2NgIDA+Hr64v4+HgAQOPGjeHp6QkHB4dSCeC7d+/g5OSEmJgYCAQCbNy4EcOHDy9xfwURFRWFfv364dWrVwAAR0dHHDhwACKRqNi2Hjt2DJcvX0ZYWBji4uLw7VeYoaEhWrRoAQcHB/Tr1w/m5uZF7v/Lly+YNm0ajh07BplMBkII1NTU4OLigsOHD+f5jMUxUszOisJrhQQaHB74X4ldYcgJQQZRwIynhrXq5jDi0s9sRUCFrwyQyWTo0aMHkpOT4evrC0tLy1L3efr0aSxYsABRUVEAAFVVVQwdOhSrVq0q9pcHpfyJj4/HkCFDcOfOHQBAvXr14OfnBxsbm2L1I5VKWQGMi4sDAJibm8PT0xOOjo4lFsCUlBQ4OTnh6dOn4HK5+PXXX7FgwYIS9VUYcrkc06ZNw759+0AIgZGREfbv3w97e/sS9xkXF4fAwEBcunQJDx8+RExMDBiGydNGR0cHzZs3R4cOHeDu7g5ra+vv9vny5Uv07t0br169Yvvicrno27cv/P39weFwEMdI4Zn5AomMDCIOD5wiiF4uhBCkEwX0uCrYLmxCxa8CoMJXBgQHB2P+/Plo1KgRDhw4UKwPxbfcuHEDs2fPRnh4OACAz+ejT58++Oeff2BgYKAskynlxIYNG/DHH39AIpGAz+dj2rRpWLp0abEFSyqV4sSJE/Dx8cHnz58BAGZmZhg/fjycnJxKJIByuRxubm64evUqAGDUqFHYtGlTqbdTvyUwMBCTJk1CWloa+Hw+5s6di0WLFiml79TUVJw8eRLnz5/H/fv38f79eygUijxtRCIRmjZtinbt2sHV1RX29vZ55rhw4UJs374dmpqaSE9PR0JCAhiGAZfLxZgxYzB3wXwsM5bhrUICTW7JjxXSGDka8tSwS2hRpNUiRXlQ4SsDpk6dipCQEMydOxeDBw8uUR9PnjyBl5cX7t69C0IIuFwunJycsG7dOtSvX1/JFlPKk7dv32LQoEGIiIgAAFhYWODw4cPF2pLLRSqV4tSpU9i1axdiY2MBAA0bNsS4cePg7OxcbNFiGAZTpkzB3r17AQBdunRBYGCg0s+NP336BHd3d4SFhQEA2rRpA39/f+jp6Sl1HIlEgqCgIJw9exahoaF48+YNpFJpnjZqampo1KgR2rZti+7du+Ovv/7Cq1evYGJiAg8PD4hEIhw+fBhRUVHQ0NCAxrh+kA/vAa1irvS+hRCCNKKAp2ptjFY1Lu1UKcWACp+SiYmJgZubGwQCAYKCgqClpVWs+1+/fg0vLy9cvnwZDMOAw+GgXbt2+Pfffws8XKdUTRiGwdKlS/Hvv/9CJpNBVVUVixYtwqxZs0rUn0wmw6lTp+Dj44OYmBgAQP369TFu3Di4uLiAx+MVq7+VK1dixYoVYBgGLVq0QHBwcLHfyz+CYRjMnz8fmzdvBsMw0NXVha+vL5ycnJQ6ztfI5XJcvnwZZ86cwe3bt/Hy5UtIJJI8NkmlUhBCoKqqinr16mH9+vVwcXFBeno69hz1h49zY+hqaBa4Srs3cBqyP3/J9/tWO/+CRuMG+e0hBFIwOCmyhCaHOqWVF1T4lMzmzZuxc+dO9OjRA0uXLi3yfbGxsZg9ezZOnToFuVwOAGjVqhXWrFlTqjMQSuUmPDwcQ4YMwZs3bwAAtra2OHToEIyNS7YCkMvlOH36NHbu3MkKYL169TBu3Dh07969WALo5+eHqVOnQiaToU6dOggODka9evVKZNf3uHjxIsaMGYPExERwuVxMmjQJq1atUvoWa0EwDIOQkBCcOHECN2/exJMnT5CamprHYYbL5aJdu3YYNWoUVPo7YQc/GVqFbHHmCp9eOxuomdRif28ytDdUDfULvCeVkWOmmin6CwyVOzlKoVDhUyIKhQK9e/dGfHw8tm3bViTHhdTUVMybNw+HDh1it2CaNGmCv/76Cy4uLmVtMqUSIJfLMWfOHOzatQsKhQIaGhpYtWoVPDw8StXnmTNnsGvXLnz8+BEAULduXYwdOxY9e/YssgBeuXIFgwYNQkZGBrS1tXHixAnY2tqW2K7CSEpKwoABA1jnH0tLSxw7dqxEoQml4ddff8X27duRkZEBhmEgl8vBMAyMjY1hYmIC/paFUGvaEEJOwa9frvBZLJ8N/Y52RRozkyjQkKsGHw0LZU6F8h2o8CmRa9euYdasWahXrx4CAgK+u/8vkUiwePFi+Pj4IDMzE0DOF9OSJUtKfC5IqdrcuHEDo0ePZs/qHB0d4efnV6otRoVCgbNnz2Lnzp348OEDAKBOnToYN24cevbsWaSzu6dPn6Jnz56Ij4+HmpoafHx84ObmVmKbvsfy5cuxevVqyOVyaGpqYvPmzXB3dy+TsQri77//xuHDh9G4cWO0bNkSLVu2hKmpKeLi4hAR+Qx+g2yho6qeJ07vawpb8TWcPrrQMRlCkAkGl0UtwaNOLuUCFT4lMmvWLFy7dg3Tp0/HqFGjCmwjl8uxatUqbNq0CSkpKQBy4o4WLFgAT0/PctneoVRepFIpxo8fj6NHj4IQAl1dXXh7e5daaBQKBc6dO4cdO3bg/fv3AAATExOMHTsWvXv3/qEAxsXFwdHREW/evAGPx8OqVaswefLkUtlUGCEhIRg6dCg+f/4MDoeDYcOGwdvbu1wSMxBCIJfLoaKiku9aDJONIRnPICpktQcUfsbX/trB746bThQ4qNEMJlzV4htNKTZU+JREXFwcevfuDQ6Hg7Nnz+bzTmMYBlu2bMHq1avZAGRtbW1Mnz4dc+bModlWKHk4ceIEpkyZgqSkJHA4HPTt2xc7duyAmppaqfplGIYVwHfv3gEAateuDQ8PD7i6uhb4hZ9LVlYWunfvjnv37oHD4WDq1KlYtWpVqewpjIyMDAwbNgzBwcEAgEaNGuHo0aMl8nxVFm8VEozOjIRGEYSvOFudQM52p6/QAg14pfv7UooGXV4oiVOnToFhGHTu3Dmf6B04cAAWFhaYO3cu4uPjoaGhgenTp+Pt27eYP38+FT1KPtzc3PD06VM4OTmBEIJjx46hefPmuH79eqn65XK56NGjB44cOYLly5ejYcOG+PTpE1asWAF3d3cEBATkc/fPRV1dHZcvX4abmxsIIdi4cSOGDRuWL2BcGWhoaOD48eNYtWoVBAIBXr16hTZt2sDX11fpY33Ny5cvsWfPHoSEhCAjIyPPNa6CQVmtEhgAArrNWW7QFZ8SYBgGffv2RUxMDDZs2MB6YZ49exbz589n0zTRbCuUkrBnzx7MnTsX6enpbBD1v//+q5QHJoZhEBwcjB07duD169cAcnKKenh4oE+fPoWmwfvll1+wadMmEEJgZ2eHs2fPQl1dvdT2FERERAT69+/PnlG6urpiz549ZZKib+bMmTh79iy4XC74fD74fD7kcjnq1q2LuIQvkBz+C3V09Yt9xmfUqws0zAv2iKVnfOUPFT4lEBISgqlTp8LY2BgnTpzAnTt3MHv2bDx58gRATrYVV1dX/PPPPzAyMqpgaylVkdjYWAwdOhR3794FADRo0AAHDhz4YfqtosIwDC5fvozt27ezD2pGRkYYM2YM+vbtW6DIeHt7Y/78+VAoFDAzM8PFixfL7P0tk8ng4eGBY8eOAQBMTU0REBCglHSAubx//x7Tp0/HuXPnkJ2dzYY0cDgc6OjooEmTJtDasxzS2vo/9Or8lu9tfVKvzvKHCp8SmD9/PoKDg9G7d29cv34dISEhbLaVrl274r///qPZVihKYd26dVi6dCkkEglUVFQwY8YM/P7770pzimIYBleuXMGOHTvw4sULADnOV6NHj4a7uztUVfM6X5w4cQIeHh6QSCQwNDTE2bNn0axZM6XYUhB79+7FrFmzkJmZWexKD9/y5s0bBAQE4NKlSwgPD0dCQgIUCgUbwA4AKioqMDExwbp169C9e3ec4qRhXfbHQuP4SgKN4yt/qPCVkqSkJHTp0gXv3r1js7lzOBzY29tj3bp1NNsKRelERUVh8ODBePbsGQCgefPmOHToEMzMzJQ2BsMwuHbtGrZt28YKoL6+PkaPHo3+/fvnEcB79+7Bzc0NKSkp0NDQwOHDh5Ve2uhr3rx5A3d3d7x8+RJATlq1gwcP/vD4ICoqCkeOHMHVq1cRHh6er8QRl8uFiYkJvnz5gszMTNSrVw+9evXCsmXL2LJiaUQO1/RwCMBVSn5NmrmlYqDCVwri4uLg7u6OO3fugMvlQiAQoGXLlli7di3NtkIpUxiGwR9//IH//vsPcrkcampq+P333zF9+nSljkMIwfXr17Ft2zZERkYCAPT09DB69Gj069ePPdd79+4dunXrhujo6DIrbfQ1crkcP//8M/bu3VtopYfnz5/j6NGjuHz5Mp4+fYqkpKQ8ffB4PNStWxdisRguLi7o06cPRCIRxo0bh/v372PUqFHw8vLKt5r2zY7FNkkMtLn8UufqTCUKTKC5OssdKnwlIDU1Fb/88gsOHjyItLQ0EELQoEEDbNy4ET169Kho8yg1iEePHmHYsGFsaEKbNm1w8OBBpZ+1EUJw48YNbN++HU+fPgWQI4AjRozAwIEDoa6ujtTUVHTt2rVMSxt9y4kTJzBhwgS21qGdnR14PB4iIiLYONlceDwe6tevD1tbW3Tv3h29e/eGhoZGvj6/fPmC6OhotGzZMt81hmGwfPUq7BYbQadlcxgJS+6kRqszVBxU+IpBdnY2m20lN6URIQRNmjTBgwcPaFgCpUKQy+WYMWMG9uzZA4ZhIBKJ8M8//5TJiosQglu3bmHbtm1sdQkdHR2MHDkSAwcOhEAgQJ8+fXDlyhUAwMiRI+Ht7V0miRmePHmCY8eO4cKFC7h79y5kMhkAsLsvAoEA9evXh52dHXr06IGePXuyW5bFRSaT4ebNm5g6dSoyMjJgbGkBbd9lyBTwaD2+KggVviIgl8vx999/Y8OGDXmyrbRo0QKxsbEYO3Yspk6dWsFWUmo6V65cgYeHB1ugtlu3bti3b1+ZhM4QQnD79m1s376dLS2kra3NrgDnzp2LPXv2AAAcHBxw/PjxUj8YPnjwAIGBgbh+/TqePXuGtLS0PPYoFArI5XJwuVxoaWlh9+7d6N27d4nHYxgGQUFBuHLlCq5fv46HDx9CKpVCXV0dN2/ehIl18xJVYM/8XwX2NbQCe4VBhe87MAyDrVu3YtWqVXmyrfz888+YNGkSevfujezsbAQGBsLU1LSCraVQcnLAjh8/HoGBgSCEQE9PD1u3bkXPnj3LZDxCCEJCQrBt2zY2fEdLSwsjRozAhw8fsGbNGjAMg+bNm+PixYtFzjvKMAwePHiAY8eO4caNG4iMjER6enqeNioqKjAzM0Pr1q3Rs2dPdO/enc13mlvpYcKECfj7779LtOI8deoUfv31V3z+/Bnx8fGsp/aMGTPwzz//AABkhMF+aRx8pLGQEwI+hwM1cPPE+TGEQAKGve4hMMZwQS26vVmBUOErhEOHDmHx4sVsZnsNDQ14eHhgyZIlUFNTw+HDh7F69WrY2dlh8+bNFWwthZKXEydOYPLkyUhOTgaHw8GAAQOwbdu2Mgn6BnIEMDQ0FNu2bcOjR48A5AigmZkZ/P392dJGFy5cKDC05+vyQDdu3MDz58/zZU4RCAQwMzND27Zt0atXLzg7Oxe4ikxOTsaAAQNw+/ZtAECLFi0QGBhY7EoPsbGxbD+52WmaNGmCq1ev5isblUbkOC9LwilZAqKYnPp+XORkZAEAc64aeqvow1lFl3pvVgYIJQ9BQUGkZcuWRCgUEqFQSHR1dcmkSZNISkoK24ZhGDJ06FAiFovJuXPnKtBaCqVwkpKSSK9evdj3spmZGblx40aZjskwDLl79y7x9PQkYrGYiMVi0qpVKyISiYi6ujqpXbs2CQ0NJQqFgty4cYPMnTuXtGvXjujr67N25v7o6OgQW1tbMm3aNBIUFERkMlmxbFmxYgXR0tIiQqGQGBkZEX9//2Ld//DhQ2JoaEi4XC7hcrlEX1+fTJo06Yf3yRmGRCsk5I08i0QrJETOMMUal1L2UOH7H7du3SL29vbsh05LS4sMHz6cfP78OV/biIgIIhaLiaOjI8nOzq4AaymUorNr1y5iaGhIhEIh0dTUJF5eXsUWkZJw7949MmHCBCIWi0nz5s2JiooKKyKampr5hE5XV5fY2dkRLy8vEhwcrBQb7969S8zMzIhQKCQaGhrE09OzSP1ev36dfc1q165NfvrpJ9KiRQty6tSpUttEqXiqlPCVxZNUeHg46dq1K9HQ0CBCoZCIRCLi5uZGXr9+Xeg9y5cvJ2KxmKxdu7bU41Mo5UF0dDTp2LEjKzItWrQgYWFhZTaeTCYjFy9eJF5eXqRFixZEVVWVcDgcAoD9UVdXJ23btiUzZ84kly9fJgqFokxsycjIIG5ubuzcraysyKtXrwptf/r0aaKrq0uEQiFp0qQJiY6OJuHh4cTHx4fI5fIysZFSvlR64UthZOSIJI6MTn9GOqY+JB1TH5JO//tvx9SHZHT6M3JEEkdSme8/xUVHR5P79++z///27VvSp08fIhKJ2KdBR0fHH34ZZGRkkI4dOxKxWEyioqKUMkcKpbxYs2YN+6Wura1NlixZohTBkclkJCgoiPz888/Ezs6OHePrH21tbWJkZERUVFQIh8MhXC6XODk5keTkZCXM7Mds3LiRtUtfX5/s2rUrXxs/Pz+ira1NhEIhsba2JklJSeViG6V8qbTOLTLCwE8aB99ieEuNERhjuMAIKpy8HlyJiYkYPHgw4uPjsXbtWvj6+uLEiROQy+UAAGtra6xZswbt27f/oV3Hjx/H0qVLYW1tjV27dil30hRKOfDy5UsMHjwYz58/BwBYWlri8OHDxconK5fLceHCBZw6dQohISGIiorKV85IQ0MDTZo0Qfv27dG3b1+0adMGXC4Xjx49wqBBgxAVFQUgJxXab7/9hpEjR0JHR0dp8yyIiIgIDBgwgC3G+3Wlh82bN2PevHlgGAbW1ta4ePFiieP+KJWbSil8cYy0RPExGf+Lj1n7VXwMIQReXl44duwYEhISIJfLWU+wRo0aYeXKlcVy9fbw8EBYWBh+//13uLq6lmyCFEoFwzAMFi1ahE2bNrEpz5YsWYJp06YV2F4mkyEoKAhnzpzB3bt3ERUVxQaM56KhoQELCwt06NAB7u7uEIvF3w0jGDduHBt0LxQKYW1tjcGDB2PkyJHQ1dVV6ny/Ri6XY8yYMWzIR506ddC9e3fs2rULhBDY29sjKCiIJqSoxlQ64YtjpPDMfIFERqaUjAj79+/H9OnTkZCQACCnxIiJiQlWrVpV7MwWucmBNTQ0EBQUVGb1xyiU8uLBgwcYNmwYW+vO3t4eBw8ehJaWFs6ePYvTp0/j7t27ePv2bT6h09TUhIWFBTp27Ah3d3e0atWq2PFymzdvxpw5cyCVSsHn89G0aVOIRCIMHDgQI0eOzFfUWZns27cPM2fORHJyMuRyOVRUVODq6oojR46USaYZSuWhUgmfjDAYm/kcbxUSaJai7EcaI0cDrhrsdl/EnJmz2A8sh8OBSCRC7969sX///mL3u2bNGhw8eBADBgzA/PnzS2wfhVKZkMvlmDx5Mvbu3ctu/wsEgnwPnVpaWmjWrBk6deqEvn37olWrVkoZP7e0UWZmJlRUVFC3bl2oq6tDVVUVAwYMwKhRo6Cvr6+Usb5lyJAhOHz4MAgh4PF4cHZ2xuHDh2mh6GpOpRI+3+xYbM/+BK1irvS+RS6X4c2XOEi2+iN67S7weDzo6enBwMAAPB4PDRo0wPHjx4s1hlQqRffu3ZGamop9+/bBwoIWjaRUXTIzM3H69GmcOXMG9+7dw/v375GdnZ2nFp2amhratm2LLl26oF+/fkot+vot9+/fh5ubG5KTk6GqqgonJye2IrxAIGAF0MDAQCnjMQyDwYMH48yZMwByjj2ioqJACIGhoSEOHDhAK6xUYyqN8KUSOdwKqXOVW9WYw+GAqyoAX1sTms0awWRIL2g2b5yvr/S0NLyPiQFPXQ3DT4bB1dEJIpEIKioq4PP5MDY2hpqaWrHsCwoKwqJFi2BhYYF9+/aVaq4USnmTnp6OkydPIigoCPfv38f79++hUCjytNHR0YGFhQXi4uIQFRUFDocDfX19bN++HS4uLmVu4/v37+Hk5ITo6GioqKhg4cKFiIuLw+XLlwHkCGC/fv0wevRoGBqWvGirXC5Hr169cOPGDXA4HMyfPx+LFi3CiRMnMHHiRKSmpoLP52PWrFn4/ffflTU9SiWi0giff3Z8oZWNc4VPr50N+DpaSHvyHFkfP4HD46LJ4ukw6NI27w0EyM6WQKLCwyz1ukqpbDxx4kTcv38fCxYsQP/+/UvdH4VSlqSnp+P48eM4e/YsHjx4gA8fPrBpt3LR09NDixYt0LlzZwwYMACNG///Q2RAQAB+/vlnpKSkgMPhYODAgdi6dWuZpTzLJTU1FU5OToiIiGBLG/Xv3x87duzApUuXAOQIYN++fTFmzJhil1+SSCRwdHTE48ePweVysXLlyjwOPbGxsejXrx8eP34MALCzs4O/v7/SVpqUykGlEb4xGZF4y0gg5PDyXcsVPovls6Hf0Q5EocCLpRvx5dJt8DU1YBvgDZ6aar77MokCDblq8NEo3bbk+/fv0a9fP6ipqeHcuXMF1vCiUCqSlJQUBAYG4vz583jw4AE+fvyYT+j09fVhaWnJbl2am5t/t8/ExESMGDECV69eBQDUqVMHe/fuRZs2bcpsHkDOiqxv377sSm/48OHYsmULXr9+jR07duDixYsghEBFRYUVwFq1av2w3+TkZDg4OODly5fg8/nw9vYu0MGNYRgsXLgQmzZtgkKhgI6ODnbt2lUuq15K+VAphE9BCLqkP4bwmzi9XL4VPgDIfBeNhyNnAwBarP0VOnbW+e5jCEEmGFwWtQSvFGeG69evx549e+Dq6kq3PiiVgsTERFboHj58iJiYmHxCZ2BgACsrK3Tp0gUDBgwoVpze1+zYsQMLFixAZmYmeDweJk6ciFWrVpWp5yPDMJg2bRp2794NAOjcuTNOnDgBPp/PCuCFCxdACAGfz4ebmxs8PDxQu3btAvv7/PkzOnfujA8fPkBVVRW+vr5wc3P7rg1XrlzByJEj2UoPnp6eWLNmDfX4rAZUCuGLYbIxJOMZRAWs9oCChU+RLcWdbqMAAE0WTYWhc8cC700nChzUaAYTbv4VYVGQy+Xo2bMnEhMTsWvXLlhb5xdYCqWs+fLlC1t09dGjR4iJicG3H11DQ0O0bNkSXbp0Qf/+/VG3bl2ljf/x40cMGTIEDx8+BACYm5vjwIEDaNGihdLGKIiVK1dixYoVYBgGzZo1w6VLl9jSRq9fv8bOnTtx/vx5VgBdXV3h4eGRpxLDmzdv4OjoiLi4OAiFQhw5cgQODg5FGj85ORkDBw7ErVu3AORUejh69CgtQ1bFqRSPLlJCUNz1WHZsPPtvFV3tQttx/9d/Sbl27RoSExNhZmYGKyurEvdDoRSHuLg4bN26FQMGDECjRo3QoEEDeHl54fTp04iOjgYA1KpVC926dcNff/2F58+f4+3btzh+/Di8vLyUKnoAYGpqihs3buC3336DQCBAVFQUOnTogJUrVyp1nG9ZsGABtmzZAoFAgGfPnsHW1hbv3r0DAJiZmWH58uU4cuQIevToAYZhcOzYMbi7u2Pp0qWIjo5GREQEOnbsiLi4OGhpaeHcuXNFFj0gx+HnwoULWLRoEfh8PiIiIiAWi+Hv719GM6aUB1VyxVfUMz6g9Cu+6dOn49atW5g1axaGDRtWoj4olB/x6dMnHD16FBcvXsTjx4/x+fPnPCs6DoeDWrVqoVWrVujatSv69euXryZceREZGYkhQ4bg5cuXAICWLVvi8OHDZboKunLlCgYPHoz09HRoa2vj+PHjsLOzy9Pm/fv32LFjB4KCgsAwDDIzM/H+/XswDAMDAwMEBwejSZMmJbYhNDQUQ4YMQWxsLDgcDoYOHYrNmzfTDC9VkEohfEU94yuyV+f/KO0Z36dPn+Dm5gY+n4+zZ8+WeR5BSs3h48ePOHbsGIKDg/HkyRO2wncuHA4HtWvXRqtWreDk5IT+/ftXKs9ChmHY1ZhcLoe6ujqWL1+OiRMnltmYz549Q48ePRAfHw81NTX4+PgUeE73/v17/Prrrzh48CDrBDNlyhR4eXmhXr16pbIhMzMTw4YNw4ULFwDkbPkeO3bsh45ClMpFpRA+oGhenRwOB1yBAHwdTWg2b5wTx9esUaF9ltarc9u2bdi2bRucnZ2xYsWKEvVBoQDAhw8f4O/vj8uXL7NC9zW5qfRsbGzQrVs39OnTp1IJXWGEhoZi+PDh7PZrhw4d4OfnV2a2f/nyBY6OjoiKigKPx8OKFSvy5Rf19/eHp6cnsrOzIRKJYGpqCi6XCy6Xi+7du2P8+PGlFkBvb28sWrQI2dnZUFdXx99//w0PD49S9UkpPyqN8H0vjq+kpDJyzFQzLVEcH8MwcHV1xefPn7F58+Z82yoUyvd48+YNjh49ikuXLiE8PBxfvnzJc53L5aJOnTr46aef4OzsjL59+5ZpYuayRC6XY8qUKThw4AAYhoGWlhY2bNiAAQMGlMl4EokEPXr0wN27d8HhcDBlyhSsXr0aQI4H6qxZs6BQKNCiRQtcunQJKSkp2LVrF06dOgWFQgEulwsXFxeMGzcODRo0KLEd36v0QKncVBrhSyNyuBaSuaUkyAmBFAxOiiyhySm+mN68eRMzZsyAqakpjh49Sl2YKd8lKioK/v7+uHr1KsLCwpCYmJjnOpfLhampKcRiMSt0ud6J1YULFy5g/Pjx+PIlZ3emR48e8PHxKZO8lwzDYNSoUTh27BiAHNGxsbHBn3/+CUIIWrdujXPnzuURoZiYmDwlyTgcDpydnTF+/Hg0bNiwRHbI5XJ4eHjg2LFjbKUHf39/6v1dyak0wgcoL1cnIQSpRIEJqrUxWrVkDgBz5szBlStXMHXqVLqFQcnHixcvEBAQgCtXriAiIgJJSUl5rnO5XNSrVw9isRguLi7o06dPjUh8nJ6ejrFjx+LMmTMghMDAwAA7duxAt27dymS8BQsWYMOGDcjOzgbDMFBVVUW3bt0QEBBQqNPJp0+f4Ovri+PHj7MC2K1bN4wfPx5mZmYlssPPzw9eXl7IzMyEQCDA77//Di8vr1LMjFKWVCrhU2Z1hoY8NewSWpRo9ZiQkIAePXoAAM6cOVMlzlooZUtERASOHTuGq1ev4unTp0hOTs5zncfjoV69erCzs0P37t3Rq1evGiF0hXH48GHMmDEDqamp4HK5GDp0KLy9vcvEA7Jz5864du0aAEBbWxtPnz7NE8dXGJ8/f4avry8CAwMhk8nA4XDQtWtXjB8/Ho0aFe47UBjv3r1D37598eLFCwCAg4MDDh06VKPfB5WVSiV8gPLr8ZUEX19fbNy4EQ4ODlizZk2J+qBUbcLDwxEQEIBr167h6dOnSE1NzXOdz+ejfv36sLW1Rc+ePdGzZ09arfsbvnz5gmHDhuHmzZsAclKe+fn5Ke28nGEYjBgxAsePH2cTbvN4PBgYGODMmTNFDq6Pi4tjBTC3iryjoyM8PT3z5C8tqk0///wzdu/eTSs9VGIqnfABJa/Anvm/CuxrvqrAXlwYhkG/fv3w8eNH/Pfff2jfvn2J+qFULR4+fIjAwEBcu3YNz549Q1paWp7rfD4fDRo0QJs2bdC9e3f07Nmz2BU+aipbt27FwoULkZWVBT6fj4kTJ+Kvv/4q1bm5XC5Hnz59cOXKFXA4HMyePRt9+vSBq6srkpOToaGhgYMHD8LR0bHIfcbFxWHPnj04evQoK4AODg7w9PRE06ZNi2UfrfRQuamUwgfkbHvul8bBRxoLOSHgczhQ+yrOTyaV4ktCIkQGeiC8HIcYD4Exhgtqlco5JjQ0FJMnT4aRkRFOnTpFnVqqIQzD4OHDhzh69Chu3LiByMhIpKen52mjoqKChg0bonXr1ujVqxe6d+9OvfVKwfv37zF48GA8efIEANC4cWMcOnSo2IIC5NTGdHJywv3798HlcvHnn39i5syZ7DhflzbauHEjRowYUaz+v3z5gj179sDf358VwE6dOmHChAnFqsMZFxeHvn370koPlZBKK3y5pBE5zsuScEqWgChGAiAnDVlyaiqSk5JQn6hgelM7OKvolsh781sWLlyIc+fOwdPTs0yDcSnlB8MwuHv3Lo4fP46bN28iMjISGRkZedoIBAKYmZmhbdu26NmzJ1xcXGhGjjJg5cqVWL16NaRSKQQCARYsWIB58+YV+f709HQ4ODjg2bNn4PP5WLduXT7ns29LG82fPx8LFy4stq0JCQmsAGZnZwMAOnbsCE9PTzRv3rxIfdBKD5UUUoWQMwyJVkjIG3kWOfMghNRr2IDY2toqrf/k5GTStm1bYmtrS2JiYpTWL6V8USgU5ObNm2Tu3LmkXbt2xMDAgAiFwjw/Ojo6xNbWlkydOpUEBQURmUxW0WbXGMLDw4mVlRX7t2jfvj358OHDD++Lj48nzZs3Z/9+/v7+hbaVyWSkd+/e7Bienp5EoVCUyN6EhASybt060r59eyIWi4lYLCbTp08n4eHhRe7j8uXLxNTUlAiFQiISiYiXl1eJ7aGUniolfF+TmppKeDwe4XA4ZOfOnUrp08/Pj4jFYvLzzz8rpT9K+aBQKMjVq1fJ7Nmzib29PdHX188ndLq6usTOzo5Mnz6dXLhwgQpdBaNQKMisWbOIpqYmEQqFxNDQkOzYsaPQ9u/evSNmZmZEKBQSfX19EhwcXKRxpkyZwr4HevToUaq/e0JCAvnvv/9Ihw4dWAH8+eefyZMnT4p0f1JSEunWrRtrj62tbZEEn6J8qqzwhYWFEYFAQAAQU1NTcvfu3VL1xzAMGThwIBGLxeTixYtKspJSEF+v3KMVEiJnmOLdL5eTS5cuES8vL9KmTRuip6eXT+j09PRImzZtiJeXF7l8+TKRy+VlNBtKabh16xZp1KhRHnFKSEjI0+bp06fsaqlWrVrF/qz/9ddfRCQSEaFQSMRiMUlKSiqVzUlJSWTDhg2kY8eOrABOnTqVPH78uEj3r1ixgmhraxOhUEiMjIzIkSNHSmUPpfhU+jO+wjh48CA8PT2Rnp4OFRUV2Nvbw8fHp8QBqE+ePMHYsWOhp6eH06dPQ0VFRckW12xSiRznpUk4JU/A6/+d1XIA5L75zLhq6M3Xh4sg/1mtXC7H5cuXcerUKdy+fRuvXr1iz1xyUVdXR5MmTdCuXTv06dMH7du3p45JVQSpVIpJkybh8OHDIIRAW1sbGzZsQP/+/XHv3j307t0baWlp0NPTQ1BQUIlqAO7fvx9Tp06FVCqFiYkJgoODS1yYN5eUlBT4+fnh4MGDyMzMBAC0bt0aEyZMQKtWrb5777179zB48GC20sOQIUOwZcsWeq5cTlRZ4Vu0aBG8vb2RlJTEZrLv3Lkz9u/fX6L+/vzzT5w4cQKjRo3C9OnTlWxtzUVGGPhJ4+BbiHcukFNFQwKGvT6KbwSja08QdPIU7ty5g6ioKNa7LhehUIimTZuiffv26NOnD9q2bUuFrooTFBQET09PJCYmgsPhwM7ODmFhYcjKyoKxsTEuXbpUKrG6evUqBg0a9N3SRiUhNTWVFcBcpyk7Ozt4enrCxsam0PsyMzMxYsQInDt3DkBOfcHAwEBa6aEcqLLCN3DgQNy8eROxsbEAAEtLS4wYMaJYHmK5ZGRkwMXFBRKJBEePHi115nZKDkWOxyQEKSkpSElJQbokCzIVHuRRH5Ax82+Q+JxUYBoaGrCwsED79u3h7u4OW1tbKnTVkPT0dIwaNQqnT5+GVCoFh8NB3bp1ce/ePRgaFj/Z/Lc8f/4cLi4ubGmjXbt2oU+fPkqwPEcADx48iP3797PhMWKxGBMmTIBYLC70vs2bN2PhwoXIzs6GmpoaVq9ejXHjxinFJkrBVFnh8/b2xr1793Du3DkkJCTgl19+wZIlS0rUV0BAAFauXAkbGxts27ZNyZbWTL6XgYcwBKmpKUhOSUFmRgays7Px7duQq60JdYkMbhefY7hLL/z0009U6GoIvr6+mDJlCrKyssDhcKCuro5Ro0Zhw4YNStkK/La00fLly/Hzzz8rwfIc0tLScODAARw4cIBNhGBjYwNPT0/Y2toWmI3q6dOn6N+/P1vpoVevXtizZw9NklBGVFnhy2XChAnw8/ODvb09goODS9THiBEjEBkZiaVLl7I5Oikl59ucq4Rh2BVdRkYmpNIChI7Hg7qaGkQiTejq6kBdKCx1zlVK1eO///7DokWLwDAMrKysoKamhtDQUABAvXr14Ofn993tw6IikUjQs2dPhISE5CttpCzS09PZFWBuyrtWrVphwoQJsLOzyyeAcrkcY8eOxdGjR9lKD4cPH/7heSGl+FR54Tt58iSGDBkCLS0tfPr0qdj3R0ZGYsSIEdDS0kJQUBDNzqEEtqe9x6b098hOSEJWRgakUmk+oePxeFBTU4empgi6urpQU1fP1w8hBGlEAc9SVNmgVB2WLFmCv//+G4QQdO7cGSdOnACfz8fGjRvx+++/QyKRgM/nY9q0aVi6dGmpdwAKKm20f/9+pe8sZGRk4NChQ9i3bx8rgNbW1pgwYQLatGmTTwD9/Pwwc+ZMZGRkQCAQ4LfffsOsWbOUalNNp8oLn1wuh4GBAWQyGa5fv17sp8G//voL/v7+GDJkCObMmVNGVlZv0tPTcerUKQQFBSH0WQSSvOeBZGcDcgXbhsfjQV1dHZqamtDR0UH4qLnI/vwFFstnQ79j4Q4Gpa2rSKkazJgxAzt27ABQsAC9e/cOAwcOREREBADAwsIChw8fVoojyK+//or169eDEAKxWIygoKAySTiemZnJCmBKSgoAwMrKCp6enrC3t88jgO/evYO7uzueP38OIKcCxeHDh2mlByVR5Q9N+Hw+W0IkICCgWPdmZWXh7NmzAIC+ffsq27RqS3p6Ovz8/DBq1Ci0aNECtWvXxrhx43DkyBHEWNQBeBzwCAcikSZq166N5s2bw7plSzRu0gTGtWsXuLorDD6HAzkhOC9L+nFjSpWDYRiMHj2aFb1Ro0bh4MGD+VZd9evXx507dzBv3jyoqKggMjISdnZ2WLt2baltWLFiBdauXQs+n4/79++jdevWrNOcMhEKhfDw8MDJkycxffp06OjoICwsDNOnT8eYMWNw8+ZNdmekfv36uHfvHsaMGQMOh4OrV6/C0tKSrXRBKR1VfsUHANOnT8fOnTshFovZulxF4eTJk1iyZAksLS3h6+tbdgZWcVJTUxEYGIhz587hwYMH+PjxIxiGydNGT08PVlZWSPtrKhR1jKCj+v1D+XsDpxVpxQcAmUSBhlw1+GgUPUEwpfLDMAzc3d3Zs/np06dj5cqVP7wvPDwcQ4YMwZs3bwDkeE4eOnQItWvXLpU9p0+fxqhRoyCRSKCvr4+zZ8+WKGawqGRlZcHf3x979+5FYmIiAKB58+bw9PREhw4d2BXgt5UeZs6ciT/++AMAIJPJwOPxqONXMakWwnfp0iW4urpCKBTi8+fPRX4TjBs3Do8fP8Zvv/2mNJfm6kBSUhICAwNx/vx5PHjwADExMfmEzsDAAJaWlnB0dES/fv3QsGFDKAhBl/THEH4Tp1cQxRE+hhBkgsFlUUvwqJNLtUAmk8HFxYV1Llm8eHGxQpHkcjnmzp2LnTt3QqFQQENDA6tWrcqXsLq4PHjwoFSljUqCRCJBQEAAdu/ezQqghYUFPD090alTJ3A4HMTFxcHd3R2PHj0CANja2sLb2xuzZ8+Gqakptm3bRsWvGFQL4WMYBoaGhpBIJLhw4QLatWv3w3tev36NQYMGQSgUltmeflXhy5cvCAwMxIULF/Dw4UPExMTkc0YxNDSEtbU1K3QFxTrGMNkYkvEMIg7vh2MWR/hSU1LwJSsDXfyuw0DOgVAohIaGBoRCYZ6f3N9paGhAXV2d/X9VVdViFTSmlC3p6elwdHREREQEeDwe1qxZgwkTJpSor5s3b2LUqFHs1qSjoyP27dsHbW3tEtv34cMHdO3alS1ttH79eowaNarE/RWV3Dji3bt3IyEhAQDQpEkTeHp6onPnzgCA3377DRs2bIBCoQAhBBoaGjAyMsJ///0HJyenH46hIASfiRRSQiDgcFCLI6iRD5PVwluAy+WiadOmePz4MY4ePYoGDRrAwMDgux6agYGBAAAXF5caJ3pfvnxBQEAALly4gMePH+PTp095hI7D4cDIyAgtW7ZE165d4e7uDlNTUwDA3bt3MWTIEJibm8Pe3h7t27eHpaUlOBwO3sZEg2gyAO/HwlccUlJSIVFIcfvePfA/fSn2/Vwut8giWZCofvv/VEhLTmJiIjp37ozXr19DRUUFW7duxeDBg0vcX/v27fHs2TN4enoiICAAly5dQvPmzbF582a4ubmVqM+6deviwYMHrDhPmTIF79+/x6JFi0psZ1FQU1PDsGHD0L9/fxw7dgy+vr548eIF5s6di8aNG8PT0xNLly5Ft27d4O7ujpSUFGRlZSEjIwNbtmyBo6Njgau+0qQLrK5UixXfly9f4OnpiVOnTkEoFMLMzAyurq74888/87RLTU3FihUrUKtWLbbG1p49e4pcW6uqEhsbi6NHj+LixYt49OgRPn/+nE/oatWqhZYtW8LJyQn9+vWDsXHB4QPHjh3DnDlzEB8fDw6HA4ZhoFAooKenB0MrC6T9OxNNTH+c+aY4Kz7CMEjIzsLcyCwIUzORkZGBzMxMZGbm/Dv3w5/7/7nXcn++TXdWWr4W0h+J5LciW9A1gUBQI4T048ePcHR0RHR0NNTU1LB//36l1qU7deoUJk2axKYx7Nu3L3bs2FHiIHC5XI7+/fuzZ5DDhg3D1q1by21LUSqVIjAwEL6+voiLiwMANGrUCF27dsV///2HyMhIKBQ5ntNqamrYvn17nqK7JUkXOEZgjOECI6hwqve2aZUXPqlUCmdnZ0RFRSE6OhoAUKtWLYwdOxbLly/P0zYkJATjxo3Dly9fkJqaChMTE2zatAnOzs7V6osnJiYGAQEBuHjxIp48eYK4uLh8QmdsbIxWrVqxT49GRkYF9vXx40dcvHgRISEhCA8Px8uXLwsUTqFQiI6dOyN6y1yY6OhVqjM+uVxeZJH89ncVJaTfE9AfiWxlFNJXr16ha9eu+PLlCzQ0NHD8+HHY29srfZzU1FSMGjUKFy5cAJDzXeDr64tOnTqVuM+pU6eyzm+dOnXCyZMnyzWZtFQqxfHjx+Hj44O4uDikp6fj8+fPEIlESEtLY2MD1dXVcfv2bVhbWxc9XeBXyAlBBlHAjKeGtermMOJW35jmKr+uVVFRgbm5Od68eZPny9jS0jJfWy0tLQA5lZUZhsGXL18wb948CAQCdOnSpdxsVjYfP36Ev78/Ll++jMePHyM+Pj7PdQ6Hgzp16qBVq1ZwdnZG3759YWBgkKcNwzAICwvDlStXEBoaisjISHz48IHNOZgLIYT9UuVyudDU1ETr1q3h7e0Nc3NzjMmIxFtGAiGUt90pAQNzrlqJzyL4fD60tLTYv39pKYmQftv2699JpVKkp6fne61LCpfLVcpKNPffpRXSR48eoWfPnkhJSYGOjg7OnTtX4OdTGWhpaSEwMBD79u3D7Nmz8fnzZ/Tq1QtjxozBv//+WyLB2rRpE+rXr4+lS5fi2rVraNu2LYKDg6Gjo6P8CRSAQCDAwIED0adPH5w8eRI+Pj5QVVVFdnY2jI2Noaenh9u3b4PL5WL8+PGYu2o59tnVRiIjg9Y36QK/B5/DgRZ4eKuQwDPzBbYLm1Rb8avyKz4g59zAw8MDZ8+ehUwmg56eHq5cuQIrK6s87eLi4uDg4IDnz5+zFR1++ukn+Pj4QF9fv4KsLz7v3r2Dv78/Ll26hPDwcHz5kvfcK1fobGxs4OzsjD59+kBPT4+9LpVKcePGDVy/fh0PHz7Eixcv8OnTpwJXMjweD0ZGRjA3N0fLli3Rrl07+Pr64sGDBzA0NMTMmTMxfPhw9sPlnx2PddkfocVV3jNVKiPHTDVT9BeUPklxZUQmk+UTxdIKqTL5WkiLeyb67NkzzJ07F1lZWTAwMMD58+dhYWFRLivSuLg4DBkyBCEhIQByYuMOHjwIa2vrEvV34MABTJkyBVKpFLVr18aFCxfQsGFDZZpcJGQyGSuAudmq6tWrB0NDQ4Q+fAD1HX+A28AEmqX4DFb3dIHVQvgA4P379+jUqRPevXsHoVCIpKSkfM4tMpkM5ubmiI6OhpaWFnr06IENGzZUetGLiopCQEAArly5gvDwcNbjKxculwtTU1PY2NjAxcUFffv2ZVc3X758wZUrV3Dr1i08efIEb968QVxcXL7wBABQVVWFiYkJmjZtChsbG3Ts2BHt2rXL95QcEBCA48ePY/bs2WjZsmWea2lEDtf0cAjAVcoHhmZuKT4lFdLCrslkshLZkZKSwu7EqKiooGnTplBRUQGPxyvySrQgkf36dxoaGlBRUfmukP7333/4888/IZFIoKKigunTp+OPP/4o0VndtWvXMHDgQLa00bFjx9CmTZsSvT6lRSaT4fTp09i1axdiYmIAAE1+n453nS2hxeGX6uGiuqcLrDbCBwBHjx7FwIEDIRQKkZycDF4B3oXGxsZITk6Gq6srfH19oaGhUQGWfp+XL1/C398fV69eRXh4OJKS8mYt4XK5qFu3LsRiMbp3744+ffpAJBIhKioKFy9exN27d/H06VO8ffuWTY30LVpaWqhbty6aNWsGOzs7dOnSBc2aNVPKwb1vdiy2Z38q1jbLtxCGATgcpBIFJlTTD19V4Vsh/d5qM/d3oaGhOH/+PBiGgUgkgr29PaRSaamEtDCKIqSZmZnYvXs3u0KqX78+li9fjiZNmuRr+yMhff78Obp37464uDioqqpi586dcHd3V+qcCuPro4Zc5HI5zpw5A/9zZ/B00QjoibQKfOhMe/oS0X4nkBr+AvLUdKhoa0LY0BTGfbtBv3N+8a7OD53VSvgAoEePHvj8+TN89+6BQbPG+eJVlixZgqioKGzfvh2qqqoVbS4A4NmzZzh69CiuXr2KiIgIJCcn57nO4/FQr1492NrawsXFBT169EBkZCSuXbuGBw8e4NmzZ4iOjkZWVla+vjkcDgwMDNCwYUNYWlqibdu2cHR0LHWWi+/xbXWG4iLJysLbt2+hbmQAG/3a8NGontst1RVvb2/88ssvYBgGLVu2RHBwcJ6QoYKEtCQr0dyf4ghpTEwM6+zF4XBgYmKSz7GLx+P9cLXJMAy8vb0RFxcHHo+HsWPHYtiwYQWuVn8kpEXl/PnzWLZsGXr16oUJEyZAV1c3z/XvHTN8uXwHL5asB2EYqNczgaZlEzCZEqQ9fQWRhRkslhWcBLu6HjNUK+FLJXLMOrkflwXZ0LOxhLq6eqWMVwkPD0dAQACuX7+Op0+f5luV8fl81K9fH2KxGI6OjtDQ0MD9+/fx8OFDREVFITY2FnK5PF+/fD4ftWvXhrm5OVq1aoUOHTqgY8eOFZLY9nv1+H6EVJqNV7GfwCQkw2LzSRz23lZujgSU0rFixQqsWLEChBC0a9cOZ8+eLXMPSJlMViwv3devX+PIkSNITU0FIQT6+vpo2bIl67RU0GerIAghePnyJVt13dDQkI13/ZpcIS2pl27uf7ds2QJfX1/w+XzUrVsX48ePx+DBg9kjHdax7JsEEgpJNu71nwJ5WgYMHO3R5Ldp4PxvN4wwDLLexUDYML/dQPVNF1gthO/reBUZw4CRSqGtKgSXWzniVR49eoTAwEBcu3YNz549Y92Pc+Hz+WjQoAFatGgBAwMDZGdnIzIyEm/fvkVCQkK+LCpATsLbOnXqwMLCAmKxGA4ODvjpp5/K1c36R5TUpTqTKKCbKkGwfX9kR8dCX18f48ePh6urK9q0aVPgFjal4pk7dy68vb0B5Oy8HD58uNKm0ZLL5ZgxYwb27NnDbseuXbsWI0aMKFBIC/PgzcjIgJ+fH549ewYAMDU1ha2tLSQSCdumqEL6I6Kjo5GQkACFQgEulwtVVVUYGBggODgYDc3NC00XmBz6BBGzVwAAftq7FsL6dYo8ZnVNF1jlha+yxaswDINHjx4hICAAN2/eRGRkJFuFORcejwcTExPo6elBVVUVaWlp+PjxY752uejo6KB+/fpo0aIFWrduDUdHR6WUYykPZITBfmkcfIoRROshMMZwQS30cHbGxYsXQQiBSCRCvXr1YGZmhp07dxYad0ipGMaPH48DBw4AAIYOHcpWW6jsXLlyBWPHjsXnz58BAE5OTvDz8yv2LsnChQvx33//gRACGxsbnDt3jt3ezT3bLK537rci+/jxY8TGxoIQAkIIeDweVFRUcPToUVh1cyg0XWD8+et4sWwTAMA+eC+4AhW83bIf0ftPsG3aXztY6NzSiQIHNZrBhFs5joaUQZUWvtJspxFCkE4U0OOqlCpehWEYhIaG4vjx47hx4wYiIyPZrY/c6zweD5qamlBTUwPDMEhNTS3Q5ZzL5bKhA1ZWVrC3t4ejo2OeUISqShqR47wsCadkCYj6X9okLoBc31Jzrhp6q+jDWeX/t6GDg4MxaNAg1rlHXV0dzZo1w5EjR2BmZlYBs6B8C8MwGDRoEFvea9KkSUopFVSeSCQSjB8/HoGBgSCEQE9PD1u3bkXPnj2L1c/WrVsxb948yOVyNGjQABcvXiw0A1JJmDx5Mk6ePAmBQABdXV107twZQ4YMQevWrfFWIcHozEhoFCB8X6/4bPz+hXrd2ki8eR9fLt5CfHBOmaPvCV8mUcBXaIEGvJJlwKmMVFnhK60DRS7fi1c5f/48NmzYgKlTp6J79+4Acj7oISEhCAwMxM2bN/HixQtW6BiGYcMEcvM5ymSyAgVZIBCgTp06aNy4MX766Sd06tQJ7dq1qxEV4IuaKDc3mfHDhw8hl8vB5XIxY8YM/PPPPxVgNeVb5HI5evbsiZs3b4LD4WDBggVYuHBhRZtVYk6cOIHJkycjOTkZHA4HAwYMwLZt24r1mTxz5gxGjRqFrKwspZc28vHxwbFjx9CrVy8MGDAgTxjW9xLEf33GZ9TTAY1+mQgOh4OMl2/xaNx8AHTFV2VQhss8UHC8CiEEu3btwqpVq/D+/XvUqVMH/fr1Y4UuKyuLzVGZK3RcLpfdfvj2DEpTUxOmpqZo3rw5GzrQvHnzSnv+UZmYOHEigoODkZKSgoyMDKioqGDbtm0YMmRIRZtWo8nKykLXrl3x+PFjcLlc/PXXX5g6dWpFm1VqkpOTMWLECFy+fBlATvjTnj170L59+yL38W1powMHDqBr166lti0hIQFbt26FpqYm6tevj3r16qFu3br4+PEjLl29At9+rVBbSzePb0Mu8Rdv4eXSjSAMA41G9aHZrBGy4xKQFPIIQOHCR8/4KhGpRA637wRJ5+aB/JZWO/+CRuMG+X7/dbyKSrYcEyZMwNGjR/NsWfJ4PHZvHcgJE+ByuXmEjsPhQF9fHw0aNIClpSXs7e3h4OBQoKcXpWjcvXsXe/fuxcSJEzFjxgzcuXMHKioqOHTokFITHFOKTnJyMhwcHPDy5Uvw+Xx4e3tj+PDhFW2WUvHx8cEvv/yCjIwM8Hg8jBs3Dn///XeRncc+fvyIrl274uPHj1BRUcF///2H0aNHl8qmkydPYv78+UhLS4NUKmXPAgUCAZo3b46UFZNh0tYGGoXsgKU+iUT0/hNIDXsORaYEAj1tCBvWhX7nNqjlWnDNQerVWYn4UVqsXOHTa2cDNZNa7O9NhvaGqmHBWVriszJQ73QIgmcszud1mQufz2eFjs/no1atWmjUqBFatWqF9u3bo3PnzhUSOlBTkMvlaN++PcLDw6GmpoZTp06VSaJjSuHExsbCwcEBHz58gKqqKnbv3g1XV9eKNqtMiImJwdChQ3Hv3j0AQMOGDXHw4MEi5xlNT09H165dER4eDg6Hg3nz5mHx4sUlskUikcDb2xu///57npyuHA4HGhoaWLJkCQQDuuGgjpymCywCVVL4CotXyaU4mf9ziYh6BcmLt4gbVHAgp56eHrp27QqxWIxOnTrBzs6uUoUO1BTS09Nhb2+P169fQyQS4dKlS0o7Q6F8nzdv3sDR0RFxcXHQ0NDAkSNH2AKp1Zm1a9di+fLlyM7OhoqKCmbNmoVFixYV6aji29JGQ4cOLXK19NxsM0eOHMHDhw8hlUqRnZ3NliISCoUQi8UIDAyEnp4eTRdYDKqc8CkIKTReJZfCVnwNpxe+1fDu/TukyWUwGLccMR8+Ijs7GwzDsDE4I0eOxJYtW5Q7GUqJ+PLlC+zt7RETEwNdXV3cvHkT9evXr2izqjXh4eFwcXFBcnIytLS0cPr0adjY2FS0WeXGy5cvMWTIEERGRgLIqf5y+PDhIr/vvi5t1LFjR5w6darAB+fU1FT4+PggICAAT548yZOVRkdHB6ampnj//j3U1NTQvn17eHt756k6okzfh+qcLrDKCd/3vJdyKeyM73ueS0COh+chUXOYcFWRkZGBu3fv4vbt23jy5AnGjh0LZ2fnUttPUQ4fP35Eu3btkJCQAGNjY4SEhOQrtURRDrdv30bfvn2Rnp4OfX19XLx4EY0bN65os8odhmHw22+/YePGjZDL5VBTU8OSJUswbdq0It2/evVqLF26FAzDwMLCAhcvXoSOjg4SExOxY8cOBAYGIiIiIk/Au76+PhwcHDB27Fh07twZsbGxGDBgABo3boz169fnK7VVHt7u1YEqJ3zfi1fJpSRbnUD1jFepzjx79gxdunRBWloaGjRogJCQEHrGqmTOnTuHYcOGQSKRoE6dOrh06VKNd9Z68OABhg8fjvfv3wMA7O3tcfDgwSI9eB06dAiTJk2CRCKBuro66tatizdv3rDbl0BO8VwHBwd4enoWeIadkZEBdXX1QrdLK0N8c2WnyvnTCzgclJVSM//rn1I1aNasGU6ePAl1dXW8ffsWDg4OSq9FV5M5cuQIBg8eDIlEAjMzM9y5c6fGix4A2NjYICwsDKNGjQKXy8Xt27dhaWnJZq4pjI8fPyIiIgKGhoaQSCRISkpitzNNTEwwatQoXL9+Ha9fv8auXbsKddzS0ND47hmhEVeA7cImaMBTQypRQF7EtY38f6FdDXlq1Vr0gCoofLU4OX8MRskL1dz+cvunVA3s7Oxw6NAhCAQCPHv2DC4uLkrLjViT2b59O8aNGweZTIYWLVrg9u3b1SKDkLLg8/nYvHkzjh8/DkNDQ6SlpWH8+PHo379/Hq/LN2/e4JdffoG1tTUsLCywdu1aREdHQ11dnfUSV1FRwerVq7F582alnZsacQXYJWyKCaq1IQWDVEaOTKLI973J/C83biojhxQMPFVrY5fQolqLHlAFhY/H4cCMqwYJ8hdSLQ0SMDDnqlWrIM2aQteuXbF9+3bweDzcvXsX/fv3L7DQLqVo/P3335g5cyYUCgVat26NGzdu0C3kQnB0dMTTp0/h6uoKDoeDoKAgNGrUCP369UPz5s1hZWWFjRs3IioqCkBOSMSUKVPw+PFjfPr0CS1atIBCocDo0aOxYcMGpdqmwuFitKoxToosMVPNFA25asgEg3SiQCZR5PwXDBpy1TBTzRQnRZYYrWpcLc/0vqXKnfEBP47jKwnVNV6lJrFjxw54eXmBEIKBAweyXnSUovN1smUnJycEBATQsJ0iEB4ejjlz5iA4OJg9r+PxeFBTU4O5uTl69eqFiRMn5vMClUgk6NmzJ0JCQsDhcDBx4sQyzXVa1HSB1Z0qKXw0XoVSGKtXr8aSJUsA5KQ7o3k9i86UKVOwe/duAED//v3h6+tL0+p9h3v37mHbtm24ePEiYmNjAeQ4h0ilUjY5vampKQ4ePIg2bfJXOM+FYRiMHj0aR48eBQD07NkTBw4coA8cZUiVfFdrcvgYIzBGBlEUWKuuOJD/lSjyEBhT0asGzJs3j3Uv37p1K5YtW1bBFlV+GIbB0KFDWdEbN24c9uzZQ0WvAG7evAkPDw80aNAAnTt3hp+fH2JjY8Hj8WBpaYnffvsNsbGx2LZtG3R1dREXF4du3bph7ty5hW6/c7lc7N27F15eXuBwODhz5gwcHBzypEykKJcqueIDaLwK5ftMmDABfn5+4HA4WL16NaZMmVLRJlVK5HI53NzccPXqVXA4HMyZMwd//PFHRZtVaWAYBleuXMGuXbtw9epVJCYmstf4fD4sLS3Rr18/jB07Frq6unnu/fjxI4YMGYKHDx8CAMzNzXHgwIHvZhratm0b5s6dy5Y2Cg4ORu3atctmcjWYKit8AI1XoXyfAQMG4OzZs+Byudi2bRuGDh1a0SZVKiQSCZydnXH//n1wuVwsXboUXl5eFW1WhcMwDIKCgrBnzx5cu3YNKSkp7DWBQABra2sMGDAAHh4eRXL6+euvv7Bq1SpIpVIIBALMmzcPCxYsKLT92bNnMXLkSLa00ZkzZ4qcH5RSNKq08AElr8Ce+b8K7GuUXIGdUnlgGAYuLi64desWVFRUcODAAfTo0aOizaoUpKeno3PnzoiMjASfz8e6devg4eFR0WZVGAqFAsePH8e+fftw69YtpKWlsddUVVVhY2ODgQMHYuTIkWxl9eLw/PlzDB48GC9fvgQAtGzZEocPHy40LvLhw4dwdXVFUlKSUksbUXKo8sIH5Gx77pfGwUcaCzkh4HM4UPsmlydDCCRg2OseAmMMF9Si25vVHLlcjg4dOiAsLIxWdPgfX758QefOnfH27VsIBALs2rUL7u7uFW1WuSOXy+Hv7w8/Pz+EhITkOVNTV1eHra0thg4diqFDhyqlQDTDMFiwYAG2bNkCuVwOdXV1LF++HBMnTiyw/cePH+Hk5IQPHz5ARUUF69atw5gxY0ptB6WaCF8uaUSO87IknJIlIIqRAMjx3sk9UjbnqqG3ij6cVXSpI0sNIj09He3atUNUVBREIhEuXrxYY7eO3r9/jy5duiA2Nhbq6uo4fPgwHB0LrsVWHZFKpThw4AAOHjyI0NBQZGVlsdc0NDTQpk0bDBs2DAMHDiwzr8rQ0FAMHz4c0dHRAIAOHTrAz8+vwJRn6enpcHJyQlhYWKlLG1G+glRT5AxDohUS8kaeRaIVEiJnmIo2iVKBJCQkkMaNGxOhUEjq1KlD3r59W9EmlTtPnz4lpqamRCgUklq1apG7d+9WtEnlQmZmJtm6dSvp2rUr0dXVJUKhkP2pVasWcXd3JwEBAUQmk5WbTTKZjHh6ehKRSESEQiExNjYmR44cKbRtnz59WJs9PDyIQqEoN1urI9VW+CiUb/nw4QOpW7cuEQqFxMzMjHz+/LmiTSo3QkNDSa1atYhQKCSmpqYkPDy8ok0qU9LS0sj69etJp06diI6OTh6xq127Nhk8eDA5ffp0hQvI+fPnSb169YhQKCQaGhpkwIABJC0trcC2U6dOZefg7OxMsrOzy9na6gMVPkqNIjIykhgbGxOhUEiaNWtW6JdMdeLixYtEX1+fFfx3795VtEllQlJSElmzZg1p164d0dLSyiN2pqamZPjw4eTixYsVLnbfkpaWRgYOHEg0NDSIUCgk9erVI+fPny+w7Zo1a9hVoo2NDUlISChna6sHVPgoNY7Q0FBWCGxsbKr1k/PRo0fZFU+zZs1IfHx8RZukVOLj48mKFStI69atiaamZh6xq1+/PhkzZgy5fv16RZtZJA4dOsQ+lIlEIuLp6Vng9uuhQ4fYv6mZmRl59epVBVhbtaHCR6mRXLx4kf3y6Ny5c7me75QXPj4+7MrHxsam2qxuo6OjyR9//EFsbGzY1U/uj7m5OfH09CShoaEVbWaJiI+PJ87Ozux8mjRpUuBcrl+/zm5dGxsbk9u3b1eAtVUXKnyUGou/vz+7Sujdu3el2wIrDf/++y8rCh07diRZWVkVbVKpePv2Lfn111+JtbU1uyX4tThMnTqVPHnypKLNVBpbtmxhdyW0tLTIvHnz8r0/X7x4QRo0aECE/9fevQZFdaZ5AP+fpruBbq4tjRjRFcWEqOhoaRaUmJB4KYkxJmCkViWSSGTdrBujyRpLJxqdVOk6Dq65gaiTVYxGtKREQ1DDkhFlKjI6ES9biEZAAwIitGBfz7Mf0DO2itwaTkM/vyq+9Gne83Qr5/+e97zvORoN+fv7U2ZmpkzV9jwcfMylpaenSwfShIQEuctxiE8++UT6TNOmTeuxZ7MlJSW0bNkyGj58uF3YabVaGjZsGC1ZsoQuXbokd5ldpqysjCIjI6XPPWrUqEc+b3V1NY0cOZI0Gg15e3tTSkqKTNX2LBx8zOVt2LBBOrC+//77cpfTKYsXL5YOlLNnz+5xZ7HFxcW0ePFiCgsLszur02q1FB4eTh999BGVlpbKXWa3+uyzz6RheT8/P1q/fr3ddqPRSC+99JL0PS1ZskSmSnsODj7GiGj58uXSQXbNmjVyl9NuNpuNEhISpM+QnJwsd0ltVlRURMnJyRQaGmoXdl5eXjR69GhatWoVlZeXy12mrIqLiyk8PFz6bqKiouy+k4f//WNjY3vsmX534OBj7J6kpCSp17xlyxa5y2kzm81GM2bMkGr/+OOP5S6pVSdPnqS3336bQkJC7MLO29ubxo0bR2vXrnWpdZZtYbPZaOnSpdJ1ab1eT+np6XbvWblypTR6MWHChF4zocnROPgYe0BcXJx0trFr1y65y2mVyWSi6OhoKfQ2bNggd0ktysvLo3nz5kkLtu//+Pj4UGRkJK1fv55qamrkLtPpnTp1yu7seNq0aXbr+dLS0qTZvM8++yxdv35dxmqdEwcfYw+w2Ww0efJk0mg05OvrS0eOHJG7pBYZDAYaN26cdKaUmpoqd0l2bDYb5eTkUHx8PPXv398u7Hx9fen555+nlJQUqq+vl7vUHsdkMlFiYqJ0dtevXz+7WZ3ff/+9NCt0wIABdO7cORmrdT4cfIw9xGKxUEREBGk0GtLpdFRQUCB3SY+ora2lESNGSCGyd+9euUsiouawy8rKotjYWGkx9v0fPz8/io6Opi+//JKH4BwkJydHug2fVqul+Ph4amxsJCKiM2fOSB0OvV7f4t1gXBEHH2OP0djYKE0m0Ov1TtVjLi8vl264rdPpKCcnR9Z6LBYL7du3j2bMmEGBgYF2YafT6Wjy5Mm0bdu2Hr+W0FkZDAZ644037O5Yk5eXR0TN/1fuz5D19fWl7du3y1usk+DgY6wFDz/R4erVq3KXRCUlJdI1Mr1eTydPnpSlDovFQjt37qRp06ZRQECAXdgFBARQTEwM7dq1i2cWdqPdu3dLd3Px8vKi5ORkslgsZDAYpBEMrVZLq1evlrtU2fWq5/Ex5mgVFRUYP348amtr0bdvXxQWFiIwMFCWWs6ePYuYmBjU19fDz88PP/zwQ7c+V9BoNCIjIwN79+5FUVERjEajtM3LywuRkZGYO3cuZs6c2WXPsmNPVlNTg/j4eJw6dQoAMHDgQGRkZGDkyJGYNWsWcnNzAQCzZ89Geno6FAqFnOXKR+7kZczZPfhEh7CwMGpoaOj2GvLz80mv15NGo6FBgwZ12yJug8FAX3zxBUVHRz/28T5xcXGUlZXV4xbK93ZbtmwhnU4nzZpdsWIF2Ww2eu+99/jRRsRnfIy1SVFREaZOnYq7d+8iLCwMp06dglqthslkgru7e5fuOzs7GwkJCTCZTAgODkZ+fj6CgoK6bH8NDQ3Yvn079u/fj3PnzsFisUjb/P39MXHiRMyfPx+TJk1y3TOGHuDatWuYNWsWzp8/DwAICwvDd999h4MHD2L16tUQRRFhYWE4evQodDqdzNV2Lw4+xtroxx9/RGxsLMxmM8aMGYOoqCgcP34c6enpGDlypMP2U1lZCYPBgKFDhyIjIwOLFi2C1WpFaGgo8vPz4efn57B93VdTU4P09HRkZWXhwoULsFqt0ja9Xo8XXngBiYmJePHFFx2+b9Z1RFHEunXrsGnTJlgsFri7u2PlypUIDg7GwoULYTabERQUhNzcXAwZMkTucrsNBx9j7bB//34kJibi7t27cHd3h06nw6JFi7B8+XKHtC+KImbOnCldW/zmm28giiJGjRqFY8eOQaPROGQ/QHPApqWl4dChQ7h06RJEUZS2BQUFITo6GgsWLEBERITD9snkUVxcjPj4eFy9ehUAMHbsWCxduhTvvvsuDAYDfHx8cODAAURGRqK+vh4KhQLe3t4yV911OPgYaweDwYCYmBgUFBSAiKBWqzF58mQcOnQIgiB0uv3CwkIsWLAApaWlMJvNUKlUmDhxIo4cOeKQCSMVFRVITU1FdnY2Ll++bBd2/fv3x6RJk7BgwQKMGTOm0/tizsVqteLDDz/Etm3bYLPZoNVq8cEHHyAtLQ1VVVVwd3fH2rVrkZmZCa1WiwMHDkCtVstddpfg4GOsHXbs2IGNGzfi8uXLMJvNAICAgAAUFha2OFRkI0IVmWEmgloQ0FdQw62FkFy1ahVSUlJw584dAM3X1I4dO9apICotLUVqaipycnJw5coVPPgnP3DgQEydOhVJSUkYPnx4h/fBeo6CggIkJCSgsrISABAVFYUbN26gpKQEZrMZWq0WwcHB2Lx5M6ZMmSJztV2Dg4+xdrh+/To2bNiAgoIClJaW4vbt2xAEAW+++Sb27Nkjva+BrMg11yHbWosrYvO0fwHA/T+2wQoPTFf2wVS1P7yF5jO5xsZGhISEoLq6GgCgVCoxYMAATJo0CWlpae2q8+LFi0hLS0NOTg7Ky8ulsBMEAYMGDUJMTAwWLlzoUtd12D+YzWYkJSVh//79ICJ4eXmhrq5Omsik0Wgwd+5cpKamPrGd9nTqnAkHH2MdUFNTgyNHjmDNmjW4evUq+vTpgwMHDiAiagIyzDfxZ3MlrERQCgI8oIDigYOBSAQjRGn7fHUQ/kWlR3zcLBw8eBAA4Ofnh6FDh2Ls2LF466238Nxzz7Va0y+//ILU1FQcPXoU169fl14XBAGhoaF45ZVXsHDhQgwcONDh3wfrmQ4fPozk5GTcuHHDbtgbAHx9fXH69GmEhobavd6RTp2z4eBjrBOICKtXr8ahQ4fg3j8IgzM24FdYoBXcoGxDz9dKhEayQbj2G24uXIPyM79gwoQJWLFiBSIiIqDRaEBEKCkpQXBw8COTW37++Wds3boVx48fl4auAEChUOCZZ57Bq6++iqSkJDz11FMO/+ysd8jPz0dMTAyamprsXhcEASNGjMDp06ehVqthIbFDnbo56kCoBOda9sLBx1gnERH+tHMHdo4Ngja4H7wFt3ZNdCFRxMWKMrjVN2KzEIzpEVF2bX/11VdIT09HTEwM1q1bhxMnTmDbtm3Iy8uThkUBwM3NDcOGDcNrr72GpKQkBAQEOPRzst5p2bJlyMrKQmVlpXRtGWgOvn79+mH69On4/X9vwnJrGa7YjO3u1A1288AfPYcgUOE8E2U4+BjrJAuJeLvp//CrzQhvRceGdox3jWhyA55298J2TRiUggAiwueff46vv/4a165dg6enJ9zd3VFXVyf9nlKpRHh4OF5//XW88847XbLGj/Vu58+fx759+1BeXo7S0lIUFRWhsbER3t7e8PHxge7pwQj89r9g9FDBq72dOiLcIRt0ChW2ap52mvDj4GOsk/5sqsRW02/waedB4WFEBAPZkOTeD/NUgUhOTsbevXthMBhARBAEAWq1Gp6enhg1ahTi4uIwf/58eHl5OfDTMFfX1NSE3bt349tvv0XN7Tpot38KRUh/+HSwUwcABtGKEDcPqVMnNw4+xjqhgayYcacYaige+YM+Pes9mKpqEPaHpejz/DgAQP2ZCyj+j0+h1Grwz99vf6Q9s82Km/W3YY1dhktFZ+yWHqjVarz88svIzMx06EJ2xh7n1q1b+OxKEU4O7QNfhdJhnbq33Lvudntt5ZxTbhjrIXLNdbASQaNwTC/2ds0t1Bsb4TY+HO7FF6BWq+Hj4wNPT08QEZRKJYce6xZKfx+cfkYPLygeG3pEhKI3/x2mqhoAwOj/2QjNoODHtiUIAjRwww5zJd5QB8g+25ODj7FOyLbWOnToRqfzh+2OG0L+cxEu/2k7bDYbKioqUFZWhrKyMl5kzrpNa526hr9flEIPAKpzT+Cf3o1vsT2lIKBJJORa6hCr1ju83vbg4GOsg2xEuCIaocGTp2rfPJyHhjMXAACmm7VPfK9SpUJfPx1qIIIEASqVCiEhIQgJCXFY3Yy1RWuduurcEwAA7dBBaCz5FdVHT2Bg0uwnDokqBQHZlloOPsZ6qipqvmWZopUzvlsn/9audhWCAFBz+08JXfvII8Yep7VOnWi2oPZ//woACPm3ebi0ahNMVTVo+PtF+P5uWIvtekCBUtEIG5Gsd3hxrlWFjPUgZiK05U837A9LMeGnPZjw0x6M2Pz7NrWtuNc+Y3JorVNXd+pvsN5phMrfFz6/exb+kc33kr1/FtiS++3db18uHHyMdZBaENBV0STea58xObTWqav+4S8AAN34MRAUCvSZ2DxruTavEKLZ8oTfdI5OHQ91MtZBfYXmxbgiUavDne0h3jso3G+fse72pE6dpeEO6grPAgCqDueh6nCetM3a2IRbJ4sQ8GLLz3B0hk4dBx9jHeQmCBis8MCvohEauDmsXSNEDFF49Ii73LPe6Umdutq8QohWK9w0nvAd/Y9ZxnevXcfdit9QnfOXFoPPWTp1HHyMdcJ0ZR+kmCrwuHGhsfs+f+Q139HDMOGnPY+++QFWIkxX9XFUiYy125M6ddVHm6/jBb02CYP+dY70ev3ZCyhe/Cnq/noWlnoDVL6PPsHdWTp1fOcWxjrBQFa82sKdWzrCSgQzRBzyGiH7Il/m2jJN1UgxVXTqVmUPaxCtWOIRLPtyBp7cwlgneAtKzFcHoZFs6Gwfku7dzT5RHcShx2Q3Ve0PpSDA6qBzo/uPKpqi8ndIe53BwcdYJ81RB2KwmwfukK1T7dwhG4a4eWCOuq+DKmOs43pzp46Dj7FOUgkK/NFzCHQKFQyitd0HCSKCQbRCp1Bho+cQp7h7PWNA7+3UcfAx5gCBCjW2ap7GIDcPNJCtzcND1nt3rQ9x83Cq55UxBvTeTh1PbmHMgSwkYrf5JnaYK6VrGh5Q2E0JF4lghChtT1QHYY66r9McFBh72E3RjKV3S9v9BPame09g38hPYGes9zOQFbmWOmRbalEqGgE0D6+I97YPUXhguqoPpqj8neKaB2Ot6U2dOg4+xrqYjQhVZIaZCGpBQF9BLfs6JsY6qjd06jj4GGOMdUhP7dRx8DHGGHMpPKuTMcaYS+HgY4wx5lI4+BhjjLkUDj7GGGMuhYOPMcaYS+HgY4wx5lI4+BhjjLkUDj7GGGMuhYOPMcaYS+HgY4wx5lI4+BhjjLkUDj7GGGMuhYOPMcaYS+HgY4wx5lI4+BhjjLkUDj7GGGMuhYOPMcaYS+HgY4wx5lL+Hwcogo1yq/cPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://github.com/ericthansen/dsc-graph-theory-shortest-path-lab\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "##generating a small world network\n",
    "G = nx.navigable_small_world_graph(3, seed=3)\n",
    "G = nx.relabel_nodes(G, dict(zip(G.nodes, ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'])))\n",
    "nx.draw(G, pos=nx.random_layout(G, seed=51), with_labels=True, node_color='#1cf0c7',\n",
    "        node_size=500, font_weight='bold', width=2, alpha=0.8)\n",
    "\n",
    "##dijkstra's alg\n",
    "'''Mark all nodes as unvisited\n",
    "Set the distance of the starting node as 0, and  ∞  for all other nodes\n",
    "Set the starting node as the current node\n",
    "Visit each of the neighbors of the current node\n",
    "For each neighbor, calculate the distance to that node traveling through the current node\n",
    "If this distance is less then the current distance recorded for that node, update the record accordingly\n",
    "Mark the current node as \"visited\"\n",
    "Of the unvisited nodes, set the one with the smallest distance to the current node\n",
    "Repeat steps 4 through 6 until one of the following:\n",
    "The algorithm terminates when the destination node is the current node\n",
    "Alternatively, if the smallest distance of the unvisited nodes is  ∞ , then no path exists to the destination node.\n",
    "Note: Dijkstra's algorithm (and NetworkX's implementations demonstrated above) returns a single path. In many cases, there may be multiple paths which are tied for the shortest distance between two nodes. In such cases, it is arbitrary which path is returned.'''\n",
    "\n",
    "\n",
    "##part 1\n",
    "def dijkstra(G, u, v):\n",
    "    \"\"\"\n",
    "    G is the graph in question\n",
    "    u is the starting node\n",
    "    v is the destination node\n",
    "    \"\"\"\n",
    "    #the default graph has no edge weights (boring!) so I suppose we can give default weight of 1\n",
    "    weight = 1\n",
    "    \n",
    "    visited = set()\n",
    "    unvisited = set(G.nodes)\n",
    "    #print(unvisited)\n",
    "    distances = {u:0}\n",
    "    for node in unvisited:\n",
    "        if node == u:\n",
    "            continue\n",
    "        distances[node] = np.inf\n",
    "    \n",
    "    current = u\n",
    "    \n",
    "    while True:\n",
    "        if current == v:\n",
    "            break\n",
    "        elif len(unvisited) <= 0:\n",
    "            break\n",
    "        else:\n",
    "            if min([distances[node] for node in unvisited]) == np.inf:\n",
    "                print('There is no path between u and v.')\n",
    "                return np.nan\n",
    "            #get neighbors of current\n",
    "            neighbors = G[current]\n",
    "            print(current, neighbors)\n",
    "            for neighbor in neighbors:\n",
    "                #check/add weights here\n",
    "                distances[neighbor] = min(distances[current] + weight, distances[neighbor])\n",
    "            visited.add(current)\n",
    "            unvisited.remove(current)\n",
    "            \n",
    "            #alt solution:\n",
    "            #current = sorted([(node, distances[node]) for node in unvisited], key=lambda x:x[1])[0][0] # Set the node with the minimum distance as the current node\n",
    "            closest_nbr = None\n",
    "            closest_nbr_dist = np.inf\n",
    "            for u in unvisited:\n",
    "                if distances[u]<closest_nbr_dist:\n",
    "                    closest_nbr = u\n",
    "                    closest_nbr_dist = distances[u]\n",
    "            current = closest_nbr\n",
    "    return distances[v]\n",
    "    \n",
    "    \n",
    "            \n",
    "    #node_vis_dict = dict(zip(G.nodes, ['u']*len(G.nodes)))\n",
    "    #print(visit_dict)\n",
    "    #print(u, v)\n",
    "    \n",
    "    \n",
    "    # Test your function here\n",
    "dijkstra(G, 'A', 'F')\n",
    "\n",
    "dijkstra(G, 'F', 'A')\n",
    "\n",
    "##from solution for reference.  \n",
    "def dijkstra(G, u, v):\n",
    "    \"\"\"\n",
    "    G is the graph in question\n",
    "    u is the starting node\n",
    "    v is the destination node\n",
    "    \n",
    "    Future: add weighting option\n",
    "    \"\"\"\n",
    "    visited = set()\n",
    "    unvisited = set(G.nodes)\n",
    "    distances = {u:0}\n",
    "    for node in unvisited:\n",
    "        if node == u:\n",
    "            continue\n",
    "        else:\n",
    "            distances[node] = np.inf\n",
    "    cur_node = u\n",
    "    weight = 1 # Set default weight for non-weighted graphs\n",
    "    while len(unvisited) > 0:\n",
    "        if cur_node == v:\n",
    "            break\n",
    "        if min([distances[node] for node in unvisited]) == np.inf:\n",
    "            print('There is no path between u and v.')\n",
    "            return np.nan\n",
    "        # Pull up neighbors\n",
    "        neighbors = G[cur_node]\n",
    "        for node in neighbors:\n",
    "            # Future update:Add weight update for weighted graphs\n",
    "            # Set either the distance through the current node or a previous shorter path\n",
    "            distances[node] = min(distances[cur_node] + weight, distances[node])\n",
    "        # Mark current node as visited\n",
    "        visited.add(cur_node)\n",
    "        unvisited.remove(cur_node)\n",
    "        cur_node = sorted([(node, distances[node]) for node in unvisited], key=lambda x:x[1])[0][0] # Set the node with the minimum distance as the current node\n",
    "    return distances[v]\n",
    "\n",
    "# Compare to NetworkX's built in method\n",
    "display(nx.dijkstra_path_length(G, 'F', 'A'))\n",
    "nx.dijkstra_path_length(G, 'A', 'F')\n",
    "#weird - aha, upon closer inspection I see there are just a couple one-directional edges.\n",
    "\n",
    "display(nx.dijkstra_path(G, 'F', 'A'))\n",
    "display(nx.dijkstra_path(G, 'A', 'F'))\n",
    "\n",
    "##part 2, with path\n",
    "def dijkstra(G, u, v, return_path_directions=True):\n",
    "    \"\"\"\n",
    "    G is the graph in question\n",
    "    u is the starting node\n",
    "    v is the destination node\n",
    "    \"\"\"\n",
    "    #the default graph has no edge weights (boring!) so I suppose we can give default weight of 1\n",
    "    weight = 1\n",
    "    path = {u:[u]}\n",
    "    \n",
    "    visited = set()\n",
    "    unvisited = set(G.nodes)\n",
    "    #print(unvisited)\n",
    "    distances = {u:0}\n",
    "    for node in unvisited:\n",
    "        if node == u:\n",
    "            continue\n",
    "        distances[node] = np.inf\n",
    "    \n",
    "    current = u\n",
    "    \n",
    "    while True:\n",
    "        if current == v:\n",
    "            break\n",
    "        elif len(unvisited) <= 0:\n",
    "            break\n",
    "        else:\n",
    "            if min([distances[node] for node in unvisited]) == np.inf:\n",
    "                print('There is no path between u and v.')\n",
    "                return np.nan\n",
    "            #get neighbors of current\n",
    "            neighbors = G[current]\n",
    "            #print(current, neighbors)\n",
    "            for neighbor in neighbors:\n",
    "                #check/add weights here\n",
    "                if distances[current] + weight < distances[neighbor]:\n",
    "                    distances[neighbor] = distances[current] + weight\n",
    "                    path[neighbor] = path[current]+[neighbor]\n",
    "                \n",
    "            visited.add(current)\n",
    "            unvisited.remove(current)\n",
    "            \n",
    "            #alt solution:\n",
    "            #current = sorted([(node, distances[node]) for node in unvisited], key=lambda x:x[1])[0][0] # Set the node with the minimum distance as the current node\n",
    "            closest_nbr = None\n",
    "            closest_nbr_dist = np.inf\n",
    "            for u in unvisited:\n",
    "                if distances[u]<closest_nbr_dist:\n",
    "                    closest_nbr = u\n",
    "                    closest_nbr_dist = distances[u]\n",
    "            current = closest_nbr\n",
    "    if return_path_directions:\n",
    "        return path[v], distances[v]\n",
    "    else:\n",
    "        return distances[v]\n",
    "# Your code here\n",
    "\n",
    "print(dijkstra(G, 'F', 'G'), nx.dijkstra_path(G, 'F', 'G'), nx.dijkstra_path_length(G, 'F', 'G'))\n",
    "print('\\n\\n')\n",
    "print(dijkstra(G, 'I', 'A'), nx.dijkstra_path(G, 'I', 'A'), nx.dijkstra_path_length(G, 'I', 'A'))\n",
    "\n",
    "\n",
    "##create a visual - wow!\n",
    "#in interest of time, taken from solution.  Very cool!\n",
    "def dijkstra(G, u, v, return_path_directions=True, show_plots=True):\n",
    "    \"\"\"\n",
    "    G is the graph in question\n",
    "    u is the starting node\n",
    "    v is the destination node\n",
    "    \n",
    "    Returns path, distance\n",
    "    \"\"\"\n",
    "    if show_plots:\n",
    "        return_path_directions = True # Must have path directions to generate plots\n",
    "    visited = set()\n",
    "    visited_edges = []\n",
    "    unvisited = set(G.nodes)\n",
    "    distances = {u:0}\n",
    "    shortest_paths = {u:[u]}\n",
    "    for node in unvisited:\n",
    "        if node == u:\n",
    "            continue\n",
    "        else:\n",
    "            distances[node] = np.inf\n",
    "    cur_node = u\n",
    "    weight = 1 # Set default weight for non-weighted graphs\n",
    "    # Create the initial plot\n",
    "    if show_plots:\n",
    "        fig = plt.figure(figsize=(20,15))\n",
    "        ax = fig.add_subplot(561)\n",
    "        nx.draw(G, pos=nx.random_layout(G, seed=9), with_labels=True, node_color='#1cf0c7',\n",
    "                node_size=500, font_weight='bold', width=2, alpha=0.8, ax=ax)\n",
    "        ax.set_title('Step 1')\n",
    "        plot_n = 2\n",
    "    while len(unvisited) > 0:\n",
    "        if cur_node == v:\n",
    "            break\n",
    "        if min([distances[node] for node in unvisited]) == np.inf:\n",
    "            print('There is no path between u and v.')\n",
    "            return np.nan\n",
    "        # Pull up neighbors\n",
    "        neighbors = G[cur_node]\n",
    "        for node in neighbors:\n",
    "            # Future update: Add weight update for weighted graphs\n",
    "            # Create a new graph of the neighbor being explored\n",
    "            if show_plots:\n",
    "                ax = fig.add_subplot(5,6,plot_n)\n",
    "                # Base Plot\n",
    "                nx.draw(G, pos=nx.random_layout(G, seed=9), with_labels=True, node_color='#1cf0c7',\n",
    "                        node_size=500, font_weight='bold', width=2, alpha=.8, ax=ax)\n",
    "                # Recolor paths to visited nodeds\n",
    "                nx.draw_networkx_edges(G, edgelist=visited_edges, pos=nx.random_layout(G, seed=9),\n",
    "                       width=3, edge_color='#00b3e6', ax=ax);\n",
    "                # Recolor current path\n",
    "                nx.draw_networkx_edges(G, edgelist=[(cur_node, node)], pos=nx.random_layout(G, seed=9),\n",
    "                       width=3, edge_color='#ffd43d', ax=ax);\n",
    "                ax.set_title('Step {}'.format(plot_n))\n",
    "                plot_n += 1\n",
    "            # Set either the distance through the current node or a previous shorter path\n",
    "            if distances[cur_node] + weight < distances[node]:\n",
    "                distances[node] = distances[cur_node] + weight\n",
    "                shortest_paths[node] = shortest_paths[cur_node] + [node]\n",
    "        # Mark current node as visited\n",
    "        visited.add(cur_node)\n",
    "        unvisited.remove(cur_node)\n",
    "        try:\n",
    "            # Will error for initial node\n",
    "            visited_edges.append((shortest_paths[cur_node][-2],cur_node))\n",
    "        except:\n",
    "            pass\n",
    "        # Update the plot for the visited node\n",
    "        if show_plots:\n",
    "            ax = fig.add_subplot(5,6,plot_n)\n",
    "            # Base Plot\n",
    "            nx.draw(G, pos=nx.random_layout(G, seed=9), with_labels=True, node_color='#1cf0c7',\n",
    "                    node_size=500, font_weight='bold', width=2, alpha=.8, ax=ax)\n",
    "            # Recolor paths to visited nodeds\n",
    "            nx.draw_networkx_edges(G, edgelist=visited_edges, pos=nx.random_layout(G, seed=9),\n",
    "                       width=3, edge_color='#00b3e6', ax=ax);\n",
    "            ax.set_title('Step {}'.format(plot_n))\n",
    "            plot_n += 1\n",
    "            if plot_n >= 29:\n",
    "                plt.show()\n",
    "                return None\n",
    "        cur_node = sorted([(node, distances[node]) for node in unvisited], key=lambda x:x[1])[0][0] # Set the node with the minimum distance as the current node\n",
    "    if return_path_directions:\n",
    "        return shortest_paths[v], distances[v]\n",
    "    else:\n",
    "        return distances[v]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'twitter.edges'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ad32a98065a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Load the Network from File\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_edgelist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'twitter.edges'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# Simplify the Node Labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelabel_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-776>\u001b[0m in \u001b[0;36mread_edgelist\u001b[1;34m(path, comments, delimiter, create_using, nodetype, data, edgetype, encoding)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf36\\lib\\site-packages\\networkx\\utils\\decorators.py\u001b[0m in \u001b[0;36m_open_file\u001b[1;34m(func_to_be_decorated, *args, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m             \u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dispatch_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m             \u001b[0mclose_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'twitter.edges'"
     ]
    }
   ],
   "source": [
    "# https://github.com/ericthansen/dsc-node-centrality\n",
    "##within network graphs, centrality and distance/connectedness are important\n",
    "#centrality\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "##\n",
    "# Load the Network from File\n",
    "G = nx.read_edgelist('twitter.edges')\n",
    "# Simplify the Node Labels\n",
    "G = nx.relabel_nodes(G, dict(zip(G.nodes, range(len(G.nodes)))))\n",
    "# Create a matplotlib figure\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "# Draw the network!\n",
    "nx.draw(G, pos=nx.spring_layout(G), with_labels=True,\n",
    "        alpha=.8, node_color='#1cf0c7', node_size=700)\n",
    "\n",
    "##degree centrality\n",
    "nx.degree(G)\n",
    "\n",
    "##closeness centrality\n",
    "# The Closeness for a central node\n",
    "print(nx.closeness_centrality(G, 4))\n",
    "# The Closeness Metric for an ostracized node\n",
    "print(nx.closeness_centrality(G, 86))\n",
    "\n",
    "##betweenness centrality\n",
    "# The Betweeness Metric for a central node\n",
    "print(nx.betweenness_centrality(G)[4])\n",
    "# The Betweeness Metric for an ostracized node\n",
    "print(nx.betweenness_centrality(G)[86])\n",
    "\n",
    "##eigenvector centrality\n",
    "# The eigenvector Metric for a central node\n",
    "print(nx.eigenvector_centrality(G)[4])\n",
    "# The eigenvector Metric for an ostracized node\n",
    "print(nx.eigenvector_centrality(G)[86])\n",
    "\n",
    "##putting it all together\n",
    "import pandas as pd\n",
    "\n",
    "degrees = nx.degree_centrality(G)\n",
    "closeness = nx.closeness_centrality(G)\n",
    "betweeness = nx.betweenness_centrality(G)\n",
    "eigs = nx.eigenvector_centrality(G)\n",
    "df = pd.DataFrame([degrees, closeness, betweeness, eigs]).transpose()\n",
    "df.columns = ['degrees', 'closeness', 'betweeness', 'eigs']\n",
    "# Some Nodes to Investigate\n",
    "islanders = [86, 87]\n",
    "penisulas = [51, 92, 98, 95]\n",
    "bridges = [52, 96]\n",
    "periphial = [57, 97]\n",
    "centers = [74, 3, 20]\n",
    "temp = {'islanders': islanders,\n",
    "       'penisulas': penisulas,\n",
    "       'bridges': bridges,\n",
    "       'periphial': periphial,\n",
    "       'centers': centers}\n",
    "node_label_dict = {}\n",
    "for label in temp.keys():\n",
    "    nodes = temp[label]\n",
    "    for node in nodes:\n",
    "        node_label_dict[node]=label\n",
    "ex_nodes = islanders + penisulas + bridges + periphial + centers\n",
    "df['group'] = df.index.map(node_label_dict)\n",
    "df.iloc[ex_nodes]\n",
    "\n",
    "##\n",
    "df.groupby('group').mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-node-centrality-lab\n",
    "##character ineraction graph from Game of Thrones!\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "#load data\n",
    "# Load edges into dataframes\n",
    "df = pd.read_csv('asoiaf-all-edges.csv')\n",
    "\n",
    "# Print the first five rows\n",
    "df.head(5)\n",
    "\n",
    "##create a graph with weights\n",
    "# Create an empty graph instance\n",
    "G = nx.Graph()\n",
    "\n",
    "# Read edge lists into dataframes\n",
    "for row in df.index:\n",
    "    source = df['Source'][row]\n",
    "    target = df['Target'][row]\n",
    "    weight = df['weight'][row]\n",
    "    G.add_edge(source, target, weight=weight)\n",
    "    \n",
    "##calculate degree\n",
    "# Your code here\n",
    "pd.DataFrame.from_dict(nx.degree_centrality(G), orient='index').sort_values(by=0, ascending=False).head(10).plot(kind='barh', color='#1cf0c7')\n",
    "plt.title('Top 10 Characters Ranked by by Degree Centrality');\n",
    "\n",
    "#closeness centrality\n",
    "# Your code here\n",
    "pd.DataFrame.from_dict(nx.closeness_centrality(G), orient='index').sort_values(by=0, ascending=False).head(10).plot(kind='barh', color='#1cf0c7')\n",
    "plt.title('Top 10 Characters Ranked by Closeness Centrality');\n",
    "\n",
    "##bteweenness\n",
    "# Your code here\n",
    "pd.DataFrame.from_dict(nx.betweenness_centrality(G), orient='index').sort_values(by=0, ascending=False).head(10).plot(kind='barh', color='#1cf0c7')\n",
    "plt.title('Top 10 Characters Ranked by Betweeness Centrality');\n",
    "\n",
    "##put it all together with eigenvector\n",
    "# Your code here\n",
    "degrees = nx.degree_centrality(G)\n",
    "closeness = nx.closeness_centrality(G)\n",
    "betweeness = nx.betweenness_centrality(G)\n",
    "eigen = nx.eigenvector_centrality(G)\n",
    "centrality = pd.DataFrame([degrees, closeness, betweeness, eigen]).transpose()\n",
    "centrality.columns = ['degrees', 'closeness', 'betweeness', 'eigenvector']\n",
    "centrality = centrality.sort_values(by='eigenvector', ascending=False)\n",
    "centrality.head()\n",
    "\n",
    "##identify key players\n",
    "# Your code here\n",
    "centrality['bridge_est'] = centrality['betweeness'] / centrality.degrees\n",
    "centrality = centrality.sort_values(by='bridge_est', ascending=False)\n",
    "centrality.head(10)\n",
    "\n",
    "##draw graph\n",
    "# Your code here\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "plt.figure(figsize=(12,12))\n",
    "nx.draw(G, with_labels=True, pos=nx.spring_layout(G),\n",
    "        alpha=0.8, node_color='#11ffcc', node_size=700);\n",
    "nx.draw_networkx_edge_labels(G,pos=nx.spring_layout(G), edge_labels=edge_labels);\n",
    "\n",
    "##this is very busy, subset it\n",
    "# Your code here\n",
    "# Read edge lists into dataframes\n",
    "threshold = 75\n",
    "colors = []\n",
    "G = nx.Graph()\n",
    "for row in df.index:\n",
    "    source = df['Source'][row]\n",
    "    target = df['Target'][row]\n",
    "    weight = df['weight'][row]\n",
    "    if weight >= threshold:\n",
    "        G.add_edge(source,target, weight=weight)\n",
    "edge_labels = nx.get_edge_attributes(G,'weight')\n",
    "for node in G.nodes:\n",
    "    if node in centrality.index[:10]:\n",
    "        colors.append('#ffdd33')\n",
    "    else:\n",
    "        colors.append('#11ffcc')\n",
    "plt.figure(figsize=(18,10))\n",
    "nx.draw(G, with_labels=True, pos=nx.spring_layout(G),\n",
    "        alpha=.8, node_color=colors, node_size=1500)\n",
    "nx.draw_networkx_edge_labels(G,pos=nx.spring_layout(G), edge_labels=edge_labels);\n",
    "\n",
    "##cool!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-network-clustering\n",
    "##focusing on k-clique clustering\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "G = nx.read_gexf('ga_graph.gexf')\n",
    "plt.figure(figsize=(12,8))\n",
    "nx.draw(G, with_labels=True, node_color='#1cf0c7',\n",
    "        alpha=.75, font_weight='bold', node_size=2*10**3, pos=nx.spring_layout(G, seed=4))\n",
    "##\n",
    "c = list(nx.algorithms.community.k_clique_communities(G, k=2))\n",
    "c\n",
    "##\n",
    "\n",
    "colors = [('teal', '#1cf0c7'),\n",
    "         ( 'workzone_yellow', '#ffd43d'),\n",
    "         ('light-blue', '#00b3e6'),\n",
    "         ('medium-blue', '#32cefe'),\n",
    "         ('gray', '#efefef'),\n",
    "         ('slate', '#2b2b2b'),\n",
    "         ('dark-blue', '#144ff')]\n",
    "color_dict = dict(colors)\n",
    "##\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "for n, ci in enumerate(c):\n",
    "    ci = G.subgraph(ci)\n",
    "    nx.draw(ci, with_labels=True, node_color=colors[n][1],\n",
    "            alpha=.75, font_weight='bold', pos=nx.spring_layout(G, seed=4))\n",
    "##\n",
    "##girvan newman algorithm in networkx\n",
    "'''As discussed, the Girvan Newman algorithm works by successively removing edges which are considered to be strong ties between subsets in the community. Typically, this is the betweenness metric. Since you are removing edges in the algorithm, betweenness is calculated for the edges as opposed to the nodes, as you have previously seen. The process is nearly identical though. First, the shortest paths between all nodes are computed using Dijkstra's algorithm. From there, an edges betweenness is the fraction of these paths that the edge is part of.'''\n",
    "c_gn = list(nx.algorithms.community.centrality.girvan_newman(G))\n",
    "print(len(c_gn), c_gn[:3])\n",
    "##\n",
    "c_gn[0]\n",
    "##\n",
    "colors = [('teal', '#1cf0c7'),\n",
    "         ( 'workzone_yellow', '#ffd43d'),\n",
    "         ('light-blue', '#00b3e6'),\n",
    "         ('medium-blue', '#32cefe'),\n",
    "         ('gray', '#efefef'),\n",
    "         ('slate', '#2b2b2b'),\n",
    "         ('dark-blue', '#1443ff')]\n",
    "##\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(18,18))\n",
    "for n, cs in enumerate(c_gn[:4]):\n",
    "    i = n//2\n",
    "    j = n%2\n",
    "    ax = axes[i][j]\n",
    "    ax.set_title('Clusters with {} Edges Removed'.format(n+1))\n",
    "    #Multiple Clusters per Groups\n",
    "    for n2, c in enumerate(cs):\n",
    "        ci = G.subgraph(c)\n",
    "        color = colors[n2][1]\n",
    "        nx.draw(ci, with_labels=True, node_color=color, ax=ax,\n",
    "                alpha=.75, font_weight='bold', pos=nx.spring_layout(G, seed=4));\n",
    "        \n",
    "##\n",
    "cs = c_gn[-1]\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "for c in cs:\n",
    "    ci = G.subgraph(c)\n",
    "    nx.draw(ci, with_labels=True, node_color=color_dict['light-blue'],\n",
    "                alpha=.75, font_weight='bold', pos=nx.spring_layout(G, seed=4));\n",
    "    \n",
    "##\n",
    "#Documentation:\n",
    "# https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.community.kclique.k_clique_communities.html\n",
    "# https://pelegm-networkx.readthedocs.io/en/latest/reference/generated/networkx.algorithms.community.centrality.girvan_newman.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-network-clustering-lab\n",
    "##making visualizations of network clusters\n",
    "import pandas as pd\n",
    "df = pd.read_csv('stack-overflow-tag-network/stack_network_links.csv')\n",
    "df.head()\n",
    "\n",
    "##transform into a networkx graph\n",
    "# Your code here\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "for row in df.index:\n",
    "    source = df.source[row]\n",
    "    target = df.target[row]\n",
    "    weight = df.value[row]\n",
    "    G.add_edge(source, target, weight=weight)\n",
    "print(len(G.nodes))\n",
    "\n",
    "##initial vis\n",
    "# Your code here\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "nx.draw(G, pos=nx.spring_layout(G, k=2), with_labels=True,\n",
    "        alpha=.8, node_size=10000, font_weight='bold', font_size=20)\n",
    "\n",
    "##initial clustering k-clique\n",
    "# Your code here\n",
    "for i in range(2,9):\n",
    "    kc_clusters = list(nx.algorithms.community.k_clique_communities(G, k=i))\n",
    "    print('With k={}, {} clusters form.'.format(i, len(kc_clusters)))\n",
    "    \n",
    "\n",
    "##visiualize\n",
    "# Your code here\n",
    "kc_clusters = list(nx.algorithms.community.k_clique_communities(G, k=5))\n",
    "\n",
    "colors = [('teal', '#1cf0c7'),\n",
    "         ('workzone_yellow', '#ffd43d'),\n",
    "         ('light-blue', '#00b3e6'),\n",
    "         ('medium-blue', '#32cefe'),\n",
    "         ('gray', '#efefef'),\n",
    "         ('dark-blue', '#1443ff')]\n",
    "color_dict = dict(colors)\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "for n, ci in enumerate(kc_clusters):\n",
    "    ci = G.subgraph(ci)\n",
    "    nx.draw(ci, pos=nx.spring_layout(G, k=2, seed=7), with_labels=True, node_color=colors[n%len(colors)][1],\n",
    "            alpha=0.8, node_size=20000, font_weight='bold', font_size=20)\n",
    "    \n",
    "## different clustering k value\n",
    "# Your code here\n",
    "kc_clusters = list(nx.algorithms.community.k_clique_communities(G, k=3))\n",
    "colors = ['#1cf0c7','#ffd43d','#00b3e6','#32cefe','#efefef','#2b2b2b', '#1443ff',\n",
    "          '#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c',\n",
    "          '#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928']\n",
    "fig = plt.figure(figsize=(35,20))\n",
    "for n, ci in enumerate(kc_clusters):\n",
    "    ci = G.subgraph(ci)\n",
    "    nx.draw(ci, pos=nx.spring_layout(G, k=2.5, seed=10), with_labels=True, node_color=colors[n],\n",
    "            alpha=0.8, node_size=20000, font_weight='bold', font_size=20)\n",
    "    \n",
    "##now use girvan-newman alg\n",
    "# Your code here\n",
    "gn_clusters = list(nx.algorithms.community.centrality.girvan_newman(G))\n",
    "for n, clusters in enumerate(gn_clusters):\n",
    "    print('After removing {} edges, there are {} clusters.'.format(n, len(clusters))) \n",
    "    \n",
    "## create a viz wrapper\n",
    "def plot_girvan_newman(G, clusters):\n",
    "    # Your code here \n",
    "    fig = plt.figure(figsize=(30,30))\n",
    "    colors = ['#1cf0c7','#ffd43d','#00b3e6','#32cefe','#efefef',\n",
    "          '#1443ff','#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99',\n",
    "          '#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99',\n",
    "          '#b15928','#8dd3c7','#ffffb3','#bebada','#fb8072','#80b1d3',\n",
    "          '#fdb462','#b3de69','#fccde5','#d9d9d9','#bc80bd','#ccebc5',\n",
    "          '#ffed6f','#bf812d','#dfc27d','#f6e8c3','#f5f5f5','#c7eae5',\n",
    "          '#80cdc1', '#35978f', '#01665e', '#003c30']\n",
    "    for n , c in enumerate(clusters):\n",
    "        ci = G.subgraph(c)\n",
    "        # K value of 2.5 also seem to work well\n",
    "        nx.draw(ci, pos=nx.spring_layout(G, k=2.66, seed=7), with_labels=True, node_color=colors[n],\n",
    "                alpha=0.8, node_size=20000, font_weight='bold', font_size=20)\n",
    "    plt.show()\n",
    "    \n",
    "##visualize for different values\n",
    "# Your code here\n",
    "plot_girvan_newman(G, gn_clusters[0])\n",
    "##\n",
    "plot_girvan_newman(G, gn_clusters[5])\n",
    "\n",
    "##\n",
    "plot_girvan_newman(G, gn_clusters[15])\n",
    "##\n",
    "plot_girvan_newman(G, gn_clusters[25])\n",
    "\n",
    "##check out cluster decay rate\n",
    "# Your code here\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "y = [len(cluster) for cluster in gn_clusters]\n",
    "x = [n+1 for n in range(len(gn_clusters))]\n",
    "plt.plot(x,y)\n",
    "plt.title('Number of Clusters versus Number of Edges Removed')\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.xlabel('Number of Edges Removed')\n",
    "plt.show()\n",
    "#\n",
    "### From solution, for reference:\n",
    "# While an initial investigation such as the one above does not appear to be particularly informative,\n",
    "# exploring additional cluster metrics reveals more interesting patterns.\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "max_cluster_size = [max([len(c) for c in cluster]) for cluster in gn_clusters]\n",
    "plt.plot(x,max_cluster_size, color=colors[0], label='Max Cluster Size')\n",
    "\n",
    "min_cluster_size = [min([len(c) for c in cluster]) for cluster in gn_clusters]\n",
    "plt.plot(x,min_cluster_size, color=colors[1], label='Minimum Cluster Size')\n",
    "\n",
    "mean_cluster_size = [np.mean([len(c) for c in cluster]) for cluster in gn_clusters]\n",
    "plt.plot(x,mean_cluster_size, color=colors[2], label='Mean Cluster Size')\n",
    "\n",
    "median_cluster_size = [np.median([len(c) for c in cluster]) for cluster in gn_clusters]\n",
    "plt.plot(x,median_cluster_size, color=colors[3], label='Median Cluster Size')\n",
    "\n",
    "single_node_clusters = [sum([1 if len(c)==1 else 0 for c in cluster]) for cluster in gn_clusters]\n",
    "plt.plot(x,single_node_clusters, color=colors[6], label='Number of Single Node Clusters')\n",
    "\n",
    "small_clusters = [sum([1 if len(c)<=5 else 0 for c in cluster ]) for cluster in gn_clusters]\n",
    "plt.plot(x,small_clusters, color=colors[5], label='Number of Small Clusters (5 or less nodes)')\n",
    "\n",
    "plt.legend(loc=(1.01,.75), fontsize=14)\n",
    "plt.title('Cluster Size Metrics versus Number of Edges Removed', fontsize=14)\n",
    "plt.xlabel('Number of Edges Removed', fontsize=14)\n",
    "plt.ylabel('Cluster Metric')\n",
    "plt.ylim(0,80)\n",
    "plt.yticks(ticks=list(range(0,80,5)))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#if we look for an \"elbow\" in the various metrics above, 15-20 seems like a good range\n",
    "\n",
    "##choose a clustering\n",
    "# Your code/response here\n",
    "print('Number of clusters:',len(gn_clusters[20]))\n",
    "plot_girvan_newman(G, gn_clusters[20])\n",
    "# This clustering representation was chosen after analyzing the plots above.\n",
    "# After the 20th edge is removed, max cluster size does not drastically drop again,\n",
    "# while small and single node clusters start to rapidly spawn. K-Clique clusters did not appear to be well developed.\n",
    "\n",
    "##\n",
    "##again from solution:\n",
    "# While there is no definitive criteria for optimizing clusters, \n",
    "# there is almost a 33% increase in the total number clusters here,\n",
    "# yet the same definitive clusters jump to the eye as above.\n",
    "# For this reason, it can be argued that the above clusters are more definitive and representative.\n",
    "print('Number of clusters:',len(gn_clusters[32]))\n",
    "plot_girvan_newman(G, gn_clusters[32])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-network-community-detection-lab\n",
    "##goal here is to idenfity clusters/communities in meetup groups in Nashville\n",
    "\n",
    "# Your code here\n",
    "\n",
    "import pandas as pd\n",
    "groups = pd.read_csv('nashville-meetup/group-edges.csv', index_col=0)\n",
    "groups.head()\n",
    "##descriptions\n",
    "# Your code here\n",
    "groups_meta = pd.read_csv('nashville-meetup/meta-groups.csv')\n",
    "groups_meta.head()\n",
    "\n",
    "##transform to network rep\n",
    "# Your code here\n",
    "import networkx as nx\n",
    "\n",
    "group_dict = dict(zip(groups_meta.group_id, groups_meta.group_name))\n",
    "G = nx.Graph()\n",
    "for row in groups.index:\n",
    "    g1 = group_dict[groups.group1[row]]\n",
    "    g2 = group_dict[groups.group2[row]]\n",
    "    weight = groups.weight[row]\n",
    "    G.add_edge(g1, g2, weight=weight)\n",
    "len(G)\n",
    "\n",
    "##first vis\n",
    "# Your code here\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nx.draw(G, pos=nx.spring_layout(G, k=2, seed=7), alpha=.8, node_color='#33ccff')\n",
    "\n",
    "##that is no good!\n",
    "##refine:\n",
    "# Your code here\n",
    "##this approach from solution\n",
    "for i in range(0,100,5):\n",
    "    print('{}th percentile: {}'.format(i,groups.weight.quantile(q=i/100)))\n",
    "    \n",
    "##this approach from solution\n",
    "threshold = 5\n",
    "G_subset = nx.Graph()\n",
    "for row in groups.index:\n",
    "    g1 = group_dict[groups.group1[row]]\n",
    "    g2 = group_dict[groups.group2[row]]\n",
    "    weight = groups.weight[row]\n",
    "    if weight > threshold:\n",
    "        G_subset.add_edge(g1, g2, weight=weight)\n",
    "plt.figure(figsize=(30,20))\n",
    "nx.draw(G_subset, pos=nx.spring_layout(G_subset, k=2, seed=5),\n",
    "        alpha=.8, node_color='#33ccff', node_size=4000,\n",
    "        with_labels=True, font_size=16, font_weight='bold')\n",
    "##\n",
    "len(G_subset)\n",
    "\n",
    "##cluster the network\n",
    "# Your code here\n",
    "gn_clusters = list(nx.algorithms.community.centrality.girvan_newman(G_subset))\n",
    "for n, clusters in enumerate(gn_clusters):\n",
    "    print('After removing {} edges, there are {} clusters.'.format(n, len(clusters)))\n",
    "    \n",
    "##determin an optimal clustering schema\n",
    "# Your code here\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "colors = ['#1cf0c7','#ffd43d','#00b3e6','#32cefe','#efefef',\n",
    "          '#1443ff','#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99',\n",
    "          '#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99',\n",
    "          '#b15928','#8dd3c7','#ffffb3','#bebada','#fb8072','#80b1d3',\n",
    "          '#fdb462','#b3de69','#fccde5','#d9d9d9','#bc80bd','#ccebc5',\n",
    "          '#ffed6f','#bf812d','#dfc27d','#f6e8c3','#f5f5f5','#c7eae5',\n",
    "          '#80cdc1', '#35978f', '#01665e', '#003c30']\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "\n",
    "x = [n+1 for n in range(len(gn_clusters))]\n",
    "\n",
    "max_cluster_size = [max([len(c) for c in cluster]) for cluster in gn_clusters]\n",
    "plt.plot(x,max_cluster_size, color=colors[0], label='Max Cluster Size')\n",
    "\n",
    "min_cluster_size = [min([len(c) for c in cluster]) for cluster in gn_clusters]\n",
    "plt.plot(x,min_cluster_size, color=colors[1], label='Minimum Cluster Size')\n",
    "\n",
    "mean_cluster_size = [np.mean([len(c) for c in cluster]) for cluster in gn_clusters]\n",
    "plt.plot(x,mean_cluster_size, color=colors[2], label='Mean Cluster Size')\n",
    "\n",
    "median_cluster_size = [np.median([len(c) for c in cluster]) for cluster in gn_clusters]\n",
    "plt.plot(x,median_cluster_size, color=colors[3], label='Median Cluster Size')\n",
    "\n",
    "single_node_clusters = [sum([1 if len(c)==1 else 0 for c in cluster]) for cluster in gn_clusters]\n",
    "plt.plot(x,single_node_clusters, color=colors[6], label='Number of Single Node Clusters')\n",
    "\n",
    "small_clusters = [sum([1 if len(c)<=5 else 0 for c in cluster ]) for cluster in gn_clusters]\n",
    "plt.plot(x,small_clusters, color=colors[5], label='Number of Small Clusters (5 or less nodes)')\n",
    "\n",
    "plt.legend(loc=(1.01,.75), fontsize=14)\n",
    "plt.title('Cluster Size Metrics versus Number of Edges Removed', fontsize=14)\n",
    "plt.xlabel('Number of Edges Removed', fontsize=14)\n",
    "plt.ylabel('Cluster Metric')\n",
    "plt.ylim(0,80)\n",
    "plt.yticks(ticks=list(range(0,80,5)))\n",
    "plt.show()\n",
    "\n",
    "##inspect the graphs...idk how we tell, but i guess there's an elbow at 20?\n",
    "##visualize the clusters\n",
    "# Your code here\n",
    "def plot_girvan_newman(G, clusters):\n",
    "    #Your code here\n",
    "    fig = plt.figure(figsize=(35,20))\n",
    "    colors = ['#1cf0c7','#ffd43d','#00b3e6','#32cefe','#efefef',\n",
    "              '#1443ff','#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99',\n",
    "              '#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99',\n",
    "              '#b15928','#8dd3c7','#ffffb3','#bebada','#fb8072','#80b1d3',\n",
    "              '#fdb462','#b3de69','#fccde5','#d9d9d9','#bc80bd','#ccebc5',\n",
    "              '#ffed6f','#bf812d','#dfc27d','#f6e8c3','#f5f5f5','#c7eae5',\n",
    "              '#80cdc1', '#35978f', '#01665e', '#003c30']\n",
    "    for n , c in enumerate(clusters):\n",
    "        ci = G.subgraph(c)\n",
    "        nx.draw(ci, pos=nx.spring_layout(G_subset, k=3.6, seed=3), with_labels=True, node_color=colors[n],\n",
    "                alpha=0.8, node_size=10000, font_weight='bold', font_size=20)\n",
    "    plt.show()\n",
    "sns.set_style('white')\n",
    "plot_girvan_newman(G_subset, gn_clusters[20])\n",
    "\n",
    "##\n",
    "# Because the Network is still dense and names are long, organizing the clusters as a Pandas dataframe is appropriate\n",
    "clusters = pd.DataFrame(gn_clusters[20]).transpose()\n",
    "clusters.columns = ['Cluster{}'.format(i) for i in range(1,len(clusters.columns)+1)]\n",
    "clusters\n",
    "\n",
    "# Many of the clusters are easily identifiable;\n",
    "# You can see that cluster 1 is a technology group,\n",
    "# cluster 2 is a social hodgepodge,\n",
    "# cluster3 is outdoor enthusiasts,\n",
    "# cluster4 is gamers, and so forth\n",
    "\n",
    "##that is cool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-network-recommendation-systems\n",
    "##idea - building a recommender system off of networks.  (though I'm not exactly seeing the network)\n",
    "\n",
    "#load data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('ml-100k/u.data', delimiter='\\t', \n",
    "                 names=['user_id' , 'item_id' , 'rating' , 'timestamp'])\n",
    "df.head()\n",
    "\n",
    "##\n",
    "col_names = ['movie_id' , 'movie_title' , 'release_date' , 'video_release_date' ,\n",
    "             'IMDb_URL' , 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "             'Childrens', 'Comedy', 'Crime' , 'Documentary', 'Drama', 'Fantasy',\n",
    "             'Film-Noir', 'Horror', 'Musical', 'Mystery' , 'Romance' , 'Sci-Fi',\n",
    "             'Thriller', 'War' ,'Western']\n",
    "movies = pd.read_csv('ml-100k/u.item', delimiter='|', encoding='latin1', names=col_names)\n",
    "movies.head()\n",
    "\n",
    "##\n",
    "#transform the data - pivot table\n",
    "user_ratings = df.pivot(index='user_id', columns='item_id', values='rating')\n",
    "user_ratings.head()\n",
    "\n",
    "##fill missing values\n",
    "for col in user_ratings:\n",
    "    mean = user_ratings[col].mean()\n",
    "    user_ratings[col] = user_ratings[col].fillna(value=mean)\n",
    "user_ratings.head()\n",
    "\n",
    "##create user matrix\n",
    "'''To create a user matrix, you must calculate the distance between users. Choosing an appropriate distance metric for this is crucial. In this instance, a simple Euclidean distance is apt to be appropriate, but in other instances an alternative metric such as cosine distance might be a more sensible choice.'''\n",
    "import numpy as np\n",
    "import datetime\n",
    "##\n",
    "u1 = user_ratings.iloc[1]\n",
    "u2 = user_ratings.iloc[2]\n",
    "def distance(v1,v2):\n",
    "    return np.sqrt(np.sum((v1-v2)**2))\n",
    "distance(u1,u2)\n",
    "\n",
    "##\n",
    "# ⏰ Expect this cell to take several minutes to run\n",
    "start = datetime.datetime.now()\n",
    "user_matrix = []\n",
    "for i, row in enumerate(user_ratings.index):\n",
    "    u1 = user_ratings[row]\n",
    "    # Matrix is symetric, so fill in values for previously examined users\n",
    "    user_distances = [entry[i] for entry in user_matrix] \n",
    "    for j, row2 in enumerate(user_ratings.index[i:]):\n",
    "        u2 = user_ratings[row2]\n",
    "        d = distance(u1,u2)\n",
    "        user_distances.append(d)\n",
    "    user_matrix.append(user_distances)\n",
    "user_similarities = pd.DataFrame(user_matrix)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed)\n",
    "\n",
    "user_similarities.head()\n",
    "\n",
    "##calculate recommendation\n",
    "def recommend_movies(user, user_similarities, user_ratings, df, n_users=20, n_items=10):\n",
    "    \"\"\"n is the number of similar users who you wish to use to generate recommendations.\"\"\"\n",
    "    # User_Similarities Offset By 1 and Must Remove Current User\n",
    "    top_n_similar_users = user_similarities[user-1].drop(user-1).sort_values().index[:n_users] \n",
    "    # Again, fixing the offset of user_ids\n",
    "    top_n_similar_users = [i+1 for i in top_n_similar_users] \n",
    "    already_watched = set(df[df.user_id == 0].item_id.unique())\n",
    "    unwatched = set(df.item_id.unique()) - already_watched\n",
    "    projected_user_reviews = user_ratings[user_ratings.index.isin(top_n_similar_users)].mean()[list(unwatched)].sort_values(ascending=False)\n",
    "    return projected_user_reviews[:n_items]\n",
    "\n",
    "##\n",
    "recommend_movies(1, user_similarities, user_ratings, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-network-recomendation-systems-lab \n",
    "\n",
    "\n",
    "##recommender system\n",
    "#load data\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "\n",
    "df = pd.read_csv('books_data.edgelist', names=['source', 'target', 'weight'], delimiter=' ')\n",
    "df.head()\n",
    "\n",
    "#load metadata\n",
    "# Your code here\n",
    "meta = pd.read_csv('books_meta.txt', sep='\\t')\n",
    "meta.head()\n",
    "\n",
    "##select books to test recommender on\n",
    "# Your code here\n",
    "dresden = meta[meta.Title.str.contains('Dresden')]\n",
    "dresden\n",
    "\n",
    "##generate recs for a few books of choice\n",
    "# Your code here\n",
    "# Well, got a couple or extraneous results in there, but perhaps good measure for comparion.\n",
    "# What does our recommender return for these books?\n",
    "rec_dict = {}\n",
    "id_name_dict = dict(zip(meta.ASIN, meta.Title))\n",
    "for row in dresden.index:\n",
    "    book_id = dresden.ASIN[row]\n",
    "    book_name = id_name_dict[book_id]\n",
    "    most_similar = df[(df.source == book_id)\n",
    "                      | (df.target == book_id)\n",
    "                     ].sort_values(by='weight', ascending=False).head(10)\n",
    "    most_similar['source_name'] = most_similar['source'].map(id_name_dict)\n",
    "    most_similar['target_name'] = most_similar['target'].map(id_name_dict)\n",
    "    recommendations = []\n",
    "    for row in most_similar.index:\n",
    "        if most_similar.source[row] == book_id:\n",
    "            recommendations.append((most_similar.target_name[row], most_similar.weight[row]))\n",
    "        else:\n",
    "            recommendations.append((most_similar.source_name[row], most_similar.weight[row]))\n",
    "    rec_dict[book_name] = recommendations\n",
    "    print('Recommendations for:', book_name)\n",
    "    for r in recommendations:\n",
    "        print(r)\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-networks-recap\n",
    "'''Graph Theory - Recap\n",
    "GitHub RepoCreate New Issue\n",
    "Introduction\n",
    "In this section you explored a new data structure: networks! While network analysis is a deep topic with many additional topics to explore, you should have a good initial introduction and enough to conduct some preliminary analyses for social networks and building recommendation systems.\n",
    "\n",
    "Networks\n",
    "You've seen that networks can represent a range of different underlying data. From directions, social networks, and customer databases, networks are a wonderful way to represent the relationships between individuals. They also make for some snazzy visuals!\n",
    "\n",
    "Paths\n",
    "The first stop along your journey was paths! Here, you investigated Dijkstra's algorithm to find the shortest path between nodes. This harked back to some of your experience scraping the web when you used recursive functions to perform breadth and depth based search techniques to transverse a json file. While you didn't directly explore this application, networks are also a natural representation for exploring internet traffic and web page structures.\n",
    "\n",
    "Centrality\n",
    "Once you had a metric to calculate the distance between nodes, you then started to investigate other important concepts of networks such as which nodes were most influential or connected within a graph. You saw how alternative metrics can provide different insights on node structure. As a quick recap:\n",
    "\n",
    "Degree-centrality: The number of edges attached to a node\n",
    "Closeness-centrality: The reciprocal of the sum of the distances to all other nodes in the network\n",
    "Betweeness-centrality: The number of shortest paths between all node pairs the node lies on divided by the maximum number of shortests-paths any one node in the network lies on\n",
    "Eigenvalue-centrality: An iterative algorithm which assigns relative influence to a node based on the number and importance of connected nodes. Can be very computationally expensive to compute for large networks. Google's PageRank algorithm is a variation of eigenvalue-centrality\n",
    "Clustering\n",
    "After discussing centrality, you then focused on larger structures within a network, breaking apart nodes into clusters to examine subgroups. While this is a common and useful application, it is an ill-defined problem mathematically, often making it difficult to definitively determine an optimal clustering schema.\n",
    "\n",
    "Recommendations\n",
    "Finally, you rounded out the section by investigating how networks can be used to provide recommendations to users. To do this, you investigated a preliminary approach known as collaborative filtering, specifically exploring user-based collaborative filtering in which similar users are identified and their preferences are used to generate recommendations to the user in question. There are many alternative approaches to recommendations systems such as using Singular Value Decomposition.\n",
    "\n",
    "Summary\n",
    "A lot was covered in this section! From this, you should have a solid introduction to networks, and some of their applications. Going forward, continue to explore ongoing developments in clustering social networks, and generating recommendations from these fascinating data structures.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-cnn-intro\n",
    "# https://github.com/ericthansen/dsc-convolutional-neural-networks-codealong\n",
    "#store the imgs\n",
    "import os, shutil\n",
    "\n",
    "##\n",
    "data_santa_dir = 'data/santa/'\n",
    "data_not_santa_dir = 'data/not_santa/'\n",
    "new_dir = 'split/'\n",
    "##\n",
    "imgs_santa = [file for file in os.listdir(data_santa_dir) if file.endswith('.jpg')]\n",
    "##\n",
    "imgs_santa[0:10]\n",
    "##\n",
    "print('There are', len(imgs_santa), 'santa images')\n",
    "##\n",
    "imgs_not_santa = [file for file in os.listdir(data_not_santa_dir) if file.endswith('.jpg')]\n",
    "##\n",
    "print('There are', len(imgs_not_santa), 'images without santa')\n",
    "##\n",
    "os.mkdir(new_dir)\n",
    "#\n",
    "train_folder = os.path.join(new_dir, 'train')\n",
    "train_santa = os.path.join(train_folder, 'santa')\n",
    "train_not_santa = os.path.join(train_folder, 'not_santa')\n",
    "\n",
    "test_folder = os.path.join(new_dir, 'test')\n",
    "test_santa = os.path.join(test_folder, 'santa')\n",
    "test_not_santa = os.path.join(test_folder, 'not_santa')\n",
    "\n",
    "val_folder = os.path.join(new_dir, 'validation')\n",
    "val_santa = os.path.join(val_folder, 'santa')\n",
    "val_not_santa = os.path.join(val_folder, 'not_santa')\n",
    "##\n",
    "train_santa\n",
    "##\n",
    "os.mkdir(test_folder)\n",
    "os.mkdir(test_santa)\n",
    "os.mkdir(test_not_santa)\n",
    "\n",
    "os.mkdir(train_folder)\n",
    "os.mkdir(train_santa)\n",
    "os.mkdir(train_not_santa)\n",
    "\n",
    "os.mkdir(val_folder)\n",
    "os.mkdir(val_santa)\n",
    "os.mkdir(val_not_santa)\n",
    "\n",
    "# train santa\n",
    "imgs = imgs_santa[:271]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(train_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# validation santa\n",
    "imgs = imgs_santa[271:371]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(val_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# test santa\n",
    "imgs = imgs_santa[371:]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(test_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# train not_santa\n",
    "imgs = imgs_not_santa[:271]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_not_santa_dir, img)\n",
    "    destination = os.path.join(train_not_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "# validation not_santa\n",
    "imgs = imgs_not_santa[271:371]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_not_santa_dir, img)\n",
    "    destination = os.path.join(val_not_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "# test not_santa\n",
    "imgs = imgs_not_santa[371:]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_not_santa_dir, img)\n",
    "    destination = os.path.join(test_not_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "print('There are', len(os.listdir(train_santa)), 'santa images in the training set')\n",
    "#\n",
    "print('There are', len(os.listdir(val_santa)), 'santa images in the validation set')\n",
    "#\n",
    "print('There are', len(os.listdir(test_santa)), 'santa images in the test set')\n",
    "#\n",
    "print('There are', len(os.listdir(train_not_santa)), 'images without santa in the train set')\n",
    "#\n",
    "print('There are', len(os.listdir(val_not_santa)), 'images without santa in the validation set')\n",
    "#\n",
    "print('There are', len(os.listdir(test_not_santa)), 'images without santa in the test set')\n",
    "#\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "##\n",
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size=(64, 64), batch_size = 180) \n",
    "\n",
    "# get all the data in the directory split/validation (200 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_folder, \n",
    "        target_size=(64, 64), batch_size = 200)\n",
    "\n",
    "# get all the data in the directory split/train (542 images), and reshape them\n",
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_folder, \n",
    "        target_size=(64, 64), batch_size=542)\n",
    "\n",
    "##\n",
    "# create the data sets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)\n",
    "val_images, val_labels = next(val_generator)\n",
    "\n",
    "# Explore your dataset again\n",
    "m_train = train_images.shape[0]\n",
    "num_px = train_images.shape[1]\n",
    "m_test = test_images.shape[0]\n",
    "m_val = val_images.shape[0]\n",
    "\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"Number of testing samples: \" + str(m_test))\n",
    "print (\"Number of validation samples: \" + str(m_val))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "print (\"test_images shape: \" + str(test_images.shape))\n",
    "print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "print (\"val_images shape: \" + str(val_images.shape))\n",
    "print (\"val_labels shape: \" + str(val_labels.shape))\n",
    "\n",
    "##\n",
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "val_img = val_images.reshape(val_images.shape[0], -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(test_img.shape)\n",
    "print(val_img.shape)\n",
    "\n",
    "##\n",
    "train_y = np.reshape(train_labels[:,0], (542,1))\n",
    "test_y = np.reshape(test_labels[:,0], (180,1))\n",
    "val_y = np.reshape(val_labels[:,0], (200,1))\n",
    "##\n",
    "# Build a baseline fully connected model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu', input_shape=(12288,))) # 2 hidden layers\n",
    "model.add(layers.Dense(7, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "##\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "histoire = model.fit(train_img,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_img, val_y))\n",
    "\n",
    "##\n",
    "results_train = model.evaluate(train_img, train_y)\n",
    "##\n",
    "results_test = model.evaluate(test_img, test_y)\n",
    "##\n",
    "results_train\n",
    "##\n",
    "results_test\n",
    "##\n",
    "####Build a CNN\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['acc'])\n",
    "##\n",
    "history = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_images, val_y))\n",
    "##\n",
    "results_train = model.evaluate(train_images, train_y)\n",
    "#\n",
    "results_test = model.evaluate(test_images, test_y)\n",
    "#\n",
    "results_train\n",
    "#\n",
    "results_test\n",
    "#\n",
    "\n",
    "###Data Augmentation!  - making extra images out of existing ones with little variations\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=40, \n",
    "                                   width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2, \n",
    "                                   shear_range=0.3, \n",
    "                                   zoom_range=0.1, \n",
    "                                   horizontal_flip=False)\n",
    "##\n",
    "names = [os.path.join(train_santa, name) for name in os.listdir(train_santa)]\n",
    "img_path = names[91]\n",
    "img = load_img(img_path, target_size=(64, 64))\n",
    "\n",
    "reshape_img = img_to_array(img) \n",
    "reshape_img = reshape_img.reshape((1,) + reshape_img.shape) \n",
    "i=0\n",
    "for batch in train_datagen.flow(reshape_img, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 3 == 0:\n",
    "        break\n",
    "plt.show()\n",
    "##\n",
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size = 180,\n",
    "        class_mode='binary') \n",
    "\n",
    "# get all the data in the directory split/validation (200 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_folder, \n",
    "        target_size=(64, 64),\n",
    "        batch_size = 32,\n",
    "        class_mode='binary')\n",
    "\n",
    "# get all the data in the directory split/train (542 images), and reshape them\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size = 32, \n",
    "        class_mode='binary')\n",
    "##\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer= 'sgd',\n",
    "              metrics=['acc'])\n",
    "\n",
    "##\n",
    "history_2 = model.fit_generator(train_generator, \n",
    "                                steps_per_epoch=25, \n",
    "                                epochs=30, \n",
    "                                validation_data=val_generator, \n",
    "                                validation_steps=25)\n",
    "\n",
    "##\n",
    "test_x, test_y = next(test_generator)\n",
    "##\n",
    "results_test = model.evaluate(test_x, test_y)\n",
    "##\n",
    "results_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/ericthansen/dsc-building-a-cnn-from-scratch\n",
    "'''Objectives\n",
    "In this lab you will:\n",
    "\n",
    "Load images from a hierarchical file structure using an image datagenerator\n",
    "Apply data augmentation to image files before training a neural network\n",
    "Build a CNN using Keras\n",
    "Visualize and evaluate the performance of CNN models'''\n",
    "\n",
    "##loading the imgs\n",
    "# Load the images; be sure to also preprocess these into tensors \n",
    "\n",
    "train_dir = 'cats_dogs_downsampled/train'\n",
    "validation_dir = 'cats_dogs_downsampled/val/'\n",
    "test_dir = 'cats_dogs_downsampled/test/' \n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import datetime\n",
    "\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "##\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(150, 150),\n",
    "                                                        batch_size=20,\n",
    "                                                        class_mode='binary')\n",
    "\n",
    "##designing the model\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "##\n",
    "#from keras import optimizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "##training and evaluating\n",
    "# Set the model to train \n",
    "# ⏰ This cell may take several minutes to run \n",
    "history = model.fit(train_generator, \n",
    "                              steps_per_epoch=100, \n",
    "                              epochs=30, \n",
    "                              validation_data=validation_generator, \n",
    "                              validation_steps=20)\n",
    "\n",
    "##plotting\n",
    "# Plot history\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))\n",
    "##\n",
    "#save the model\n",
    "# Your code here; save the model for future reference \n",
    "model.save('cats_dogs_downsampled_data.h5')\n",
    "\n",
    "##data augmentation\n",
    "'''Recall that data augmentation is typically always a necessary step when using a small dataset as this one which you have been provided. As such, if you haven't already, implement a data augmentation setup.'''\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "##\n",
    "# Add data augmentation to the model setup and set the model to train; \n",
    "# See warnings above if you intend to run this block of code \n",
    "# ⏰ This cell may take several hours to run \n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40, \n",
    "                                   width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2, \n",
    "                                   shear_range=0.2, \n",
    "                                   zoom_range=0.2, \n",
    "                                   horizontal_flip=True, \n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "history = model.fit(train_generator, \n",
    "                              steps_per_epoch=100, \n",
    "                              epochs=30, \n",
    "                              validation_data=validation_generator, \n",
    "                              validation_steps=20)\n",
    "##save model\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training with data augmentation took a total of {}'.format(elapsed))\n",
    "\n",
    "### Save the model \n",
    "model.save('cats_dogs_downsampled_with_augmentation_data.h5')\n",
    "##final evaluation\n",
    "# Your code here \n",
    "# Perform a final evaluation using the test set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, \n",
    "                                                  target_size=(150, 150), \n",
    "                                                  batch_size=20, \n",
    "                                                  class_mode='binary')\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=20)\n",
    "print('test acc:', test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-visualizing-intermediate-activations\n",
    "##loading previous model\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('cats_dogs_downsampled_with_augmentation_data.h5')\n",
    "# As a reminder \n",
    "model.summary()  \n",
    "\n",
    "##loading an images from training set\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "##\n",
    "img_path = 'images/dog.1100.jpg'\n",
    "\n",
    "\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "\n",
    "#Follow the Original Model Preprocessing\n",
    "img_tensor /= 255.\n",
    "\n",
    "#Check tensor shape\n",
    "print(img_tensor.shape)\n",
    "\n",
    "#Preview an image\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()\n",
    "\n",
    "##visualizing a layer\n",
    "from keras import models\n",
    "\n",
    "##\n",
    "# Extract model layer outputs\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "\n",
    "# Rather then a model with a single output, we are going to make a model to display the feature maps\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "##\n",
    "model.summary()\n",
    "##\n",
    "# Returns an array for each activation layer\n",
    "activations = activation_model.predict(img_tensor)\n",
    "\n",
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)\n",
    "\n",
    "# We slice the third channel and preview the results\n",
    "plt.matshow(first_layer_activation[0, :, :, 3], cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "##\n",
    "# Repeating the process for another channel (the 30th)\n",
    "activations = activation_model.predict(img_tensor)\n",
    "\n",
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)\n",
    "\n",
    "plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "##\n",
    "##can visl 32 channels from first activation function\n",
    "fig, axes = plt.subplots(8, 4, figsize=(12,24))\n",
    "for i in range(32):\n",
    "    row = i//4\n",
    "    column = i%4\n",
    "    ax = axes[row, column]\n",
    "    first_layer_activation = activations[0]\n",
    "    ax.matshow(first_layer_activation[0, :, :, i], cmap='viridis')\n",
    "    \n",
    "##repeating for all layers\n",
    "fig, axes = plt.subplots(2,4, figsize=(12,8))\n",
    "\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "for i in range(8):\n",
    "    row = i//4\n",
    "    column = i%4\n",
    "    ax = axes[row, column]\n",
    "    cur_layer = activations[i]\n",
    "    ax.matshow(cur_layer[0, :, :, 29], cmap='viridis')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.set_title(layer_names[i])\n",
    "    \n",
    "#things get more \"abstract\" - note this is the 29th channel (i.e. filter, I think) for each layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-visualizing-activation-functions-lab\n",
    "##load a model\n",
    "# Your code here\n",
    "from keras.models import load_model\n",
    "model = load_model('cats_dogs_downsampled_with_augmentation_data.h5')\n",
    "model.summary()\n",
    "##load an img\n",
    "# Your code here\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "filename = 'dog.1100.jpg'\n",
    "img = image.load_img(filename, target_size=(150, 150))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "##transform img to a tensor\n",
    "# Your code here\n",
    "import numpy as np\n",
    "\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "\n",
    "# Follow the Original Model Preprocessing\n",
    "img_tensor /= 255.\n",
    "\n",
    "# Check tensor shape\n",
    "print(img_tensor.shape)\n",
    "\n",
    "# Preview an image\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()\n",
    "\n",
    "##plot feature maps\n",
    "# Your code here\n",
    "from keras import models\n",
    "import math \n",
    "\n",
    "# Extract model layer outputs for conv and max pooling layers (leave out the dense layers at end)\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "\n",
    "# Create a model for displaying the feature maps\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "activations = activation_model.predict(img_tensor)\n",
    "\n",
    "# Extract Layer Names for Labelling\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "total_features = sum([a.shape[-1] for a in activations])\n",
    "print(\"total features: {}\".format(total_features))\n",
    "\n",
    "n_cols = 16\n",
    "n_rows = math.ceil(total_features / n_cols)\n",
    "\n",
    "\n",
    "iteration = 0\n",
    "fig , axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols, n_rows*1.5))\n",
    "\n",
    "for layer_n, layer_activation in enumerate(activations):\n",
    "    n_channels = layer_activation.shape[-1]\n",
    "    for ch_idx in range(n_channels):\n",
    "        row = iteration // n_cols\n",
    "        column = iteration % n_cols\n",
    "    \n",
    "        ax = axes[row, column]\n",
    "\n",
    "        channel_image = layer_activation[0,\n",
    "                                         :, :,\n",
    "                                         ch_idx]\n",
    "        # Post-process the feature to make it visually palatable\n",
    "        channel_image -= channel_image.mean()\n",
    "        channel_image /= channel_image.std()\n",
    "        channel_image *= 64\n",
    "        channel_image += 128\n",
    "        channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "\n",
    "        ax.imshow(channel_image, aspect='auto', cmap='viridis')\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        \n",
    "        if ch_idx == 0:\n",
    "            ax.set_title(layer_names[layer_n], fontsize=10)\n",
    "        iteration += 1\n",
    "\n",
    "fig.subplots_adjust(hspace=1.25)\n",
    "plt.savefig('Intermediate_Activations_Visualized.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-cnn-recap \n",
    "'''Introduction\n",
    "Well done! In this section you learned all about convolutional neural networks! You should now have enough of an introductory primer to be able to do some image recognition tasks on your own!\n",
    "\n",
    "Key Takeaways\n",
    "Remember that the essence of a CNN is the convolutional operation. A window is slided across the image based on a stride size. Padding can be used to prevent shrinkage and to make sure pixels at the edge of an image deserve the necessary attention. Each convolution then works to adjust the weights of the kernel through backpropagation during training. Going back to the general architecture, max pooling is typically used between convolutional layers to reduce the dimensionality.\n",
    "\n",
    "Overall, CNNs are a useful model for image recognition due to their ability to recognize visual patterns at varying scales. After developing the convolutional and pooling layers to form a base, the end of the network architecture still connects back to a densely connected network to perform classification.\n",
    "\n",
    "Summary\n",
    "In this section you learned all about convolutional neural networks! From here, you'll learn more about tuning neural networks and other neural network architectures!'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-transfer-learning-intro\n",
    "# https://github.com/ericthansen/dsc-using-pretrained-networks\n",
    "'''Objectives¶\n",
    "You will be able to:\n",
    "\n",
    "Describe the benefits of using pretrained networks\n",
    "Explain how pre-trained neural networks are used for feature extraction\n",
    "Explain what \"freezing\" and \"unfreezing\" a layer means in a neural network'''\n",
    "'''Exampled of pretrained networks¶\n",
    "Keras has several pretrained models available. Here is a list of pretrained image classification models. All these models are available in keras.applications and were pretrained on the ImageNet dataset, a dataset with 1.4 million labeled images and 1,000 different classes.\n",
    "\n",
    "DenseNet\n",
    "InceptionResNetV2\n",
    "InceptionV3\n",
    "MobileNet\n",
    "NASNet\n",
    "ResNet50\n",
    "VGG16\n",
    "VGG19\n",
    "Xception'''\n",
    "# https://keras.io/applications/\n",
    "#look at structure\n",
    "from keras.applications import MobileNet\n",
    "conv_base = MobileNet(weights='imagenet', \n",
    "                      include_top=True)\n",
    "\n",
    "# This is a deep and complex network!\n",
    "conv_base.summary()\n",
    "\n",
    "'''You'll learn about two ways to use pre-trained networks:\n",
    "\n",
    "Feature extraction: Here, you use the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch.\n",
    "\n",
    "Fine-tuning: When fine-tuning, you'll \"unfreeze\" a few top layers from the model and train them again together with the densely connected classifier. Note that you are changing the parts of the convolutional layers here that were used to detect the more abstract features. By doing this, you can make your model more relevant for the classification problem at hand.'''\n",
    "\n",
    "#feature extraction\n",
    "'''Feature Extraction\n",
    "Feature extraction with convolutional neural networks means that you take the convolutional base of a pretrained network, run new data through it, and train a new classifier on top of the output (a new densely connected classifier). Why use convolutional base but new dense classifier? Generally, patterns learned by the convolutional layers are more generalizable.\n",
    "\n",
    "Note that, if your dataset differs a lot from the dataset used when pretraining, it might even be worth only using part of the convolutional base (see \"fine tuning\")\n",
    "\n",
    "Also, with feature extraction, there are two ways running the model:\n",
    "\n",
    "You can run the convolutional base over your dataset, save its output, then use this data as input to a standalone, densely connected network. This solution is pretty fast to run, and you need to run the convolutional base first for every input image. The problem here is, however, that you can't use data augmentation as we've seen before.\n",
    "You can extend the convolutional base by adding dense layers on top, and running everything altogether on the input data. This way, you can use data augmentation, but as every input image goes through the convolutional base every time, this technique is much more time-consuming. It's almost impossible to do this without a GPU'''\n",
    "\n",
    "#fine tuning\n",
    "'''Fine tuning\n",
    "Fine tuning is similar to feature extraction in that you reuse the convolution base and retrain the dense, fully connected classifier layers to output a new prediction. In addition, fine tuning also works by retraining the frozen weights for the convolutional base. This allows these weights to be tweaked for the current scenario, hence the name. To do this, you'll freeze part of the model while tuning specific layers.'''\n",
    "# http://cs231n.stanford.edu/syllabus.html\n",
    "# https://www.dlology.com/blog/gentle-guide-on-how-yolo-object-localization-works-with-keras/\n",
    "# https://www.dlology.com/blog/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-using-pretrained-networks-codealong\n",
    "##\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "#\n",
    "import os, shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "#\n",
    "train_folder = 'split/train'\n",
    "test_folder = 'split/test'\n",
    "val_folder = 'split/validation'\n",
    "#\n",
    "datagen = ImageDataGenerator(rescale=1./255) \n",
    "batch_size = 10\n",
    "##vgg19 feature extractoin\n",
    "from tensorflow.keras.applications import VGG19\n",
    "cnn_base = VGG19(weights='imagenet', \n",
    "                 include_top=False, \n",
    "                 input_shape=(64, 64, 3))\n",
    "                 #input_shape=(224, 224, 3)) #turn this on if include_top=True\n",
    "    \n",
    "#\n",
    "cnn_base.summary()\n",
    "##\n",
    "def extract_features(directory, sample_amount):\n",
    "    features = np.zeros(shape=(sample_amount, 2, 2, 512)) \n",
    "    labels = np.zeros(shape=(sample_amount))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory, target_size=(64, 64), \n",
    "        batch_size = 10, \n",
    "        class_mode='binary')\n",
    "    i=0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = cnn_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch \n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i = i + 1\n",
    "        if i * batch_size >= sample_amount:\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "#\n",
    "# You should be able to divide sample_amount by batch_size\n",
    "train_features, train_labels = extract_features(train_folder, 540) \n",
    "validation_features, validation_labels = extract_features(val_folder, 200) \n",
    "test_features, test_labels = extract_features(test_folder, 180)\n",
    "\n",
    "train_features = np.reshape(train_features, (540, 2 * 2 * 512))\n",
    "validation_features = np.reshape(validation_features, (200, 2 * 2 * 512))\n",
    "test_features = np.reshape(test_features, (180, 2 * 2 * 512))\n",
    "#\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=2*2*512))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(validation_features, validation_labels))\n",
    "##\n",
    "results_test = model.evaluate(test_features, test_labels)\n",
    "results_test\n",
    "#\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epch = range(1, len(train_acc) + 1)\n",
    "plt.plot(epch, train_acc, 'g.', label='Training Accuracy')\n",
    "plt.plot(epch, val_acc, 'g', label='Validation acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epch, train_loss, 'r.', label='Training loss')\n",
    "plt.plot(epch, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "##\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Feature extraction method 1 took {} to execute.'.format(elapsed))\n",
    "startp = datetime.datetime.now() # Set new start time for new process method\n",
    "##feature extraction method 2\n",
    "model = models.Sequential()\n",
    "model.add(cnn_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(132, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "##freezing\n",
    "# You can check whether a layer is trainable (or alter its setting) through the layer.trainable attribute\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "# Similarly, you can check how many trainable weights are in the model\n",
    "print(len(model.trainable_weights))\n",
    "\n",
    "#\n",
    "cnn_base.trainable = False\n",
    "##check\n",
    "# You can check whether a layer is trainable (or alter its setting) through the layer.trainable attribute\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "# Similarly, we can check how many trainable weights are in the model\n",
    "print(len(model.trainable_weights))\n",
    "\n",
    "#\n",
    "# Get all the data in the directory split/train (542 images), and reshape them\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=40, \n",
    "                                   width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2, \n",
    "                                   shear_range=0.2, \n",
    "                                   zoom_range=0.2, \n",
    "                                   horizontal_flip=True, \n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_folder,  \n",
    "                                                    target_size=(64, 64),  \n",
    "                                                    batch_size= 20, \n",
    "                                                    class_mode= 'binary') \n",
    "\n",
    "# Get all the data in the directory split/validation (200 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(val_folder,  \n",
    "                                                                       target_size=(64, 64),  \n",
    "                                                                       batch_size=20, \n",
    "                                                                       class_mode='binary')\n",
    "\n",
    "# Get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(test_folder,  \n",
    "                                                                        target_size=(64, 64), \n",
    "                                                                        batch_size=180,\n",
    "                                                                        class_mode='binary')\n",
    "\n",
    "test_images, test_labels = next(test_generator)\n",
    "##compile as usual\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "##and fit\n",
    "# ⏰ This cell may take several minutes to run\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=27,\n",
    "                              epochs=10,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=10)\n",
    "\n",
    "##\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epch = range(1, len(train_acc) + 1)\n",
    "plt.plot(epch, train_acc, 'g.', label='Training Accuracy')\n",
    "plt.plot(epch, val_acc, 'g', label='Validation acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epch, train_loss, 'r.', label='Training loss')\n",
    "plt.plot(epch, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "##\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - startp\n",
    "print('Feature extraction method 2 took {} to execute.'.format(elapsed))\n",
    "elapsed = end - start\n",
    "print('Total running time of notebook thus far: {}'.format(elapsed))\n",
    "startp = datetime.datetime.now() # Set new start time for new process method\n",
    "##\n",
    "##fine tuning\n",
    "model.summary()\n",
    "\n",
    "#\n",
    "cnn_base.summary()\n",
    "##reminder on fine tuning - feature extraction comes first.  then train final layers, then unfreeze last few layers by unfreezing\n",
    "#entire thing and then refreezing beginning layers\n",
    "cnn_base.trainable = True\n",
    "#\n",
    "cnn_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in cnn_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        \n",
    "##recompile\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=optimizers.RMSprop(lr=1e-4), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "##fit\n",
    "# ⏰ This cell may take several minutes to run\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=27,\n",
    "                              epochs=10,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=10)\n",
    "##\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epch = range(1, len(train_acc) + 1)\n",
    "plt.plot(epch, train_acc, 'g.', label='Training Accuracy')\n",
    "plt.plot(epch, val_acc, 'g', label='Validation acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epch, train_loss, 'r.', label='Training loss')\n",
    "plt.plot(epch, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "##final eval on test set\n",
    "# ⏰ This cell may take several minutes to run\n",
    "\n",
    "# test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "#                                                   target_size=(150, 150),\n",
    "#                                                   batch_size=20,\n",
    "#                                                   class_mode='binary')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
    "print('test acc:', test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-image-classification-lab\n",
    "##this is a bit of a clusterfuck; it's a good runthrough I suppose but the cloud-jupyter version doesn't run right\n",
    "#and the local version needs some extra TLC.  Just look at the repo.\n",
    "\n",
    "# https://github.com/ericthansen/dsc-transfer-learning-recap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b3f7bc68787e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# https://github.com/ericthansen/dsc-generating-word-embeddings-lab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# https://github.com/ericthansen/dsc-deep-nlp-intro\n",
    "\n",
    "# https://github.com/ericthansen/dsc-word-embeddings\n",
    "    # good visuals\n",
    "\n",
    "# https://github.com/ericthansen/dsc-using-word2vec\n",
    "# - this is very useful summary - don't hesitate to check it out again\n",
    "    # http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    # http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "    # https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "    \n",
    "    # number of features (300) is what google used.  i wonder how many is saturation and gets to diminishing returns\n",
    "    #because this would suggest the number of \"ideas\" contained in the training corpus?\n",
    "    \n",
    "    #negative sampling - 2-5 for larger set, 5-25 for smaller\n",
    "\n",
    "\n",
    "# https://github.com/ericthansen/dsc-generating-word-embeddings-lab\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "##it seems i need to install gensim?\n",
    "#pip install --upgrade gensim\n",
    "\n",
    "##also, on cloud version (or local), need to unzip the News_Category_Dataset_v2.zip file through terminal\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "df = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "df.head()\n",
    "\n",
    " #preparing data\n",
    "df['combined_text'] = df['headline'] + ' ' +  df['short_description']\n",
    "data = df['combined_text'].map(word_tokenize)\n",
    "#\n",
    "data[:5]\n",
    "##training the model\n",
    "model = Word2Vec(data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "##\n",
    "model.train(data, total_examples = model.corpus_count, epochs=10)\n",
    "#\n",
    "wv = model.wv\n",
    "##examining word vectors\n",
    "wv.most_similar('Texas')\n",
    "\n",
    "##negative - nonsense\n",
    "wv.most_similar(negative='Texas')\n",
    "\n",
    "#see entire vector\n",
    "wv['Texas']\n",
    "#\n",
    "wv.vectors\n",
    "\n",
    "##now see if you can get king-man+woman to get queen\n",
    "#spoiler - doesn't work locally like it did in Solution, but still neat\n",
    "wv.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "#wow, totally not reproducible\n",
    "#still, cool and very interactive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-classification-with-word-embeddings\n",
    "## see outline link\n",
    "# vague documentation: https://keras.io/layers/embeddings/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-classification-with-word-embeddings-codealong\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import word2vec\n",
    "##need to unzip the file first\n",
    "df = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "df = df.sample(frac=0.2)\n",
    "print(len(df))\n",
    "df.head()\n",
    "\n",
    "##transform it a bit\n",
    "target = df['category']\n",
    "df['combined_text'] = df['headline'] + ' ' + df['short_description']\n",
    "data = df['combined_text'].map(word_tokenize).values\n",
    "\n",
    "##pretrained GloVe model - need to download:\n",
    "# http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# to get file glove.6B.50d.txt\n",
    "##getting the total vocab\n",
    "total_vocabulary = set(word for headline in data for word in headline)\n",
    "\n",
    "len(total_vocabulary)\n",
    "print('There are {} unique tokens in the dataset.'.format(len(total_vocabulary)))\n",
    "##get vectors out of Glove\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector\n",
    "            \n",
    "##check if it worked\n",
    "glove['school']\n",
    "\n",
    "##create mean word embeddings (using a custom vectorizer function and pipeline)\n",
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])\n",
    " \n",
    "##the pipline:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])##\n",
    "##\n",
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]\n",
    "##Run this; may tak ea while\n",
    "# ⏰ This cell may take several minutes to run\n",
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]\n",
    "#check:\n",
    "scores\n",
    "##Deep Learning with word embeddings\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "## convert to one-hot\n",
    "y = pd.get_dummies(target).values\n",
    "##tokenizer\n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(df['combined_text']))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(df['combined_text'])\n",
    "X_t = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)\n",
    "\n",
    "##create model\n",
    "model = Sequential()\n",
    "\n",
    "##model layers\n",
    "embedding_size = 128\n",
    "model.add(Embedding(20000, embedding_size))\n",
    "model.add(LSTM(25, return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(41, activation='softmax'))\n",
    "\n",
    "##compile\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#\n",
    "model.summary()\n",
    "\n",
    "# ⏰ This cell may take several minutes to run\n",
    "model.fit(X_t, y, epochs=3, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ericthansen/dsc-sequence-model-use-cases\n",
    "#-includes some links to auto-complete cat drawing things\n",
    "# https://github.com/ericthansen/dsc-understanding-recurrent-neural-networks\n",
    "# https://github.com/ericthansen/dsc-lstms-and-grus\n",
    "# https://github.com/ericthansen/dsc-deep-nlp-recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
